Index,Text,Clarity of Review,Justification of Scores,Depth of Analysis,Fairness and Objectivity,Constructiveness of Feedback,Engagement with Related Work,Accuracy in Understanding,Consistency of Evaluation,Identification of Novelty,Ethical Considerations and Responsibility
B11bwYgfM-R1,"The idea of using cross-task transfer performance to do task clustering is not new.  Please refer to the paper \u201cDiscovering structure in multiple learning tasks: The TC algorithm\u201d published in ICML 1996.  One issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks, which is useful for learning from multiple tasks.  For example, in binary classification tasks, a very small S_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each other.  So the use of cross-task transfer performance and the task clustering approach can only capture positive correlations between tasks but ignore the negative task relations which are also important to the sharing among tasks in multi-task learning. \n\nProblem (2) is identical to robust PCA and Theorem 3.1 is common in matrix completion literature.  I don\u2019t see much novelty.  Appendix A seems obvious but it cannot prove the validity of the assumption made in problem (2).  Based on previous works such as \u201cMulti-task Sparse Structure Learning with Gaussian Copula Models\u201d and \u201cLearning Sparse Task Relations in Multi-Task Learning\u201d, when the number of tasks is large, the task relation exhibits the sparse structure.  I don\u2019t know whether the low-rank structure does exist in the cross-task transfer performance or not. \n\nThe two parts in this paper are not new.  The combination of the two parts seems a bit incremental and does not bring much novelty.",1,1,1,1,1,1,1,1,1,1
B11bwYgfM-R2,"This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j). \n\nThe matrix completion approach is based on robust PCA.  When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds).  This can be computationally demanding. \n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering.  However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable. \n\nThere have been a number of MTL methods based on task clustering.  For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI) \n[2] A dirty model for multi-task learning (NIPS) \n[3] Clustered multi-task learning: A convex formulation (NIPS) \n[4] Convex multitask learning with flexible task clusters (ICML) \n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model.  This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model).  However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not. \n\nFor few-shot learning, the authors mentioned that the \\alpha's are adaptable parameters but did not mention how they are adapted. \n\nExperimental results are not convincing. \n- Comparison with existing clustered MTL methods mentioned above are missing. \n- As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported. \n- As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL.",1,1,1,1,1,-1,1,1,1,1
B11bwYgfM-R3,"The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces.  Prior techniques which can address some of these aspects do not necessarily work with deep learning, which is a key focus of the paper.  The authors suggest computing a similarity matrix amongst the tasks.  Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster.  If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module. \nTo deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix.  The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems. \n\nI think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice.  But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form. \n\n1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples).  In this paper, on the other hand, the clusters are obtained in a manner which only accounts for pairwise similarities of tasks, using a pairwise similarity metric which is quite different from how the cluster is eventually used.  This seems quite suboptimal. \n2. The pairwise similarity measure appears to be one that might have a high false negative rate.  That is, it might rate many tasks as dissimilar even when they are not.  This is because you train individual model on i and apply it to j.  It is possible that this model does not do well, but there is an equally good model for i which also does well on j.  Such a model would indeed be found if i and j are put in the same cluster, but the method would fail to do so, leading to high fragmentation. \n3. I do not see how you apply the model from task i to task j when the two have different output spaces.  Since this is a major motivation of the paper, I actually do not see how the setup makes sense! \n4. It seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix. \n5. The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless.  Without good reasoning from the authors, I see no reason why the entries in the row of a matrix should have a normal-like distribution.  At the very least, I would consider using regret to the model of the task, and compute some quantiles on that which is still suspect in the matrix completion setting. \n6 .In the evaluation, why are just 12 tasks used in the Amazon dataset?  Why don't you present evaluation results on all tasks in the multitask setting? \n7. Why is average accuracy the right thing?  If the error rates are different for different tasks, it is not sensible to measure raw accuracies. \n\nThe authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539) \n\nBesides these major issues, there are also a few minor issues I have with the paper.  I do not see why there's need for a proof for the matrix completion result.  This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803).  Given this, the presentation in the paper makes the idea look more novel than it is.  I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method.",1,1,1,1,1,1,1,1,1,1
B12Js_yRb-R1,"\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. \n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals. \n\nClarity:\n- The paper is well written and clarity is good.  Figure 2 & 3 helps the readers understand the core algorithm. \n\nPros:\n- De-duplication modules of inter and intra object edges are interesting. \n- The proposed method improves the baseline by 5% on counting questions. \n\nCons:\n- The proposed model is pretty hand-crafted.  I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016). \n- One major bottleneck of the model is that the proposals are not jointly finetuned.  So if the proposals are missing a single object, this cannot really be counted.  In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals.  The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is. \n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain. \n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017). \n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).  This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance. \n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions. \\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL \\n\nConclusion:\n- I feel that the motivation is good,  but the proposed model is too hand-crafted.  Also, key experiments are missing: 1) NMS baseline  2) Comparison with VQA counting work  (Chattopadhyay et al., 2017).  Therefore I recommend reject. \n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017. \n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016. \n\nUpdate:\nThank you for the rebuttal.  The paper is revised and I saw NMS baseline is added.  I understood the reason not to compare with certain related work.  The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy.  However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.",1,1,1,1,1,1,1,1,1,1
B12Js_yRb-R2,"Summary\n - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism.  The authors propose a differentiable counting component, which explicitly counts the number of objects.  Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals.  In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset. \n\nStrengths\n - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.  \n - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model. \n - The proposed model can be easily applied to any VQA model using soft attention.  \n - The paper is well written and the contribution is clear. \n\nWeaknesses\n - Although the proposed model is helpful to model counting information in VQA,  it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. \n\nComments\n - It is not clear if the value of count \""c\"" is same with the final answer in counting questions. \n\n",1,1,-1,1,1,-1,1,1,1,1
B12Js_yRb-R3,"This paper tackles the object counting problem in visual question answering.  It is based on the two-stage method that object proposals are generated from the first stage with attention.  It proposes many heuristics to use the object feature and attention weights to find the correct count.   In general, it treats all object proposals as nodes on the graph.  With various agreement measures, it removes or merges edges and count the final nodes.  The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset.  The experimental results on counting are promising.   Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems.   Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications.  I thus believe the overall contribution is not sufficient for ICLR. \n\nPros:\n1. Well written paper with clear presentation of the method.  \n2. Useful for object counting problem. \n3. Experimental performance is convincing.  \n\nCons:\n1. The application range of the method is very limited.  \n2. The technique is built on a lot of heuristics without theoretical consideration.  \n\nOther comments and questions:\n\n1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel.   It may also lead to simpler solutions.  For example, it can be used for deduplication using A (eq 1) as the similarity matrix.  \n\n2. Can the author provide analysis on scalability the proposed method?  When the number of objects is very large, the graph could be huge.  What are the memory requirements and computational complexity of the proposed method?   \nIn the end of section 3, it mentioned that \""without normalization,\"" the method will not scale to an arbitrary number of objects.  I think that it will only be a problem for extremely large numbers.  I wonder whether the proposed method scales.  \n\n3. Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result?  Theoritically, it solves the soft attention problems.  \n\n4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.  \n\n[1] Kulesza, Alex, and Ben Taskar. \""Determinantal point processes for machine learning.\"" Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012): 123-286.",1,1,1,1,1,-1,1,1,1,1
B13EC5u6W-R1,"* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label.  As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc). \n\n* This is quite an interesting paper with a sensible goal.  It seems like the method could be more informative than the other methods.   However, there are quite a number of problems, as explained below.\n\n* The explanation of eqs 1 and 2 is quite poor.  \\alpha in (1) seems to be \\gamma in Alg 1 (line 5). \""L_target is a target objective which can be a negative class probability ..\"" this assumes that the example is a positive class.  Could we not also apply this to negative examples? \n\n\""or in the case of heart failure, predicted BNP level\"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level?  Also specific details should be reserved until a general explanation of the problem has been made. \n\n* The trade-off parameter \\gamma is a \""fiddle factor\"" -- how was this set for the lung image and MNIST examples?  Were these values different? \n\n* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks.  It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings. \n\n* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s?  Why have you not reported results for these kinds of tasks? \n\n* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side). \n\nThe reconstructions show poor detail relative to the originals.   This loss of detail could be a limitation. \n\n* A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8.  A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for. \n\n* The references section is highly inadequate -- no venues of publication are given.  If these are arXiv give the proper ref.  Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014. \n\n* Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold. \n\n* Text:\n\nsec 2 para 4. \""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set.\"" ?? \n\n* p 3 bottom -- give size of dataset\n\n* p 5 AUC curve -> ROC curve\n\n* p 6 Fig 4 use text over each image to better specify the details given in the caption.\n\n\n\n",1,1,1,-1,1,1,1,1,1,1
B13EC5u6W-R2,"The main contribution of the paper is a method that provides visual explanations of classification decisions.  The proposed method uses \n - a generator trained in a GAN setup\n - an autoencoder to obtain a latent space representation\n - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it).  \nThe method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided. \n\n\n - The paper proposes a (I believe) novel method to obtain visual explanations.  The results are visually compelling  although most results are shown on a medical dataset - which I feel is very hard for most readers to follow.  The MNIST explanations help a lot.   It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader. \n\n - The paper shows that the results are plausible using a neat trick.  The authors train their system with the testdata included which leads to very different visualizations.  It would be great if this analysis could be performed for MNIST as well. \n\n\nFrom the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction.\nAlso, there is the work by Hendricks et al on Generating Visual Explanations.  This should probably also be referenced. \n\nminor comments: \n- some figures with just two parts are labeled \""from left to right\"" - it would be better to just write left: ... right: ...\ n- figure 2: do these images correspond to each other?  If yes, it would be good to show them pairwise.\n- figure 5: please explain why the saliency map is relevant.  This looks very noisy and non-interesting.\n\n",1,1,-1,1,1,-1,1,1,1,1
B13EC5u6W-R3,"The authors address two important issues: semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image.  This focus is mainly on medical image classification but the approach could potentially be useful in many more areas.  The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed.  The encoder is then used as a feature extractor for classification and regression of targets (e.g. heard disease).  The visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class.  This gives an image that is similar to the original but with features that caused the classification of the disease removed.  The resulting image can be subtracted from the original encoding to highlight problematic areas.  The approach is evaluated on an in-house dataset and a public NIH dataset, demonstrating good performance, and illustrative visual rationales are also given for MNIST. \n\nThe idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales.  There are a few limitations, e.g. the difficulty of evaluating the rationales, and the fact that the resolution is fixed to 128x128 (which means discarding many pixels collected via ionizing radiation), but these are readily acknowledged by the authors in the conclusion. \n\nComments:\n1) There are a few details missing, like the batch sizes used for training (it is difficult to relate epochs to iterations without this).  Also, the number of hidden units in the 2 layer MLP from para 5 in Sec 2. \n2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error. \n3) Sec 2 para 4: \""the reconstruction loss on the validation set was similar to the reconstruction loss on the validation set\"" -- perhaps you could be a little more precise here.  E.g. learning curves would be useful.\n4) Sec 2 para 5: \""paired with a BNP blood test that is correlated with heart failure\"" I suspect many readers of ICLR, like myself, will not be well versed in this test, correlation with HF, diagnostic capacity, etc., so a little further explanation would be helpful here.  The term \""correlated\"" is a bit too broad, and it is difficult for a non-expert to know exactly how correlated this is.  It is also a little confusing that you begin this paragraph saying that you are doing a classification task, but then it seems like a regression task which may be postprocessed to give a classification.  Anyway, a clearer explanation would be helpful.  Also, if this test is diagnostic, why use X-rays for diagnosis in the first place? \n5) I would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale, as this would have practical implications.. \n6) Sec 2 para 7: \""L_target is a target objective which can be a negative class probability or in the case of heart failure, predicted BNP level\"" -- for predicted BNP level, are you treating this as a probability and using cross entropy here, or \nmean squared error?. \n7) As always, it would be illustrative if you could include some examples of failure cases, which would be helpful both in suggesting ways of improving the proposed technique, and in providing insight into where it may fail in practical situations.",1,1,1,1,1,-1,1,1,1,1
B13njo1R--R1,"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially.  The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task.  The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.  \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network? \n- What data do you use for the distillation?  Section 4.1 states\""We use a method similar to the DAGGER algorithm\"", but what is your method.  If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?  \n- I do not understand the purpose of \""input injection\"" nor where it is used in the paper.   \n\nStrengths:\n- The method is simple but novel.   The results support the method's utility.  \n- The testbed is nice; the tasks seem significantly different from each other.  It seems that no reward shaping is used. \n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker. \n\nWeaknesses:\n- Figure 2: the plots are too small. \n- Distilling may hurt performance ( Figure 2.d) \n- The method lacks details (see Questions above) \n- No comparisons with prior work are provided.  The paper cites many previous approaches to this but does not compare against any of them.  \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.  \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work.  The method is clear  but not precisely described.  The results are promising.  I think that this is a good approach to the problem that could be used in real-world scenarios.  With some filling out, this could be a great paper.",1,1,1,1,1,1,1,1,1,1
B13njo1R--R2,"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum. \n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty. \n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain. \n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case. \n- Are the experiments single runs?  Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour. \n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks? \n- How were the network architecture and network size chosen, especially for the multitasker?  Would policies generalize to later tasks better with larger, or smaller networks? \n- Was any kind of regularization used, how does it influence task performance vs. transfer? \n- I find figure 1 (c) somewhat confusing.  Is performance maintained only on the last 2 tasks, or all previously seen tasks?  That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n",1,1,1,1,1,-1,1,1,1,1
B13njo1R--R3,"Hi, \n\nThis was a nice read.  I think overall it is a good idea.  But I find the paper lacking a lot of details and to some extend confusing.  \nHere are a few comments that I have:\n\nFigure 2 is very confusing for me.  Please first of all make the figures much larger.  ICLR does not have a strict page limit, and the figures you have are hard to impossible to read.  So you train in (a) on the steps task until 350k steps?  Is (b), (d),(c) in a sequence or is testing moving from plain to different things?  The plot does not explicitly account for the distillation phase.  Or at least not in an intuitive way.  But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted.  \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines.  Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all.  So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough).  \n\nWhere exactly is input injection used?  Is it experiments from figure 3.  What input is injecting?  What do you do when you go back to the task that doesn't have the input, feed 0?  What happens if 0 has semantics ?  \n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix.  And do try to copy a bit more of them in the main text where reasonable.  \n\nWhat is the role of PLAID?  Is it to learn a continual learning solution?  So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills?  Will this be feasible?  Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning?  \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?   And even then maximize transfer with respect to what?  Frames required from the environment?  If that are you reusing the frames you used during training to distill?  Can we afford to keep all of those frames around?  If not we have to count the distillation frames as well.  Also more baselines are needed.  A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way.   Or at least have a good argument of why this is suboptimal compared to PLAID.  \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score.  But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read.  Providing at least one more baseline (if not more considering the other things cited by them). \n\n",1,1,1,1,1,-1,1,1,1,1
B14TlG-RW-R1,"Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention.  The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference.  Authors also propose a paraphrasing based data augmentation method which helps in improving the performance.  Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference. \n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written. \n\n1. Why don\u2019t you report your model performance without data augmentation in Table 1?  Is it because it does not achieve SOTA?  The proposed data augmentation is a general one and it can be used to improve the performance of other models as well.  So it does not make sense to compare your model + data augmentation against other models without data augmentation.  I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models. \n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n 3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset.  That will show the generality of the proposed architecture.  Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision. \n\n4. Are you willing to release your code to reproduce the results?\ n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\ n2. In the first contribution bullet point, \u201cthat exclusive built upon\u201d should be \u201cthat is exclusively built upon\u201d.\n",1,1,1,1,1,-1,1,1,1,1
B14TlG-RW-R2,"This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally. \n\nFirstly, I suggest the authors rewrite the end of the introduction.  The current version tends to mix everything together and makes the misleading claim.  When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1.  But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method.  \n\nSecondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows: \n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC.  The novelty is limited  but it is a good idea to speed up the RC models.  However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs.  Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct. \n\n(2) I feel that the model design is the main reason for the good overall RC performance.  However, in the paper there is no motivation about why the architecture was designed like this.  Moreover, the whole model architecture is only evaluated on the SQuAD dataset.  As a result, it is not convincing that the system design has good generalization.  If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself. \n\n(3) I like the idea of data augmentation with paraphrasing.  Currently, the improvement is only marginal,  but there seems many other things to play with.  For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\ n\nI am looking forward to the test performance of this work on SQuAD.",1,1,1,1,1,1,1,1,1,1
B14TlG-RW-R3,"This paper presents a reading comprehension model using convolutions and attention.  This model does not use any recurrent operation but it is not per se simpler than a recurrent model.  Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.   On SQuAD dataset, their results show some small improvements using the proposed augmentation technique.  Their best results, however, do not outperform the best results reported on the leader board. \n\nOverall, this is an interesting study on SQuAD dataset.  I would like to see results on more datasets and more discussion on the data augmentation technique.  At the moment, the description in section 3 is fuzzy in my opinion.  Interesting information could be:\n- how is the performance of the NMT system?  \n- how many new data points are finally added into the training data set? \n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n",1,-1,-1,1,1,-1,1,1,1,1
B14uJzW0b-R1,"Summary: \nThe paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem  - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem). \nGiven this architecture, the authors focus on characterizing the objective landscape of such a problem. \nThe techniques used depend on previous work.  According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. \n\nOriginality: \nThe paper heavily depends on the approach followed by Brutzkus and Globerson, 2017.  To this end, slighly novel. \n\nImportance: \nUnderstanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work.  I would say that the topic is an important one. \n\nPresentation/Clarity: \nTo the best of my understanding, the paper has some misconceptions.  The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.  In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?).  Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units.  \nThese are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). \n\nAnother issue is the fact that, on my humble opinion, the main text looks like a long proof.  It would be great to have more intuitions. \n\nComments:\n1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other.  While the authors already identify that this might be a restriction, it still does not lessen the fact that the configuration considered is a really specific one. \n\n2. The paper reads like a collection of lemmas, with no verbose connection.  It was hard to read and understand their value, just because mostly the text was structured as one lemma after the other. \n\n3. It is not clear from the text whether the setting is already considered in Brutzkus and Globerson, 2017.  Please clarify how your work is different/new from previous works.\n",1,1,1,1,1,1,1,1,1,1
B14uJzW0b-R2,"In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network.  Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points.  Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods.  In general I found this paper clearly written and technically sound.  I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures.  \n\nContribution: \nAs discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units.  The analysis becomes considerably more complicated,  and the contribution seems to be novel and significant.  I am not sure why did the authors mentioned the work on over-parameterization though.  It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small).  \n\nComments on the Assumptions:\n- Please explain the motivation behind the standard Gaussian assumption of the input vector x.  \n- Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6).  \nWithout extra justifications, it seems that the theoretical result only holds for an artificial problem setting.  While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results.  \n\nGeneral Comment:  \nThe technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs.  From the analysis in the main paper, I believe the theoretical contribution is correct and sound.  While I appreciate the technical contributions,  in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples).  Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector,  and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems.",1,1,-1,1,1,-1,1,1,1,1
B14uJzW0b-R3,"This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer.  As a result, a gradient descent algorithm converges to the unique solution.  This works address a conjecture proposed by Tian (2017). \n\nWhile it is clearly written, my main concern is whether this model is significant enough.  The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis,  but it makes the model considerably simpler than any practical setting.\n\n",1,-1,-1,1,1,1,1,1,1,1
B16yEqkCZ-R1,"The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states.  The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes.  The proposed technique is validated both empirically and theoretically.  \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN.  Nonetheless, there are some criticisms than can be made of both the method and the evaluations:\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning.  Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally.  There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable.  No empirical results on the effect of the parameter are given. \n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning. The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies.   This problem does not seem to be really solved by this method.  Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier.  While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties.  As such the method wouldn\u2019t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state.  It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited.  \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method.  The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]). \n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  \u2018if the fear model reaches arbitrarily high accuracy\u2019 or \u2018if the probability is negligible\u2019. \n\nIt is wasn\u2019t clear to me that the properties described in Theorem 1 actually hold.  The motivation in the appendix is very informal and no clear derivation is provided.  The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states.  However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states.   It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty.  It is therefore not clear to me that any claims can be made about its performance without additional assumptions. \n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state.  Any optimal policy would therefore need to spend some time e in the danger state, on average.  A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards.  E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax.  By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin).   This seems to contradict the theorem.  It wasn\u2019t clear what assumptions the authors make to exclude situations like this. \n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \""Improved deep reinforcement learning for robotics through distribution-based experience retention,\"" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952.\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835.",1,1,1,1,1,1,1,1,1,1
B16yEqkCZ-R2,"\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.   The fear model is trained in parallel to predict catastrophic states.    Its output is used to penalize the Q learning target.  \n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states.   Because it does not experience it any more.   Shouldn\u2019t the agent stop learning at some point in time?   Why does it need to keep collecting good data?   How about giving more weight to catastrophic data (e.g., replicating it)\ n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation? \n\nWhy not specify catastrophic states with a large negative reward? \n\nIt seems that catastrophe states need to be experienced at least once. \nIs that acceptable for the autonomous car hitting a pedestrian?\n",1,-1,-1,1,1,-1,1,1,1,1
B16yEqkCZ-R3,"The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL).  The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting.  The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that \u201cDQNs  are susceptible to periodically repeating mistakes\u201d.  I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues. \n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions .In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel.  \n\nStill, many of the design choices appear quite arbitrary and can most likely be improved upon.  In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal.  Instead I view the proposed techniques mostly as useful inspiration for future papers to build on.  As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest.  The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong.  \n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning.  A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. \n",1,1,1,1,1,-1,1,1,1,1
B16_iGWCW-R1,"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. \n\nWhile the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:\n\n- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners.  It's not clear what kind of loss function is really being optimised here.  It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end. \n- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them.  And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup. \n- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners. \n- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method)  2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST)  3) comparison to simply ensembling with random initialisations. \n\nOther comments:\n- Paper would benefit from writing improvements to make it read better. \n-  \""to replace the softmax error function (used in deep learning)\"": I don't think we have softmax error function",1,1,1,1,1,-1,1,1,1,1
B16_iGWCW-R2,"In conventional boosting methods, one puts a weight on each sample.  The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.   Thus the learned weak learner at this round will make different mistakes. \nThis idea however is difficult to be applied to deep learning with a large amount of data.  This paper instead designed a new boosting method which puts large weights on the category with large error in this round.    In other words samples in the same category will have the same weight \n\nError bound is derived.   Experiments show its usefulness  though experiments are limited\n",1,-1,-1,1,1,-1,1,1,-1,-1
B16_iGWCW-R3,"This paper applies the boosting trick to deep learning.  The idea is quite straightforward, and the paper is relatively easy to follow.  The proposed algorithm is validated on several image classification datasets. \n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper.  The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against. \n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise. \n3. In Eq. (3), \\tilde{D} is not defined. \n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$.   \n5. How many layers is the DenseNet-BC used in this paper?  Why the error rate reported here is higher than that in the original paper? \nTypo: \nIn Session 3 Line 7, there is a missing reference. \nIn Session 3 Line 10, \u201c1,00 object classes\u201d should be \u201c100 object classes\u201d. \nIn Line 3 of the paragraph below Equation 5, \u201cclasse\u201d should be \u201cclass\u201d.\n",1,1,-1,1,1,-1,1,1,1,1
B1ae1lZRb-R1,"The authors investigate knowledge distillation as a way to learn low precision networks.  They propose three training schemes to train a low precision student network from a teacher network.  They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\n Pros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\n Cons:\n(-) The gap with full precision models is still large \n(-)  Transferability of the learned low precision models to other tasks is not discussed\n\n The authors tackle a very important problem, the one of learning low precision models without comprosiming performance.  For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks.  One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3).  As a matter of fact, under some scenarios performance even decreases.  \n\nThe authors do not discuss the gains of their best low-precision regime in terms of computation and memory. \n\nFinally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k).  How good are the low-precision models trained by the authors at transferring to other tasks?  Is it possible to transfer student-teacher training practices to other tasks?",1,1,1,1,1,-1,1,1,-1,1
B1ae1lZRb-R2,"The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network.  Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly.  The paper shows an interesting result that the distilled low precision network actually performs better than high precision network. \n\nI found the paper interesting  but the contribution seems quite limited. \n\nPros:\n1. The paper is well written and easy to read.\n2.  The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally. \n\nCons:\n1. The name Apprentice seems a bit confusing with apprenticeship learning. \n2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights). \n3. It is unclear how the proposed method outperforms other methods based on fine-tuning.  It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.",1,1,1,1,1,-1,1,1,-1,1
B1ae1lZRb-R3,Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation. \nScheme A consists of training a high precision teacher jointly with a low precision student.  Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode. \n\nReview:\nThe paper is well written.  The experiments are clear and the three different schemes provide good analytical insights. \nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n Comments:\nTensorflow citation is missing.\n Conclusion is short and a few directions for future research would have been useful.,1,1,-1,1,1,-1,1,1,1,1
B1al7jg0b-R1,"The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report.  Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference.  Figure 2 is almost identical, again a reference to the original would be better. \nConceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression.  What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach.  The fact that additional conceptors can be trained does not appear new for the approach described here.  If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version. \n\nThe evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting.  The evaluation shows that the approach performs better classifying MNIST digits than another approach.  This is nice but doesn't really tell me much about overcoming catastrophic forgetting. \n",1,1,1,1,1,1,1,1,-1,1
B1al7jg0b-R2,"[Reviewed on January 12th]\n\nThis article applies the notion of \u201cconceptors\u201d -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks.  It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation.   After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation.  Follows a section of experiments on variants of MNIST commonly used for continual learning.\n\n Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea.  The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.   The numeric examples, although quite toy, provide a clear illustration. \n\nA few things are still missing to back the strong claims of this paper:\n* Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer.   This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures. \n* It could also be welcome to use a more grounded vocabulary, e.g. on p.2 \u201cFigure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources\u201d could be much more simply said as \u201cFigure 1 shows the ellipses corresponding to three sets of R^3 points\u201d.  Being less grandiose would make the value of this article nicely on its own.\n*  Some examples beyond the contrived MNIST toy examples would be welcome.  For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation.  I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.  Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST.  The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further.  If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication. \n\nThe later point would normally make me attribute a score of \""6: Marginally above acceptance threshold\"" by current DL community standards,  but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.\n",1,1,1,1,1,-1,1,1,1,1
B1al7jg0b-R3,"This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors.  This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \""unused\"" until the present task in hand.\n\n The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.  In Section 2 the authors review conceptors.  This method is algebraic method closely related to spanning sub spaces and SVD.  The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally.  In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.  The authors provide a version with batch SGD as well.\n\n In Section 4, the authors show their method on permuted MNIST.  They compare the method to EWC with the same architecture.  They show that their method more efficiently suffers on permuted MNIST from less degradation.  Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance. \n\nIn general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life.  Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\n I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.   What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods.  Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.",1,1,1,-1,1,-1,1,1,1,1
B1bgpzZAZ-R1,"This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension.   I found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compelling. \n\nIn my opinion the main weakness of the paper is the focus on the RACE dataset.   This dataset has not attracted much attention and most work in reading comprehension has now moved to the SQUAD dataset for which there is an active leader board.   I realize that SQUAD is not explicitly multiple choice and that this is a challenge for an answer elimination architecture.   However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span.   In any case, competing with an active leader board would be much more compelling.",1,1,-1,1,1,1,1,1,1,1
B1bgpzZAZ-R2,"In this paper, a model is built for reading comprehension with multiple choices.  The model consists of three modules: encoder, interaction module and elimination module.  The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations.  Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored. \n\nThe elimination module is interesting,  but the usefulness of \u201celimination\u201d is not well justified for two reasons.  First, the improvement of the proposed model over the previous state of the art is limited.  Second, the model is built upon GAR until the elimination module, then according to Table 1 it seems to indicate that the elimination module does not help significantly (0.4% improvement).  \n\nIn order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e. after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy).  Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps. \n\nOther issues:\n\n1) Is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $\\tilde{x}_i$?  Even though the authors find the orthogonal vectors, they\u2019re gated summed together very soon.  It would be better to show how much \u201celimination\u201d and \u201csubtraction\u201d effect the final performance, besides the effect of subtraction gate.\n\n2)  A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.\n\n 3) $c_i$ in page 5 is not defined.  What\u2019s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function? \n\n4) It would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance, besides the case study in Figure 3 .\n\nMinor issues:\n\n1) In Eqn. (4), it would be better to use a vector as the input of softmax.\n\n 2) It would be easier for discussion if the authors could assign numbers to every equation.",1,1,1,1,1,-1,1,1,1,1
B1bgpzZAZ-R3,"This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations. \n\nIt is a well-written paper,  however, I am not very convinced by its motivation, the proposed model and the experimental results.  \n\nFirst of all, the improvement is rather limited.  It is only 0.4 improvement overall on the RACE dataset;  although it outperforms GAR on 7 out of 13 categories;  but why is it worse on the other 6 categories?  I don\u2019t see any convincing explanations here. \n\nSecondly, in terms of the development of reading comprehension models, I don\u2019t see why we need to care about eliminating the irrelevant options.  It is hard to generalize to any other RC/QA tasks.  If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to.  The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option.  Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined.  \n\nSome visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.\n",1,1,1,-1,1,-1,1,1,1,1
B1CEaMbR--R1,"This paper presents some reviews on clustering methods with deep learning.  Based on the review taxonomy, the authors presents a mixed objective which aims for bretter clustering performance.  The proposed method is then tested on two image data sets.\n\n The claimed main contribution of the paper is the taxonomy.  There are no new things in such kind of reviews.  The taxonomy gives no scientific axioms.  Therefore the impact or actual contribution to the ICLR community is very limited.\n\n The proposed clustering method is problematic.  It is hard to set the paramter alpha.  The experimental results are also disappointing.  For example, the COIL20 accuracy is only 0.762, much worse than the state of the art.  Moreover, results on only two image data sets are not sufficient for convincing.",1,1,-1,-1,1,-1,1,1,-1,-1
B1CEaMbR--R2,"In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods.  Finally, the authors propose a new method by using one unexplored combination of taxonomy features. \n\nThe paper is well-written and easy to follow . The proposed combination is straightforward,  but lack of novelty.  From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining.  I am not convinced that these changes will lead to a significant difference.  The performance of the proposed method and DEPICK are also similar in table 1.  \n\nIn addition, the experiments section is not comprehensive enough as well the author only tested on two datasets.  More datasets should be tested for evaluation.  In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications.  The authors should finish the experiments on comparison methods and fill the entries in Table 1.\n\n In summary, the proposed method is lack of novelty compare to existing methods.  The survey part is nice,  however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios.  Therefore, I think the paper cannot be accepted at this stage.\n",1,1,1,1,1,-1,1,1,1,1
B1CEaMbR--R3,"The paper is mostly a survey about clustering methods with neural networks.  \n\nSection 2 presents a taxonomy for the different neural network clustering methods.  A rich lists of the possible components of the neural network-based clustering methods are given, that include the different neural network architectures, feature to use for clustering, loss functions used and more.  In Section 3, a few methods from the literature are classified according to the proposed taxonomy.  Furthermore, in Section 4 a new method is proposed, that is to combine the best parts of the already existing models in the literature.  Unfortunately, the experiments is Section 5 reveal that the proposed method yields results that are at most comparable with the existing methods.  \n\nThe paper is written well and provides good insights (mostly taxonomy) on the existing methods for neural network-based clustering.  However, the paper lacks novel content.  The novel content of the paper sums up to the proposed method, that is composed of building blocks of existing models, and fails to impress in experimental results.  It could be that this paper belongs to another venue that is more appropriate for survey papers.   Also, it overall rather appears short.\n",1,1,-1,-1,1,-1,1,1,-1,1
B1CNpYg0--R1,"This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.  The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings.  The method leads to better performance than using no external resources,  but not as high performance as using Glove embeddings.   The paper is clearly written, and has useful ablation experiments.   However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word.   As the authors note, this kind of character level modelling has been used in many previous works.   \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,   but I don\u2019t know the area well enough to make specific suggestions   \n- I\u2019m a little skeptical about how often this method would really be useful in practice.   It seems to assume that you don\u2019t have much unlabelled text (or you\u2019d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well.   All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.  \n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.   It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n",1,1,1,1,1,1,1,1,1,1
B1CNpYg0--R2,"\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas: \n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older). \n\n* Using character-level models a la Ling et al. \n\n* Using dictionary embeddings a la Hill et al. \n\nNone of these ideas are new before but I haven\u2019t seen them combined in this way before.  This is a very practical idea, well-explained with a thorough set of experiments across three different tasks.  The paper is not surprising  but this seems like an effective technique for people who want to build effective systems with whatever data they\u2019ve got. \n",1,-1,-1,1,1,1,1,1,1,1
B1CNpYg0--R3,"This paper examines ways of producing word embeddings for rare words on demand.  The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets.  The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task. \n\nThe contribution is clear  but not huge.  In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category.  The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible.  More things could have been considered.  Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet.  The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3).  However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is. \n\nThe paper is clearly written.  \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf .  While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK.  However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words.  That method could also be considered as a possible approach to compare against here. \n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.).  While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics. \n - p.2: You use pretrained GloVe vectors that you do not update.  My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \""only\"" 6 billion words \u2013 2 orders of magnitude less data. \n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector? \n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1)  but well below current best single models of around 76-78 F1.  The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective.  This model shows the rather strong performance of spelling models \u2013 at least on this task \u2013 which he again benefit from training in the context of the end objective.  \n - Fig 2: It's weird that only the +dict (left) model learns to connect \""In\"" and \""where\"".  The point made in the text between \""Where\"" and \""overseas\"" is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words \""where\"" and \""in\"" both commonly expressing a location. \n - Table 2: These results are interestingly different.  Dict is much more useful than spelling here.  I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge. \n - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD).  My impression is that most papers on NLI use much larger vocabs, no? \n - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct. \n - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold.\n",1,1,1,1,1,1,1,1,1,1
B1D6ty-A--R1,"After reading the rebuttal:\n\nThe authors addressed some of my theoretical questions.  I think the paper is borderline, leaning towards accept. \n\nI do want to note my other concerns:\n\nI suspect the theoretical results obtained here are somewhat restricted to the least-squares, autoencoder loss.   \n\nAnd note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better.  The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs.  So it is not clear if the proposed algorithm is better for training the classifier, which may be of more interest. \n\n=============================================================\n\nThis paper presents an algorithm for training deep neural networks.  Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers.  \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders.  Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation.  As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015.  This itself can be a small, nice contribution. \n\nWhat concerns me is the extension to multiple layers.  Some questions are not clear from section 3.4:\n1.  Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it?  A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result. \n\n2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence.  Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time.  As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime.  In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations). \n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD.  Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n",1,1,1,1,1,1,1,1,1,1
B1D6ty-A--R2,"The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks.  The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem.  Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable.  This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established.  They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer.  The ideas are interesting,  but I have some concerns regarding this work. \n\nMajor comments:\n\n1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over, It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem?  The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem.  However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via \\phi_1 and another via \\phi_2. \n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations.  This is an interesting result, and useful in its own right.  However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about.  Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems. \n\n3. Experimental results for classification are not convincing enough.  If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS.  \n\n4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3).  I would recommend the authors to rerun these experiments but truncate the iterations early enough.  If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result.",1,1,1,1,1,-1,1,1,1,1
B1D6ty-A--R3,"In this paper an alternating optimization approach is explored for training Auto Encoders (AEs). \nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase. \nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD.  The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs. \n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading.  While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer. \nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply. \nThe authors should mention this point. \n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.  \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.  Concretely: Is there any known theory for such objectives?  What guarantees can we hope to achieve? \n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice .\n\nThe experimental part is satisfactory, and seems to be done in a decent manner.  \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm. \nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n",1,1,1,1,1,1,1,1,1,1
B1DmUzWAW-R2,"The authors propose a model for sequence classification and sequential decision making.  The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution.  The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making. \n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control). \n\nThe experiment section definitely demonstrate the effort put into this work.  However, my primary concern is that the model seems somewhat lacking in novelty.  Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO.  The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding.  I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making.  I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information). \n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning.  I think this information would be very useful to the community in terms of what to take away from this paper.  In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains. \n\nOverall, I am inclined to accept this paper on the basis of its experimental results.  However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain). \n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation \n- In the experiment section for Omniglot, when the authors say \""1200 classes for training and 432 for testing\"", it sounds like the authors are performing zero-shot learning.  How does this particular model generalize to classes not seen during training?",1,1,-1,1,1,1,1,1,-1,1
B1DmUzWAW-R3,"The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning.  Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods.  The experiment results seem solid and the proposed structure is with simple design and highly generalizable.  The concern is that the contribution is quite incremental from the theoretical side  though it involves large amount of experimental efforts, which could be impactful.  Please see the major comment below.\n\nOne major comment:\n- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side.  \nSpecifically, for two folds: \n(1) in general, some more work in investigating the task space would be nice.  The paper assumes the tasks are \u201crelated\u201d or \u201csimilar\u201d and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution.  But what exactly should the distribution be like to be learnable and how to quantify such \u201crelated\u201d or \u201csimilar\u201d relationship across tasks?  \n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods?  The result from the paper seems to answer with \u201cin all cases\u201d but then that always brings the issue of \u201coverfitting\u201d or parameter tuning issue. \n\nMore detailed comments:\n- On Page 1, \u201cthe optimal strategy for an arbitrary range of tasks\u201d lacks definition of \u201crange\u201d; also, in the setting in this paper, these tasks should share \u201csimilarity\u201d or follow the same \u201cdistribution\u201d and thus such \u201carbitrariness\u201d is actually constrained. \n\n- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined.  It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the \u201ctask distribution\u201d is too large to be meaningful.  One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable. \n\n- For results in Table 1 and Table 2, how are the confidence intervals computed?  Is it over multiple runs or within the same run?  It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests.  The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming \u201cnot statistically-significantly different\u201d because no significance test is reported.  \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?",1,1,1,1,1,-1,1,1,1,1
B1e5ef-C--R1,"The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.   This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings.   They then show that the result matrix satisfies a restricted isometry condition.    Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.  \n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible.   I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.",1,-1,-1,1,1,-1,1,1,-1,1
B1KJJf-R--R2,"This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples.  The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs.   In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs.    Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results,  and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique.  \n\nStrengths:\n\n- Reasonable approach, quality is good \n\n- The DSL is richer than that of previous related work like Balog et al. (2016). \n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting. \n\nWeaknesses:\n\n- There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work.  Using beam search on tree outputs is a bit of a minor contribution. \n\n- The baselines are just minor variants of the proposed method.  It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset. \n\n- Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given. \n\nQuestions:\n\n- Why not compare against Seq2Seq + Search? \n\n- How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions.  I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do. \n\n- Are there plans to release the dataset?  Could you provide a large sample of the data at an anonymized link?  I'd re-evaluate my rating after looking at the data in more detail.\n",1,1,1,1,1,1,1,1,-1,-1
B1KJJf-R--R3,This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network.  This work to my knowledge is the first to use a DSL closer to a full language. \n\nThe paper is very clear and easy to follow.  One way it could be improved is if it were compared with another system.  The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work.,1,-1,-1,1,1,-1,-1,1,-1,1
B1l8BtlCb-R1,"This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words.  The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs.   To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations.   For training, apply knowledge distilation for better training followed by fine tuning by reinforce.   Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding. \n\nThe motivation is clear and proposed methods are very sound.  Experiments are carried out very carefully. \n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency.  I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough. \n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment. \n\n- What is the impact of the external word aligner quality?  For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner.  \n\n- The positional attention is rather unclear and it would be better to revise it.  Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1l8BtlCb-R2,"This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models).  The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described).  The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search.  \n\nOverall, this is an interesting paper.  It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores.   While the Ro->En results are goodG this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more.  Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1l8BtlCb-R3,"This paper can be seen as an extension of the paper \""attention is all you need\"" that will be published at nips in a few weeks (at the time I write this review).  \n\nThe goal here is to make the target sentence generation non auto regressive.  The authors propose to introduce a set of latent variables to represent the fertility of each source words.  The number of target words can be then derived and they're all predicted in parallel. \n\nThe idea is interesting and trendy.  However, the paper is not really stand alone.  A lot of tricks are stacked to reduce the performance degradation.  However, they're sometimes to briefly described to be understood by most readers.  \n\nThe training process looks highly elaborate with a lot of hyper parameters.  Maybe you could comment on this.  \n\nFor instance, the use fertility supervision during training could be better motivated and explained.  Your choice of IBM 2 is wired since it doesn't include fertility.  Why not IBM 4, for instance ?  How you use IBM model for supervision.  This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1Lc-Gb0Z-R1,"The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x).  Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches.  Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation.  More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z). \n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen.  Deciding on these targets is formulated as a combinatorial optimization problem.  Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization.  The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation.  At each layer, the targets can be chosen using a variety of search algorithms such as beam search. \n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks. \n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions.  The experimental results are encouraging,  though I have a few questions below that prevent me for now from rating the paper higher. \n\nComments and questions:\n\n1) How computationally expensive is FTP?  The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details. \n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks?  I do not see a comparison.  If that is so, should this paper also compare with STE?  How do we know if generalizing saturated STE is more worthwhile than generalizing STE? \n\n3) It took me a while to understand the authors' subtle comparison with target propagation, where they say \""Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization. \"" It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.)  Focusing on discrete targets gains the benefits of quantized networks.  If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear. \n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation? \n\n5) In Table 1, the significance of the last two columns is unclear.  It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good).  I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?",1,1,1,-1,1,1,1,1,1,-1
B1Lc-Gb0Z-R2,"This paper examines the problem of optimizing deep networks of hard-threshold units.  This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks.  While none of the contributions are especially novel,  the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work.",1,1,-1,1,-1,-1,-1,-1,-1,-1
B1Lc-Gb0Z-R3,"The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it.  The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy.  The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems.  However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem.  These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such.  Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently.  They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet.  The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting.  The results are moderately convincing in favor of the proposed approach.  Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem.  Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community.  \n\nThere are a few flaws/weaknesses in the paper though, making it somewhat lose.  \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.  Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier.  \n- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.   \n- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition \n- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach.  The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. \n",1,1,1,-1,1,1,1,1,1,-1
B1lMMx1CW-R1,"The paper proposes a new neural network based method for recommendation., \n\nThe main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed. \n\nThis contribution is not bad for an empirical paper.  There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works. \n\nThere's not much detail about the data (it is after all an industrial paper).  It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration.  As it is, I am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere. \n\nOther than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible.  There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution.  Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1lMMx1CW-R2,"Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning.  They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.) \n\nMost of the originality comes from integrating time decay of purchases into the learning framework.  Rest of presented work is more or less standard. \n\nPaper may be useful to practitioners who are looking to implement something like this in production.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1lMMx1CW-R3,"This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history.  The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay.  And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability.  It also showed good online A/B test performance, which indicates that this approach has been tested in real world. \n\nTwo small concerns:\n1. In Section 3.3. I am not fully sure why the proposed predictor model is able to win over LSTM.  As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model.  Some insights might be useful there. \n2. The title of this paper is weird.  Suggest to rephrase \""unreasonable\"" to something more positive.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
B1mSWUxR--R1,"This paper dives deeper into understand reward augmented maximum likelihood training.  Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule.  Please elaborate on this. \n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates? \n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.  However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),  Please explain the difference.",1,1,-1,-1,1,1,1,1,-1,-1
B1mSWUxR--R2,"This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule. \n\nI have a few questions on the motivation and the results. \n- In the section \""Open Problems in RAML\"", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q.  This is not true.  The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q.  It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau). \n- Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule,  Can anything be said about this?  Are the KL divergence small enough to guarantee similar predictive rewards? \n- In Theorem 2, when does the exponential tail bound assumption hold? \n- In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML.  Are the differences actually significant?  Are the differences between SQDML/RAML and ML significant?  In addition, how should \\tau be chosen in these experiments?\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1mSWUxR--R3,"The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML.  Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary.  The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule.  (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically.  (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize.  \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric.  Given a reward function, one can define the Bayes decision rule.  The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.  The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).  Of course, the moving-out is biased but the replacing is unbiased.  \n\nThe second contribution is partially valid,  although I doubt how much improvement one can get from SQDML.  The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).  In fact, this step can result in biased estimation because the replacement is inside the nonlinear function.  When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example.  However, when x is not repeated frequently, both RAML and SQDML are biased.  Experiment in section 4.1.2 do not validate significant improvement, either. \n\nThe numerical results are relatively weak.  The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.  However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau.  Is there any guidelines to choose \\tau?  For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper.   These results are also lower than the state of the art performance.  \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.  This is a direct result from definition of conditional probability.  No Bayesian elements, like prior or likelihood appears here. \n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator.  Compared with the n-gram replacement used in the paper, which one is better? \n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.  This will make the comparison much more clear. \n(4). What is Theorem 2 trying to convey?  Although \\tau goes to 0, there is still a gap between Q and Q'.  This seems to suggest that for small \\tau, Q' is not a good approximation of Q.  Are the assumptions in Theorem 2 reasonable?  There are several typos in the proof of Theorem 2. \n(5). In section 4.2.2, the authors write \""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\"".  Could you explain it in more details?\n\n",1,1,1,-1,1,1,1,1,1,-1
B1n8LexRZ-R1,"In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\ n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance).  In the literature, specially in machine learning, there is ``fever\u2019\u2019 about HMC, in my opinion, partially unjustified. \n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point. \n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader. \n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes \n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques,  \n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223\u2013242, April 2001, \n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337\u2013348, 199.\u2028 \n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives.",1,1,1,-1,1,1,1,1,-1,-1
B1n8LexRZ-R2,"The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly.  Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution.  The derivations look correct to me.  In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC.  It showed that the proposed method could mix between the modes in the posterior.  Although the method could mix well when applied to those particular experiments,  it lacks theoretical justifications why the method could mix well.",1,1,1,-1,-1,1,1,1,1,-1
B1n8LexRZ-R3,"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions.  These functions are trained to maximize expected squared jump distance. \nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016. \n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases.  A few downsides are commented on below.\n\n The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2.  \nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper.  I can't see where the number \""124x\"" in sec 5.1 stems from.  As a user, I would be interested in the typical computational cost of both \""MCMC sampler training\"" and MCMC sampler usage (inference?), compared to competing methods.  This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation.  \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC? \n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC). \n\nThe number \""124x ESS\"" in sec5.1 seems at odds with the number in the abstract, \""50x\"". \n\n# Minor errors\n- sec1: \""The sampler is trained to minimize a variation\"": should be maximize\n\""as well as on a the real-world\"" \n- sec3.2 \""and 1/2 v^T v the kinetic\"": \""energy\"" missing\n - sec4: the acronym L2HMC is not expanded anywhere in the paper\n The sentence \""We will denote the complete augmented...p(d)\"" might be moved to after \""from a uniform distribution\"" in the same paragraph.  \nIn paragraph starting \""We now update x\"":\n    - specify for clarity: \""the first update, which yields x' \""/ \""the second update, which yields x''  \""\n     - \""only affects $x_{\\bar{m}^t}$\"": should be $x'_{\\bar{m}^t}$  (prime missing)\n     - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \""mask(x',m^t)\""?\n     - inside zeta_2 and zeta_3, do you not mean $m^t\"" and $\\bar{m}^t$ ?\n - sec5: add reference for first mention of \""A NICE MC\""\n - Appendix A: \n    - \""Let's\"" -> \""Let\""\n    - eq12 should be x''=...\n-  Appendix C: space missing after \""Section 5.1\"" \n- Appendix D1: \""In this section is presented\"" : sounds odd\ n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1NGT8xCZ-R1,"This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al. \nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem. \n\nThe only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model.   \n\nThe authors perform numerous empirical experiments on several types of problems.  They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks.   On the downside, all these experiments concern predictive (discriminative) problems.  Given the paper title, I would have expected some experiments in a generative context.  Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus. \nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016). \n\nMinor comments:\n- Paragraph below Equation 6:  The meaning of $\\phi(\\psi)$ is unclear  \n- Equation (7): phi and psi seems inverted  \n- Section 4: The acronym MLP is used but never defined. \n\n=== update ===\nI lowered my score and confidence, see my new post below.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1NGT8xCZ-R2,"This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances.   \nHowever, I have the following concerns on novelty.  \n\n1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation.   At this point, the novelty is weaken. \n2. In the introduction, the authors mentioned \u201climitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. \u201d However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features.  It is not clear why mSDA cannot handle time-series data but DAuto can.   DAuto does not consider the sequence/ordering of data either.  \n3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss).  As far as I know, this kind of loss is commonly used in most existing methods.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1NGT8xCZ-R3,"The authors propose a probabilistic framework for semi-supervised learning and domain adaptation.  By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.   The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation.  A domain confusion loss is added to learn domain-invariant feature representations.  The authors compared Dauto with several baseline methods on several datasets and showed improvement. \n\nThe paper is well-organized and easy to follow.  The probabilistic framework itself is quite straight-forward.  The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme.  \n\nThe proposed DAuto is essentially DANN+autoencoder.   The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator.  It would be interesting to see if the additional auto-encoder part help address the issue.  \n\nThe experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017).  \n\nIt could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block.  DAuto does seem to offer more boost in domain pairs that are less similar.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1nLkl-0Z-R1,"I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution.  The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \""expected policy gradient\"" by Ciosek & Whiteson.  All these seems very sound and interesting. \n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical).  Such representation style is highly discouraging and brings about un-necessary readability difficulties.  \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is).  So I suggest moving them to the Appendix and make the major focus more narrowed down.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
B1nLkl-0Z-R2,"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.   It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.   Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG. \n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.   The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.   The inclusion of proof summaries in the main text would strengthen this aspect of the paper. \n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.   In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact. \n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1nLkl-0Z-R3,"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.   The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies. \n\nTo be honest, I didn't really \""get\"" this paper.\n*  As far I understand, all of the original work policy gradients involved stochastic policies.   Many are/were Gaussian.\n*  All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*  As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution. \n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas. \n\nI thought the little 2-mode MOG was a nice example of the premise of the model. \n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.   Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.   Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.   They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \""Very well, run your algorithm 10x longer and see where you end up!\""   I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published. \n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n",1,-1,-1,-1,-1,1,-1,-1,-1,-1
B1nxTzbRZ-R1,"The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft.  \n\nI find the problem of defogging quite interesting, even though it is a bit too Starcraft-specific some findings could perhaps be translated to other partially observed environments. \nAuthors use the dataset provided for Starcraft: Brood war by Lin et al, 2017. \n\nMy impression about the paper is that even though it touches a very interesting problem,  it neither is written well nor it contains much of a novelty in terms of algorithms, methods or network architectures. \n\nDetailed comments:\n* Authors should at very least cite (Vinyals et al, 2017) and explain why the environment and the dataset released for Starcraft 2 is less suited than the one provided by Lin et al. \n* Problem statement in section 3.1 should certainly be improved.  Authors introduce rather heavy notation which is then used in a confusing way.  For example, what is the top index in $s_t^{3-p}$ supposed to mean?  The notation is not much used after sec. 3.1, for example, figure 1 does not use it.  \n* A related issue, is that the definition of metrics is very informal and, again, does not use the already defined notation.  Including explicit formulas would be very helpful, because, for example, it looks like when reported in table 1 the metrics are spatially averaged, yet I could not find an explicit notion of that. \n* Authors seem to only consider deterministic defogging models.  However, to me it seems that even in 15 game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it.  At least the concept of stochastic predictions should be discussed\n* The rule-based baselines are not described in detail.  What does \u201cusing game rules to infer the existence of unit types\u201d mean? \n* Another detail which I found missing is whether authors use just a screen, a mini-map or both.  In the game of Starcraft, only screen contains information about unit-types, but it\u2019s field of view is limited.  Hence, it\u2019s unclear to me whether a model should infer hidden information based on just a single screen + minimap observation (or a history of them) or due to how the dataset is constructed, all units are observed without spatial limitations of the screen. \n",1,1,1,-1,1,1,1,1,-1,1
B1nxTzbRZ-R2,"The authors introduce the task of \""defogging\"", by which they mean attempting to infer the contents of areas in the game StarCraft hidden by \""the fog of war\"". \n\nThe authors train a neural network to solve the defogging task, define several evaluation metrics, and argue that the neural network beats several naive baseline models.  \n\nOn the positive side, the task is a nice example of reasoning about a complex hidden state space, which is an important problem moving forwards in deep learning. \n\nOn the negative side, from what I can tell, the authors don't seem to have introduced any fundamentally new architectural choices in their neural network, so the contribution seems fairly specific to mastering StarCraft, but at the same time, the authors don't evaluate how much their defogger actually contributes to being able to win StarCraft games.  \n\nGranted, being able to infer hidden states is of course an important problem,  but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win StarCraft competitions, meaning that, at least as the paper is currently framed, the critical evaluation metric would be showing that a defogger helps to win games.  \n\nTwo ways I could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build \""defoggers\"" for other domains (and spelling out more explicitly what domains the authors expect their insights to generalize to), or doubling down on the StarCraft application specifically and showing that the defogger helps to win games.   A minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to one. \n \nAll that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable. \n\nMinor points:\n- Is there a benefit to having a model that jointly predicts unit presence and count, rather than having two separate models (e.g., one that feeds into the next)?   Could predicting presence or absence separately be a way to encourage sparsity, since absence of a unit is already representable as a count of zero?   The choice to have one model seems especially peculiar given the authors say they couldn't get one set of weights that works for both their classification and regression tasks \n- Notation: I believe the space U is never described in the main text.  What components precisely does an element of U have? \n- The authors say they use gameplay from no later than 11 minutes in the game to avoid the difficulties of increasing variance.  How long is a typical game?    Is this a substantial fraction of the time of the games studied?    If it is not, then perhaps the defogger would not help so much at winning.  \n- The F1 performance increases are somewhat small.   The L1 performance gains are bigger, but the authors only compare L1 on true positives.   This means they might have very bad error on false positives.   (The authors state they are favoring the baseline in this comparison, but it would be nice to have those numbers.)\n- I don't understand when the authors say the deep model has better memory than baselines (which includes a perfect memory baseline)",1,1,1,1,1,1,1,1,-1,-1
B1nxTzbRZ-R3,"# Summary\nThis paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state.  This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map.  The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft. \n\n[Pros]\n- Interesting problem \n\n[Cons]\n- The proposed method is not much novel. \n- The evaluation is a bit limited to two specific downstream prediction tasks. \n\n# Novelty and Significance\n- The problem considered in this paper is interesting. \n- The proposed method is not much novel.  \n- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks.  It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks.  Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible. \n\n# Quality\n- The experimental result is not much comprehensive.  The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks.  It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks). \n\n# Clarity\n- I did not fully understand the learning objective.  Does the model try to reconstruct the state of the current time-step or the future?  The learning objective is not clearly defined.  In Section 4.1, the target x and y have time steps from t1 to t2.  What is the range of t1 and t2?  If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results.",1,1,1,-1,1,-1,-1,1,-1,-1
B1nZ1weCZ-R1,"The paper present online algorithms for learning multiple sequential problems.  The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm.  The contributions are interesting and experimental results seem promising.  But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages).  Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known.  This is a very strong assumption.  In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption.  Therefore, I am not convinced that the paper is ready for publication at ICLR'18. \n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks.  The gap is so large that I am not convinced on the fairness of the comparison . For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge. \n* I do not see how the single output layer is defined. \n* As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm. \n* Section 7.1. It is not clear why degradation does not happen.  It seems to be only an experimental fact.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1nZ1weCZ-R2,"\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.   They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL.   The domain is video games.     All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).  \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL \n\n- a number of approaches developed for the basic idea \n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games) \n\n- paper is overall well written/clear \n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling) \nCouldn't comparisons be made, in some way, to other multitask work? \n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks  ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!). \n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling). \n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section). \n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space).  Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper. \n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features.  Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting. \n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch? \n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ?  Could go either way since the network\n   has to allocate resources to learn other games too.   But is there a pattern? \n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5. \n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?  Why not explore both\n  possibilities?  what are the pros/cons? (section 3)\n\n\n\n\n",1,1,1,1,1,1,1,1,1,-1
B1nZ1weCZ-R3,"In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games.  A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works. \n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper.  Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper.  The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs. \n\nAlthough performance is still reduced compared to single task learning in some cases,  this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale. \n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\",1,1,1,-1,1,1,1,1,1,-1
B1p461b0W-R1,"The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise.  It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \""structured noise\"", and (3) Samples from out-of-dataset classes.  The experiments show robustness to these types of noise.  \n\nReview: \nThe claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise.  This is supported by the literature on \""data cleaning\"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.   While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy. \n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).  The paper did not get a chance to test these differences, and I can only raise a few hypotheses.  First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle.  This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.    Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise.   Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general.   \n\nWithout such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers.   \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search.   For example, for Conv4, how many channels at each layer?   how was the net initialized?  which hyper parameters were tuned and with which values?  were hyper parameters tuned on a separate validation set?  How was the train/val/test split done, etc.  These details are useful for judging technical correctness. \n-- Figure 8 failed to show for me.  \n-- Figure 9,10, need to specify which noise model was used.\n\n\n\n\n\n",1,1,1,1,-1,-1,-1,-1,-1,-1
B1p461b0W-R2,"The authors study the effect of label noise on classification tasks.  They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  \n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.  Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015).  Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains.  \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules.  \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.  However, the previous settings can be reinterpreted in the authors setting.  I found the formulation of the \\alpha to be non-intuitive and confusing at times.  The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.  In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels).  This can be improved to help readers understand better.  \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels.  \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.  \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n",1,1,1,1,1,1,1,1,-1,1
B1p461b0W-R3,"The authors study the effect of label noise on classification tasks.  They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  \n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.  Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015).  Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains.  \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules.  \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.  However, the previous settings can be reinterpreted in the authors setting.  I found the formulation of the \\alpha to be non-intuitive and confusing at times.  The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.  In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels).  This can be improved to help readers understand better.  \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels.  \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.  \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n",1,1,1,1,1,1,1,1,-1,1
B1QgVti6Z-R1,"This paper studies empirical risk in deep neural networks.  Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks. \nResults for deep linear neural networks are puzzling.  Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression.  So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play?  \nAlso this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D.  In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1QgVti6Z-R2,"This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs).  Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.  The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.  2) Uniform convergence of the empirical risk to population risk.  3) Generalization bound based on stability.  The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations. \n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points.  Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW.  Should the risk bound only depends on the dimensions of the matrix W? \n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations.  Thus, the authors need to refine BM's result for comparison.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1QgVti6Z-R3,"Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape.   The analysis answers:\n\n1) When empirical gradients are close to true gradients\ n2) When empirical isolated saddle points are close to true isolated saddle points \n3) When the empirical risk is close to the true risk. \n\nThe answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities.   I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct.  \n\nAnother reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.   Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.    An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.   However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one.  \n\nOverall, I believe this paper is a nice contribution to the deep learning theory literature.  However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem.    Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters.   I suspect their upper bounds will be loose,  but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds",1,1,1,-1,1,1,1,1,1,-1
B1QRgziT--R1,"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.  The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator.  This Lipschitz property has already been proposed by recent methods and has showed some success.  However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.  This is demonstrated in comparison to weight normalization in Figure 4.  The experimental results are very good and give strong support for the proposed normalization. \n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs.  The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.  I am recommending acceptance,  though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.  More details in the comments below. \n\nComments:\n1. One concern about this paper is that it doesn\u2019t fully answer the reasons why this normalization works better.  I found the discussion about rank to be very intuitive,  The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency.  I would like to see the same spectra included.  \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?  One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.  What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win? \n3. Section 4 needs some careful editing for language and grammar.\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1QRgziT--R2,"This paper proposes \""spectral normalization\"" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function.  The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.  Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods. \n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.  The experimental results seem solid and seem to support the authors' claims.  I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.  Like the anonymous commenter, I also initially thought that the proposed \""spectral normalization \"" is basically the same as \""spectral norm regularization\"", but given the authors' feedback on this I think the differences should be made more explicit in the paper. \n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication. \n\nSmall Nits: \n\nSection 4: \""In order to evaluate the efficacy of our experiment\"": I think you mean \""approach\"". \n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \""As we prophesied ...\"", and in the paragraph below \n * \""... is a tad slower ...\"".",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1QRgziT--R3,"The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator.  The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \""spectrally normalized\"" objective. \n\nI think the methodology presented in this paper is neat and the experimental results are encouraging.  However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato).  For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2.  I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients.  Please clarify this. \n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator.  Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?",1,1,1,-1,1,1,1,1,-1,-1
B1spAqUp--R1,"This paper is well written and easy to follow.  The authors propose pixel deconvolutional layers for convolutional neural networks.  The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers.  \nThe method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer.  PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer.  The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset).  \n\nThe authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower).  In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation).  Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm).  More comments and further exploration on this results should be done.  Why no performance boost?  Is it because of the residual connection?  Or other component of DeepLab?  Is the proposed layer really useful once a powerful model is used? \n\nI also think the experiments on VAE are not conclusive.  The authors simply show set of generated images.  First, it is difficult to see the different of the image generated using deconv and PixelDCL.  Second, a set of 20 qualitative images does not (and cannot) validate any research idea.",1,1,1,-1,1,1,1,1,-1,-1
B1spAqUp--R2,"Paper summary:\nThis paper proposes a technique to generalize deconvolution operations used in standard CNN architectures.  Traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixels.  This work proposes to do sequential prediction of adjacent pixel features (via intermediate feature maps) resulting in more spatially smooth outputs for deconvolution layer.  This new layer is referred to as \u2018pixel deconvolution layer\u2019 and it is demonstrated on two tasks of semantic segmentation and face generation. \n\n\nPaper Strengths:\n- Despite being simple technique, the proposed pixel deconvolution layer is novel and interesting",1,-1,-1,-1,-1,-1,1,-1,1,-1
B1spAqUp--R3,"This paper proposed the new approach for feature upsampling called pixel deconvolution, which aims to resolve checkboard artifact of conventional deconvolution.  By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent. \n\nOverall, the paper is clearly written and easy to understand the main motivation and methods.  However, the checkboard artifact is a well-known problem of deconvolution network, and has been addressed by several approaches which are simpler than the proposed pixel deconvolution.  For example, it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent, and bilinear additive upsampling proposed in Wonja et al., 2017 also demonstrated its effectiveness as an alternative for deconvolution.   Comparisons against these approaches would make the paper stronger.   Besides, comparisons/discussions based on extensive analysis on various deconvolution architectures presented in Wonja et al., 2017 would also be interesting. \n\nWonja et al, The Devil is in the Decoder, In BMVC, 2017\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1tC-LT6W-R1,"The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time.  \n\nThe proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters,  b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters. \n\nThe experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy.  \n\nI have the following questions regarding the experiments:\n1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data?  It is not explicitly stated.  I hope it is indeed the former, else I have a major concern with the efficacy of the algorithm as ultimately, we care about the test performance of the compressed models in comparison to uncompressed model.  \n\n2. In B.1 the authors use an increasing number units in the hidden layers of the GRUs as opposed to a fixed size like in Deep Speech 2, an obvious baseline that is missing from the  experiments is the comparison with *exact* same GRU (with  768, 1024, 1280, 1536 hidden units) *without any compression*.  \n\n3.  What do different points in Fig 3 and 4 represent.  What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4.  I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1.  \n\nIt is interesting that although L2 regularization does not lead to low \\nu parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization.  The authors point it out, but a further investigation might be interesting.  \n\nWriting:\n1. The GRU model for which the algorithm is proposed is not introduced until the appendix.  While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like \u201c\\lambda_rec\u201d and \u201c\\lambda_norec\u201d",1,1,1,-1,1,-1,1,1,-1,-1
B1tC-LT6W-R2,"The authors propose a strategy for compressing RNN acoustic models in order to deploy them for embedded applications.  The technique consists of first training a model by constraining its trace norm, which allows it to be well-approximated by a truncated SVD in a second fine-tuning stage.  Overall, I think this is interesting work,  but I have a few concerns which I\u2019ve listed below:\n\n1. Section 4, which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit \u201cdisjoint\u201d from the rest of the paper.  I had a number of clarification questions spefically on this section:\n- Am I correct that the results in this section do not use the trace-norm regularization at all?  It would strengthen the paper significantly if the experiments presented on WSJ in the first section were also conducted on the \u201cinternal\u201d task with more data. \n- How large are the training/test sets used in these experiments (for test sets, number of words, for training sets, amount of data in hours (is this ~10,000hrs), whether any data augmentation such as multi-style training was done, etc.) \n- What are the \u201ctier-1\u201d and \u201ctier-2\u201d models in this section?  It would also aid readability if the various models were described more clearly in this section, with an emphasis on structure, output targets, what LMs are used, how are the LMs pruned for the embedded-size models, etc.  Also, particularly given that the focus is on embedded speech recognition, of which the acoustic model is one part, I would like a few more details on how decoding was done, etc. \n- The details in appendix B are interesting, and I think they should really be a part of the main paper.  That being said, the results in Section B.5, as the authors mention, are somewhat preliminary,  and I think the paper would be much stronger if the authors can re-run these experiments were models are trained to convergence. \n- The paper focuses fairly heavily on speech recognition tasks, and I wonder if it would be more suited to a conference on speech recognition.  \n\n2. Could the authors comment on the relative training time of the models with the trace-norm regularizer, L2-regularizer and the unconstrained model in terms of convergence time. \n\n3. Clarification question: For the WSJ experiments was the model decoded without an LM?  If no LM was used, then the choice of reporting results in terms of only CER is reasonable,  but I think it would be good to also report WERs on the WSJ set in either case. \n\n4. Could the authors indicate the range of values of \\lambda_{rec} and \\lambda_{nonrec} that were examined in the work?  Also, on a related note, in Figure 2, does each point correspond to a specific choice of these regularization parameters? \n\n5. Figure 4: For the models in Figure 4, it would be useful to indicate the starting CER of the stage-1 model before stage-2 training to get a sense of how stage-2 training impacts performance. \n\n6. Although the results on the WSJ set are interesting,  I would be curious if the same trends and conclusions can be drawn from a larger dataset -- e.g., the internal dataset that results are reported on later in the paper, or on a set like Switchboard.  I think these experiments would strengthen the paper. \n\n7. The experiments in Section 3.2.3 were interesting, since they demonstrate that the model can be warm-started from a model that hasn\u2019t fully converged.  Could the authors also indicate the CER of the model used for initialization in addition to the final CER after stage-2 training in Figure 5. \n\n8. In Section 4, the authors mention that quantization could be used to compress models further although this is usually degrades WER by 2--4% relative. I think the authors should consider citing previous works which have examined quantization for embedded speech recognition [1], [2]. In particular, note that [2] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after training. \nReferences:\n[1] Vincent Vanhoucke, Andrew Senior, and Mark Mao, \u201cImproving the speed of neural networks on cpus,\u201d in Deep Learning and Unsupervised Feature Learning Workshop, NIPS, 2011.\n[2] Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin, \u201cOn the efficient representation and execution of deep acoustic models,\u201d Proc. of Interspeech, pp. 2746 -- 2750, 2016.\n\n9. Minor comment: The authors use the term \u201cwarmstarting\u201d to refer to the process of training NNs by initializing from a previous model.  It would be good to clarify this in the text.",1,1,1,1,1,1,1,1,-1,-1
B1tC-LT6W-R3,"Paper is well written and clearly explained.  The paper is a experimental paper as it has more content on the experimentation  and less content on problem definition and formulation.  The experimental section is strong and it has evaluated across different datasets and various scenarios.  However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue.  It only considers a slight modification into the loss function by adding a trace norm regularization.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1tExikAW-R1,"The idea is clearly stated (but lacks some details) and I enjoyed reading the paper.  \n\nI understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better.  From the adversary's standpoint, it would be easier to manipulate inputs than latent variables.  On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation. \n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more.  I could not imagine how VAE and T are trained simultaneously. \n\nIn Section 3.2, the authors listed a couple of loss functions.  How were these loss functions are combined?  The final optimization problem that is used for training of the propose VAE should be formally defined.  Also, the detailed specification of the VAE should be detailed. \n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner,  however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines.  For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks?  Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1tExikAW-R2,"This paper misses the point of what VAEs (or GANs, in general) are used for.  The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples.  The use of these techniques for compressing is still unclear and their quality today is too low.  So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense.  \n\nBut let\u2019s assume that at some point they can be used as the authors propose.  In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing.  Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless?  I think this will make the first attack useless.  \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE.  This is even less doable, because the encoder and decoder are not train remotely.  They are train of the same machine or cluster in a controlled manner by the person that would use the system.  Once it is train it will give away the decoder and keep the encoder for sending information.\n\n",1,1,1,1,1,-1,1,1,-1,-1
B1tExikAW-R3,"This paper is concerned with both security and machine learning.  \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified. \nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input;  2) make minimal change in the middle so that the attack is not detectable.  \n\nThis paper is concerned with both security and machine learning, but there is no clear contributions to either field.  From the machine learning perspective, the proposed \""attacking\"" method is standard without any technical novelty.  From the security perspective, the scenarios are too simplistic.  The encoding-decoding mechanism being attacked is too simple without any security enhancement.  This is an unrealistic scenario.  For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them.  Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community.  \n\nI am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas.  For example, what are the necessary and sufficient conditions for an attacking method to be undetectable?  Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient?  Is there any reference for them?  Why do we require the correspondence between the classification confidence of tranformed and original data?  Would it be enough to match the DISTRIBUTION of the confidence?",1,-1,1,-1,-1,1,-1,-1,-1,-1
B1twdMCab-R1,"This paper proposes a model for adding background knowledge to natural language understanding tasks.  The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction.  The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion). \n\nI think the paper does a fairly good job at doing what it does,  it is just hard to get excited by it.  \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic.  But then they just concept net to augment text.  This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation.  As is, I don't really see how this motivation has anything to do with getting things out of a KB.  A KB is usually a pretty static entity, and things are added to it at a slow pace. \n\n* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI.  Specifically they take text and add common sense knowledge from concept net.  The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis.  However, is this statement enough to cross the acceptance threshold of ICLR?  Seems a bit marginal to me. \n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them.  To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way.  For example, have another RNN read the assertions and somehow integrate that.  The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated.  There are no comparisons to other possibilities.  As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful.  As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution. \n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis.  The conclusion is that reading background knowledge from concept net boost performance using some architecture.  This is nice to know but I think does not cross the acceptance threshold.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1twdMCab-R2,"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems . They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format.  However, in this paper, the way it is added is simply by updating word representations based on this extra text.  This seems too simple to really be the right way to add background knowledge.  \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing;  the paper never says exactly how, not even if you read the supplementary material).  This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!   As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning). \n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \""reading\"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%).  A note on the QA results: The QA results are certainly good enough to be in the range of \""good systems\"",  but none of the results really push the SOTA.  The best SQuAD (devset) results are shown as several percent below the SOTA.  In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission,  but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD.  Similar remarks perhaps apply to the NLI results. \n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller,  but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge. \n\nBiggest question:\n - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge? \n\nMinor notes:\n - The paper was very well written/edited.  The only real copyediting I noticed was in the conclusion: and be used \u2794 and can be used; that rely on \u2794 that relies on. \n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems? \n - On p.3 above sec 3.1: What is u?  Was that meant to be z? \n - On p.8, I'm a bit suspicious of the \""Is additional knowledge used? \"" experiment which trains with knowledge and then tests without knowledge.  It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone. \n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs.  This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n",1,1,1,-1,1,1,1,1,-1,-1
B1twdMCab-R3,"The quality of this paper is good.  The presentation is clear  but I find lack of description of a key topic.  The proposed model is not very innovative but works fine for the DQA task.  For the TE task, the proposed method does not perform better than the state-of-the-art systems.  \n\n- As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM. \n- The reference of ESIM is not correct. \n- Figure 1 is hard to understand.  \n- What corpus did you use to pre-train word vectors?  \n- As the proposed method was successful for the QA task,  you need to explain QA data sets and how the questions are solved. \n- I also expect performance and  error analysis of the task results.   \n- To claim \""task-agnostic\"", you need to try to apply your method to other NLP tasks as well. \n- Page 3. \\Sigma is not defined.",1,1,1,-1,1,1,1,1,-1,-1
B1uvH_gC--R1,"The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks.  The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension. \n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000.  Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example.  In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE.  Please add the Swiss roll example. \n* Distance preservation appears more and more like a dated DR paradigm.  Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation.  Computation accuracy of the geodesic distances in high-dimensional spaces can be poor.  This could be discussed and some experiments on very HD data should be reported. \n* Some key historical references are overlooked, like the SAMMANN.  There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago.  Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity).  Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too. \n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach.  The motivation in the end of page 3 seems to be computational only. \n* Section 3 should be further detailed (step 2 in particular). \n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress.  The running times are not in favor of the proposed method.  The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold. \n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...').  What is the take-home message of the paper?",1,1,1,1,-1,-1,-1,-1,-1,-1
B1uvH_gC--R2,"The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension.  They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure.  The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework. \n\nThe paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional). \n\nThe abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc. \n\nIn the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities.  This is in general not true.  Even if the number of parameters is small, learning them might require complex computations on the whole data set.  On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N). \n\nIn section 2.1, the authors claim \""Spectral techniques are non-parametric in nature\""; this is wrong again.  E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words. \n\nIn section 2.2, it says \""observation that the double centering...\"".  Can you provide a citation for this? \n\nIn section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity.  It is not quite clear from the text what the resulting complexity would be.  With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2).  Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is. \n\nFigure 3, contrary to text, does not provide a visualisation to the sampling mechanism. \n\nIn the experiments section, can you provide a citation for ADAM and explain how the parameters were selected?  Also, it is not meaningful to measure the quality of a visualisation via the MDS fit.  There are more useful approaches to this task, such as the quality framework [*]. \n\nIn figure 4a, x-axis should be \""number of landmarks\"". \n\nIt is not clear why the equation 6 holds.  Citation? \nIt is also not clear how exactly the equation 7 is evaluated.  It says \""By varying the number of layers and the number of nodes...\"", but the nodes and layer are not a part of the equation. \n\nThe notation for equation 8 is not explained.  Again, use [*].\n\n[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.\n",1,1,1,1,1,1,1,1,-1,-1
B1uvH_gC--R3,"The key contribution of the paper is a new method for nonlinear dimensionality reduction.  \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling.  While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry.  The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets).  \n\nThe paper revisits mostly familiar ideas.  The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is.  This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution. \n\nMoreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice.  However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with \""holes\"" in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc).  \n\nFinally, the experimental results are somewhat uninspiring.  It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x.  \n\nThe horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions. \n",1,1,1,1,-1,-1,-1,-1,-1,-1
B1wN2f2-G-R1,"This paper surveys models for collaborative filtering with user/item covariate.  \n\nOverall the authors seems to have captured the essence of a large number of popular CF models and I found that the proposed model classification is reasonable.  The notation also make it easy to understand the differences between different models.  In that sense this paper could be useful to researchers wanting to better understand this field.  It may also be useful to develop further insights into current models (although the authors do not go that route).  \n\nThe impact of this paper may be limited in this community since it is a survey about a fairly niche topic (a subset of recommender systems) that may not be of central interest at ICLR.  Overall, I think this paper would be a better fit in a recsys, applied ML or information retrieval journal. \n\n\nA few comments: \n\nI find that there are several ways the paper could make a stronger contribution:  \n1) Use the unifying notation to discuss strengths and weaknesses of current approaches (ideally with insights about possible future approaches).\ n2) Report the results of a large study of many of the surveyed models on a large number of datasets.  Ideally further insights could be derived from these results.\ n3) Provide a common code framework with all methods\ n4) Add a discussion on more structured sources of covariates (e.g., social networks).  This could probably more or less easily be added as a subsection using the current classification. \n\n- A similar classification of collaborative filtering models with covariates is proposed in this thesis (p.41):\nhttps://tspace.library.utoronto.ca/bitstream/1807/68831/1/Charlin_Laurent_201406_PhD_thesis.pdf \n\n- The paper is well written overall  but the current version of the paper contains several typos.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1wN2f2-G-R2,"This paper provides a survey of attribute-aware collaborative filtering.  In particular, it classifies existing methods into four different categories, according to the representation of the interactions of users, items and attributes.  Furthermore, the authors also provide the probabilistic interpretation of the models.  In addition, preliminary experiments comparing among different categories are also provided.  \n\nThere have existed several works which also provide surveys of attribute-aware collaborative filtering . Hence, the contribution of this paper is limited, although the authors claim two differences between their work and the existing ones.  In particular, the advantages and disadvantages of different categories are not systematically compared, and hence the readers cannot get insightful comments and suggestions from this survey.\ n\nIn general, survey papers are not very suitable for publication at conferences. \n",1,1,-1,-1,-1,1,1,-1,-1,-1
B1wN2f2-G-R3,"This paper reviews the existing literature on attribute-based collaborative filtering.  The author categories the existing works int four categories. \n\nWhile the categorization is reasonable , there is no proposed new work beyond the existing approaches.  No new insight is being discussed.  Such survey style paper is not appropriate to for ICLR.",1,1,-1,-1,-1,-1,-1,-1,-1,-1
B1X0mzZCW-R1,"This paper suggests a simple yet effective approach for learning with weak supervision.  This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision.    The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \""fidelity\"" of the weak label when training the student at the final step.   The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students.   The suggested method seems to work well on several document classification tasks.   \n\nOverall, I liked the paper.   I would like the authors to consider the following questions - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few).   First, I'd suggest acknowledging these works and discussing the differences to your work.  Second - Is your approach applicable to these frameworks?   It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve.  \n\n- Can this approach be applied to semi-supervised learning?  Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? \n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data.   Is it clear that this step in needed?  Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student?  \n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1X0mzZCW-R2,"The problem of interest is to train deep neural network models with few labelled training samples.  The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data.  The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching.  The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.  The teacher also supplies an uncertainty estimate to each predicted label.  How about the heuristic function?  This is used for learning initial feature representation of the student model.  Crucially, the teacher model will also rely on these learned features.  Labelled data and unlabelled data are therefore lie in the same dimensional space.  \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.  Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points.   Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data.   On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited.   As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations).   It will be informative to provide results with a single GP model.   \n2)\tFrom modifying learning rates to weighting samples.   Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty.   Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n",1,1,1,-1,1,1,1,1,-1,-1
B1X0mzZCW-R3,"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.   This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.   The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.   The authors evaluate their proposed methods on one toy problem and two real-world problems.   The paper is well written, easy to follow, and have good experimental study.    My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work  . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?   To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?   (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?   (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?   Why not learning the representation using an unsupervised learning method (unsupervised pre training)?   This should be at least one of the baselines.  \n(4) the idea of using surrogate labels to learn representation is also not new.   One example work is \""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\"". The authors didn't compare their method with this one.",1,-1,1,-1,1,1,1,1,-1,-1
B1X4DWWRb-R1,"The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X.  The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x.  If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still \""work\"" and the distribution of X can also change without affecting the accuracy of the predictions.  \n\nWhat is proposed seems to be twofold:\n- instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound.  The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size. \n- as an additional ingredient the authors also propose \""representation learning\"" by mapping x to some representation Phi(x).  \nThe goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance.  \n\nPros: \n- The problem is relevant and also appears in similar form in domain adaptation and transfer learning.  \n- The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al.  \n\nCons:\n- I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others.  \n- The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail.  Especially the second half of page 5 is at times very hard to understand as it is so dense.  \n- The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_\\Phi, C^\\mathcal{F}_{n,\\delta} and D^{\\Phi,\\mathcal{H}}_\\delta.  Why would we expect these quantities to be small or bounded?  How does that compare to the assumptions needed for standard inverse probability weighting?   \n- I appreciate that it is difficult to find good test datasets for evaluating causal estimator.    The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?).  The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.\n\n\n\n\n\n\n\n\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1X4DWWRb-R2,"This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function.  Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs. \nOverall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs.  \n\nThe main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift.  However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
B1X4DWWRb-R3,"Summary:\nThis paper proposes a new approach to tackle the problem of prediction under\nthe shift in design, which consists of the shift in policy (conditional\ndistribution of treatment given features) and the shift in domain (marginal \ndistribution of features). \n\nGiven labeled samples from a source domain and unlabeled samples from a target\ndomain, this paper proposes to minimize the risk on the target domain by \njointly learning the shift-invariant representation and the re-weighting \nfunction for the induced representations.  According to Lemma 1 and its finite\nsample version in Theorem 1, the risk on the target domain can be upper bounded\nby the combination of 1) the re-weighted empirical risk on the source domain;  \nand 2) the distributional discrepancy between the re-weighted source domain and\nthe target domain.  These theoretical results justify the objective function\nshown in Equation 8.  \n\nExperiments on the IHDP dataset demonstrates the advantage of the proposed\napproach compared to its competing alternatives. \n\nComments:\n1) This paper is well motivated.  For the task of prediction under the shift in\ndesign, shift-invariant representation learning (Shalit 2017) is biased even in\nthe inifite data limit.  On the other hand, although re-weighting methods are\nunbiased, they suffer from the drawbacks of high variance and unknown optimal\nweights.  The proposed approach aims to overcome these drawbacks. \n\n2) The theoretical results justify the optimization procedures presented in\nsection 5.  Experimental results on the IHDP dataset confirm the advantage of\nthe proposed approach.  \n\n3) I have some questions on the details.  In order to make sure the second \nequality in Equation 2 holds, p_mu (y|x,t) = p_pi (y|x,t) should hold as well. \nIs this a standard assumption in the literature? \n\n4) Two drawbacks of previous methods motivate this work, including the bias of\nrepresentation learning and the high variance of re-weighting.  According to\nLemma 1, the proposed method is unbiased for the optimal weights in the large\ndata limit.  However, is there any theoretical guarantee or empirical evidence\nto show the proposed method does not suffer from the drawback of high variance? \n\n5) Experiments on synthetic datasets, where both the shift in policy and the\nshift in domain are simulated and therefore can be controlled, would better \ndemonstrate how the performance of the proposed approach (and thsoe baseline \nmethods) changes as the degree of design shift varies.  \n\n6) Besides IHDP, did the authors run experiments on other real-world datasets, \nsuch as Jobs, Twins, etc?",1,1,1,-1,1,1,1,1,1,-1
B1ydPgTpW-R1,"The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits.  The method uses a learnable character embedding to transform the data, but is an end-to-end approach . The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features.  \nHere are my concerns:\n1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model.  That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000.  \n\n2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2.  If so, the evaluation setting of dividing data into three *random* sets of training, validation, and test, in 5.3 doesn't seem to be the right and most appropriate choice.  It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1ydPgTpW-R2,"Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \""luckiness\"" of the license plate number.   The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings.  Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.   The proposed pipeline converts each character to an embedding with the only sentence of description being \""Each character is converted by a lookup table to a vector representation, known as character embedding.\""    Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined.  Finally they find a \""a feature vector for each plate by summing up the output of the last recurrent layer overtime. \ and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates.  In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better.  \n\nSpecific Comments on Sections: \nComments: Sec 1,2\nIn these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data. \n\nSec 3\nThe author does not mention the following reference: \""Deep learning for stock prediction using numerical and textual information\"" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks. \n\nSec 4\nWhat are the characters embedded with? This is important to specify.  Is it Word2vec or something else?  What does the lookup table consist of?  References should be added to the relevant methods.  \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison.  LSTMs as well could have been a point of comparison.  \n\nSec 6\n Nothing too insightful is said about the RNN Model.  \n\nSec 7\nThe ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table. \n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN.  However comparisons are lacking and the paper is not presented very scientifically.   The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.\n",1,1,1,-1,1,1,1,-1,-1,-1
B1ydPgTpW-R3,"The authors present a deep neural network that evaluates plate numbers.  The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context.  I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow.  There are some typos and grammatical problems (indicated below),  but nothing really serious. \n\nSo, the paper is relevant and well presented.  However, I find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality.  Even though the significance of the work is apparent given the good results of the proposed neural network,  I believe that such material is more appropriate to a focused applied meeting.  However, even for that sort of setting I think the paper requires some additional work, as some final parts of the paper have not been tested yet (the interesting part of explanations).  Hence I don't think the submission is ready for publication at this moment. \n\nConcerning the text, some questions/suggestions:\n- Abstract, line 1: I suppose \""In the Chinese society...\""--- are there many Chinese societies? \n- The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text.  \n- Footnote 1, line 2: \""an exchange\"". \n- Page 2, line 12: \""prices.  Among\"".\n- Please add commas/periods at the end of equations. \n- There are problems with capitalization in the references.",1,1,1,-1,-1,-1,-1,-1,-1,-1
B1Z3W-b0W-R1,"This paper proposes an iterative inference scheme for latent variable models that use inference networks.  Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al.  The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network). \n\nMy main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach.  It appears that the iterative method should result in \""direct improvement with additional samples and inference iterations\"".  I am supposing this is at the test time.  It is not clear exactly when this will be useful.  \n\nI believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al.  2014 (they used this method to perform data imputation). \n\nThe paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE. \n\nIn summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method.",1,-1,1,-1,-1,-1,-1,-1,-1,-1
B1Z3W-b0W-R2,"This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution.  The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm.  \n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family).   This source of error is often ignored in the literature,  although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). \n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. \n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs.  They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables. \n\nI think this is good and potentially important work,  although I do have some questions/concerns about the results in Table 1 (see below). \n\n(Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset.  A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm. \n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014).  First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance.  Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1.  An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter. \n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}? \n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most).  This is much smaller than the gap in Figure 5(a). \n* The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture.  I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats.",1,1,1,1,1,1,1,1,1,-1
B1Z3W-b0W-R3,"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two.   In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients.   The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate. \n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context.   From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016).   Indeed the paper perhaps anticipates this perspective and preemptively offers that \""variational inference is a qualitatively different optimization problem\"" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work.   But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient.   That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. \n\nBeyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE.   While these results are enlightening,  most of the conclusions are not entirely unexpected.   For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result.   It would certainly seem strange if this were not the case.   And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance.   Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016). \n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison?   For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration.   In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained. \n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving  but the iterative inference model has mostly saturated. \n* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.",1,1,1,-1,1,1,1,1,-1,-1
B1ZvaaeAZ-R1,"The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision.  The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks.  \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision. \n\nNegatives\n- While the exhaustive analysis is extremely useful  the overall technical contribution of the paper that of widening the networks is fairly small.  \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches.  However, the results are more focused on compute cost.  Also large batches are used mainly during training where memory is generally not a huge issue.  Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes.  It might help to emphasize the speed-up in compute more in the contributions.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
B1ZvaaeAZ-R2,"This is a well-written paper with good comparisons to a number of earlier approaches.  It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs.  Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper. \n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction. \n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design.  It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today.  \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit) \n- Moving the state of the art in low precision forward \n- Strong potential impact, especially on constrained power environments (but not limited to them) \n- Uses same hyperparameters as original training, making the process of using this much simpler. \n\nCons/Questions\n- They mention not quantizing the first and last layer of every network.  How much does that impact the overall compute?  \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model?  This could be interesting for low power case, even if the \""effective compute\"" is larger than the baseline.\n",1,1,1,-1,1,1,1,1,1,-1
B1ZvaaeAZ-R3,"This paper presents an simple and interesting idea to improve the performance for neural nets.  The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced).  The paper is aiming to solve a practical problem, and has done some solid research work to validate that.   In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach.    \n\nIn addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet.  The results are convincing to me.  \n\nOn the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study.  So the novelty is limited. \n\nSo overall given limited novelty  but the paper presents useful results,  I would recommend borderline leaning towards reject.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ0hF1Z0b-R1,"Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy.  The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example.  The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework. \n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale.  The paper uses the right composition tools like moments accountant to get strong privacy guarantees.  The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm.  Both these contributions are important in the effectiveness of the overall algorithm. \n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models.  I did not find strong algorithmic ideas in the paper.  I found the paper to be lacking in that respect",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJ0hF1Z0b-R2,"\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee.  The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed. \n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent \n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point. \n\nCorrectness: The paper is technically correct. \n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline. \n2. Clipping is supposed to help with the exploding gradients problem.  Do you have an idea why a low threshold hurts the performances?  Is it because it reduces the amplitude of the updates (and thus simply slows down the training)? \n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)? \n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee. \n2. Strong experimental setup that analyses in details the proposed extensions. \n3. Experiments performed on public datasets. \n\nCons\n-------\n\nNone \n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \""is given in Figure 1\"" -> \""is given in Algorithm 1\""\n\n Note\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.",1,1,1,-1,1,-1,1,1,-1,-1
BJ0hF1Z0b-R3,"This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models.  It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model. \n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community.  This work makes a pretty significant contribution to such topic.  It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy.  The experiment shows good privacy and utility. \n\nThe presentation of the paper can be improved a bit.  For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper.  Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ4prNx0W-R1,"Quality\nThe paper is well-written and clear, and includes relevant comparisons to previous work (NPI and recursive NPI). \n\nClarity\nThe paper is clearly written. \n\nOriginality\nTo my knowledge the method proposed in this work is novel.  It is the first to study constructing minimal training sets for NPI given a black-box oracle.  However, as pointed out by the authors, there is a lot of similar prior work in software testing. \n\nSignificance\nThe work could be potentially significant,  but there are some very strong assumptions made in the paper that could limit the impact.  If the NPI has access to a black-box oracle, it is not clear what is the use of training an NPI in the first place.  It would be very helpful to describe a potential scenario where the proposed approach could be useful.  Also, it is assumed that the number of possible inputs is finite (also true for the recursive NPI paper), and it is not clear what techniques or lessons of this paper might transfer to tasks with perceptual inputs.  The main technical contribution is the search procedure to find minimal training sets and pare down the observation size, and the empirical validation of the idea on several algorithmic tasks. \n\nPros\n- Greatly improves the data efficiency of recursive NPI. \n- Training and verification sets are automatically generated by the proposed method. \n\nCons\n- Requires access to a black-box oracle to construct the dataset. \n- Not clear that the idea will be useful in more complex domains with unbounded inputs.\n",1,1,1,-1,1,1,1,1,1,-1
BJ4prNx0W-R2,"In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.  In particular, they aim at generating a complete set that fully specifies the behavior of the oracle.  The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation.  The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.  The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.  Their experiments show that this improve technique can produce complete training sets for three programs. \n\nIt is nice to see the application of ideas from different areas for learning-related questions.  However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all?  Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data.  If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter.  One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter.  During research, we have multiple executable oracles and need to produce good training data from them.  The authors' technique may let us do this data-generation easily.  But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ4prNx0W-R3,"Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion.  It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem.  Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions.  In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper.  They iteratively explore all\npossible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive\noracle ensures that the procedure converges.  As a running example, they show how this can be be done for bubblesort.  The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed.  It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper.  \n\nI am not an expert in this area, so it is difficult for me to judge the technical merit of the work.  My feeling from reading the paper is that it is rather incremental over Cai et al.  I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant.  But these are also the same set of experiments performed by Cai et al.  \n\nGiven the original number of traces generated is huge, I do not understand, why this method is at all practical.  This also explains why the authors have just tested the performance on extremely small sized data. It will not scale.  So, I am hesitant accepting the paper.  I would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ78bJZCZ-R1,"The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor.  While the RWA was an interesting idea  with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods. \n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original.  On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU  - except for MultiCopy where it trains faster,  but not to better results and it looks like the difference is between few and very-few training steps anyway.  The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM.  So the results are not strongly convincing, and the paper lacks any mention of newer work on attention.  This year strong improvements over state-of-the-art have been achieved using attention for translation (\""Attention is All You Need\"") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition).  To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJ78bJZCZ-R2,"This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage.  The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions. \n\nThe proposed method is using Elman nets as the base RNN.  I think the same method can be applied to GRUs or LSTMs . Some parameters might be redundant,  however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination. \n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA? \nOverall, the proposed method seems to be very useful for the RWA.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ78bJZCZ-R3,"Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention.  The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t. \n\nPros:\n- Addresses an issue of RWAs. \n\nCons:\n-The paper addresses a problem with an issue with RWAs.  But it is not clear to me why would that be an important contribution. \n-The writing needs more work. \n-The experiments are lacking and the results are not good enough. \n\nGeneral Comments:\n\nThis paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address.  I would still like to have a better justification on why should we care about RWA and fixing that model.  \n\nThe writing of this paper seriously needs more work.   The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c. \n\nThe experiments are only on toyish and small scale tasks.  According to the results the model doesn't really do better than a simple LSTM or GRU.",1,-1,1,-1,-1,-1,-1,-1,-1,-1
BJ8vJebC--R1,"This paper investigates the impact of character-level noise on various flavours of neural machine translation.  It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side.  The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables.  They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data.  Unfortunately, they are not able to show any types of synthetic noise helping address natural noise.  However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise. \n\nThis is a thorough exploration of a mostly under-studied problem.  The paper is well-written and easy to follow.  The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty.  The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work.  Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others.  In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT.  The proposed meanChar architecture doesn\u2019t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn\u2019t extremely satisfying.  Furthermore, the use of these replacement tables means that even when the noise is natural, it\u2019s still kind of artificial.  Finally, this paper doesn\u2019t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference. \n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial.  First of all, errors learned from the noisy data sources are constrained to exist within a word.  This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words.  Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data.  This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur).  I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6).  I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner.  On a related note, the line in the abstract stating that \u201c... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise\u201d implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren\u2019t given the chance. \n\nSection 7.2 on the richness of natural noise is extremely interesting,  but maybe less so to an ICLR audience.  From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies. \n\nI have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).\n\n[edited for typos]",1,1,1,1,1,1,1,1,1,-1
BJ8vJebC--R2,"This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural.  The results are not surprising:\n\n* NMT is terrible with noise. \n\n* But it improves on each noise type when it is trained on that noise type. \n\nWhat I like about this paper is that:\n\n1) The experiments are very carefully designed and thorough. \n\n2) This problem might actually matter.  Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish.  But as the paper shows, it\u2019s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow.  So this paper could have real-world impact. \n\n3) Most importantly, it shows that NMT\u2019s handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different.  So solving the problem of natural noise is not so simple\u2026 it\u2019s a *real* problem.  Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale.  So these methods could be applied in the real world.  (It would be excellent if an outcome of this paper was that commercial MT providers answered it\u2019s call to provide more realistic noise by actually providing examples.) \n\nThere are no fancy new methods or state-of-the-art numbers in this paper.  But it\u2019s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR.",1,1,1,-1,1,1,1,1,-1,-1
BJ8vJebC--R3,"This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust. \n\nOverall the paper is a clearly written, well described report of several experiments.  It shows convincingly that standard NMT models completely break down on both natural \""noise\"" and various types of input perturbations.  It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.  The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations. \n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.  The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps)  but the impact may be.  I wonder if you could put this in the context of \""training with input noise\"", which has been studied in Neural Network for a while (at least since the 1990s).  I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise.  Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet. \n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \""noisy\"" training set and adding that to clean data?  Or introducing noise \""on-line\"" as part of the training?  If fixed, what sizes were tried?  More information on the experimental design would help. \n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \""Rand\"" line for German, or the Swap/Mid/Rand lines for Czech.  It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance. \n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way. \n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks.",1,1,1,-1,1,1,1,1,-1,-1
BJaU__eCZ-R1,"Quality\n\nThis is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging. \n\nClarity\n\nThe paper is well-written.  Methods and results are clearly described.  The authors state significant improvements in classification using generated data.  These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets. \n\nOriginality\n\nThis is one of the first uses of GANs in the context of neuroimaging.""  \n\nSignificance \n\nThe approach outlined in this paper may spawn a new research direction. \n\nPros\n\nWell-written and original contribution demonstrating the use of GANs in the context of neuroimaging. \n\nCons\n\nThe focus on neuroimaging might be less relevant to the broader AI community",1,1,1,-1,1,1,1,1,1,-1
BJaU__eCZ-R2,"This paper proposes to use 3D conditional GAN models to generate\nfMRI scans.  Using the generated images, paper reports improvement\nin classification accuracy on various tasks. \n\nOne claim of the paper is that a generative model of fMRI\ndata can help to caracterize and understand variability of scans\nacross subjects. \n\nArticle is based on recent works such as Wasserstein GANs and AC-GANs\nby (Odena et al., 2016). \n\nDespite the rich literature of this recent topic the related work\nsection is rather convincing. \n\nModel presented extends IW-GAN by using 3D convolution and also\nby supervising the generator using sample labels. 1\n\nMajor:\n\n- The size of the generated images is up to 26x31x22 which is limited\n(about half the size of the actual resolution of fMRI data).  As a\nconsequence results on decoding learning task using low resolution\nimages can end up worse than with the actual data (as pointed out). \nWhat it means is that the actual impact of the work is probably limited. \n\n- Generating high resolution images with GANs even on faces for which\nthere is almost infinite data is still a challenge.  Here a few thousand\ndata points are used.  So it raises too concerns: First is it enough? \nUsing so-called learning curves is a good way to answer this.  Second\nis what are the contributions to the state-of-the-art of the 2\nmethods introduced?  Said differently, as there\nis no classification results using images produced by an another\nGAN architecture it is hard to say that the extra complexity\nproposed here (which is a bit contribution of the work) is actually\nnecessary. \n\nMinor:\n\n- Fonts in figure 4 are too small.\n",1,1,1,-1,1,1,1,1,-1,-1
BJaU__eCZ-R3,"The work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding. \nThe contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models). \nConcerning the first objective the empirical results do not provide meaningful support that the generative model is really effective.  The improvement is really tiny and a statistical test (not included in the analysis) probably wouldn't pass a significant threshold.   This analysis is missing a straw man.  It is not clear whether the difference in the evaluation measures is related to the greater number of examples or by the specific generative model. \nConcerning the contribution of the model, one novelty is the conditional formulation of the discriminator.  The design of the empirical evaluation doesn't address the analysis of the impact of this new formulation.  It is not clear whether the supposed improvement is related to the conditional formulation.  \nFigure 3 and Figure 5 illustrate the brain maps generated for Collection 1952 with ICW-GAN and for collection 503 with ACD-GAN.  It is not clear how the authors operated the choices of these figures.  From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods.  The pairwise brain maps would support the interpretation of the generated data.  It is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding (classification) relies on artifacts or confounds. \n\nMinor comments\n- typos: \""a first application or this\"" => \""a first application of this\"" (p.2)\n- \""qualitative quality\"" (p.2)",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJB7fkWR--R1,"In this paper, the authors propose a new approach for learning underlying structure of visually distinct games. \n\nThe proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games.  \n\nThe network architecture is suitably described and seems reasonable to learn simultaneously similar games, which are visually distinct.  However, the authors do not explain how this architecture can be used to do the domain adaptation.  \nIndeed, if some games have been learnt by the proposed algorithm, the authors do not precise what modules have to be retrained to learn a new game.  This is a critical issue, because the experiments show that there is no gain in terms of performance to learn a shared embedding manifold (see DA-DRL versus baseline in figure 5). \nIf there is a gain to learn a shared embedding manifold, which is plausible, this gain should be evaluated between a baseline, that learns separately the games, and an algorithm, that learns incrementally the games.  \nMoreover, in the experimental setting, the games are not similar but simply the same. \n\nMy opinion is that this paper is not ready for publication.  The interesting issues are referred to future works.\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJB7fkWR--R2,"This paper introduces a method to learn a policy on visually different but otherwise identical games.  While the idea would be interesting in general,  unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario.  Any sort of remotely convincing experiment is left to 'future work'. \n\nThe experimental setup is 4x4 grid world with different basic shape or grey level rendering.  I am quite convinced that any somewhat correctly setup vanilla deep RL algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box. \n\nFigure 5: Looks to me like the baseline is actually doing much better than the proposed methods? \n\nFigure 6: Looking at those 2D PCAs, I am not sure any of those method really abstracts the rendering away.  Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJB7fkWR--R3,"- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics.  The key idea is that the agent learns a shared representations for tasks with different visual statistics \n\n- A lot of important references  touching on very similar ideas are missing.  For e.g. \""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks\"", \""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\"", \""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\"".  \n\n- This paper has a lot of orthogonal details.  For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context.  \n\n- Only single runs for the results are shown in plots.  How statistically valid are the results? \n\n- In the last section authors mention the intent to do future work on atari and other env.  Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication",1,1,1,-1,1,-1,-1,-1,-1,-1
BJcAWaeCW-R1,"The authors try to combine the power of GANs with hierarchical community structure detections.  While the idea is sound,  many design choices of the system is questionable.  The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers.  I do not recommend the acceptance of this draft. \n\nCompared with GAN, traditional graph analytics is model-specific and non-adaptive to training data.  This is also the case for hierarchical community structures.  By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic.  In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm.  \n\nThe evaluation remains superficial with minimal quantitative comparisons.  Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic.   They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs.   \n\nThe writing of the draft leaves much to be desired.  The description of the architecture is confusing with design choices never clearly explained.  Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification.  Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJcAWaeCW-R2,"Quality: The work has too many gaps for the reader to fill in.  The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.  I am not sure how this is achieved in this work.  The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.  Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.  How can such a limited technique be generalizing?  There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\ n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.  It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,  although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.  From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function.  If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?  This is a key point not discussed in the manuscript.  \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data.  And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output? \n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster.  There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work.  There is no contribution in the GAN / neural network aspect.  It is also unclear whether the model generalizes.  I don\u2019t think this is a good fit for ICLR. \n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases.  The work itself, however, falls short of the goal.  At best the generator seems to be working but I fear it is overfitting.  The contribution for ICLR is rather minimal, unfortunately",1,1,1,1,1,1,1,1,-1,-1
BJcAWaeCW-R3,"The proposed approach, GTI, has many free parameters: number of layers L, number of communities in each layer, number of non-overlapping subgraphs M, number of nodes in each subgraph k, etc.   No analysis is reported on how these affect the performance of GTI. \n\nGTI uses the Louvain hierarchical community detection method to identify the hierarchy in the graph and METIS to partition the communities.   How important are these two methods to the success of GTI? \n\nWhy is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution (as stated in Section 2.1)? \n\nWhy is the stride for the convolutional/deconvoluational layers set to 2 (as stated in Section 2.1)? \n\nEquation 1 has a symbol E in it.   E is defined (in Section 2.2) to be \""all the inter-subgraph (community) edges identified by the Louvain method for each hierarchy. \""  However, E can be intra-community because communities are partitioned by METIS.   More discussion is needed about the role of edges in E.  \n\nEquation 3 sparsifies (i.e. prunes the edges) of a graph -- namely $re_{G}$.   However, it is not clear how one selects a $re^{i}{G}$ from among the various i values.   The symbol i is an index into $CV_{i}$, the cut-value of the i-th largest unique weight-value. \n\nWas the edge-importance reported in Section 2.3 checked against various measures of edge importance such as edge betweenness? \n\nTable 1 needs more discussion in terms of retained edge percentage for ordered stages.   Should one expect a certain trend in these sequences? \n\nAlmost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes. \n\nThe discussion on graph sampling does not include how much of the graph was sampled.   Thus, the comparisons in Tables 2 and 3 are not fair.\n\nThe most realistic graph generator is the BTER model.  See http://www.sandia.gov/~tgkolda/bter_supplement/ and http://www.sandia.gov/~tgkolda/feastpack/doc_bter_match.html.\n\nA minor point: The acronym GTI is never defined.",1,1,1,-1,1,1,1,1,-1,-1
BJDEbngCZ-R1,"The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective.  I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved. \n2) The authors state in the abstract (and elsewhere): \""... showing that (model free) policy gradient methods globally converge to the optimal solution ...\"". This is misleading and NOT true.  The authors show the convergence of the objective but not of the iterates sequence.  This should be rephrased elsewhere. \n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJDEbngCZ-R2,"I find this paper not suitable for ICLR.  All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.",1,1,-1,-1,-1,-1,-1,-1,-1,-1
BJDEbngCZ-R3,"The paper studies the global convergence for policy gradient methods for linear control problems.  \n(1) The topic of this paper seems to have minimal connection with ICRL.  It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully.  \n\n(2) I am not convinced if the main results are novel.  The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming.  The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration.  \n\n(3) The main results of this paper seem technical sound.  However, the results seem a bit limited because it does not apply to neural-network function approximator.  It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted.  I might have missed something here.  I strongly suggest that these results be submitted to a more suitable venue.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJDH5M-AW-R1,"Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network. \nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images.  Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%.  They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction. \n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting.  However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date.  \nSee for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.  \n- The presentation of the results is not very clear.   See specific comments below.\n- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.  \n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations.   Bold everywhere or nowhere \nResults: The tables are difficult to read and should be clarified: \nWhat does the l2 metric stands for ?  \nHow about min, max ? \nAccuracy -> classification accuracy \nModels -> 3D models\nDescribe each metric (Adversarial, Miss-classified, Correct)\n",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJDH5M-AW-R2,"The authors present a method to enable robust generation of adversarial visual\ninputs for image classification. \n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation. \n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution. \n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains. \n\nThe paper is clear to follow and the objective employed appears to be sound.  I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples. \n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each \n   experiment?  Is it a PCFG?  Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?)  And on a related note, how were the number of sampled \n   transformations chosen? \n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input. \n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective.  I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required. \n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\ n   what effect the monte-carlo approximation of the objective has on things. \n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment. \n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from.  There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one. \n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models?  Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason?  Similar questions for the 5\n   models in the 3D rendering experiment. \n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \""over a variety of viewpoints\"".  Were these\n   viewpoints chosen randomly? \n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves.  For now, I think the\nsubmission is good for a weak accept  \u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept.",1,1,1,-1,1,1,1,1,1,-1
BJDH5M-AW-R3,"The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations.  The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results. \n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive.  This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting. \n\nHowever, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al.  This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle). \n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al).  If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary.  This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world. \n\nMinor comments:\nSection 1.1: \""a affine\"" -> \""an affine\""\nTypo in section 3.4: \""of a of a \""\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse. \n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017.",1,1,1,-1,1,1,1,1,-1,-1
BJehNfW0--R1,"The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution.  Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support.  The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally.  The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > \\sqrt{N}.  The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2.  One should note that the birthday theorem assumes uniform sampling.   In the revised versions, it has been clarified that the tested distribution is not assumed to be uniform but the distribution has \""effectively\"" small support size using an indistinguishability notion.  Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity.  Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity.  Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above.  A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN. \n\nEvaluation: \nSignificance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value.  \n\nClarity: The paper is written well and the issues raised are well motivated and proper background is given.  \n\nOriginality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new.  \n\nQuality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs. \n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJehNfW0--R2,"This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples.  The main goal is to quantify mode collapse in state-of-the-art generative models.  The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse. \nUsing the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1). \nThe results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models. \nBidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs. \nFinally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator. \n\nThis is a very interesting area and exciting work.  The main idea behind the proposed test is very insightful.  The main theoretical contribution stimulates and motivates much needed further research in the area.  In my opinion both contributions suffer from some significant limitations.  However, given how little we know about the behavior of modern generative models, it is a good step in the right direction. \n\n\n1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity.  The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2.  My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity.  Anecdotally, real-life distributions are far from uniform, so this should be a common issue.  I would still use the test as a part of a suite of measurements, but I would not solely rely on it.  I feel that the authors should give a more prominent disclaimer to potential users of the test. \n\n2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.  The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution.  As long as they yield collisions at the same rate, these two generative models are \u2018equally diverse\u2019. Isn\u2019t coverage of equal importance? \n\n3. The other main contribution of the paper is Theorem 3, which shows\u2014via a very particular construction on the generator and encoder\u2014that bidirectional GANs can also suffer from serious mode collapse.  I welcome and are grateful for any theory in the area.  This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice.  In light of this, the statement in the introduction that \u201cencoder-decoder training objectives cannot avoid mode collapse\u201d might need to be qualified.  In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.",1,1,1,1,1,1,1,1,1,-1
BJehNfW0--R3,"The article \""Do GANs Learn the Distribution?  Some Theory and Empirics\"" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images.  The authors argue that GANs in fact generate the distributions with fairly low support. \n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.  This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017).  The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective. \n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets.  We definitely see many very similar images in fairly small sample generated.  So, the general claim is supported.  However, if you look closely at some pictures, you can see that they are very different though reported as similar.  For example, some deer or truck pictures.  That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates. \n\nTo sum up, I think that the general idea looks very natural and the results are supportive.  On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJgd7m0xRZ-R1,"The authors propose a defense against attacks on the security of one-class SVM based anonaly detectors.  The core idea is to perform a random projection of the data (which is supposed to decrease the impact from adversarial distortions).  The approach is empirically tested on the following data: MNIST, CIFAR, and SVHN. \n\nThe paper is moderately well written and structured.  Command of related work is ok,  but some relevant refs are missing (e.g., Kloft and Laskov, JMLR 2012).   The empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distortions.   This is encouraging.  Right now, there is no theoretical justification for the approach, nor even a (in my opinion) convincing movitation/Intuition behind the approach.  Also, the attack model should formally introduced. \n\nIn summary, I d like to encourage the authors to further investigate into their approach, but I am not convinced by the manuscript in the current form.  It lacks both in sound theoretical justification and intuitive motivation of the approach.  The experiments, however, show clearly advantages of the approach  (again, here further experiments are necessary, e.g., varying the dose of adversarial points).",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJgd7m0xRZ-R2,"In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data.   While the intuition is nice and interesting,  the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robustness. \n\nDetails:\nhave been successfully in anomaly detection --> have been successfully used in anomaly detectionP \n\n\""The adversary would select a random subset of anomalies, push them towards the normal data cloud and inject these perturbed points into the training set\"" -- This seems backwards.   As in the example that follows, if the adversary wants to make anomalies seem normal at test time, it should move normal points outward from the normal point cloud (eg making a 9 look like a weird 7). \n\nAs s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2.    From (a) to (b) the red points move closer to the center while in (c) they move further away (why?).    The blue points seem to consistently become more dense from (a) to (c).  \n\nThe attack model is too rough.    It seems that without bounding D, we can make the model arbitrarily bad, no?    Assumption 1 alludes to this but doesn't specify what is \""small\""?    Also the attack model is described without considering if the adversary knows the learner's algorithm.    Even if there is randomness, can the adversary take actions that account for that randomness?  \n\nDoes selecting a projection based on compactness remove the randomness?  \n\nExperiments -- why/how would you have distorted test data?    Making an anomaly seem normal by distorting it is easy.  \n\nI don't see experiments comparing having random projections and not.    This seems to be the fundamental question -- do random projects help in the train_D | test_C case?  \n\nExperiments don't vary the attack much to understand how robust the method is.",1,1,1,-1,1,-1,-1,-1,-1,-1
BJgd7m0xRZ-R3,"Although the problem addressed in the paper seems interesting,  but there lacks of evidence to support some of the arguments that the authors make.  And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference.  Detailed critiques are as following:1. The idea proposed by the authors seems too quite simple.  It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM. \n2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5.  It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change. \n3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that.  And PCA as a non-random projection would a nice baseline to compare against. \n4. The paper seems to misuse the term \u201cFalse positive rate\u201d as the y label of figure 3(d/e/f).  The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive.  So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a).  From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified.  The paper should come up with a better term for that evaluation. \n5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d).  Need more experiment on more dimensionality to prove that. \n6. In the appendix B results part, sometimes the word \u2019S_attack\u2019 is typed wrong. And the values in  \u201cdistorted/distorted\u201d columns in Table 5 do not match up with the ones in Figure 3(c).",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJgPCveAW-R1,"This paper examines sparse connection patterns in upper layers of convolutional image classification networks.   Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks. \n\nWhile it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions.   I've described many of the points I was confused by in more detailed comments below. \n\n\nDetailed comments and questions:\n\n\nThe distribution of connections in \""windows\"" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image.   But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure.   So are do the \""windows\"" correspond to spatial windows, and if so, how?   Or are they different (maybe arbitrary) groupings over the feature maps? \n\nAlso a bit confusing is the notation \""conv2\"", \""conv3\"", etc.   These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example).   But here it seems just to indicate the number of \""CL\"" layers: 2.  And p.1 says that the \""CL\"" layers are those often referred to as \""FC\"" layers, not \""conv\"" (though they may be convolutionally applied with spatial 1x1 kernels). \n\nThe heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets.   For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop.   So some regions are more important than others, and the top half may be more important than an equally spaced global view.   So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the \""scatter\"" metric described. \n\nAnother broader question I have is in the distinction between lower and upper layers (those referred to as \""feature extracting\"" and \""classification\"" in this paper).   It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret).   So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the \""classification\"" function is pushed down to lower layers, as the upper layers are reduced in size.   How would they respond to similar reductions? \n\nI'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?   I'm also unsure whether the windows are over spatial extent only, or over features.",1,1,1,-1,1,1,1,1,-1,-1
BJgPCveAW-R2,"The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.  Numerical experiments show that such sparse networks can have similar performance to fully connected ones.  They introduce a concept of \u201cscatter\u201d that correlates with network performance.  Although  I found the results useful and potentially promising,  I did not find much insight in this paper. \nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.  Once the signals from different windows are intermixed, how do you even define the windows?   \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n",1,1,-1,-1,-1,-1,-1,-1,-1,-1
BJgPCveAW-R3,"The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters, \n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices. \nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers. \n2) The authors reject the technique of 'Deep compression' as being impractical.  I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network. \n\np3. What does 'normalized' mean?  Batch-norm? \np3. Are you using an L2 weight penalty?  If not, your fully-connected baseline may be unnecessarily overfitting the training data. \np3. Table 1. Where do the choice of CL Junction densities come from?  Did you do a grid search to find the optimal level of sparsity at each level? \np7-8. I had trouble following the left/right & front/back notation. \np8. Figure 7. How did you decide which data points to include in the plots?",1,1,1,-1,1,1,1,1,-1,-1
BJhxcGZCW-R1,"SUMMARY.\n\nThe paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting. \nThe model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities. \nThe encoder maps the input to a probabilistic latent space. \nFinally, a generator is used to generate entity pairs give a relation. \n\n----------\n\nOVERALL JUDGMENT\nThe paper presents a clever use of VAEs for generating entity pairs conditioning on relations. \nMy main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set. \nIf this is the case, all the analysis and results obtained are almost meaningless. \nI suggest the authors make clear if they used the split training, validation, test. \nUntil then it is not possible to draw any conclusion from this work. \n\nAssuming the experimental setting is correct, it is not clear to me the reason of having the representation of r (one-hot-vector of the relation) also in the decoding/generation part. \nThe hidden representation obtained by the encoder should already capture information about the relation. \nIs there a specific reason for doing so?\n\n",1,-1,1,-1,-1,-1,-1,-1,-1,-1
BJhxcGZCW-R2,"The authors suggest using a variational autoencoder to infer binary relationships between medical entities.  The model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed before.   \nWhile the paper is very well-written  I have certain concerns regarding the motivation, model, and evaluation methodology followed: \n\n1) A stronger motivation for this model is required.  Having a generative model for causal relationships between symptoms and diseases is \""intriguing\"" yet I am really struggling with the motivation of getting such a model from word co-occurences in a medical corpus.  I can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model.  \n\n2) The word embeddings used seem to be sufficient to capture the \""knowledge\"" included in the corpus.  An ablation study of the impact of word embeddings on this model is required.  \n\n3) The authors do not describe how the data from xywy.com were annotated.  Were they annotated by experts in the medical domain or random users? \n\n4) The metric of quality is particularly ad-hoc",1,1,1,-1,1,1,1,1,-1,-1
BJhxcGZCW-R3,"In the medical context, this paper describes the classic problem of \""knowledge base completion\"" from structured data only (no text).   The authors argue for the advantages of a generative VAE approach (but without being convincing).   They do not cite the extensive literature on KB completion.   They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods. \n\nThe authors seem unaware of a large literature on \""knowledge base completion.\""  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015],  \n\nThe paper claims that operating on pre-structured data only (without using text) is an advantage.   I don't find the argument convincing.   There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. \""universal schema\"" [Riedel et al, 2014]. \n\nThe paper claims that \""discriminative approaches\"" need to iterate over all possible entity pairs to make predictions.   In their generative approach they say they find outputs by \""nearest neighbor search.\""   But the same efficient search is possible in many of the classic \""discriminatively-trained\"" KB completion models also. \n\nIt is admirable that the authors use an interesting (and to my knowledge novel) data set.   But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995.   The method is evaluated only against their own VAE-based alternatives.   It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.\n",1,1,1,1,1,1,1,1,-1,-1
BJIgi_eCZ-R1,"The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \""fully-aware\"" of all levels of abstraction, e.g. word-level, phrase-level, etc.  In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD.  They also propose an attention mechanism that works better than others (Symmetric + ReLU). \n\nStrengths:\n- The paper is well-written and clear. \n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field. \n- The multi-level attention is novel and indeed seems to work, with convincing ablations. \n- Nice engineering achievement, reaching the top of the leaderboard (in early October). \n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances.  Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA). \n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard.  Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution. \n\n\nMinor:\n- Probably figure 4 can be drawn better.  Not easy to understand nor concrete. \n- Section 3.2 GRU citation should be Cho et al. [2]. \n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.  Could you perform ablation without contextualized embedding (CoVe)? \n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.  EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014.",1,1,1,-1,1,1,1,1,-1,-1
BJIgi_eCZ-R2,"The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive.  That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level . The network proposed here, FusionHet, fixes problem.  Importantly, the model achieves state-of-the-art performance of the SQuAD dataset. \n\nThe paper is very well-written and easy to follow.  I found the architecture very intuitively laid out, even though this is not my area of expertise.  Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work!  What most impressed me, however, was the literature review.  Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work.  Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2.  \n\nAll in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJIgi_eCZ-R3,"(Score before author revision: 4)\n(Score after author revision: 7)\n\nI think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly.  I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference). \n\nTherefore I would now support accepting this paper. \n\n------------(Original review below) -----------------------\n\nThe authors present an enhancement to the attention mechanism called \""multi-level fusion\"" that they then incorporate into a reading comprehension system.  It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores. \n\ni.e. the authors form a vector \""HoW\"" (called history of the word), that is defined as a concatenation of several vectors:\n\nHoW_i = [g_i, c_i, h_i^l, h_i^h]\n\nwhere g_i = glove embeddings, c_i = COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word. \n\nThe attention score is then a function of these concatenated vectors i.e. \\alpha_{ij} = \\exp(S(HoW_i^C, HoW_j^Q)) \n\nResults on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match).  The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers. \n\nThe authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best.  \n\nComments:\n\n-I feel overall the contribution is not very novel.   The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset (e.g. some combination of attention between question/context and LSTMs over question/context).  The only novelty is these \""HoW\"" inputs to the extra attention mechanism that takes a richer word representation into account. \n\n-I feel the model is seems overly complicated for the small gain (i.e. 75.7->76.0 Exact Match), especially on a relatively exhausted dataset (SQuAD) that is known to have lots of pecularities (see anonymous comment below).  It is possible the gains just come from having more parameters. \n\n-The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. \\alpha_{ij}^l, \\alpha_{ij}^h, \\alpha_{ij}^u) it will learn to attend to \""different regions for different level\"".  However, there is nothing enforcing this and the gains just probably come from having more parameters/complexity.",1,1,1,-1,1,1,1,1,-1,-1
BJInEZsTb-R1,"This paper introduces a generative approach for 3D point clouds.  More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper).  In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations.  \n\nThe results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6).  \n\nOne question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches.  Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results,  whereas the details seem to be washed out for the point cloud results presented in this paper.  \n\nI would like to see comparison experiments with voxel based approaches in the next update for the paper.  \n\n[1]\n@article{tatarchenko2017octree,\n  title={Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs},\n  author={Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},\n  journal={arXiv preprint arXiv:1703.09438},\n  year={2017}\n}\n\nIn light of the authors' octree updates score is updated.  I expect these updates to be reflected in the final version of the paper itself as well.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJInEZsTb-R2,"3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc. \n\nCurrent approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval. \n\nThe lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds. \n\nConsidered paper is one of the first approaches to learn GAN-type generative models. \nUsing PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model. \n\nThe paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures.  \n\nThus I think that the paper should be published.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJInEZsTb-R3,"Summary:\n\nThis paper proposes generative models for point clouds.  First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.).  Then, they train generative models over the auto-encoder's latent space, both using a \""latent-space GAN\"" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model.  To generate point clouds, they sample a latent code and pass it to the decoder.  They also introduce a \""raw point cloud GAN\"" (r-GAN) that, instead of generating a latent code, directly produces a point cloud.\ n\nThey evaluate the methods on several metrics.  First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset.  Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance.  Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled. \n\nPros:\n\n- It is interesting that the latent space models are most successful, including the relatively simple GMM-based model.  Is there a reason that these models have not been as successful in other domains?\ n\n- The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs.  Due to the simplicity of the method, this paper could be a useful baseline for future work. \n\n- The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper. \n\nCons:\n\n- How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ? \n\n- The paper simultaneously proposes methods for generating point clouds, and for evaluating them.  The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs.  Although the performance on automated metrics is encouraging,  it is hard to conclude much about under what circumstances one representation or model is better than another. \n\n- The technical approach is not particularly novel.  The auto-encoder performs fairly well,  but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance.  The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM). \n\n- While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem.\ n\n- The paper could possibly be clearer by integrating more of the \""background\"" section into later sections.  Some of the GAN figures could also benefit from having captions. \n\nOverall, I think that this paper could serve as a useful baseline for generating point clouds,  but I am not sure that the contribution is significant enough for acceptance.\n",1,1,1,-1,1,1,-1,1,-1,-1
BJjBnN9a--R1,"The paper introduces the notion of continuous convolutional neural networks.  \nThe main idea of the paper is to project examples into an RK Hilbert space\nand performs convolution and filtering into that space.  Interestingly, the\nfilters defined in the Hilbert space  have parameters that are learnable. \n\nWhile the idea may be novel and interesting,  \nMost data that are available for learning are in discrete forms and hopefully,\nthey have been digitalized according to Shannon theory.  This means that they bring\nall necessary information for rebuilding their continuous counterpart.  Hence, it is\nnot clear why projecting them back into continuous functions is of interest.  \n\nAnother point that is not clear or at least misleading is the so-called Hilbert Maps. \nAs far as I understand, Equation (4) is not an embedding into an Hilbert space but\nis more a proximity space representation [1].  Hence, the learning framework of the\nauthors can be casted more as a learning with similarity function than learning\ninto a RKHS [2].  A proper embedding would have mapped $x$ into a function\nbelonging to $\\mH$.  In addition, it seems that all computations are done\ninto a \\ell^2 space instead of in the RKHS (equations 5 and 11).  \nLearning good similarity functions is also not novel [3] and Equations\n(6) and (7) corresponds to learning these similarity functions. \nAs far as I remember, there exists also some paper from the nineties that\nlearn the parameters of RBF networks but unfortunately I have not been able to\ngoogle some of them. \n\n\nPart 3 is the most interesting part of the paper,  however it would have been\ngreat if the authors provide other kernel functions with closed-form convolution \nformula that may be relevant for learning. \nThe proposed methodology is evaluated on some standard benchmarks in vision.  While\nresults are pretty good,  it is not clear how the various cluster sets have been obtained\nand what are their influence on the performances (if they are randomly initialized, it \nwould be great to see standard deviation of performances with respect to initializations). \nI would also be great to have intuitions on why a single continuous filter works betters\nthan 20 discrete ones (if this behaviour is consistent accross initialization). \n\nOn the overall, while the idea may be of interested,  the paper lacks in motivations\nin connecting to relevant previous works and in providing insights on why it works. \nHowever, performance results seem to be competitive and that's the reader may\nbe eager for insights. \n\n\nminor comments\n---------------\n\n* the paper employs vocabulary that is not common in ML.  eg. I am not sure what\noccupancy values, or inducing points are.  \n\n* Supposingly that the authors properly consider computation in RKHS, then \\Sigma_i\nshould be definite positive right?  how update in (7) is guaranteed to be DP?  \nThis constraints may not be necessary if instead they used proximity space representation. \n\n\n\n\n\n[1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf\n[2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf\n[3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification.",1,1,1,1,-1,-1,-1,-1,-1,-1
BJjBnN9a--R2,"This paper aims to provide a continuous variant of CNN.   The main idea is to apply CNN on Hilbert maps of the data.  The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix.  A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets. \n\nThe paper is well written and provides some new insights on incorporating kernels in CNN. \n\nThe kernel matrix in Eq. 5 is not symmetric and the kernel function in Eq. 3 is not defined over a pair of inputs.  In this case, the projections of the data via the kernel are not necessarily in a RKHS.  The connection between Hilbert maps and RKHS in that sense is not clear in the paper. \n\nThe size of a kernel matrix depends on the sample size.  In large scale situations, working with the kernel matrix can be computational expensive.  It is not clear how this issue is addressed in this paper. \n\nIn section 2.2, how \\mu_i and \\sigma_i are computed? \n\nHow the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJjBnN9a--R3,"This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases.  A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs. \n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.  Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters.  Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN. \n\nWhile the idea is interesting and might be a good alternative to standard CNNs,  the paper falls short in terms of providing experimental validation that would demonstrate the latter point.  It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.  They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results.  The CIFAR-10, STL-10, and SVHN results are disappointing.  CCNNs do not outperform the prior CNN results listed in Table 2,3,4.  Moreover, these tables do not even cite more recent higher-performing CNNs.  See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper. \n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?   To be competitive, deeper CCNNs would likely need to be trained.   I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.   Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?   Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.  \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table.   This would assist in understanding tradeoffs in the design space.  \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?   If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?  \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN?   Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training.   Such dependence for CCNNs appears to be a weakness in comparison.",1,1,1,-1,1,1,1,1,1,-1
BJJLHbb0--R1,"1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection\n2.  It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods\n3.  Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process?  can it be a trainable parameter? \n4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJJLHbb0--R2,"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.  The paper is rigorous and ideas are clearly stated.  The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.  My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.  I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n\n1. The framework uses the class information, i.e., \u201conly data samples from the normal class are used for training\u201d, but it is still considered unsupervised.  Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.  I would like to see a plot of the sample energy as a function of the number of data points.  Is there an elbow that indicates the threshold cut?  Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data). \n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?  \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results? \n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model).  Those approaches should at least be discussed in the related work, if not compared against. \n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.  Could you provide a comparison with EM? \n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant?  Does it well describe the new space?  Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?  Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians. \n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies  ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets. \n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate  ...\u201d - it is not clear to me, it does look better than the other ones, but not clear.   If there is a clear separation from a different view, show that one instead.   We don't need the same view for all methods.   \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic! \n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.  \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean? \n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE.  Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.  They are the best in terms of precision. \n4. Is the error in Table 2 averaged over multiple runs? If yes, how many? \n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented.  It can be further improved as mentioned in the comments. \n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read.  \n\nOriginality \u2013 Fairly original, but it still needs some work to justify it better.  \n\nSignificance \u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n",1,1,1,-1,1,1,1,1,1,-1
BJJLHbb0--R3,"Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space.  The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples. \n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.  The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training. \n\nThe paper demonstrates improvements in a number of public datasets.  Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field.  Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection. \n\nCriticisms\n\nBased on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method.  Little to no detail about these features is included.  Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled.  Why did the choices that were made in the paper yield this success?  How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results? \n\nQuality\n\nThis paper does not set out to produce a novel network architecture.  Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.  This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJjquybCW-R1,"This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks.  I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \""wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples. \n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data. \n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum.  These solutions achieve zero squared-loss. \n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1).  Intuitively, (1) is an easy result.  Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent.  The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small. \n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima;  (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure;  (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero.  If an optimization algorithm falls onto these solutions, it will be hard to escape. \n\nOverall, the paper presents several incremental improvement over existing theories.  However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJjquybCW-R2,"This paper analyzes the expressiveness and loss surface of deep CNN.  I think the paper is clearly written, and has some interesting insights.",1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJjquybCW-R3,"This paper analyzes the loss function and properties of CNNs with one \""wide\"" layer, i.e., a layer with number of neurons greater than the train sample size.  Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums.  I like the presentation and writing of this paper.  However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \""wide\""-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.  The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.  This is not surprising.  It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error.",-1,-1,-1,-1,-1,-1,-1,-1,-1,-1
BJk7Gf-CZ-R1,"The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks.  The paper is an extension of Kawaguchi'16.  It also provides some sufficient conditions for the non-linear cases.  \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16.  I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it.  As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique.  Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum.  This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJk7Gf-CZ-R2,"Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks.  In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion.  This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are .\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach. \n\nQuality:\nThe quality is very good.  The paper is technically correct and nontrivial.  All proofs are provided and easy to follow. \n\nClarity:\nThe paper is very clear.  Related work is clearly cited, and the novelty of the paper well explained.  The technical proofs of the paper are in appendices, making the main text very smooth. \n\nOriginality:\nThe originality is weak.  It extends a series of recent papers correctly cited.  There is some originality in the proof which differs from recent related papers. \n\nSignificance:\nThe result is not completely surprising,  but it is significant given the lack of theory and understanding of deep learning.  Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJk7Gf-CZ-R3,"\n-I think title is misleading, as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to \u2026 deep LINEAR networks \n\n- Theorems 2.1, 2.2 and the observation (2) are nice! \n \n- Theorem 2.2 there is no discussion about the nature of the saddle point is it strict?  Does this theorem imply that the global optima can be reached from a random initialization?  Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary. \n\n- I\u2019m a bit puzzled by Theorems 4.1 and 4.2 and why they are useful.  Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result?  Further discussion would be helpful.\n",1,1,1,-1,1,-1,1,1,1,-1
BJ_wN01C--R1,"In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs.  Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning.  Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space),  but the initial results are quite promising.\n\n It would also be interesting to understand more fully how performance scales to larger networks.  If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales.  Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?\n\n As a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c.",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ_wN01C--R2,"The authors provide a novel, interesting, and simple algorithm capable of training with limited memory.   The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well.   However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs.   Second, the relationship to existing work needs to be explained better.\n\n Pro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\n Con:\nThe relationship to stochastic gradient markov chain monte carlo needs to be explained better.   In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly. \nThe evidence is presented on very small input data.   With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger.   Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases?  This would require a smaller ratio of included parameters. \n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics.  In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688). \n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May.  Bridging the gap between stochastic gradient MCMC and stochastic optimization.  In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n  [3] Patterson, S. and Teh, Y.W., 2013.  Stochastic gradient Riemannian Langevin dynamics on the probability simplex.  In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n""",1,1,1,-1,-1,-1,-1,-1,-1,-1
BJ_wN01C--R3,"This paper presents an iterative approach to sparsify a network already during training.  During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold.  This is a big advantage when training is performed on hardware with computational limitations, in comparison to \""post-hoc\"" sparsification methods, that compress the network after training. \nThe method is derived by considering the \""rewiring\"" of an (artificial) neural network as a stochastic process.  This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach.  References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\n Especially the stochastic gradient method in [2] is strongly related to the existing approach. \n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.  The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-  By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n - The method is specifically designed for online learning with limited hardware ressources.\n\n Negative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C).  Especially the results on MNIST suggest that this method is most advantageous for very high compression levels.  However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n - A detailled discussion of the relation to previously existing very similar work is missing (see above) \n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure.  Readibility could be slightly increased by putting the figures on the respective pages.\n",1,1,1,1,1,1,1,1,-1,-1
