Index,Text,Clarity of Review,Justification of Scores,Depth of Analysis,Fairness and Objectivity,Constructiveness of Feedback,Engagement with Related Work,Accuracy in Understanding,Consistency of Evaluation,Identification of Novelty,Ethical Considerations and Responsibility
B11bwYgfM-R1,"The idea of using cross-task transfer performance to do task clustering is not new.  Please refer to the paper \u201cDiscovering structure in multiple learning tasks: The TC algorithm\u201d published in ICML 1996.  One issue of the use of cross-task transfer performance to measure task relations is that it ignores the negative correlations between tasks, which is useful for learning from multiple tasks.  For example, in binary classification tasks, a very small S_{ij} indicates that by changing the sign of the classification function these two tasks are useful to each other.  So the use of cross-task transfer performance and the task clustering approach can only capture positive correlations between tasks but ignore the negative task relations which are also important to the sharing among tasks in multi-task learning. \n\nProblem (2) is identical to robust PCA and Theorem 3.1 is common in matrix completion literature.  I don\u2019t see much novelty.  Appendix A seems obvious but it cannot prove the validity of the assumption made in problem (2).  Based on previous works such as \u201cMulti-task Sparse Structure Learning with Gaussian Copula Models\u201d and \u201cLearning Sparse Task Relations in Multi-Task Learning\u201d, when the number of tasks is large, the task relation exhibits the sparse structure.  I don\u2019t know whether the low-rank structure does exist in the cross-task transfer performance or not. \n\nThe two parts in this paper are not new.  The combination of the two parts seems a bit incremental and does not bring much novelty.",1,1,1,1,-1,1,1,1,1,-1
B11bwYgfM-R2,"This paper proposes a method for multitask and few-shot learning by completing a performance matrix (which measures how well the classifier for task i performs on task j). \n\nThe matrix completion approach is based on robust PCA.  When used for multitask learning (MTL) with N tasks, the method has to first train one classifier for each task (and so train a total of N classifiers), and then evaluate the performance of each classifier on each and every task (and so involves N^2 testing rounds).  This can be computationally demanding. \n\nThe key assumption in the paper is that task classifier i that performs well on task j means tasks i and j belong to the same cluster, and if task classifier i does not perform well on task j, then tasks i and j belong to different cluster. The proposed algorithm then uses these performance values to perform task clustering.  However, in MTL, we usually assume that there are not enough samples to learn each task, and so this performance matrix may not be reliable. \n\nThere have been a number of MTL methods based on task clustering.  For example,\n[1] A convex formulation for learning task relationships in multi-task learning (UAI) \n[2] A dirty model for multi-task learning (NIPS) \n[3] Clustered multi-task learning: A convex formulation (NIPS) \n[4] Convex multitask learning with flexible task clusters (ICML) \n[5] Integrating low-rank and group-sparse structures for robust multi-task learning (KDD)\n[6] Learning incoherent sparse and low-rank patterns from multiple tasks (KDD)\nIn particular, [5] assumes that the combined weight matrix (for all the tasks) follows the robust PCA model.  This is thus very similar to the proposed method (which assumes that the performance matrix follows the robust PCA model).  However, a disadvantage of the proposed method is that it is a two-step approach (first perform task clustering, then re-learn the cluster weights), while [5] is not. \n\nFor few-shot learning, the authors mentioned that the \\alpha's are adaptable parameters but did not mention how they are adapted. \n\nExperimental results are not convincing. \n- Comparison with existing clustered MTL methods mentioned above are missing. \n- As mentioned above, the proposed method can be computationally expensive (when used for MTL), but no timing results are reported. \n- As the authors mentioned in section 4.2, most of the tasks have a significant amount of training data (and single-task baselines achieve good results), and so this is not a good benchmark dataset for MTL.",1,1,1,1,1,1,1,1,1,-1
B11bwYgfM-R3,"The authors propose techniques for multitask and few shot learning, where the number of tasks is potentially very large, and the different tasks might have different output spaces.  Prior techniques which can address some of these aspects do not necessarily work with deep learning, which is a key focus of the paper.  The authors suggest computing a similarity matrix amongst the tasks.  Given such a matrix, they propose to do multitask learning by clustering the similarity matrix, and learning a single model for each cluster.  If the tasks in a cluster have different output spaces, then a separate output layer is learned for each task in the cluster following a common encoding module. \nTo deal with the large number of tasks, the authors further propose computing a few randomly sampled entries of the similarity matrix, and then using ideas from robust matrix completion to induce the full matrix.  The resulting algorithm is evaluated on a standard amazon reviews benchmark from multitask learning, as well as two datasets from intent classification in dialog systems. \n\nI think there are some interesting ideas in this paper, and the use of matrix completion techniques to deal with a large number of tasks is nice.  But I believe there are important drawbacks in the framing and basic methodology and evaluation which make the paper unfit for publication in its current form. \n\n1. The prior works which do task clustering and multitask learning typically focus on how one might induce clusters which work well with the multitask learning methods used (see e.g. Kang et al. which is cited, as well as Kshirsagar et al. in ECML 2017 as two examples).  In this paper, on the other hand, the clusters are obtained in a manner which only accounts for pairwise similarities of tasks, using a pairwise similarity metric which is quite different from how the cluster is eventually used.  This seems quite suboptimal. \n2. The pairwise similarity measure appears to be one that might have a high false negative rate.  That is, it might rate many tasks as dissimilar even when they are not.  This is because you train individual model on i and apply it to j.  It is possible that this model does not do well, but there is an equally good model for i which also does well on j.  Such a model would indeed be found if i and j are put in the same cluster, but the method would fail to do so, leading to high fragmentation. \n3. I do not see how you apply the model from task i to task j when the two have different output spaces.  Since this is a major motivation of the paper, I actually do not see how the setup makes sense! \n4. It seems odd to put absolute errors on task j instead of regret to the model trained on j in the similarity matrix. \n5. The inducing of edges in the Y matrix by comparing to a mean and standard deviation is completely baseless.  Without good reasoning from the authors, I see no reason why the entries in the row of a matrix should have a normal-like distribution.  At the very least, I would consider using regret to the model of the task, and compute some quantiles on that which is still suspect in the matrix completion setting. \n6 .In the evaluation, why are just 12 tasks used in the Amazon dataset?  Why don't you present evaluation results on all tasks in the multitask setting? \n7. Why is average accuracy the right thing?  If the error rates are different for different tasks, it is not sensible to measure raw accuracies. \n\nThe authors also seem to miss a potentially relevant baseline in Cross-Stitch Networks (https://arxiv.org/abs/1604.03539) \n\nBesides these major issues, there are also a few minor issues I have with the paper.  I do not see why there's need for a proof for the matrix completion result.  This appears to be a direct application of Chandrasekaran et al, and in fact matrix completion has been used for clustering before (https://arxiv.org/abs/1104.4803).  Given this, the presentation in the paper makes the idea look more novel than it is.  I also think that the authors might benefit from dropping the whole few-shot learning angle here, and instead do a more thorough job of evaluating their multitask learning method.",1,1,1,1,1,1,1,1,1,-1
B12Js_yRb-R1,"\nSummary: \n- This paper proposes a hand-designed network architecture on a graph of object proposals to perform soft non-maximum suppression to get object count. \n\nContribution:\n- This paper proposes a new object counting module which operates on a graph of object proposals. \n\nClarity:\n- The paper is well written and clarity is good.  Figure 2 & 3 helps the readers understand the core algorithm. \n\nPros:\n- De-duplication modules of inter and intra object edges are interesting. \n- The proposed method improves the baseline by 5% on counting questions. \n\nCons:\n- The proposed model is pretty hand-crafted.  I would recommend the authors to use something more general, like graph convolutional neural networks (Kipf & Welling, 2017) or graph gated neural networks (Li et al., 2016). \n- One major bottleneck of the model is that the proposals are not jointly finetuned.  So if the proposals are missing a single object, this cannot really be counted.  In short, if the proposals don\u2019t have 100% recall, then the model is then trained with a biased loss function which asks it to count all the objects even if some are already missing from the proposals.  The paper didn\u2019t study what is the recall of the proposals and how sensitive the threshold is. \n- The paper doesn\u2019t study a simple baseline that just does NMS on the proposal domain. \n- The paper doesn\u2019t compare experiment numbers with (Chattopadhyay et al., 2017). \n- The proposed algorithm doesn\u2019t handle symmetry breaking when two edges are equally confident (in 4.2.2 it basically scales down both edges).  This is similar to a density map approach and the problem is that the model doesn\u2019t develop a notion of instance. \n- Compared to (Zhou et al., 2017), the proposed model does not improve much on the counting questions. \\n- Since the authors have mentioned in the related work, it would also be more convincing if they show experimental results on CL \\n\nConclusion:\n- I feel that the motivation is good,  but the proposed model is too hand-crafted.  Also, key experiments are missing: 1) NMS baseline  2) Comparison with VQA counting work  (Chattopadhyay et al., 2017).  Therefore I recommend reject. \n\nReferences:\n- Kipf, T.N., Welling, M., Semi-Supervised Classification with Graph Convolutional Networks. ICLR 2017. \n- Li, Y., Tarlow, D., Brockschmidt, M., Zemel, R. Gated Graph Sequence Neural Networks. ICLR 2016. \n\nUpdate:\nThank you for the rebuttal.  The paper is revised and I saw NMS baseline is added.  I understood the reason not to compare with certain related work.  The rebuttal is convincing and I decided to increase my rating, because adding the proposed counting module achieve 5% increase in counting accuracy.  However, I am a little worried that the proposed model may be hard to reproduce due to its complexity and therefore choose to give a 6.",1,1,1,1,1,1,1,1,1,1
B12Js_yRb-R2,"Summary\n - This paper mainly focuses on a counting problem in visual question answering (VQA) using attention mechanism.  The authors propose a differentiable counting component, which explicitly counts the number of objects.  Given attention weights and corresponding proposals, the model deduplicates overlapping proposals by eliminating intra-object edges and inter-object edges using graph representation for proposals.  In experiments, the effectiveness of proposed model is clearly shown in counting questions on both a synthetic toy dataset and the widely used VQA v2 dataset. \n\nStrengths\n - The proposed model begins with reasonable motivation and shows its effectiveness in experiments clearly.  \n - The architecture of the proposed model looks natural and all components seem to have clear contribution to the model. \n - The proposed model can be easily applied to any VQA model using soft attention.  \n - The paper is well written and the contribution is clear. \n\nWeaknesses\n - Although the proposed model is helpful to model counting information in VQA,  it fails to show improvement with respect to a couple of important baselines: prediction from image representation only and from the combination of image representation and attention weights. \n\nComments\n - It is not clear if the value of count \""c\"" is same with the final answer in counting questions. \n\n",1,1,1,1,-1,-1,1,1,1,-1
B12Js_yRb-R3,"This paper tackles the object counting problem in visual question answering.  It is based on the two-stage method that object proposals are generated from the first stage with attention.  It proposes many heuristics to use the object feature and attention weights to find the correct count.   In general, it treats all object proposals as nodes on the graph.  With various agreement measures, it removes or merges edges and count the final nodes.  The method is evaluated on one synthetic toy dataset and one VQA v2 benchmark dataset.  The experimental results on counting are promising.   Although counting is important in VQA, the method is solving a very specific problem which cannot be generalized to other representation learning problems.   Additionally, this method is built on a series of heuristics without sound theoretically justification, and these heuristics cannot be easily adapted to other machine learning applications.  I thus believe the overall contribution is not sufficient for ICLR. \n\nPros:\n1. Well written paper with clear presentation of the method.  \n2. Useful for object counting problem. \n3. Experimental performance is convincing.  \n\nCons:\n1. The application range of the method is very limited.  \n2. The technique is built on a lot of heuristics without theoretical consideration.  \n\nOther comments and questions:\n\n1. The determinantal point processes [1] should be able to help with the correct counting the objects with proper construction of the similarity kernel.   It may also lead to simpler solutions.  For example, it can be used for deduplication using A (eq 1) as the similarity matrix.  \n\n2. Can the author provide analysis on scalability the proposed method?  When the number of objects is very large, the graph could be huge.  What are the memory requirements and computational complexity of the proposed method?   \nIn the end of section 3, it mentioned that \""without normalization,\"" the method will not scale to an arbitrary number of objects.  I think that it will only be a problem for extremely large numbers.  I wonder whether the proposed method scales.  \n\n3. Could the authors provide more insights on why the structured attention (etc) did not significantly improve the result?  Theoritically, it solves the soft attention problems.  \n\n4. The definition of output confidence (section 4.3.1) needs more motivation and theoretical justification.  \n\n[1] Kulesza, Alex, and Ben Taskar. \""Determinantal point processes for machine learning.\"" Foundations and Trends\u00ae in Machine Learning 5.2\u20133 (2012): 123-286.",1,1,1,1,1,-1,1,1,1,-1
B13EC5u6W-R1,"* This paper models images with a latent code representation, and then tries to modify the latent code to minimize changes in image space, while changing the classification label.  As the authors indicate, it lies in the space of algorithms looking to modify the image while changing the label (e.g. LIME etc). \n\n* This is quite an interesting paper with a sensible goal.  It seems like the method could be more informative than the other methods.   However, there are quite a number of problems, as explained below.\n\n* The explanation of eqs 1 and 2 is quite poor.  \\alpha in (1) seems to be \\gamma in Alg 1 (line 5). \""L_target is a target objective which can be a negative class probability ..\"" this assumes that the example is a positive class.  Could we not also apply this to negative examples? \n\n\""or in the case of heart failure, predicted BNP level\"" -- this doesn't make sense to me -- surely it would be necessary to target an adjusted BNP level?  Also specific details should be reserved until a general explanation of the problem has been made. \n\n* The trade-off parameter \\gamma is a \""fiddle factor\"" -- how was this set for the lung image and MNIST examples?  Were these values different? \n\n* In typical ICLR style the authors use a deep network to learn the encoder and decoder networks.  It would be v interesting (and provide a good baseline) to use a shallow network (i.e. PCA) instead, and elucidate what advantages the deep network brings. \n\n* The example of 4/9 misclassification seems very specific. Does this method also work on say 2s and 3s?  Why have you not reported results for these kinds of tasks? \n\n* Fig 2: better to show each original and reconstructed image close by (e.g. above below or side-by-side). \n\nThe reconstructions show poor detail relative to the originals.   This loss of detail could be a limitation. \n\n* A serious problem with the method is that we are asked to evaluate it in terms of images like Fig 4 or Fig 8.  A serious study would involve domain experts and ascertain if Fig 4 conforms with what they are looking for. \n\n* The references section is highly inadequate -- no venues of publication are given.  If these are arXiv give the proper ref.  Others are published in conferences etc, e.g. Goodfellow et al is in Advances in Neural Information Processing Systems 27, 2014. \n\n* Overall: the paper contains an interesting idea, but given the deficiencies raised above I judge that it falls below the ICLR threshold. \n\n* Text:\n\nsec 2 para 4. \""reconstruction loss on the validation set was similar to the reconstruction loss on the validation set.\"" ?? \n\n* p 3 bottom -- give size of dataset\n\n* p 5 AUC curve -> ROC curve\n\n* p 6 Fig 4 use text over each image to better specify the details given in the caption.\n\n\n\n",1,1,1,1,1,1,1,1,1,1
B13EC5u6W-R2,"The main contribution of the paper is a method that provides visual explanations of classification decisions.  The proposed method uses \n - a generator trained in a GAN setup\n - an autoencoder to obtain a latent space representation\n - a method inspired by adversarial sample generation to obtain a generated image from another class - which can then be compared to the original image (or rather the reconstruction of it).  \nThe method is evaluated on a medical images dataset and some additional demonstration on MNIST is provided. \n\n\n - The paper proposes a (I believe) novel method to obtain visual explanations.  The results are visually compelling  although most results are shown on a medical dataset - which I feel is very hard for most readers to follow.  The MNIST explanations help a lot.   It would be great if the authors could come up with an additional way to demonstrate their method to the non-medical reader. \n\n - The paper shows that the results are plausible using a neat trick.  The authors train their system with the testdata included which leads to very different visualizations.  It would be great if this analysis could be performed for MNIST as well. \n\n\nFrom the related work, it would be nice to mention that generative models (p(x|c)) also often allow for explaining their decisions, e.g. the work by Lake and Tenenbaum on probabilistic program induction.\nAlso, there is the work by Hendricks et al on Generating Visual Explanations.  This should probably also be referenced. \n\nminor comments: \n- some figures with just two parts are labeled \""from left to right\"" - it would be better to just write left: ... right: ...\ n- figure 2: do these images correspond to each other?  If yes, it would be good to show them pairwise.\n- figure 5: please explain why the saliency map is relevant.  This looks very noisy and non-interesting.\n\n",1,1,1,1,1,1,1,1,1,-1
B13EC5u6W-R3,"The authors address two important issues: semi-supervised learning from relatively few labelled training examples in the presence of many unlabelled examples, and visual rationale generation: explaining the outputs of the classifiier by overlaing a visual rationale on the original image.  This focus is mainly on medical image classification but the approach could potentially be useful in many more areas.  The main idea is to train a GAN on the unlabeled examples to create a mapping from a lower-dimensional space in which the input features are approximately Gaussian, to the space of images, and then to train an encoder to map the original images into this space minimizing reconstruction error with the GAN weights fixed.  The encoder is then used as a feature extractor for classification and regression of targets (e.g. heard disease).  The visual rationales are generated by optimizing the encoded representation to simultaneously reconstruct an image close to the original and to minimize the probability of the target class.  This gives an image that is similar to the original but with features that caused the classification of the disease removed.  The resulting image can be subtracted from the original encoding to highlight problematic areas.  The approach is evaluated on an in-house dataset and a public NIH dataset, demonstrating good performance, and illustrative visual rationales are also given for MNIST. \n\nThe idea in the paper is, to my knowledge, novel, and represents a good step toward the important task of generating interpretable visual rationales.  There are a few limitations, e.g. the difficulty of evaluating the rationales, and the fact that the resolution is fixed to 128x128 (which means discarding many pixels collected via ionizing radiation), but these are readily acknowledged by the authors in the conclusion. \n\nComments:\n1) There are a few details missing, like the batch sizes used for training (it is difficult to relate epochs to iterations without this).  Also, the number of hidden units in the 2 layer MLP from para 5 in Sec 2. \n2) It would be good to include PSNR/MSE figures for the reconstruction task (fig 2) to have an objective measure of error. \n3) Sec 2 para 4: \""the reconstruction loss on the validation set was similar to the reconstruction loss on the validation set\"" -- perhaps you could be a little more precise here.  E.g. learning curves would be useful.\n4) Sec 2 para 5: \""paired with a BNP blood test that is correlated with heart failure\"" I suspect many readers of ICLR, like myself, will not be well versed in this test, correlation with HF, diagnostic capacity, etc., so a little further explanation would be helpful here.  The term \""correlated\"" is a bit too broad, and it is difficult for a non-expert to know exactly how correlated this is.  It is also a little confusing that you begin this paragraph saying that you are doing a classification task, but then it seems like a regression task which may be postprocessed to give a classification.  Anyway, a clearer explanation would be helpful.  Also, if this test is diagnostic, why use X-rays for diagnosis in the first place? \n5) I would have liked to have seen some indicative times on how long the optimization takes to generate a visual rationale, as this would have practical implications.. \n6) Sec 2 para 7: \""L_target is a target objective which can be a negative class probability or in the case of heart failure, predicted BNP level\"" -- for predicted BNP level, are you treating this as a probability and using cross entropy here, or \nmean squared error?. \n7) As always, it would be illustrative if you could include some examples of failure cases, which would be helpful both in suggesting ways of improving the proposed technique, and in providing insight into where it may fail in practical situations.",1,1,1,1,1,-1,1,1,1,-1
B13njo1R--R1,"This paper aims to learn a single policy that can perform a variety of tasks that were experienced sequentially.  The approach is to learn a policy for task 1, then for each task k+1: copy distilled policy that can perform tasks 1-k, finetune to task k+1, and distill again with the additional task.  The results show that this PLAID algorithm outperforms a network trained on all tasks simultaneously.  \n\nQuestions:\n- When distilling the policies, do you start from a randomly initialized policy, or do you start from the expert policy network? \n- What data do you use for the distillation?  Section 4.1 states\""We use a method similar to the DAGGER algorithm\"", but what is your method.  If you generate trajectories form the student network, and label them with the expert actions, does that mean all previous expert policies need to be kept in memory?  \n- I do not understand the purpose of \""input injection\"" nor where it is used in the paper.   \n\nStrengths:\n- The method is simple but novel.   The results support the method's utility.  \n- The testbed is nice; the tasks seem significantly different from each other.  It seems that no reward shaping is used. \n- Figure 3 is helpful for understanding the advantage of PLAID vs MultiTasker. \n\nWeaknesses:\n- Figure 2: the plots are too small. \n- Distilling may hurt performance ( Figure 2.d) \n- The method lacks details (see Questions above) \n- No comparisons with prior work are provided.  The paper cites many previous approaches to this but does not compare against any of them.  \n- A second testbed (such as navigation or manipulation) would bring the paper up a notch.  \n\nIn conclusion, the paper's approach to multitask learning is a clever combination of prior work.  The method is clear  but not precisely described.  The results are promising.  I think that this is a good approach to the problem that could be used in real-world scenarios.  With some filling out, this could be a great paper.",1,1,1,1,1,1,1,1,1,-1
B13njo1R--R2,"This paper describes PLAID, a method for sequential learning and consolidation of behaviours via policy distillation; the proposed method is evaluated in the context of bipedal motor control across several terrain types, which follow a natural curriculum. \n\nPros:\n- PLAID masters several distinct tasks in sequence, building up \u201cskills\u201d by learning \u201crelated\u201d tasks of increasing difficulty. \n- Although the main focus of this paper is on continual learning of \u201crelated\u201d tasks, the authors acknowledge this limitation and convincingly argue for the chosen task domain. \n\nCons:\n- PLAID seems designed to work with task curricula, or sequences of deeply related tasks; for this regime, classical transfer learning approaches are known to work well (e.g finetunning), and it is not clear whether the method is applicable beyond this well understood case. \n- Are the experiments single runs?  Due to the high amount of variance in single RL experiments it is recommended to perform several re-runs and argue about mean behaviour. \n\nClarifications:\n- What is the zero-shot performance of policies learned on the first few tasks, when tested directly on subsequent tasks? \n- How were the network architecture and network size chosen, especially for the multitasker?  Would policies generalize to later tasks better with larger, or smaller networks? \n- Was any kind of regularization used, how does it influence task performance vs. transfer? \n- I find figure 1 (c) somewhat confusing.  Is performance maintained only on the last 2 tasks, or all previously seen tasks?  That\u2019s what the figure suggests at first glance, but that\u2019s a different goal compared to the learning strategies described in figures 1 (a) and (b).\n",1,1,1,1,1,-1,1,1,1,-1
B13njo1R--R3,"Hi, \n\nThis was a nice read.  I think overall it is a good idea.  But I find the paper lacking a lot of details and to some extend confusing.  \nHere are a few comments that I have:\n\nFigure 2 is very confusing for me.  Please first of all make the figures much larger.  ICLR does not have a strict page limit, and the figures you have are hard to impossible to read.  So you train in (a) on the steps task until 350k steps?  Is (b), (d),(c) in a sequence or is testing moving from plain to different things?  The plot does not explicitly account for the distillation phase.  Or at least not in an intuitive way.  But if the goal is transfer, then actually PLAID is slower than the MultiTasker because it has an additional cost to pay (in frames and times) for the distillation phase right? Or is this counted.  \n\nGoing then to Figure 3, I almost fill that the MultiTasker might be used to simulate two separate baselines.  Indeed, because the retention of tasks is done by distilling all of them jointly, one baseline is to keep finetuning a model through the 5 stages, and then at the end after collecting the 5 policies you can do a single consolidation step that compresses all.  So it will be quite important to know if the frequent integration steps of PLAID are helpful (do knowing 1,2 and 3 helps you learn 4 better? Or knowing 3 is enough).  \n\nWhere exactly is input injection used?  Is it experiments from figure 3.  What input is injecting?  What do you do when you go back to the task that doesn't have the input, feed 0?  What happens if 0 has semantics ?  \n\nPlease say in the main text that details in terms of architecture and so on are given in the appendix.  And do try to copy a bit more of them in the main text where reasonable.  \n\nWhat is the role of PLAID?  Is it to learn a continual learning solution?  So if I have 100 tasks, do I need to do 100-way distillation at the end to consolidate all skills?  Will this be feasible?  Wouldn't the fact of having data from all the 100 tasks at the end contradict the traditional formulation of continual learning?  \n \nOr is it to obtain a multitask solution while maximizing transfer (where you always have access to all tasks, but you chose to sequentilize them to improve transfer)?   And even then maximize transfer with respect to what?  Frames required from the environment?  If that are you reusing the frames you used during training to distill?  Can we afford to keep all of those frames around?  If not we have to count the distillation frames as well.  Also more baselines are needed.  A simple baseline is just finetunning as going from one task to another, and just at the end distill all the policies found through out the way.   Or at least have a good argument of why this is suboptimal compared to PLAID.  \n\nI think the idea of the paper is interesting and I'm willing to increase (and indeed decrease) my score.  But I want to make sure the authors put a bit more effort into cleaning up the paper, making it more clear and easy to read.  Providing at least one more baseline (if not more considering the other things cited by them). \n\n",1,1,1,1,1,-1,1,1,1,-1
B14TlG-RW-R1,"Summary:\n\nThis paper proposes a non-recurrent model for reading comprehension which used only convolutions and attention.  The goal is to avoid recurrent which is sequential and hence a bottleneck during both training and inference.  Authors also propose a paraphrasing based data augmentation method which helps in improving the performance.  Proposed method performs better than existing models in SQuAD dataset while being much faster in training and inference. \n\nMy Comments:\n\nThe proposed model is convincing and the paper is well written. \n\n1. Why don\u2019t you report your model performance without data augmentation in Table 1?  Is it because it does not achieve SOTA?  The proposed data augmentation is a general one and it can be used to improve the performance of other models as well.  So it does not make sense to compare your model + data augmentation against other models without data augmentation.  I think it is ok to have some deterioration in the performance as you have a good speedup when compared to other models. \n\n2. Can you mention your leaderboard test accuracy in the rebuttal?\n\n 3. The paper can be significantly strengthened by adding at least one more reading comprehension dataset.  That will show the generality of the proposed architecture.  Given the sufficient time for rebuttal, I am willing to increase my score if authors report results in an additional dataset in the revision. \n\n4. Are you willing to release your code to reproduce the results?\ n\n\nMinor comments:\n\n1. You mention 4X to 9X for inference speedup in abstract and then 4X to 10X speedup in Intro. Please be consistent.\ n2. In the first contribution bullet point, \u201cthat exclusive built upon\u201d should be \u201cthat is exclusively built upon\u201d.\n",1,1,1,1,1,-1,1,1,1,1
B14TlG-RW-R2,"This paper proposes two contributions: first, applying CNNs+self-attention modules instead of LSTMs, which could result in significant speedup and good RC performance; second, enhancing the RC model training with passage paraphrases generated by a neural paraphrasing model, which could improve the RC performance marginally. \n\nFirstly, I suggest the authors rewrite the end of the introduction.  The current version tends to mix everything together and makes the misleading claim.  When I read the paper, I thought the speeding up mechanism could give both speed up and performance boost, and lead to the 82.2 F1.  But it turns out that the above improvements are achieved with at least three different ideas: (1) the CNN+self-attention module; (2) the entire model architecture design; and (3) the data augmentation method.  \n\nSecondly, none of the above three ideas are well evaluated in terms of both speedup and RC performance, and I will comment in details as follows: \n\n(1) The CNN+self-attention was mainly borrowing the idea from (Vaswani et al., 2017a) from NMT to RC.  The novelty is limited  but it is a good idea to speed up the RC models.  However, as the authors hoped to claim that this module could contribute to both speedup and RC performance, it will be necessary to show the RC performance of the same model architecture, but replacing the CNNs with LSTMs.  Only if the proposed architecture still gives better results, the claims in the introduction can be considered correct. \n\n(2) I feel that the model design is the main reason for the good overall RC performance.  However, in the paper there is no motivation about why the architecture was designed like this.  Moreover, the whole model architecture is only evaluated on the SQuAD dataset.  As a result, it is not convincing that the system design has good generalization.  If in (1) it is observed that using LSTMs in the model instead of CNNs could give on par or better results, it will be necessary to test the proposed model architecture on multiple datasets, as well as conducting more ablation tests about the model architecture itself. \n\n(3) I like the idea of data augmentation with paraphrasing.  Currently, the improvement is only marginal,  but there seems many other things to play with.  For example, training NMT models with larger parallel corpora; training NMT models with different language pairs with English as the pivot; and better strategies to select the generated passages for data augmentation.\ n\nI am looking forward to the test performance of this work on SQuAD.",1,1,1,1,1,1,1,1,1,-1
B14TlG-RW-R3,"This paper presents a reading comprehension model using convolutions and attention.  This model does not use any recurrent operation but it is not per se simpler than a recurrent model.  Furthermore, the authors proposed an interesting idea to augment additional training data by paraphrasing based on off-the-shelf neural machine translation.   On SQuAD dataset, their results show some small improvements using the proposed augmentation technique.  Their best results, however, do not outperform the best results reported on the leader board. \n\nOverall, this is an interesting study on SQuAD dataset.  I would like to see results on more datasets and more discussion on the data augmentation technique.  At the moment, the description in section 3 is fuzzy in my opinion.  Interesting information could be:\n- how is the performance of the NMT system?  \n- how many new data points are finally added into the training data set? \n- what do \u2018data aug\u2019 x 2 or x 3 exactly mean?\n",1,1,1,1,1,-1,1,1,1,-1
B14uJzW0b-R1,"Summary: \nThe paper considers the problem of a single hidden layer neural network, with 2 RELU units (this is what I got from the paper - as I describe below, it was not clear at all the setting of the problem  - if I'm mistaken, I will also wait for the rest of the reviews to have a more complete picture of the problem). \nGiven this architecture, the authors focus on characterizing the objective landscape of such a problem. \nThe techniques used depend on previous work.  According to the authors, this paper extends(?) previous results on a NN with a single layer with a single unit. \n\nOriginality: \nThe paper heavily depends on the approach followed by Brutzkus and Globerson, 2017.  To this end, slighly novel. \n\nImportance: \nUnderstanding the landscape (local vs global minima vs saddle points) is an important direction in order to further understand when and why deep neural networks work.  I would say that the topic is an important one. \n\nPresentation/Clarity: \nTo the best of my understanding, the paper has some misconceptions.  The title is not clear whether the paper considers a two layer RELU network or a single layer with with two RELU units.  In the abstract the authors state that it has to do with a two-layer RELU network with two hidden units (per layer? in total?).  Later on, in Section 3, the expression at the bottom of page 2 seems to consider a single-layer RELU network, with two units.  \nThese are crucial for understanding the contribution of the paper; while reading the paper, I assumed that the authors consider the case of a single hidden unit with K = 2 RELU activations (however, that complicated my understanding on how it compares with state of the art). \n\nAnother issue is the fact that, on my humble opinion, the main text looks like a long proof.  It would be great to have more intuitions. \n\nComments:\n1. The paper mainly focuses on a specific problem instance, where the weight vectors are unit-normed and orthogonal to each other.  While the authors already identify that this might be a restriction, it still does not lessen the fact that the configuration considered is a really specific one. \n\n2. The paper reads like a collection of lemmas, with no verbose connection.  It was hard to read and understand their value, just because mostly the text was structured as one lemma after the other. \n\n3. It is not clear from the text whether the setting is already considered in Brutzkus and Globerson, 2017.  Please clarify how your work is different/new from previous works.\n",1,1,1,1,1,1,1,1,1,-1
B14uJzW0b-R2,"In this paper the authors studied the theoretical properties of manifold descent approaches in a standard regression problem, whose regressor is a simple neural network.  Leveraged by two recent results in global optimization, they showed that with a simple two-layer ReLU network with two hidden units, the problem with a standard MSE population loss function does not have spurious local minimum points.  Based on the results by Lee et al, which shows that first order methods converge to local minimum solution (instead of saddle points), it can be concluded that the global minima of this problem can be found by any manifold descent techniques, including standard gradient descent methods.  In general I found this paper clearly written and technically sound.  I also appreciate the effort of developing theoretical results for deep learning, even though the current results are restrictive to very simple NN architectures.  \n\nContribution: \nAs discussed in the literature review section, apart from previous results that studied the theoretical convergence properties for problems that involves a single hidden unit NN, this paper extends the convergence results to problems that involves NN with two hidden units.  The analysis becomes considerably more complicated,  and the contribution seems to be novel and significant.  I am not sure why did the authors mentioned the work on over-parameterization though.  It doesn't seem to be relevant to the results of this paper (because the NN architecture proposed in this paper is rather small).  \n\nComments on the Assumptions:\n- Please explain the motivation behind the standard Gaussian assumption of the input vector x.  \n- Please also provide more motivations regarding the assumption of the orthogonality of weights: w_1^\\top w_2=0 (or the acute angle assumption in Section 6).  \nWithout extra justifications, it seems that the theoretical result only holds for an artificial problem setting.  While the ReLU activation is very common in NN architecture, without more motivations I am not sure what are the impacts of these results.  \n\nGeneral Comment:  \nThe technical section is quite lengthy, and unfortunately I am not available to go over every single detail of the proofs.  From the analysis in the main paper, I believe the theoretical contribution is correct and sound.  While I appreciate the technical contributions,  in order to improve the readability of this paper, it would be great to see more motivations of the problem studied in this paper (even with simple examples).  Furthermore, it is important to discuss the technical assumptions on the 1) standard Gaussianity of the input vector,  and 2) the orthogonality of the weights (and the acute angle assumption in Section 6) on top of the discussions in Section 8.1, as they are critical to the derivations of the main theorems.",1,1,1,1,1,1,1,1,1,-1
B14uJzW0b-R3,"This paper considers a special deep learning model and shows that in expectation, there is only one unique local minimizer.  As a result, a gradient descent algorithm converges to the unique solution.  This works address a conjecture proposed by Tian (2017). \n\nWhile it is clearly written, my main concern is whether this model is significant enough.  The assumptions K=2 and v1=v2=1 reduces the difficulty of the analysis,  but it makes the model considerably simpler than any practical setting.\n\n",1,1,1,1,-1,-1,1,1,1,-1
B16yEqkCZ-R1,"The paper addresses the problem of learners forgetting rare states and revisiting catastrophic danger states.  The authors propose to train a predictive \u2018fear model\u2019 that penalizes states that lead to catastrophes.  The proposed technique is validated both empirically and theoretically.  \n\nExperiments show a clear advantage during learning when compared with a vanilla DQN.  Nonetheless, there are some criticisms than can be made of both the method and the evaluations:\n\nThe fear radius threshold k_r seems to add yet another hyperparameter that needs tuning.  Judging from the description of the experiments this parameter is important to the performance of the method and needs to be set experimentally.  There seems to be no way of a priori determine a good distance as there is no way to know in advance when a catastrophe becomes unavoidable.  No empirical results on the effect of the parameter are given. \n\nThe experimental results support the claim that this technique helps to avoid catastrophic states during initial learning. The paper however, also claims to address the longer term problem of revisiting these states once the learner forgets about them, since they are no longer part of the data generated by (close to) optimal policies.   This problem does not seem to be really solved by this method.  Danger and safe state replay memories are kept, but are only used to train the catastrophe classifier.  While the catastrophe classifier can be seen as an additional external memory, it seems that the learner will still drift away from the optimal policy and then need to be reminded by the classifier through penalties.  As such the method wouldn\u2019t prevent catastrophic forgetting, it would just prevent the worst consequences by penalizing the agent before it reaches a danger state.  It would therefore  be interesting to see some long running experiments and analyse how often catastrophic states (or those close to them) are visited.  \n\nOverall, the current evaluations focus on performance and give little insight into the behaviour of the method.  The paper also does not compare to any other techniques that attempt to deal with catastrophic forgetting and/or the changing state distribution ([1,2]). \n\nIn general the explanations in the paper often often use confusing and  imprecise language, even in formal derivations, e.g.  \u2018if the fear model reaches arbitrarily high accuracy\u2019 or \u2018if the probability is negligible\u2019. \n\nIt is wasn\u2019t clear to me that the properties described in Theorem 1 actually hold.  The motivation in the appendix is very informal and no clear derivation is provided.  The authors seem to indicate that a minimal return can be guaranteed because the optimal policy spends a maximum of epsilon amount of time in the catastrophic states and the alternative policy simply avoids these states.  However, as the alternative policy is learnt on a different reward, it can have a very different state distribution, even for the non-catastrophics states.   It might attach all its weight to a very poor reward state in an effort to avoid the catastrophe penalty.  It is therefore not clear to me that any claims can be made about its performance without additional assumptions. \n\nIt seems that one could construct a counterexample using a 3-state chain problem (no_reward,danger, goal) where the only way to get to the single goal state is to incur a small risk of visiting the danger state.  Any optimal policy would therefore need to spend some time e in the danger state, on average.  A policy that learns to avoid the danger state would then also be unable to reach the goal state and receive rewards.  E.g pi* has stationary distribution (0,e,1-e) and return 0*0+e*Rmin + (1-e)*Rmax.  By adding a sufficiently high penalty, policy pi~ can learn to avoid the catastrophic state with distribution (1,0,0) and then gets return 1*0+ 0*Rmin+0*Rmax= 0 < n*_M - e (Rmax - Rmin) = e*Rmin + (1-e)*Rmax - e (Rmax - Rmin).   This seems to contradict the theorem.  It wasn\u2019t clear what assumptions the authors make to exclude situations like this. \n\n[1] T. de Bruin, J. Kober, K. Tuyls and R. Babu\u0161ka, \""Improved deep reinforcement learning for robotics through distribution-based experience retention,\"" 2016 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), Daejeon, 2016, pp. 3947-3952.\n[2] Kirkpatrick, J., Pascanu, R., Rabinowitz, N., Veness, J., Desjardins, G., Rusu, A. A., ... & Hassabis, D. (2017). Overcoming catastrophic forgetting in neural networks. Proceedings of the National Academy of Sciences, 201611835.",1,1,1,1,1,1,1,1,1,-1
B16yEqkCZ-R2,"\nSUMMARY\n\nThe paper proposes an RL algorithm that combines the DQN algorithm with a fear model.   The fear model is trained in parallel to predict catastrophic states.    Its output is used to penalize the Q learning target.  \n\n\n\nCOMMENTS\n\nNot convinced about the fact that an agent forgets about catastrophic states.   Because it does not experience it any more.   Shouldn\u2019t the agent stop learning at some point in time?   Why does it need to keep collecting good data?   How about giving more weight to catastrophic data (e.g., replicating it)\ n\nIs the catastrophic scenario specific to DRL or RL in general with function approximation? \n\nWhy not specify catastrophic states with a large negative reward? \n\nIt seems that catastrophe states need to be experienced at least once. \nIs that acceptable for the autonomous car hitting a pedestrian?\n",1,-1,1,1,1,-1,1,1,-1,1
B16yEqkCZ-R3,"The paper studies catastrophic forgetting, which is an important aspect of deep reinforcement learning (RL).  The problem formulation is connected to safe RL, but the emphasis is on tasks where a DQN is able to learn to avoid catastrophic events as long as it avoids forgetting.  The proposed method is novel, but perhaps the most interesting aspect of this paper is that they demonstrate that \u201cDQNs  are susceptible to periodically repeating mistakes\u201d.  I believe this observation, though not entirely novel, will inspire many researchers to study catastrophic forgetting and propose improved strategies for handling these issues. \n\nThe paper is accurate, very well written (apart from a small number of grammatical mistakes) and contains appealing motivations to its key contributions .In particular, I find the basic of idea of introducing a component that represents fear natural, promising and novel.  \n\nStill, many of the design choices appear quite arbitrary and can most likely be improved upon.  In fact, it is not difficult to design examples for which the proposed algorithm would be far from optimal.  Instead I view the proposed techniques mostly as useful inspiration for future papers to build on.  As a source of inspiration, I believe that this paper will be of considerable importance and I think many people in our community will read it with great interest.  The theoretical results regarding the properties of the proposed algorithm are also relevant, and points out some of its benefits, though I do not view the results as particularly strong.  \n\nTo conclude, the submitted manuscript contains novel observations and results and is likely to draw additional attention to an important aspect of deep reinforcement learning.  A potential weakness with the paper is that the proposed strategies appear to be simple to improve upon and that they have not convinced me that they would yield good performance on a wider set of problems. \n",1,1,1,1,1,-1,1,1,1,-1
B16_iGWCW-R1,"This paper consider a version of boosting where in each iteration only class weights are updated rather than sample weights and apply that to a series of CNNs for object recognition tasks. \n\nWhile the paper is comprehensive in their derivations (very similar to original boosting papers and in many cases one to one translation of derivations), it lacks addressing a few fundamental questions:\n\n- AdaBoost optimises exponential loss function via functional gradient descent in the space of weak learners.  It's not clear what kind of loss function is really being optimised here.  It feels like it should be the same, but the tweaks applied to fix weights across all samples for a class doesn't make it not clear what is that really gets optimised at the end. \n- While the motivation is that classes have different complexities to learn and hence you might want each base model to focus on different classes, it is not clear why this methods should be better than normal boosting: if a class is more difficult, it's expected that their samples will have higher weights and hence the next base model will focus more on them.  And crudely speaking, you can think of a class weight to be the expectation of its sample weights and you will end up in a similar setup. \n- Choice of using large CNNs as base models for boosting isn't appealing in practical terms, such models will give you the ability to have only a few iterations and hence you can't achieve any convergence that often is the target of boosting models with many base learners. \n- Experimentally, paper would benefit with better comparisons and studies: 1) state-of-the-art methods haven't been compared against (e.g. ImageNet experiment compares to 2 years old method)  2) comparisons to using normal AdaBoost on more complex methods haven't been studied (other than the MNIST)  3) comparison to simply ensembling with random initialisations. \n\nOther comments:\n- Paper would benefit from writing improvements to make it read better. \n-  \""to replace the softmax error function (used in deep learning)\"": I don't think we have softmax error function",1,1,1,1,1,-1,1,1,1,-1
B16_iGWCW-R2,"In conventional boosting methods, one puts a weight on each sample.  The wrongly classified samples get large weights such that in the next round those samples will be more likely to get right.   Thus the learned weak learner at this round will make different mistakes. \nThis idea however is difficult to be applied to deep learning with a large amount of data.  This paper instead designed a new boosting method which puts large weights on the category with large error in this round.    In other words samples in the same category will have the same weight \n\nError bound is derived.   Experiments show its usefulness  though experiments are limited\n",1,1,-1,1,-1,-1,1,-1,1,-1
B16_iGWCW-R3,"This paper applies the boosting trick to deep learning.  The idea is quite straightforward, and the paper is relatively easy to follow.  The proposed algorithm is validated on several image classification datasets. \n\nThe paper is its current form has the following issues:\n1. There is hardly any baseline compared in the paper.  The proposed algorithm is essentially an ensemble algorithm, there exist several works on deep model ensemble (e.g., Boosted convolutional neural networks, and Snapshot Ensemble) should be compared against. \n2. I did not carefully check all the proofs, but seems most of the proof can be moved to supplementary to keep the paper more concise. \n3. In Eq. (3), \\tilde{D} is not defined. \n4. Under the assumption $\\epsilon_t(l) > \\frac{1}{2\\lambda}$, the definition of $\\beta_t$ in Eq.8 does not satisfy $0 < \\beta_t < 1$.   \n5. How many layers is the DenseNet-BC used in this paper?  Why the error rate reported here is higher than that in the original paper? \nTypo: \nIn Session 3 Line 7, there is a missing reference. \nIn Session 3 Line 10, \u201c1,00 object classes\u201d should be \u201c100 object classes\u201d. \nIn Line 3 of the paragraph below Equation 5, \u201cclasse\u201d should be \u201cclass\u201d.\n",1,1,1,1,1,1,1,1,-1,-1
B1ae1lZRb-R1,"The authors investigate knowledge distillation as a way to learn low precision networks.  They propose three training schemes to train a low precision student network from a teacher network.  They conduct experiments on ImageNet-1k with variants of ResNets and multiple low precision regimes and compare performance with previous works\n\n Pros:\n(+) The paper is well written, the schemes are well explained\n(+) Ablations are thorough and comparisons are fair\n Cons:\n(-) The gap with full precision models is still large \n(-)  Transferability of the learned low precision models to other tasks is not discussed\n\n The authors tackle a very important problem, the one of learning low precision models without comprosiming performance.  For scheme-A, the authors show the performance of the student network under many low precision regimes and different depths of teacher networks.  One observation not discussed by the authors is that the performance of the student network under each low precision regime doesn't improve with deeper teacher networks (see Table 1, 2 & 3).  As a matter of fact, under some scenarios performance even decreases.  \n\nThe authors do not discuss the gains of their best low-precision regime in terms of computation and memory. \n\nFinally, the true applications for models with a low memory footprint are not necessarily related to image classification models (e.g. ImageNet-1k).  How good are the low-precision models trained by the authors at transferring to other tasks?  Is it possible to transfer student-teacher training practices to other tasks?",1,1,1,1,1,-1,1,1,1,-1
B1ae1lZRb-R2,"The paper aims at improving the accuracy of a low precision network based on knowledge distillation from a full-precision network.  Instead of distillation from a pre-trained network, the paper proposes to train both teacher and student network jointly.  The paper shows an interesting result that the distilled low precision network actually performs better than high precision network. \n\nI found the paper interesting  but the contribution seems quite limited. \n\nPros:\n1. The paper is well written and easy to read.\n2.  The paper reported some interesting result such as that the distilled low precision network actually performs better than high precision network, and that training jointly outperforms the traditional distillation method (fixing the teacher network) marginally. \n\nCons:\n1. The name Apprentice seems a bit confusing with apprenticeship learning. \n2. The experiments might be further improved by providing a systematic study about the effect of precisions in this work (e.g., producing more samples of precisions on activations and weights). \n3. It is unclear how the proposed method outperforms other methods based on fine-tuning.  It is also quite possible that after fine-tuning the compressed model usually performs quite similarly to the original model.",1,1,1,1,1,-1,1,1,1,-1
B1ae1lZRb-R3,Summary:\nThe paper presents three different methods of training a low precision student network from a teacher network using knowledge distillation. \nScheme A consists of training a high precision teacher jointly with a low precision student.  Scheme B is the traditional knowledge distillation method and Scheme C uses knowledge distillation for fine-tuning a low precision student which was pretrained in high precision mode. \n\nReview:\nThe paper is well written.  The experiments are clear and the three different schemes provide good analytical insights. \nUsing scheme B  and C student model with low precision could achieve accuracy close to teacher while compressing the model.\n\n Comments:\nTensorflow citation is missing.\n Conclusion is short and a few directions for future research would have been useful.,1,1,1,1,1,-1,1,1,1,1
B1al7jg0b-R1,"The paper leaves me guessing which part is a new contribution, and which one is already possible with conceptors as described in the Jaeger 2014 report.  Figure (1) in the paper is identical to the one in the (short version of) the Jaeger report but is missing an explicit reference.  Figure 2 is almost identical, again a reference to the original would be better. \nConceptors can be trained with a number of approaches (as described both in the 2014 Jaeger tech report and in the JMLR paper), including ridge regression.  What I am missing here is a clear indication what is an original contribution of the paper, and what is already possible using the original approach.  The fact that additional conceptors can be trained does not appear new for the approach described here.  If the presented approach was an improvement over the original conceptors, the evaluation should compare the new and the original version. \n\nThe evaluation also leaves me a little confused in an additional dimension: the paper title and abstract suggested that the contribution is about overcoming catastrophic forgetting.  The evaluation shows that the approach performs better classifying MNIST digits than another approach.  This is nice but doesn't really tell me much about overcoming catastrophic forgetting. \n",1,1,1,1,1,1,1,1,1,1
B1al7jg0b-R2,"[Reviewed on January 12th]\n\nThis article applies the notion of \u201cconceptors\u201d -- a form of regulariser introduced by the same author a few years ago, exhibiting appealing boolean logic pseudo-operations -- to prevent forgetting in continual learning,more precisely in the training of neural networks on sequential tasks.  It proposes itself as an improvement over the main recent development of the field, namely Elastic Weight Consolidation.   After a brief and clear introduction to conceptors and their application to ridge regression, the authors explain how to inject conceptors into Stochastic Gradient Descent and finally, the real innovation of the paper, into Backpropagation.  Follows a section of experiments on variants of MNIST commonly used for continual learning.\n\n Continual learning in neural networks is a hot topic, and this article contributes a very interesting idea.  The notion of conceptors is appealing in this particular use for its interpretation in terms of regularizer and in terms of Boolean logic.   The numeric examples, although quite toy, provide a clear illustration. \n\nA few things are still missing to back the strong claims of this paper:\n* Some considerations of the computational costs: the reliance on the full NxN correlation matrix R makes me fear it might be costly, as it is applied to every layer of the neural networks and hence is the largest number of units in a layer.   This is of course much lighter than if it were the covariance matrix of all the weights, which would be daunting, but still deserves to be addressed, if only with wall time measures. \n* It could also be welcome to use a more grounded vocabulary, e.g. on p.2 \u201cFigure 1 shows examples of conceptors computer from three clouds of sample state points coming from a hypothetical 3-neuron recurrent network that was drive with input signals from three difference sources\u201d could be much more simply said as \u201cFigure 1 shows the ellipses corresponding to three sets of R^3 points\u201d.  Being less grandiose would make the value of this article nicely on its own.\n*  Some examples beyond the contrived MNIST toy examples would be welcome.  For example, the main method this article is compared to (EWC) had a very strong section on Reinforcement learning examples in the Atari framework, not only as an illustration but also as a motivation.  I realise not everyone has the computational or engineering resources to try extensively on multiple benchmarks from classification to reinforcement learning.  Nevertheless, without going to that extreme, it might be worth adding an extra demo on something bigger than MNIST.  The authors transparently explain in their answer that they do not (yet!) belong to the deep learning community and hope finding some collaborations to pursue this further.  If I may make a suggestion, I think their work would get much stronger impact by  doing it the reverse way: first finding the collaboration, then adding this extra empirical results, which then leads to a bigger impact publication. \n\nThe later point would normally make me attribute a score of \""6: Marginally above acceptance threshold\"" by current DL community standards,  but because there is such a pressing need for methods to tackle this problem, and because this article can generate thinking along new lines about this, I give it a 7 : Good paper, accept.\n",1,1,1,1,1,1,1,1,1,-1
B1al7jg0b-R3,"This paper introduces a method for learning new tasks, without interfering previous tasks, using conceptors.  This method originates from linear algebra, where a the network tries to algebraically infer the main subspace where previous tasks were learned, and make the network learn the new task in a new sub-space which is \""unused\"" until the present task in hand.\n\n The paper starts with describing the method and giving some context for the method and previous methods that deal with the same problem.  In Section 2 the authors review conceptors.  This method is algebraic method closely related to spanning sub spaces and SVD.  The main advantage of using conceptors is their trait of boolean logics: i.e., their ability to be added and multiplied naturally.  In section 3 the authors elaborate on reviewed ocnceptors method and show how to adapt this algorithm to SGD with back-propagation.  The authors provide a version with batch SGD as well.\n\n In Section 4, the authors show their method on permuted MNIST.  They compare the method to EWC with the same architecture.  They show that their method more efficiently suffers on permuted MNIST from less degradation.  Also, they compared the method to EWC and IMM on disjoint MNIST and again got the best performance. \n\nIn general, unlike what the authors suggest, I do not believe this method is how biological agents perform their tasks in real life.  Nevertheless, the authors show that their method indeed reduce the interference generated by a new task on the old learned tasks.\n\n I think that this work might interest the community since such methods might be part of the tools that practitioners have in order to cope with learning new tasks without destroying the previous ones.   What is missing is the following: I think that without any additional effort, a network can learn a new task in parallel to other task, or some other techniques may be used which are not bound to any algebraic methods.  Therefore, my only concern is that in this comparison the work bounded to very specific group of methods, and the question of what is the best method for continual learning remained open.",1,1,1,1,1,1,1,1,1,-1
B1bgpzZAZ-R1,"This paper gives an elaboration on the Gated Attention Reader (GAR) adding gates based on answer elimination in multiple choice reading comprehension.   I found the formal presentation of the model reasonably clear the the empirical evaluation reasonably compelling. \n\nIn my opinion the main weakness of the paper is the focus on the RACE dataset.   This dataset has not attracted much attention and most work in reading comprehension has now moved to the SQUAD dataset for which there is an active leader board.   I realize that SQUAD is not explicitly multiple choice and that this is a challenge for an answer elimination architecture.   However, it seems that answer elimination might be applied to each choice of the initial position of a possible answer span.   In any case, competing with an active leader board would be much more compelling.",1,1,1,1,1,-1,1,1,1,-1
B1bgpzZAZ-R2,"In this paper, a model is built for reading comprehension with multiple choices.  The model consists of three modules: encoder, interaction module and elimination module.  The major contributions are two folds: firstly, proposing the interesting option elimination problem for multi-step reading comprehension;  and secondly, proposing the elimination module where a eliminate gate is used to select different orthogonal factors from the document representations.  Intuitively, one answer option can be viewed as eliminated if the document representation vector has its factor along the option vector ignored. \n\nThe elimination module is interesting,  but the usefulness of \u201celimination\u201d is not well justified for two reasons.  First, the improvement of the proposed model over the previous state of the art is limited.  Second, the model is built upon GAR until the elimination module, then according to Table 1 it seems to indicate that the elimination module does not help significantly (0.4% improvement).  \n\nIn order to show the usefulness of the elimination module, the model should be exactly built on the GAR with an additional elimination module (i.e. after removing the elimination module, the performance should be similar to GAR but not something significantly worse with a 42.58% accuracy).  Then we can explicitly compare the performance between GAR and the GAR w/ elimination module to tell how much the new module helps. \n\nOther issues:\n\n1) Is there any difference to directly use $x$ and $h^z$ instead of $x^e$ and $x^r$ to compute $\\tilde{x}_i$?  Even though the authors find the orthogonal vectors, they\u2019re gated summed together very soon.  It would be better to show how much \u201celimination\u201d and \u201csubtraction\u201d effect the final performance, besides the effect of subtraction gate.\n\n2)  A figure showing the model architecture and the corresponding QA process will better help the readers understand the proposed model.\n\n 3) $c_i$ in page 5 is not defined.  What\u2019s the performance of only using $s_i$ for answer selection or replacing $x^L$ with $s_i$ in score function? \n\n4) It would be better to have the experiments trained with different $n$ to show how multi-hop effects the final performance, besides the case study in Figure 3 .\n\nMinor issues:\n\n1) In Eqn. (4), it would be better to use a vector as the input of softmax.\n\n 2) It would be easier for discussion if the authors could assign numbers to every equation.",1,1,1,1,1,-1,1,1,1,-1
B1bgpzZAZ-R3,"This paper proposes a new reading comprehension model for multi-choice questions and the main motivation is that some options should be eliminated first to infer better passage/question representations. \n\nIt is a well-written paper,  however, I am not very convinced by its motivation, the proposed model and the experimental results.  \n\nFirst of all, the improvement is rather limited.  It is only 0.4 improvement overall on the RACE dataset;  although it outperforms GAR on 7 out of 13 categories;  but why is it worse on the other 6 categories?  I don\u2019t see any convincing explanations here. \n\nSecondly, in terms of the development of reading comprehension models, I don\u2019t see why we need to care about eliminating the irrelevant options.  It is hard to generalize to any other RC/QA tasks.  If the point is that the options can add useful information to induce better representations for passage/question, there should be some simple baselines in the middle that this paper should compare to.  The two baselines SAR and GAR both only induce a representation from paragraph/question, and finally compare to the representation of each option.  Maybe a simple baseline is to merge the question and all the options and see if a better document representation can be defined.  \n\nSome visualizations/motivational examples could be also useful to understand how some options are eliminated and how the document representation has been changed based on that.\n",1,1,1,1,1,-1,1,1,1,-1
B1CEaMbR--R1,"This paper presents some reviews on clustering methods with deep learning.  Based on the review taxonomy, the authors presents a mixed objective which aims for bretter clustering performance.  The proposed method is then tested on two image data sets.\n\n The claimed main contribution of the paper is the taxonomy.  There are no new things in such kind of reviews.  The taxonomy gives no scientific axioms.  Therefore the impact or actual contribution to the ICLR community is very limited.\n\n The proposed clustering method is problematic.  It is hard to set the paramter alpha.  The experimental results are also disappointing.  For example, the COIL20 accuracy is only 0.762, much worse than the state of the art.  Moreover, results on only two image data sets are not sufficient for convincing.",1,1,1,1,-1,1,1,1,1,-1
B1CEaMbR--R2,"In this paper the authors give a nice review of clustering methods with deep learning and a systematic taxonomy for existing methods.  Finally, the authors propose a new method by using one unexplored combination of taxonomy features. \n\nThe paper is well-written and easy to follow . The proposed combination is straightforward,  but lack of novelty.  From table 1, it seems that the only differences between the proposed method and DEPICK is whether the method uses balanced assignment and pretraining.  I am not convinced that these changes will lead to a significant difference.  The performance of the proposed method and DEPICK are also similar in table 1.  \n\nIn addition, the experiments section is not comprehensive enough as well the author only tested on two datasets.  More datasets should be tested for evaluation.  In addition, It seems that nearly all the experiments results from comparison methods are borrowed from the original publications.  The authors should finish the experiments on comparison methods and fill the entries in Table 1.\n\n In summary, the proposed method is lack of novelty compare to existing methods.  The survey part is nice,  however extensive experiments should be conducted by running existing methods on different datasets and analyzing the pros and cons of the methods and their application scenarios.  Therefore, I think the paper cannot be accepted at this stage.\n",1,1,1,1,1,1,1,1,1,-1
B1CEaMbR--R3,"The paper is mostly a survey about clustering methods with neural networks.  \n\nSection 2 presents a taxonomy for the different neural network clustering methods.  A rich lists of the possible components of the neural network-based clustering methods are given, that include the different neural network architectures, feature to use for clustering, loss functions used and more.  In Section 3, a few methods from the literature are classified according to the proposed taxonomy.  Furthermore, in Section 4 a new method is proposed, that is to combine the best parts of the already existing models in the literature.  Unfortunately, the experiments is Section 5 reveal that the proposed method yields results that are at most comparable with the existing methods.  \n\nThe paper is written well and provides good insights (mostly taxonomy) on the existing methods for neural network-based clustering.  However, the paper lacks novel content.  The novel content of the paper sums up to the proposed method, that is composed of building blocks of existing models, and fails to impress in experimental results.  It could be that this paper belongs to another venue that is more appropriate for survey papers.   Also, it overall rather appears short.\n",1,1,1,1,1,1,1,1,1,-1
B1CNpYg0--R1,"This paper describes a method for computing representations for out-of-vocabulary words, e.g. based on their spelling or dictionary definitions.  The main difference from previous approaches is that the model is that the embeddings are trained end-to-end for a specific task, rather than trying to produce generically useful embeddings.  The method leads to better performance than using no external resources,  but not as high performance as using Glove embeddings.   The paper is clearly written, and has useful ablation experiments.   However, I have a couple of questions/concerns:\n- Most of the gains seem to come from using the spelling of the word.   As the authors note, this kind of character level modelling has been used in many previous works.   \n- I would be slightly surprised if no previous work has used external resources for training word representations using an end-task loss,   but I don\u2019t know the area well enough to make specific suggestions   \n- I\u2019m a little skeptical about how often this method would really be useful in practice.   It seems to assume that you don\u2019t have much unlabelled text (or you\u2019d use Glove), but you probably need a large labelled dataset to learn how to read dictionary definitions well.   All the experiments use large tasks - it would be helpful to have an experiment showing an improvement over character-level modelling on a smaller task.  \n- The results on SQUAD seem pretty weak - 52-64%, compared to the SOTA of 81.   It seems like the proposed method is quite generic, so why not apply it to a stronger baseline?\n",1,1,1,1,1,1,1,1,1,-1
B1CNpYg0--R2,"\nThis paper illustrates a method to compute produce word embeddings on the fly for rare words, using a pragmatic combination of existing ideas: \n\n* Backing off to a separate decoder for rare words a la Luong and Manning (https://arxiv.org/pdf/1604.00788.pdf, should be cited, though the idea might be older). \n\n* Using character-level models a la Ling et al. \n\n* Using dictionary embeddings a la Hill et al. \n\nNone of these ideas are new before but I haven\u2019t seen them combined in this way before.  This is a very practical idea, well-explained with a thorough set of experiments across three different tasks.  The paper is not surprising  but this seems like an effective technique for people who want to build effective systems with whatever data they\u2019ve got. \n",1,1,1,1,1,1,1,1,1,-1
B1CNpYg0--R3,"This paper examines ways of producing word embeddings for rare words on demand.  The key real-world use case is for domain specific terms, but here the techniques are demonstrated on rarer words in standard data sets.  The strength of this paper is that it both gives a more systematic framework for and builds on existing ideas (character-based models, using dictionary definitions) to implement them as part of a model trained on the end task. \n\nThe contribution is clear  but not huge.  In general, for the scope of the paper, it seems like what is here could fairly easily have been made into a short paper for other conferences that have that category.  The basic method easily fits within 3 pages, and while the presentation of the experiments would need to be much briefer, this seems quite possible.  More things could have been considered.  Some appear in the paper, and there are some fairly natural other ones such as mining some use contexts of a word (such as just from Google snippets) rather than only using textual definitions from wordnet.  The contributions are showing that existing work using character-level models and definitions can be improved by optimizing representation learning in the context of the final task, and the idea of adding a learned linear transformation matrix inside the mean pooling model (p.3).  However, it is not made very clear why this matrix is needed or what the qualitative effect of its addition is. \n\nThe paper is clearly written.  \n\nA paper that should be referred to is the (short) paper of Dhingra et al. (2017): A Comparative Study of Word Embeddings\nfor Reading Comprehension https://arxiv.org/pdf/1703.00993.pdf .  While it in no way covers the same ground as this paper it is relevant as follows: This paper assumes a baseline that is also described in that paper of using a fixed vocab and mapping other words to UNK.  However, they point out that at least for matching tasks like QA and NLI that one can do better by assigning random vectors on the fly to unknown words.  That method could also be considered as a possible approach to compare against here. \n\nOther comments:\n - The paper suggests a couple of times including at the end of the 2nd Intro paragraph that you can't really expect spelling models to perform well in representing the semantics of arbitrary words (which are not morphological derivations, etc.).  While this argument has intuitive appeal, it seems to fly in the face of the fact that actually spelling models, including in this paper, seem to do surprisingly well at learning such arbitrary semantics. \n - p.2: You use pretrained GloVe vectors that you do not update.  My impression is that people have had mixed results, sometimes better, sometimes worse with updating pretrained vectors or not. Did you try it both ways?\n - fn. 1: Perhaps slightly exaggerates the point being made, since people usually also get good results with the GloVe or word2vec model trained on \""only\"" 6 billion words \u2013 2 orders of magnitude less data. \n - p.4. When no definition is available, is making e_d(w) a zero vector worse than or about the same as using a trained UNK vector? \n - Table 1: The baseline seems reasonable (near enough to the quality of the original Salesforce model from 2016 (66 F1)  but well below current best single models of around 76-78 F1.  The difference between D1 and D3 does well illustrate that better definition learning is done with backprop from end objective.  This model shows the rather strong performance of spelling models \u2013 at least on this task \u2013 which he again benefit from training in the context of the end objective.  \n - Fig 2: It's weird that only the +dict (left) model learns to connect \""In\"" and \""where\"".  The point made in the text between \""Where\"" and \""overseas\"" is perfectly reasonable, but it is a mystery why the base model on the right doesn't learn to associate the common words \""where\"" and \""in\"" both commonly expressing a location. \n - Table 2: These results are interestingly different.  Dict is much more useful than spelling here.  I guess that is because of the nature of NLI, but it isn't 100% clear why NLI benefits so much more than QA from definitional knowledge. \n - p.7: I was slightly surprised by how small vocabs (3k and 5k words) are said to be optimal for NLI (and similar remarks hold for SQuAD).  My impression is that most papers on NLI use much larger vocabs, no? \n - Fig 3: This could really be drawn considerably better: make the dots bigger and their colors more distinct. \n - Table 3: The differences here are quite small and perhaps the least compelling, but the same trends hold.\n",1,1,1,1,1,1,1,1,1,-1
B1D6ty-A--R1,"After reading the rebuttal:\n\nThe authors addressed some of my theoretical questions.  I think the paper is borderline, leaning towards accept. \n\nI do want to note my other concerns:\n\nI suspect the theoretical results obtained here are somewhat restricted to the least-squares, autoencoder loss.   \n\nAnd note that the authors show that the proposed algorithm performs comparably to SGD, but not significantly better.  The classification result (Table 1) was obtained on the autoencoder features instead of training a classifier on the original inputs.  So it is not clear if the proposed algorithm is better for training the classifier, which may be of more interest. \n\n=============================================================\n\nThis paper presents an algorithm for training deep neural networks.  Instead of computing gradient of all layers and perform updates of all weight parameters at the same time, the authors propose to perform alternating optimization on weights of individual layers.  \n\nThe theoretical justification is obtained for single-hidden-layer auto-encoders.  Motivated by recent work by Hazan et al 2015, the authors developed the local-quasi-convexity of the objective w.r.t. the hidden layer weights for the generalized RELU activation.  As a result, the optimization problem over the single hidden layer can be optimized efficiently using the algorithm of Hazan et al 2015.  This itself can be a small, nice contribution. \n\nWhat concerns me is the extension to multiple layers.  Some questions are not clear from section 3.4:\n1.  Do we still have local-quasi-convexity for the weights of each layer, when there are multiple nonlinear layers above it?  A negative answer to this question will somewhat undermine the significance of the single-hidden-layer result. \n\n2. Practically, even if the authors can perform efficient optimization of weights in individual layers, when there are many layers, the alternating optimization nature of the algorithm can possibly result in overall slower convergence.  Also, since the proposed algorithm still uses gradient based optimizers for each layer, computing the gradient w.r.t. lower layers (closer to the inputs) are still done by backdrop, which has pretty much the same computational cost of the regular backdrop algorithm for updating all layers at the same time.  As a result, I am not sure if the proposed algorithm is on par with / faster than the regular SGD algorithm in actual runtime.  In the experiments, the authors plotted the training progress w.r.t. the minibatch iterations, I do not know if the minibatch iteration is a proxy for actual runtime (or number of floating point operations). \n\n3. In the experiments, the authors found the network optimized by the proposed algorithm generalize better than regular SGD.  Is this result consistent (across dataset, random initializations, etc), and can the authors elaborate the intuition behind?\n",1,1,1,1,1,1,1,1,1,-1
B1D6ty-A--R2,"The authors propose an alternating minimization framework for training autoencoders and encoder-decoder networks.  The central idea is that a single encoder-decoder network can be cast as an alternating minimization problem.  Each minimization problem is not convex but is quasi-convex and hence one can use stochastic normalized gradient descent to minimize w.r.t. each variable.  This leads to the proposed algorithm called DANTE which simply minimizes w.r.t. each variable using stochastic normalized gradient algorithm to minimize w.r.t. each variable The authors start with this idea and introduce a generalized ReLU which is specified via a subgradient function only whose local quasi-convexity properties are established.  They then extend these idea to multi-layer encoder-decoder networks by performing greedy layer-wise training and using the proposed algorithms for training each layer.  The ideas are interesting,  but I have some concerns regarding this work. \n\nMajor comments:\n\n1. When dealing with a 2 layer network where there are 2 matrices W_1, W_2 to optimize over, It is not clear to me why optimizing over W_1 is a quasi-convex optimization problem?  The authors seem to use the idea that solving a GLM problem is a quasi-convex optimization problem.  However, optimizing w.r.t. W_1 is definitely not a GLM problem, since W_1 undergoes two non-linear transformations one via \\phi_1 and another via \\phi_2. \n\n2. Theorem 3.4, 3.5 establish  SLQC properties with generalized RELU activations.  This is an interesting result, and useful in its own right.  However, it is not clear to me why this result is even relevant here. The main application of this paper is autoencoders, which are functions from R^d -> R^d. However, GLMs are functions from R^d ---> R. So, it is not at all clear to me how Theorem 3.4, 3.5 and eventually 3.6 are useful for the autoencoder problem that the authors care about.  Yes they are useful if one was doing 2-layer neural networks for binary classification, but it is not clear to me how they are useful for autoencoder problems. \n\n3. Experimental results for classification are not convincing enough.  If, one looks at Table 1. SGD outperforms DANTE on ionosphere dataset and is competent with DANTE on MNIST and USPS.  \n\n4. The results on reconstruction do not show any benefits for DANTE over SGD (Figure 3).  I would recommend the authors to rerun these experiments but truncate the iterations early enough.  If DANTE has better reconstruction performance than SGD with fewer iterations then that would be a positive result.",1,1,1,1,1,-1,1,1,1,-1
B1D6ty-A--R3,"In this paper an alternating optimization approach is explored for training Auto Encoders (AEs). \nThe authors treat each layer as a generalized linear model, and suggest to use the stochastic normalized GD of [Hazan et al., 2015] as the minimization algorithm in each (alternating) phase. \nThen they apply the suggested method to several single layer and multi layer AEs, comparing its performance to standard SGD.  The paper suggests an interesting approach and provides experimental evidence for its usefulness, especially for multi-layer AEs. \n\n\nSome comments on the theoretical part:\n-The theoretical part is partly misleading.  While it is true that every layer can be treated a generalized linear model, the SLQC property only applies for the last layer. \nRegarding the intermediate layers, we may indeed treat them as generalized linear models, but with non-monotone activations, and therefore the SLQC property does not apply. \nThe authors should mention this point. \n\n-Showing that generalized ReLU is SLQC with a polynomial dependence on the domain is interesting.  \n\n-It will be interesting if the authors can provide an analysis/relate to some theory related to alternating minimization of bi-quasi-convex objectives.  Concretely: Is there any known theory for such objectives?  What guarantees can we hope to achieve? \n\n\nThe extension to muti-layer AEs makes sense and seems to works quite well in practice .\n\nThe experimental part is satisfactory, and seems to be done in a decent manner.  \nIt will be useful if the authors could relate to the issue of parameter tuning for their algorithm. \nConcretely: How sensitive/robust is their approach compared to SGD with respect to hyperparameter misspecification.\n",1,1,1,1,1,-1,1,1,1,-1
B1DmUzWAW-R2,"The authors propose a model for sequence classification and sequential decision making.  The model interweaves attention layers, akin to those used by Vaswani et al, with temporal convolution.  The authors demonstrate superior performance on a variety of benchmark problems, including those for supervised classification and for sequential decision making. \n\nUnfortunately, I am not an expert in meta-learning, so I cannot comment on the difficulty of the tasks (e.g. Omniglot) used to evaluate the model or the appropriateness of the baselines the authors compare against (e.g. continuous control). \n\nThe experiment section definitely demonstrate the effort put into this work.  However, my primary concern is that the model seems somewhat lacking in novelty.  Namely, it interweaves the Vaswani style attention with with temporal convolutions (along with TRPO.  The authors claim that Vaswani model does not incoporate positional information, but from my understanding, it actually does so using positional encoding.  I also do not see why the Vaswani model cannot be lightly adapted for sequential decision making.  I think comparison to such a similar model would strengthen the novelty of this paper (e.g. convolution is a superior method of incorporating positional information). \n\nMy second concern is that the authors do not provide analysis and/or intuitions on why the proposed models outperform prior art in few-shot learning.  I think this information would be very useful to the community in terms of what to take away from this paper.  In retrospect, I wish the authors would have spent more time doing ablation studies than tackling more task domains. \n\nOverall, I am inclined to accept this paper on the basis of its experimental results.  However I am willing to adjust my review according to author response and the evaluation of the experiment section by other reviewers (who are hopefully more experienced in this domain). \n\nSome minor feedback/questions for the authors:\n- I would prefer mathematical equations as opposed to pseudocode formulation \n- In the experiment section for Omniglot, when the authors say \""1200 classes for training and 432 for testing\"", it sounds like the authors are performing zero-shot learning.  How does this particular model generalize to classes not seen during training?",1,1,1,1,1,1,1,1,1,-1
B1DmUzWAW-R3,"The paper proposes a general neural network structure that includes TC (temporal convolution) blocks and Attention blocks for meta-learning, specifically, for episodic task learning.  Through intensive experiments on various settings including few-shot image classification on Omniglot and Mini-ImageNet, and four reinforcement learning applications, the authors show that the proposed structure can achieve highly comparable performance wrt the corresponding specially designed state-of-the-art methods.  The experiment results seem solid and the proposed structure is with simple design and highly generalizable.  The concern is that the contribution is quite incremental from the theoretical side  though it involves large amount of experimental efforts, which could be impactful.  Please see the major comment below.\n\nOne major comment:\n- Despite that the work is more application oriented, the paper would have been stronger and more impactful if it includes more work on the theoretical side.  \nSpecifically, for two folds: \n(1) in general, some more work in investigating the task space would be nice.  The paper assumes the tasks are \u201crelated\u201d or \u201csimilar\u201d and thus transferrable; also particularly in Section 2, the authors define that the tasks follow the same distribution.  But what exactly should the distribution be like to be learnable and how to quantify such \u201crelated\u201d or \u201csimilar\u201d relationship across tasks?  \n(2) in particular, for each of the experiments that the authors conduct, it would be nice to investigate some more on when the proposed TC + Attention network would work better and thus should be used by the community; some questions to answer include: when should we prefer the proposed combination of TC + attention blocks over the other methods?  The result from the paper seems to answer with \u201cin all cases\u201d but then that always brings the issue of \u201coverfitting\u201d or parameter tuning issue. \n\nMore detailed comments:\n- On Page 1, \u201cthe optimal strategy for an arbitrary range of tasks\u201d lacks definition of \u201crange\u201d; also, in the setting in this paper, these tasks should share \u201csimilarity\u201d or follow the same \u201cdistribution\u201d and thus such \u201carbitrariness\u201d is actually constrained. \n\n- On Page 2, the notation and formulation for the meta-learning could be more mathematically rigid; the distribution over tasks is not defined.  It is understandable that the authors try to make the paradigm very generalizable; but the ambiguity or the abstraction over the \u201ctask distribution\u201d is too large to be meaningful.  One suggestion would be to split into two sections, one for supervised learning and one for reinforcement learning; but both share the same design paradigm, which is generalizable. \n\n- For results in Table 1 and Table 2, how are the confidence intervals computed?  Is it over multiple runs or within the same run?  It would be nice to make clear; in addition, I personally prefer either reporting raw standard deviations or conduct hypothesis testing with specified tests.  The confidence intervals may not be clear without elaboration; such is also concerning in the caption for Table 3 about claiming \u201cnot statistically-significantly different\u201d because no significance test is reported.  \n\n- At last, some more details in implementation would be nice (package availability, run time analysis); I suppose the package or the source code would be publicly available afterwards?",1,1,1,1,1,-1,1,1,1,1
B1e5ef-C--R1,"The main insight in this paper is that LSTMs can be viewed as producing a sort of sketch of tensor representations of n-grams.   This allows the authors to design a matrix that maps bag-of-n-gram embeddings into the LSTM embeddings.   They then show that the result matrix satisfies a restricted isometry condition.    Combining these results allows them to argue that the classification performance based on LSTM embeddings is comparable to that based on bag-of-n-gram embeddings.  \n\nI didn't check all the proof details, but based on my knowledge of compressed sensing theory, the results seem plausible.   I think the paper is a nice contribution to the theoretical analysis of LSTM word embeddings.",1,1,1,1,-1,-1,1,1,1,-1
B1e5ef-C--R2,"The interesting paper provides theoretical support for the low-dimensional vector embeddings computed using LSTMs or simple techniques, using tools from compressed sensing.   The paper also provides numerical results to support their theoretical findings.   The paper is well presented and organized.  \n\n-In theorem 4.1, the embedding dimension $d$ is depending on $T^2$, and it may scale poorly with respect to $T$.",1,1,1,1,1,-1,1,1,1,-1
B1e5ef-C--R3,"My review reflects more from the compressive sensing perspective, instead that of deep learners. \n\nIn general, I find many of the observations in this paper interesting.  However, this paper is not strong enough as a theory paper; rather, the value lies perhaps in its fresh perspective. \n\nThe paper studies text embeddings through the lens of compressive sensing theory.  The authors proved that, for the proposed embedding scheme, certain LSTMs with random initialization are at least as good as the linear classifiers; the theorem is almost a direction application of the RIP of random Rademacher matrices.  Several simplifying assumptions are introduced, which rendered the implication of the main theorem vague,  but it can serve as a good start for the hardcore statistical learning-theoretical analysis to follow. \n\nThe second contribution of the paper is the (empirical) observation that, in terms of sparse recovery of embedded words, the pretrained embeddings are better than random matrices, the latter being the main focus of compressive sensing theory.  Partial explanations are provided, again using results in compressive sensing theory.  In my personal opinion, the explanations are opaque and unsatisfactory.  An alternative route is suggested in my detailed review. \nFinally, extensive experiments are conducted and they are in accordance with the theory. \n\nMy most criticism regarding this paper is the narrow scope on compressive sensing, and this really undermines the potential contribution in Section 5. \n\nSpecifically, the authors considered only Basis Pursuit estimators for sparse recovery, and they used the RIP of design matrices as the main tool to argue what is explainable by compressive sensing and what is not.  This seems to be somewhat of a tunnel-visioning for me: There are a variety of estimators in sparse recovery problems, and there are much less restrictive conditions than RIP of the design matrices that guarantee perfect recovery. \n\nIn particular, in Section 5, instead of invoking [Donoho&Tanner 2005], I believe that a more plausible approach is through [Chandrasekaran et al. 2012].  There, a simple deterministic condition (the null space property) for successful recovery is proved.  It would be of direct interest to check whether such condition holds for a pretrained embedding (say GloVe) given some BoWs.  Furthermore, it is proved in the same paper that Restricted Strong Convexity (RSC) alone is enough to guarantee successful recovery; RIP is not required at all.  While, as the authors argued in Section 5.2, it is easy to see that pretrained embeddings can never possess RIP, they do not rule out the possibility of RSC. \n\nExactly the same comments above apply to many other common estimators (lasso, Dantzig selector, etc.) in compressive sensing which might be more tolerant to noise.  \n\nSeveral minor comments:\n\n1. Please avoid the use of \u201cinformation theory\u201d, especially \u201cclassical information theory\u201d, in the current context.   These words should be reserved to studies of Channel Capacity/Source Coding `a la Shannon.  I understand that in recent years people are expanding the realm of information theory, but as compressive sensing is a fascinating field that deserves its own name, there\u2019s no need to mention information theory here. \n\n2. In Theorem 4.1, please be specific about how the l2-regularization is chosen. \n\n3. In Section 4.1, please briefly describe why you need to extend previous analysis to the Lipschitz case.  I understood the necessity only through reading proofs. \n\n4. Can the authors briefly comment on the two assumptions in Section 4, especially the second one (on n- cooccurrence)? Is this practical? \n\n5. Page 1, there is a typo in the sentence preceding [Radfors et al., 2017]. \n\n6. Page 2, first paragraph of related work, the sentence \u201cOur method also closely related to ...\u201d is incomplete. \n\n7. Page 2, second paragraph of related work, \u201cPagliardini also introduceD a linear ...\u201d\n\n8.  Page 9, conclusion, the beginning sentence of the second paragraph is erroneous. \n\n[1] Venkat Chandrasekaran, Benjamin Recht, Pablo A. Parrilo, Alan S. Willsky, \u201cThe Convex Geometry of Linear Inverse Problems\u201d, Foundations of Computational Mathematics, 2012.",1,1,1,1,1,1,1,1,1,-1
B1EVwkqTW-R1,"After reading the rebuttal:\n\nThis paper does have encouraging results.   But as mentioned earlier, it still lacks systematic comparisons with existing (and strongest) baselines, and perhaps a better understanding the differences between approaches and the pros and cons.   The writing also needs to be improved.   So I think the paper is not ready for publication and my opinion remains.  \n===========================================================\n\nThis paper presents an algorithm for few shot learning.   The idea is to first learn representation of data using the siamese networks architecture, which predicts if a pair of two samples are similar (e.g., from the same class) or not using a SVM hinge loss, and then finetune the classifier using few labeled examples (with possibly a different set of labels).   I think the idea of representation learning using a somewhat artificial task makes sense in this setting. \n\nI have several concerns for this submission.  \n1. I am not very familiar with the literature of few shot learning.   I think a very related approach that learns the representation using pretty much the same information is the contrastive loss:\n-- Hermann and Blunsom.   Multilingual Distributed Representations without Word Alignment. ICLR 2014. \nThe intuition is similar: similar pairs shall have higher similarity in the learned representation, than dissimilar pairs, by a large margin.  This approach is useful even when there is only weak supervision to provide the \""similarity/dissimilarity\"" information.  I wonder how does this approach compare with the proposed method. \n\n2. The experiments are conducted on a small dataset OMNIGLOT and TIMIT.  I do not understand why the compared methods are not consistently used in both experiments.  Also, the experiment of speaker classification on TIMIT (where the inputs are audio segments with different durations and sampling frequency) is a quite nonstandard task; I do not have a sense of how challenging it is.  It is not clear why CNN transfer learning (the authors did not give details about how it works) performs even worse than the non-deep baseline, yet the proposed method achieves very high accuracy.  It would be nice to understand/visualize what information have been extracted in the representation learning phase.  \n\n3. Relatively minor: The writing of this paper is readable,  but could be improved.  It sometimes uses vague/nonstandard terminology (\""parameterless\"") and statement.  The term \""siamese kernel\"" is not very informative: yes, you are learning new representations of data using DNNs, but this feature mapping does not have the properties of RKHS; also you are not solving the SVM dual problem as one typically does for kernel SVMs.  In my opinion the introduction of SVM can be shortened, and more focuses can be put on related deep learning methods and few shot learning.",1,1,1,1,1,1,1,1,1,-1
B1EVwkqTW-R2,"Make SVM great again with Siamese kernel for few-shot learning  \n\n** PAPER SUMMARY **\n\nThe author proposes to combine siamase networks with an SVM for pair classification.  The proposed approach is evaluated on few shot learning tasks, on omniglot and timit.   \n\n\n** REVIEW SUMMARY **\n\nThe paper is readable  but it could be more fluent.  It lacks a few references and important technical aspects are not discussed.  It contains a few errors.  Empirical contribution seems inflated on omniglot as the authors omit other papers reporting better results.  Overall, the contribution is modest at best.v\n\n** DETAILED REVIEW **\n\nOn mistakes, it is wrong to say that an SVM is a parameterless classifier.  It is wrong to cite (Boser et al 92) for the soft-margin SVM.  I think slack variables come from (Cortes et al 95).  \""consistent\"" has a specific definition in machine learning https://en.wikipedia.org/wiki/Consistent_estimator , you must use a different word in 3.2.  You mention that a non linear SVM need a similarity measure, it actually need a positive definite kernel which has a specific definition, https://en.wikipedia.org/wiki/Positive-definite_kernel . \n\nOn incompleteness, it is not obvious how the classifier is used at test time.  Could you explain how classes are predicted given a test problem?  The setup of the experiments on TIMIT is extremely unclear.  What are the class you are interested in?  How many classes and examples does the testing problems have?  \n\nOn clarity, I do not understand why you talk again about non-linear SVM in the last paragraph of 3.2. since you mention at the end of page 4 that you will only rely on linear SVMs for computational reasons.  You need to mention explicitely somewhere that (w,\\theta) are optimized jointly.  The sentence \""this paper investigates only the one versus rest approach\"" is confusing, as you have only two classes from the SVM perspective i.e. pairs (x1,x2) where both examples come from the same class and pairs (x1,x2) where they come from different class.  So you use a binary SVM, not one versus rest.  You need to find a better justification for using L2-SVM than \""L2-SVM loss variant is considered to be the best by the author of the paper\"", did you try classical SVM and found them performing worse?  Also could you motivate your choice for L1 norm as opposed to L2 in Eq 3? \n\nOn empirical evaluation, I already mentioned that it impossible to understand what the classification problem on TIMIT is.  I suspect it might be speaker identification.  So I will focus on the omniglot experiments.  \n\nFew-Shot Learning Through an Information Retrieval Lens, Eleni Triantafillou, Richard Zemel, Raquel Urtasun, NIPS 2017 [arxiv July'17]\n\nand the reference therein give a few more recent baselines than your table.  Some of the results are better   This dataset offers a clearer experimental setup than your TIMIT setting and has abundant published baseline results.   Also, most work typically use omniglot as a proof of concept and consider mini-imagenet as a more challenging set.",1,1,1,1,1,1,1,1,1,1
B1EVwkqTW-R3,"Summary: \nThe paper proposes to pre-train a deep neural network to learn a similarity function and use the features obtained by this pre-trained network as input to an SVM model.  The SVM is trained for the final classification task at hand using the last layer features of the deep network.  The motivation behind all this is to learn the input features to the SVM as opposed to hand-crafting them, and use the generalization ability of the SVM to do well on tasks which have only a handful of training examples.  The authors apply their technique to two datasets, namely, the Omniglot dataset and the TIMIT dataset and show that their model does a reasonable job in these two tasks.  \n\nWhile the paper is reasonably clearly written and easy to read  I have a number of objections to it.  \n\nFirst, I did not see any novel idea presented in this paper.  Lots of people have tried pre-training a neural network on auxiliary task(s) and using the features from it as input to the final SVM classifier.  People have also specifically tried to train a siamese network and use its features as input to the SVM.  These works go way back to the years 2005 - 2007, when deep learning was not called deep learning.  Unless I have missed something completely, I did not see any novel idea proposed in this paper.  \n\nSecond, the experiments are quite underwhelming and does not fully support the superiority claims of the proposed approach.  For example, the authors compare their model against rather weak baselines.  I would have liked the experiments to be more thorough, with comparison to the state of the art models for the two datasets. \n""",1,1,1,1,-1,1,1,1,1,-1
B1Gi6LeRZ-R1,"Overall: Authors defined a new learning task that requires a DNN to predict mixing ratio between sounds from two different classes.  Previous approaches to training data mixing are (1) from random classes, or (2) from the same class.  The presented approach mixes sounds from specific pairs of classes to increase discriminative power of the final learned network.  Results look like significant improvements over standard learning setups. \n\nDetailed Evaluation: The approach presented is simple, clearly presented, and looks effective on benchmarks.  In terms of originality, it is different from warping training example for the same task and it is a good extension of previously suggested example mixing procedures with a targeted benefit for improved discriminative power.  The authors have also provided extensive analysis from the point of views (1) network architecture, (2) mixing method, (3) number of labels / classes in mix, (4) mixing layers -- really well done due-diligence across different model and task parameters. \n\nMinor Asks:\n(1) Clarification on how the error rates are defined.  Especially since the standard learning task could be 0-1 loss and this new BC learning task could be based on distribution divergence (if we're not using argmax as class label). \n(2) #class_pairs targets as analysis - The number of epochs needed is naturally going to be higher since the BC-DNN has to train to predict mixing ratios between pairs of classes.  Since pairs of classes could be huge if the total number of classes is large, it'll be nice to see how this scales.  I.e. are we talking about a space of 10 total classes or 10000 total classes?  How does num required epochs get impacted as we increase this class space? \n(3) Clarify how G_1/20 and G_2/20 is important / derived - I assume it's unit conversion from decibels. \n(4) Please explain why it is important to use the smoothed average of 10 softmax predictions in evaluation...  what happens if you just randomly pick one of the 10 crops for prediction?",1,1,1,1,1,1,1,1,1,-1
B1Gi6LeRZ-R2,"This manuscript proposes a method to improve the performance of a generic learning method by generating \""in between class\"" (BC) training samples.  The manuscript motivates the necessity of such technique and presents the basic intuition.  The authors show how the so-called BC learning helps training different deep architectures for the sound recognition task. \n\nMy first remark regards the presentation of the technique.  The authors argue that it is not a data augmentation technique, but rather a learning method.  I strongly disagree with this statement, not only because the technique deals exactly with augmenting data, but also because it can be used in combination to any learning method (including non-deep learning methodologies). \n\nIn this regard, I would have expected comparison with other state-of-the-art data augmentation techniques.  The usefulness of the BC technique is proven to a certain extent (see paragraph below) but there is not comparison with state-of-the-art.  In other words, the authors do not compare the proposed method with other methods doing data augmentation.  This is crucial to understand the advantages of the BC technique. \n\nThere is a more fundamental question for which I was not able to find an explicit answer in the manuscript.  Intuitively, the diagram shown in Figure 4 works well for 3 classes in dimension 2.  If we add another class, no matter how do we define the borders, there will be one pair of classes for which the transition from one to another will pass through the region of a third class.  The situation worsens with more classes.  However, this can be solved by adding one dimension, 4 classes and 3 dimensions seems something feasible.  One can easily understand that if there is one more class than the number of dimensions, the assumption should be feasible, but beyond it starts to get problematic.  This discussion does not appear at all in the manuscript and it would be an important limitation of the method, specially when dealing with large-scale data sets. \n\nOverall I believe the paper is not mature enough for publication. \n\nSome minor comments:\n- 2.1: We introduce --> We discussion\n- Pieczak 2015a did not propose the extraction of MFCC. \n- the x_i and t_i of section 3.2.2 should not be denoted with the same letters as in 3.2.1. \n- The correspondence with a semantic feature space is too pretentious, specially since no experiment in this direction is shown. \n- I understand that there is no mixing in the test phase, perhaps it would be useful to recall it.",1,1,1,1,1,1,1,1,-1,-1
B1Gi6LeRZ-R3,"The propose data augmentation and BC learning is relevant, much robust than frequency jitter or simple data augmentation.  \n\nIn equation 2, please check the measure of the mixture.  Why not simply use a dB criteria ? \n\nThe comments about applying a CNN to local features or novel approach to increase sound recognition could be completed with some ICLR 2017 work towards injected priors using Chirplet Transform. \n\nThe authors might discuss more how to extend their model to image recognition, or at least of other modalities as suggested. \n\nSection 3.2.2 shall be placed later on, and clarified. \n\nDiscussion on mixing more than two sounds leads could be completed by associative properties, we think... ?\n",1,-1,1,1,1,1,1,-1,1,-1
B1gJ1L2aW-R1,"The paper considers a problem of adversarial examples applied to the deep neural networks.  The authors conjecture that the intrinsic dimensionality of the local neighbourhood of adversarial examples significantly differs from the one of normal (or noisy) examples.  More precisely, the adversarial examples are expected to have intrinsic dimensionality much higher than the normal points (see Section 4).   Based on this observation they propose to use the intrinsic dimensionality as a way to separate adversarial examples from the normal (and noisy) ones during the test time.  In other words, the paper proposes a particular approach for the adversarial defence. \n\nIt turns out that there is a well-studied concept in the literature capturing the desired intrinsic dimensionality: it is called the local intrinsic dimensionality (LID, Definition 1) .  Moreover, there is a known empirical estimator of LID, based on the k-nearest neighbours.  The authors propose to use this estimator in computing the intrinsic dimensionalities for the test time examples.  For every test-time example X the resulting Algorithm 1 computes LID estimates of X activations computed for all intermediate layer of DNN. These values are finally used as features in classifying adversarial examples from normal and noisy ones.  \n\nThe authors empirically evaluate the proposed technique across multiple state-of-the art adversarial attacks, 3 datasets (MNIST, CIFAR10, and SVHN) and compare their novel adversarial detection technique to 2 other ones recently reported in the literature.  The experiments support the conjecture mentioned above and show that the proposed technique *significantly* improves the detection accuracy compared to 2 other methods across all attacks and datasets (see Table 1). \n\nInterestingly, the authors also test whether adversarial attacks can bypass LID-based detection methods by incorporating LID in their design.  Preliminary results show that even in this case the proposed method manages to detect adversarial examples most of the time. In other words, the proposed technique is rather stable and can not be easily exploited. \n\nI really enjoyed reading this paper.  All the statements are very clear, the structure is transparent and easy to follow.  The writing is excellent.  I found only one typo (page 8, \""We also NOTE that...\""),  otherwise I don't actually have any comments on the text. \n\nUnfortunately, I am not an expert in the particular field of adversarial examples, and can not properly assess the conceptual novelty of the proposed method.  However, it seems that it is indeed novel and given rather convincing empirical justifications, I would recommend to accept the paper. \n",1,1,-1,1,-1,-1,1,1,1,-1
B1gJ1L2aW-R2,"This paper tried to analyze the subspaces of the adversarial examples neighborhood.  More specifically, the authors used Local Intrinsic Dimensionality to analyze the intrinsic dimensional property of the subspaces.   The characteristics and theoretical analysis of the proposed method are discussed and explained.   This paper helps others to better understand the vulnerabilities of DNNs.",1,-1,-1,1,-1,-1,1,-1,-1,-1
B1gJ1L2aW-R3,"The authors clearly describe the problem being addressed in the manuscript and motivate their solution very clearly.   The proposed solution seems very intuitive and the empirical evaluations demonstrates its utility.   My main concern is the underlying assumption (if I understand correctly) that the adversarial attack technique that the detector has to handle needs to be available at the training time of the detector.   Especially since the empirical evaluations are designed in such a way where the training and test data for the detector are perturbed with the same attack technique.  However, this does not invalidate the contributions of this manuscript. \n\nSpecific comments/questions:\n- (Minor) Page 3, Eq 1: I think the expansion dimension cares more about the probability mass in the volume rather than the volume itself even in the Euclidean setting. \n- Section 4: The different pieces of the problem (estimation, intuition for adversarial subspaces, efficiency) are very well described. \n- Alg 1, L3: Is this where the normal exmaples are converted to adversarial examples using some attack technique?  \n- Alg 1, L12: Is LID_norm computed using a leave-one-out estimate?  Otherwise, r_1(.) for each point is 0, leading to a somewhat \""under-estimate\"" of the true LID of the normal points in the training set.  I understand that it is not an issue in the test set. \n- Section 4 and Alg 1: S we do not really care about the \""labels/targets\"" of the examples.  All examples in the dataset are considered \""normal\"" to start with.   Is this assuming that the \""initial training set\"" which is used to obtain the \""pre-trained DNN\"" free of adversarial examples?  \n- Section 5, Experimental Setup: Seems like normal points in the test set would get lesser values if we are not doing the \""leave-one-out\"" version of the estimation.  \n- Section 5: The authors have done a great job at evaluating every aspect of the proposed method.\n",1,1,1,1,1,-1,1,1,1,-1
B1hcZZ-AW-R1,"This paper proposes to use reinforcement learning instead of pre-defined heuristics to determine the structure of the compressed model in the knowledge distillation process. \n\nThe draft is well-written, and the method is clearly explained.  However, I have the following concerns for this draft:\n\n1. The technical contribution is not enough.  First, the use of reinforcement learning is quite straightforward.  Second, the proposed method seems not significantly different from the architecture search method in [1][2] \u2013 their major difference seems to be the use of \u201cremove\u201d instead of \u201cadd\u201d when manipulating the parameters.  It is unclear whether this difference is substantial, and whether the proposed method is better than the architecture search method. \n\n2. I also have concern with the time efficiency of the proposed method.  Reinforcement learning involves multiple rounds of knowledge distillation, and each knowledge distillation is an independent training process that requires many rounds of forward and backward propagations.  Therefore, the whole reinforcement learning process seems very time-consuming and difficult to be generalized to big models and large datasets (such as ImageNet).  It would be necessary for the authors to make direct discussions on this issue, in order to convince others that their proposed method has practical value. \n\n[1] Zoph, Barret, and Quoc V. Le. \""Neural architecture search with reinforcement learning.\"" ICLR (2017).\n[2] Baker, Bowen, et al. \""Designing Neural Network Architectures using Reinforcement Learning.\"" ICLR (2017).\n",1,1,1,1,1,1,1,1,1,-1
B1hcZZ-AW-R2,"Summary:\nThe manuscript introduces a principled way of network to network compression, which uses policy gradients for optimizing two policies which compress a strong teacher into a strong but smaller student model.  The first policy, specialized on architecture selection, iteratively removes layers, starting with architecture of the teacher model.  After the first policy is finished, the second policy reduces the size of each layer by iteratively outputting shrinkage ratios for hyperparameters such as kernel size or padding.  This organization of the action space, together with a smart reward design achieves impressive compression results, given that this approach automates tedious architecture selection.  The reward design favors low compression/high accuracy over high compression/low performance while the reward still monotonically increases with both compression and accuracy.  As a bonus, the authors also demonstrate how to include hard constraints such as parameter count limitations into the reward model and show that policies trained on small teachers generalize to larger teacher models. \n\nReview:\nThe manuscript describes the proposed algorithm in great detail and the description is easy to follow.  The experimental analysis of the approach is very convincing and confirms the author\u2019s claims.  \nUsing the teacher network as starting point for the architecture search is a good choice, as initialization strategies are a critical component in knowledge distillation.  I am looking forward to seeing work on the research goals outlined in the Future Directions section. \n\nA few questions/comments:\n1) I understand that L_{1,2} in Algorithm 1 correspond to the number of layers in the network, but what do N_{1,2} correspond to?  Are these multiple rollouts of the policies?  If so, shouldn\u2019t the parameter update theta_{{shrink,remove},i} be outside the loop over N and apply the average over rollouts according to Equation (2)?  I think I might have missed something here. \n2) Minor: some of the citations are a bit awkward, e.g. on page 7: \u201calgorithm from Williams Williams (1992).  I would use the \\citet command from natbib for such citations and \\citep for parenthesized citations, e.g. \u201c... incorporate dark knowledge (Hinton et al., 2015)\u201d or \u201cThe MNIST (LeCun et al., 1998) dataset...\u201d  \n3) In Section 4.6 (the transfer learning experiment), it would be interesting to compare the performance measures for different numbers of policy update iterations. \n4) Appendix: Section 8 states \u201cBelow are the results\u201d, but the figure landed on the next page.  I would either try to force the figures to be output at that position (not in or after Section 9) or write \""Figures X-Y show the results\"".  Also in Section 11, Figure 13 should be referenced with the \\ref command \n5) Just to get a rough idea of training time: Could you share how long some of the experiments took with the setup you described (using 4 TitanX GPUs)? \n6) Did you use data augmentation for both teacher and student models in the CIFAR10/100 and Caltech256 experiments? \n7) What is the threshold you used to decide if the size of the FC layer input yields a degenerate solution? \n\nOverall, this manuscript is a submission of exceptional quality and if minor details of the experimental setup are added to the manuscript, I would consider giving it the full score.",1,1,1,1,1,-1,1,1,1,1
B1hcZZ-AW-R3,"On the positive side the paper is well written and the problem is interesting.  \n\nOn the negative side there is very limited innovation in the techniques proposed, that are indeed small variations of existing methods. \n",1,1,-1,1,-1,-1,1,1,1,-1
B1IDRdeCW-R1,"This paper investigates numerically and theoretically the reasons behind the empirical success of binarized neural networks.  Specifically, they observe that:\n\n(1) The angle between continuous vectors sampled from a spherical symmetric distribution and their binarized version is relatively small in high dimensions (proven to be about 37 degrees when the dimension goes to infinity), and this demonstrated empirically to be true for the binarized weight matrices of a convenet.  \n\n(2) Except the first layer, the dot product of weights*activations in each layer is highly correlated with the dot product of (binarized weights)*activations in each layer.  There is also a strong correlation between (binarized weights)*activations and (binarized weights)*(binarized activations).  This is claimed to entail that the continuous weights of the binarized neural net approximate the continuous weights of a non-binarized neural net trained in the same manner. \n\n(3) To correct the issue with the first layer in (2) it is suggested to use a random rotation, or simply use continues weights in that layer. \n\nThe first observation is interesting, is explained clearly and convincingly, and is novel to the best of my knowledge. \n\nThe second observation is much less clear to me.  Specifically,\na.\tThe author claim that \u201cA sufficient condition for \\delta u to be the same in both cases is L\u2019(x = f(u)) ~ L\u2019(x = g(u))\u201d.  However, I\u2019m not sure if I see why this is true: in a binarized neural net, u also changes, since the previous layers are also binarized.  \nb.\tRelated to the previous issue, it is not clear to me if in figure 3 and 5, did the authors binarize the activations of that specific layer or all the layers? \nc.\tFor BNNs, where both the weights and activations are binarized, shouldn\u2019t we compare weights*activations to (binarized weights)*(binarized activations)? \nd.\tTo make sure, in figure 4, the permutation of the activations was randomized (independently) for each data sample?   \n\nThe third observation seems less useful to me.   Though a random rotation may improve angle preservation in certain cases (as demonstrated in Figure 4), it may hurt classification performance (e.g., distinguishing between 6 and 9 in MNIST).   Furthermore, since it uses non-binary operations, it is not clear if this rotation may have some benefits (in terms of resource efficiency) over simply keeping the input layer non-binarized.  \n\nTo summarize, the first part is interesting and nice,   the second part was not clear to me, and the last part does not seem very useful.   \n\n%%% After Author's response %%%\na. My mistake. Perhaps it should be clarified in the text that u are the weights.  I thought that g(u) is a forward propagation function, and therefore u is the neural input (i.e., pre-activation). \n\nFollowing the author's response and revisions, I have raised my grade.\n",1,1,1,1,1,-1,1,1,1,-1
B1IDRdeCW-R2,"This paper presents three observations to understand binary network in Courbariaux, Hubara et al. (2016).  My main concerns are on the usage of the given observations.  \n\n1. Can the observations be used to explain more recent works? \n\nIndeed, Courbariaux, Hubara et al. (2016) is a good and pioneered work on the binary network.  However, as the authors mentioned, there are more recent works which give better performance than this one.  For example, we can use +1, 0, -1 to approximate the weights.  Besides, [a] has also shown a carefully designed post-processing binary network can already give very good performance.  So, how can the given observations be used to explain more recent works? \n\n2. How can the given observations be used to improve Courbariaux, Hubara et al. (2016)? \n\nThe authors call their findings theory.  From this perspective, I wish to see more mathematical analysis rather than just doing experiments and showing some interesting observations.  Besides, giving interesting observations is not good enough.  I wish to see how they can be used to improve binary networks. \n\nReference\n[a]. Network sketching: exploiting binary structure in deep CNNs. CVPR 2017",1,1,1,1,1,1,1,1,-1,-1
B1IDRdeCW-R3,"This paper tries to analyze the effectiveness of binary nets from a perspective originated from the angular perturbation that binarization process brings to the original weight vector.  It further explains why binarization is able to preserve the model performance by analyzing the weight-activation dot product with \""Dot Product Proportionality Property. \"" It also proposes \""Generalized Binarization Transformation\"" for the first layer of a neural network. \n\nIn general, I think the paper is written clearly and in detail.  Some typos and minor issues are listed in the \""Cons\"" part below. \n\nPros:\nThe authors lead a very nice exploration into the binary nets in the paper, from the most basic analysis on the converging angle between original and binarized weight vectors, to how this convergence could affect the weight-activation dot product, to pointing out that binarization affects differently on the first layer.  Many empirical and theoretical proofs are given, as well as some practical tricks that could be useful for diagnosing binary nets in the future. \n\nCons:\n* it seems that there are quite some typos in the paper, for example:\n    1. Section 1, in the second contribution, there are two \""then\""s. \n    2. Section 1, the citation format of \""Bengio et al. (2013)\"" should be \""(Bengio et al. 2013)\"". \n* Section 2, there is an ordering mistake in introducing Han et al.'s work, DeepComporession actually comes before the DSD.  \n* Fig 2(c), the correlation between the theoretical expectation and angle distribution from (b) seems not very clear. \n* In appendix, Section 5.1, Lemma 1. Could you include some of the steps in getting g(\\row) to make it clearer?  I think the length of the proof won't matter a lot since it is already in the appendix, but it makes the reader a lot easier to understand it.\n",1,1,1,1,1,1,1,1,1,1
B1jscMbAW-R1,"This paper proposes to add new inductive bias to neural network architecture - namely a divide and conquer strategy know from algorithmics.  Since introduced model has to split data into subsets, it leads to non-differentiable paths in the graph, which authors propose to tackle with RL and policy gradients.  The whole model can be seen as an RL agent, trained to do splitting action on a set of instances in such a way, that jointly trained predictor T quality is maximised (and thus its current log prob: log p(Y|P(X)) becomes a reward for an RL agent).  Authors claim that model like this (strengthened with pointer networks/graph nets etc. depending on the application) leads to empirical improvement on three tasks - convex hull finding, k-means clustering and on TSP.   However, while results on convex hull task are good,  k-means ones use a single, artificial problem (and do not test DCN, but rather a part of it), and on TSP DCN performs significantly worse than baselines in-distribution, and is better when tested on bigger problems than it is trained on.  However the generalisation scores themselves are pretty bad thus it is not clear if this can be called a success story. \n\nI will be happy to revisit the rating if the experimental section is enriched. \n\nPros:\n- very easy to follow idea and model \n- simple merge or RL and SL in an end-to-end trainable model\n- improvements over previous solutions \n\nCons:\n- K-means experiments should not be run on artificial dataset, there are plenty of benchmarking datasets out there.  In current form it is just a proof of concept experiment rather than evaluation (+ if is only for splitting, not for the entire architecture proposed).  It would be also beneficial to see the score normalised by the cost found by k-means itself (say using Lloyd's method), as otherwise numbers are impossible to interpret.  With normalisation, claiming that it finds 20% worse solution than k-means is indeed meaningful.  \n- TSP experiments show that \""in distribution\"" DCN perform worse than baselines, and when generalising to bigger problems they fail more gracefully, however the accuracies on higher problem are pretty bad, thus it is not clear if they are significant enough to claim success.  Maybe TSP is not the best application of this kind of approach (as authors state in the paper - it is not clear how merging would be applied in the first place).  \n- in general - experimental section should be extended, as currently the only convincing success story lies in convex hull experiments \n\nSide notes:\n- DCN is already quite commonly used abbreviation for \""Deep Classifier Network\"" as well as \""Dynamic Capacity Network\"", thus might be a good idea to find different name. \n- please fix \\cite calls to \\citep, when authors name is not used as part of the sentence, for example:\nGraph Neural Network Nowak et al. (2017) \nshould be\nGraph Neural Network (Nowak et al. (2017)) \n\n# After the update\n\nEvaluation section has been updated threefold:\n- TSP experiments are now in the appendix rather than main part of the paper \n- k-means experiments are Lloyd-score normalised and involve one Cifar10 clustering \n- Knapsack problem has been added \n\nPaper significantly benefited from these changes, however experimental section is still based purely on toy datasets (clustering cifar10 patches is the least toy problem, but if one claims that proposed method is a good clusterer one would have to beat actual clustering techniques to show that), and in both cases simple problem-specific baseline (Lloyd for k-means, greedy knapsack solver) beats proposed method.  I can see the benefit of trainable approach here,  the fact that one could in principle move towards other objectives, where deriving Lloyd alternative might be hard; however current version of the paper still does not show that. \n\nI increased rating for the paper,  however in order to put the \""clear accept\"" mark I would expect to see at least one problem where proposed method beats all basic baselines (thus it has to either be the problem where we do not have simple algorithms for it, and then beating ML baseline is fine; or a problem where one can beat the typical heuristic approaches).\n\n",1,1,1,1,1,-1,1,1,1,1
B1jscMbAW-R2,"This paper studies problems that can be solved using a dynamic programming approach and proposes a neural network architecture called Divide and Conquer Networks (DCN) to solve such problems.  The network has two components: one component learns to split the problem and the other learns to combine solutions to sub-problems.  Using this setup, the authors are able to beat sequence to sequence baselines on problems that are amenable to such an approach.  In particular the authors test their approach on computing convex hulls, computing a minimum cost k-means clustering, and the Euclidean Traveling Salesman Problem (TSP) problem.  In all three cases, the proposed solution outperforms the baselines on larger problem instances.",1,-1,-1,1,-1,-1,1,-1,-1,-1
B1jscMbAW-R3,"Summary of paper:\n\nThe paper proposes a unique network architecture that can learn divide-and-conquer strategies to solve algorithmic tasks. \n\nReview:\n\nThe paper is clearly written.  It is sometimes difficult to communicate ideas in this area, so I appreciate the author's effort in choosing good notation.  Using an architecture to learn how to split the input, find solutions, then merge these is novel.  Previous work in using recursion to solve problems (Cai 2017) used explicit supervision to learn how to split and recurse.  The ideas and formalism of the merge and partition operations are valuable contributions.   \n\nThe experimental side of the paper is less strong.   There are good results on the convex hull problem, which is promising.   There should also be a comparison to a k-means solver in the k-means section as an additional baseline.   I'm also not sure TSP is an appropriate problem to demonstrate the method's effectiveness.   Perhaps another problem that has an explicit divide and conquer strategy could be used instead.   It would also be nice to observe failure cases of the model.   This could be done by visually showing the partition constructed or seeing how the model learned to merge solutions.. \n\nThis is a relatively new area to tackle, so while the experiments section could be strengthened, I think the ideas present in the paper are important and worth publishing. \n\nQuestions:\n\n1. What is \\rho on page 4?.  I assume it is some nonlinearity, but this was not specified. \n2. On page 5, it says the merge block takes as input two sequences.  I thought the merge block was defined on sets?  \n\nTypos:\n1. Author's names should be enclosed in parentheses unless part of the sentence. \n2. I believe \""then\"" should be removed in the sentence \""...scale invariance, then exploiting...\"" on page 2.",1,1,1,1,1,1,1,1,1,-1
B1J_rgWRW-R1,"This paper presents several theoretical results regarding the expressiveness and learnability of ReLU-activated deep neural networks.  I summarize the main results as below:\n\n(1) Any piece-wise linear function can be represented by a ReLU-acteivated DNN.  Any smooth function can be approximated by such networks. \n\n(2) The expressiveness of 3-layer DNN is stronger than any 2-layer DNN. \n\n(3) Using a polynomial number of neurons, the ReLU-acteivated DNN can represent a piece-wise linear function with exponentially many pieces \n\n(4) The ReLU-activated DNN can be learnt to global optimum with an exponential-time algorithm. \n\nAmong these results (1), (2), (4) are sort of known in the literature.  This paper extends the existing results in some subtle ways.  For (1), the authors show that the DNN has a tighter bound on the depth. For (4), although the algorithm is exponential-time, it guarantees to compute the global optimum. \n\nThe stronger results of (1), (2), (4) all rely on the specific piece-wise linear nature of ReLU.  Other than that, I don't get much more insight from the theoretical result.  When the input dimension is n, the representability result of (1) fails to show that a polynomial number of neurons is sufficient.  Perhaps an exponential number of neurons is necessary in the worst case, but it will be more interesting if the authors show that under certain conditions a polynomial-size network is good enough. \n\nResult (3) is more interesting as it is a new result.  The authors present a constructive proof to show that ReLU-activated DNN can represent many linear pieces.   However, the construction seems artificial and these functions don't seem to be visually very complex. \n\nOverall, this is an incremental work in the direction of studying the representation power of neural networks.  The results might be of theoretical interest",1,1,1,1,1,1,1,1,1,-1
B1J_rgWRW-R2,"The paper presents a series of definitions and results elucidating details about the functions representable by ReLU networks, their parametrisation, and gaps between deep and shallower nets.  \n\nThe paper is easy to read,  although it does not seem to have a main focus (exponential gaps vs. optimisation vs. universal approximation).  The paper makes a nice contribution to the details of deep neural networks with ReLUs,  although I find the contributed results slightly overstated.  The 1d results are not difficult to derive from previous results.  The advertised new results on the asymptotic behaviour assume a first layer that dominates the size of the network.  The optimisation method appears close to brute force and is limited to 2 layers.  \n\nTheorem 3.1 appears to be easily deduced from the results from Montufar, Pascanu, Cho, Bengio, 2014.  For 1d inputs, each layer will multiply the number of regions at most by the number of units in the layer, leading to the condition w\u2019 \\geq w^{k/k\u2019}.  Theorem 3.2 is simply giving a parametrization of the functions, removing symmetries of the units in the layers.  \n\nIn the list at the top of page 5. Note that, the function classes might be characterized in terms of countable properties, such as the number of linear regions as discussed in MPCB, but still they build a continuum of functions.  Similarly, in page 5 ``Moreover, for fixed n,k,s, our functions are smoothly parameterized''.  This should not be a surprise.  \n\nIn the last paragraph of Section 3 ``m = w^k-1'' This is a very big first layer.  This also seems to subsume the first condition, s\\geq  w^k-1 +w(k-1) for the network discussed in Theorem 3.9.  In the last paragraph of Section 3 ``To the best of our knowledge''. . In the construction presented here, the network\u2019s size is essentially in the layer of size m..  Under such conditions, Corollary 6 of MPCB also reads as s^n..  Here it is irrelevant whether one artificially increases the depth of the network by additional, very narrow, layers, which do not contribute to the asymptotic number of units..  \n\nThe function class Zonotope is a composition of two parts.  It would be interesting to consider also a single construction, instead of the composition of two constructions..  \n\nTheorem 3.9 (ii) it would be nice to have a construction where the size becomes 2m + wk when k\u2019=k..  \n\nSection 4, while interesting, appears to be somewhat disconnected from the rest of the paper..  \n\nIn Theorem 2.3. explain why the two layer case is limited to n=1..  \n\nAt some point in the first 4 pages it would be good to explain what is meant by ``hard\u2019\u2019 functions (e.g. functions that are hard to represent, as opposed to step functions, etc.) \n""",1,1,1,1,1,1,1,1,1,-1
B1J_rgWRW-R3,"The paper presents an analysis and characterization of ReLU networks (with a linear final layer) via the set of functions these networks can model, especially focusing on the set of \u201chard\u201d functions that are not easily representable by shallower networks.   It makes several important contributions, including extending the previously published bounds by Telgarsky et al. to tighter bounds for the special case of ReLU DNNs, giving a construction for a family of hard functions whose affine pieces scale exponentially with the dimensionality of the inputs, and giving a procedure for searching for globally optimal solution of a 1-hidden layer ReLU DNN with linear output layer and convex loss.   I think these contributions warrant publishing the paper at ICLR 2018.   The paper is also well written, a bit dense in places, but overall well organized and easy to follow.  \n\nA key limitation of the paper in my opinion is that typically DNNs do not contain a linear final layer.   It will be valuable to note what, if any, of the representation analysis and global convergence results carry over to networks with non-linear (Softmax, e.g.) final layer.   I also think that the global convergence algorithm is practically unfeasible for all but trivial use cases due to terms like D^nw, would like hearing authors\u2019 comments in case I\u2019m missing some simplification. \n\nOne minor suggestion for improving readability is to explicitly state, whenever applicable, that functions under consideration are PWL.   For example, adding PWL to Theorems and Corollaries in Section 3.1 will help.    Similarly would be good to state, wherever applicable, the DNN being discussed is a ReLU DNN.",1,1,1,1,1,-1,1,1,1,-1
B1KJJf-R--R1,"This paper presents a seq2Tree model to translate a problem statement in natural \nlanguage to the corresponding functional program in a DSL.  The model uses\nan RNN encoder to encode the problem statement and uses an attention-based\ndoubly recurrent network for generating tree-structured output.  The learnt model is \nthen used to perform Tree-beam search using a search algorithm that searches \nfor different completion of trees based on node types.  The evaluation is performed\non a synthetic dataset and shows improvements over seq2seq baseline approach. \n\nOverall, this paper tackles an important problem of learning programs from \nnatural language and input-output example specifications.  Unlike previous\nneural program synthesis approaches that consider only one of the specification \nmechanisms (examples or natural language), this paper considers both of them \nsimultaneously.  However, there are several issues both in the approach and the \ncurrent preliminary evaluation, which unfortunately leads me to a reject score, \nbut the general idea of combining different specifications is quite promising. \n\nFirst, the paper does not compare against a very similar approach of Parisotto et al.\nNeuro-symbolic Program Synthesis (ICLR 2017) that uses a similar R3NN network\nfor generating the program tree incrementally by decoding one node at a time. \nCan the authors comment on the similarity/differences between the approaches? \nWould it be possible to empirically evaluate how the R3NN performs on this dataset? \n\nSecond, it seems that the current model does not use the input-output examples at \nall for training the model.  The examples are only used during the search algorithm.\nSeveral previous neural program synthesis approaches (DeepCoder (ICLR 2017), \nRobustFill (ICML 2017)) have shown that encoding the examples can help guide \nthe decoder to perform efficient search.  It would be good to possibly add another \nencoder network to see if encoding the examples as well help improve the accuracy. \n\nSimilar to the previous point, it would also be good to evaluate the usefulness of\nencoding the problem statement by comparing the final model against a model in which\nthe encoder only encodes the input-output examples. \n\nFinally, there is also an issue with the synthetic evaluation dataset.  Since the \nproblem descriptions are generated syntactically using a template based approach, \nthe improvements in accuracy might come directly from learning the training templates\ninstead of learning the desired semantics.  The paper mentions that it is prohibitively \nexpensive to obtain human-annotated set, but can it be possible to at least obtain a \nhandful of real tasks to evaluate the learnt model?  There are also some recent \ndatasets such as WikiSQL (https://github.com/salesforce/WikiSQL) that the authors\nmight consider in future. \n\nQuestions for the authors:\n\nWhy was MAX_VISITED only limited to 100?  What happens when it is set to 10^4 or 10^6? \n\nThe Search algorithm only shows an accuracy of 0.6% with MAX_VISITED=100.  What would\nthe performance be for a simple brute-force algorithm with a timeout of say 10 mins?\n\nTable 3 reports an accuracy of 85.8% whereas the text mentions that the best result\nis 90.1% (page 8)?\n\nWhat all function names are allowed in the DSL (Figure 1)?  \n\nCan you clarify the contributions of the paper in comparison to the R3NN? \n\nMinor typos:\n\npage 2: allows to add constrains --> allows to add constraints \npage 5: over MAX_VISITED programs has been --> over MAX_VISITED programs have been\n\n",1,1,1,1,1,1,1,1,1,-1
B1KJJf-R--R2,"This paper tackles the problem of doing program synthesis when given a problem description and a small number of input-output examples.  The approach is to use a sequence-to-tree model along with an adaptation of beam search for generating tree-structured outputs.   In addition, the paper assembles a template-based synthetic dataset of task descriptions and programs.    Results show that a Seq2Tree model outperforms a Seq2Seq model, that adding search to Seq2Tree improves results,  and that search without any training performs worse, although the experiments assume that only a fixed number of programs are explored at test time regardless of the wall time that it takes a technique.  \n\nStrengths:\n\n- Reasonable approach, quality is good \n\n- The DSL is richer than that of previous related work like Balog et al. (2016). \n\n- Results show a reasonable improvement in using a Seq2Tree model over a Seq2Seq model, which is interesting. \n\nWeaknesses:\n\n- There are now several papers on using a trained neural network to guide search, and this approach doesn't add too much on top of previous work.  Using beam search on tree outputs is a bit of a minor contribution. \n\n- The baselines are just minor variants of the proposed method.  It would be stronger to compare against a range of different approaches to the problem, particularly given that the paper is working with a new dataset. \n\n- Data is synthetic, and it's hard to get a sense for how difficult the presented problem is, as there are just four example problems given. \n\nQuestions:\n\n- Why not compare against Seq2Seq + Search? \n\n- How about comparing wall time against a traditional program synthesis technique (i.e., no machine learning), ignoring the descriptions.  I would guess that an efficiently-implemented enumerative search technique could quickly explore all programs of depth 3, which makes me skeptical that Figure 4 is a fair representation of how well a non neural network-based search could do. \n\n- Are there plans to release the dataset?  Could you provide a large sample of the data at an anonymized link?  I'd re-evaluate my rating after looking at the data in more detail.\n",1,1,1,1,1,1,1,1,1,1
B1KJJf-R--R3,This paper introduces a technique for program synthesis involving a restricted grammar of problems that is beam-searched using an attentional encoder-decoder network.  This work to my knowledge is the first to use a DSL closer to a full language. \n\nThe paper is very clear and easy to follow.  One way it could be improved is if it were compared with another system.  The results showing that guided search is a potent combination whose contribution would be made only stronger if compared with existing work.,1,1,1,1,1,1,1,1,1,-1
B1l8BtlCb-R1,"This work proposes non-autoregressive decoder for the encoder-decoder framework in which the decision of generating a word does not depends on the prior decision of generated words.  The key idea is to model the fertility of each word so that copies for source words are fed as input to the encoder part, not the generated target words as inputs.   To achieve the goal, authors investigated various techniques: For inference, sample fertility space for generating multiple possible translations.   For training, apply knowledge distilation for better training followed by fine tuning by reinforce.   Experiments for English/German and English/Romanian show comparable translation qualities with speedup by non-autoregressive decoding. \n\nThe motivation is clear and proposed methods are very sound.  Experiments are carried out very carefully. \n\nI have only minor concerns to this paper:\n\n- The experiments are designed to achieve comparable BLEU with improved latency.  I'd like to know whether any BLUE improvement might be possible under similar latency, for instance, by increasing the model size given that inference is already  fast enough. \n\n- It'd also like to see other language pairs with distorted word alignment, e.g., Chinese/English, to further strengthen this work, though  it might have little impact given that attention already capture sort of alignment. \n\n- What is the impact of the external word aligner quality?  For instance, it would be possible to introduce a noise in the word alignment results or use smaller data to train a model for word aligner.  \n\n- The positional attention is rather unclear and it would be better to revise it.  Note that equation 4 is simply mentioning attention computation, not the proposed positional attention.",1,1,1,1,1,-1,1,1,1,-1
B1l8BtlCb-R2,"This paper describes an approach to decode non-autoregressively for neural machine translation (and other tasks that can be solved via seq2seq models).  The advantage is the possibility of more parallel decoding which can result in a significant speed-up (up to a factor of 16 in the experiments described).  The disadvantage is that it is more complicated than a standard beam search as auto-regressive teacher models are needed for training and the results do not reach (yet) the same BLEU scores as standard beam search.  \n\nOverall, this is an interesting paper.  It would have been good to see a speed-accuracy curve which plots decoding speed for different sized models versus the achieved BLUE score on one of the standard benchmarks (like WMT14 en-fr or en-de) to understand better the pros and cons of the proposed approach and to be able to compare models at the same speed or the same BLEU scores.   While the Ro->En results are goodG this particular language pair has not been used much by others; it would have been more interesting to stay with a single well-used language pair and benchmark and analyze why WMT14 en->de and de->en are not improving more.  Finally it would have been good to address total computation in the comparison as well -- it seems while total decoding time is smaller total computation for NAT + NPD is actually higher depending on the choice of s.\n",1,1,1,1,1,-1,1,1,1,-1
B1l8BtlCb-R3,"This paper can be seen as an extension of the paper \""attention is all you need\"" that will be published at nips in a few weeks (at the time I write this review).  \n\nThe goal here is to make the target sentence generation non auto regressive.  The authors propose to introduce a set of latent variables to represent the fertility of each source words.  The number of target words can be then derived and they're all predicted in parallel. \n\nThe idea is interesting and trendy.  However, the paper is not really stand alone.  A lot of tricks are stacked to reduce the performance degradation.  However, they're sometimes to briefly described to be understood by most readers.  \n\nThe training process looks highly elaborate with a lot of hyper parameters.  Maybe you could comment on this.  \n\nFor instance, the use fertility supervision during training could be better motivated and explained.  Your choice of IBM 2 is wired since it doesn't include fertility.  Why not IBM 4, for instance ?  How you use IBM model for supervision.  This a simple example, but a lot of things in this paper is too briefly described and their impact not really evaluated.",1,1,1,1,1,1,1,1,1,-1
B1Lc-Gb0Z-R1,"The paper studies learning in deep neural networks with hard activation functions, e.g. step functions like sign(x).  Of course, backpropagation is difficult to adapt to such networks, so prior work has considered different approaches.  Arguably the most popular is straight-through estimation (Hinton 2012, Bengio et al. 2013), in which the activation functions are simply treated as identity functions during backpropagation.  More recently, a new type of straight-through estimation, saturated STE (Hubara et al., 2016) uses 1[|z|<1] as the derivative of sign(z). \n\nThe paper generalizes saturated STE by recognizing that other discrete targets of each activation layer can be chosen.  Deciding on these targets is formulated as a combinatorial optimization problem.  Once the targets are chosen, updating the weights of each layer to minimize the loss on those targets is a convex optimization.  The targets are heuristically updated through the layers, starting out the output using the proposed feasibility target propagation.  At each layer, the targets can be chosen using a variety of search algorithms such as beam search. \n\nExperiments show that FTP often outperforms saturated STE on CIFAR and ImageNet with sign and quantized activation functions, reaching levels of performance closer to the full-precision activation networks. \n\nThis paper's ideas are very interesting, exploring an alternative training method to backpropagation that supports hard-threshold activation functions.  The experimental results are encouraging,  though I have a few questions below that prevent me for now from rating the paper higher. \n\nComments and questions:\n\n1) How computationally expensive is FTP?  The experiments using ResNet indicate it is not prohibitively expensive, but I am eager for more details. \n\n2) Does (Hubara et al., 2016) actually compare their proposed saturated STE with the orignal STE on any tasks?  I do not see a comparison.  If that is so, should this paper also compare with STE?  How do we know if generalizing saturated STE is more worthwhile than generalizing STE? \n\n3) It took me a while to understand the authors' subtle comparison with target propagation, where they say \""Our framework can be viewed as an instance of target propagation that uses combinatorial optimization to set discrete targets, whereas previous approaches employed continuous optimization. \"" It seems that the difference is greater than explicitly stated, that prior target propagation used continuous optimization to set *continuous targets*. (One could imagine using continuous optimization to set discrete targets such as a convex relaxation of a constraint satisfaction problem.)  Focusing on discrete targets gains the benefits of quantized networks.  If I am understanding the novelty correctly, it would strengthen the paper to make this difference clear. \n\n4) On a related note, if feasible target propagation generalizes saturated straight through estimation, is there a connection between (continuous) target propagation and the original type of straight through estimation? \n\n5) In Table 1, the significance of the last two columns is unclear.  It seems that ReLU and Saturated ReLU are included to show the performance of networks with full-precision activation functions (which is good).  I am unclear though on why they are compared against each other (bolding one or the other) and if there is some correspondence between those two columns and the other pairs, i.e., is ReLU some kind of analog of SSTE and Saturated ReLU corresponds to FTP-SH somehow?",1,1,1,1,1,1,1,1,1,-1
B1Lc-Gb0Z-R2,"This paper examines the problem of optimizing deep networks of hard-threshold units.  This is a significant topic with implications for quantization for computational efficiency, as well as for exploring the space of learning algorithms for deep networks.  While none of the contributions are especially novel,  the analysis is clear and well-organized, and the authors do a nice job in connecting their analysis to other work.",1,1,-1,1,-1,1,1,1,1,-1
B1Lc-Gb0Z-R3,"The paper discusses the problem of optimizing neural networks with hard threshold and proposes a novel solution to it.  The problem is of significance because in many applications one requires deep networks which uses reduced computation and limited energy.  The authors frame the problem of optimizing such networks to fit the training data as a convex combinatorial problems.  However since the complexity of such a problem is exponential, the authors propose a collection of heuristics/approximations to solve the problem.  These include, a heuristic for setting the targets at each layer, using a soft hinge loss, mini-batch training and such.  Using these modifications the authors propose an algorithm (Algorithm 2 in appendix) to train such models efficiently.  They compare the performance of a bunch of models trained by their algorithm against the ones trained using straight-through-estimator (SSTE) on a couple of datasets, namely, CIFAR-10 and ImageNet.  The authors formulation of the problem as one of combinatorial optimization and proposing Algorithm 1 is also quite interesting.  The results are moderately convincing in favor of the proposed approach.  Though a disclaimer here is that I'm not 100% sure that SSTE is the state of the art for this problem.  Overall i like the originality of the paper and feel that it has a potential of reasonable impact within the research community.  \n\nThere are a few flaws/weaknesses in the paper though, making it somewhat lose.  \n- The authors start of by posing the problem as a clean combinatorial optimization problem and propose Algorithm 1.  Realizing the limitations of the proposed algorithm, given the assumptions under which it was conceived in, the authors relax those assumptions in the couple of paragraphs before section 3.1 and pretty much throw away all the nice guarantees, such as checks for feasibility, discussed earlier.  \n- The result of this is another algorithm (I guess the main result of the paper), which is strangely presented in the appendix as opposed to the main text, which has no such guarantees.   \n- There is no theoretical proof that the heuristic for setting the target is a good one, other than a rough intuition \n- The authors do not discuss at all the impact on generalization ability of the model trained using the proposed approach.  The entire discussion revolves around fitting the training set and somehow magically everything seem to generalize and not overfit. \n",1,1,1,1,-1,-1,1,1,1,-1
B1lMMx1CW-R1,"The paper proposes a new neural network based method for recommendation., \n\nThe main finding of the paper is that a relatively simple method works for recommendation, compared to other methods based on neural networks that have been recently proposed. \n\nThis contribution is not bad for an empirical paper.  There's certainly not that much here that's groundbreaking methodologically, though it's certainly nice to know that a simple and scalable method works. \n\nThere's not much detail about the data (it is after all an industrial paper).  It would certainly be helpful to know how well the proposed method performs on a few standard recommender systems benchmark datasets (compared to the same baselines), in order to get a sense as to whether the improvement is actually due to having a better model, versus being due to some unique attributes of this particular industrial dataset under consideration.  As it is, I am a little concerned that this may be a method that happens to work well for the types of data the authors are considering but may not work elsewhere. \n\nOther than that, it's nice to see an evaluation on real production data, and it's nice that the authors have provided enough info that the method should be (more or less) reproducible.  There's some slight concern that maybe this paper would be better for the industry track of some conference, given that it's focused on an empirical evaluation rather than really making much of a methodological contribution.  Again, this could be somewhat alleviated by evaluating on some standard and reproducible benchmarks.",1,1,1,1,1,-1,1,1,1,1
B1lMMx1CW-R2,"Authors describe a procedure of building their production recommender system from scratch, begining with formulating the recommendation problem, label data formation, model construction and learning.  They use several different evaluation techniques to show how successful their model is (offline metrics, A/B test results, etc.) \n\nMost of the originality comes from integrating time decay of purchases into the learning framework.  Rest of presented work is more or less standard. \n\nPaper may be useful to practitioners who are looking to implement something like this in production.",1,1,-1,1,-1,-1,1,1,1,-1
B1lMMx1CW-R3,"This paper presents a practical methodology to use neural network for recommending products to users based on their past purchase history.  The model contains three components: a predictor model which is essentially a RNN-style model to capture near-term user interests, a time-decay function which serves as a way to decay the input based on when the purchase happened, and an auto-encoder component which makes sure the user's past purchase history get fully utilized, with the consideration of time decay.  And the paper showed the combination of the three performs the best in terms of precision@K and PCC@K, and also with good scalability.  It also showed good online A/B test performance, which indicates that this approach has been tested in real world. \n\nTwo small concerns:\n1. In Section 3.3. I am not fully sure why the proposed predictor model is able to win over LSTM.  As LSTM tends to mitigate the vanishing gradient problem which most likely would exist in the predictor model.  Some insights might be useful there. \n2. The title of this paper is weird.  Suggest to rephrase \""unreasonable\"" to something more positive.",1,1,1,1,1,-1,1,1,1,-1
B1mSWUxR--R1,"This paper dives deeper into understand reward augmented maximum likelihood training.  Overall, I feel that the paper is hard to understand and that it would benefit from more clarity, e.g., section 3.3 states that decoding from the softmax q-distribution is similar to the Bayes decision rule.  Please elaborate on this. \n\nDid you compare to minimum bayes risk decoding which chooses the output with the lowest expected risk amongst a set of candidates? \n\nSection 4.2.2 says that Ranzato et al. and Bahdanau et al. require sampling from the model distribution.  However, the methods analyzed in this paper also require sampling (cf. Appendix D.2.4 where you mention a sample size of 10),  Please explain the difference.",1,1,1,1,1,1,1,1,-1,-1
B1mSWUxR--R2,"This paper interprets reward augmented maximum likelihood followed by decoding with the most likely output as an approximation to the Bayes decision rule. \n\nI have a few questions on the motivation and the results. \n- In the section \""Open Problems in RAML\"", both (i) and (ii) are based on the statement that the globally optimal solution of RAML is the exponential payoff distribution q.  This is not true.  The globally optimal solution is related to both the underlying data distribution P and q, and not the same as q.  It is given by q'(y | x, \\tau) = \\sum_{y'} P(y' | x) q(y | y', \\tau). \n- Both Theorem 1 and Theorem 2 do not directly justify that RAML has similar reward as the Bayes decision rule,  Can anything be said about this?  Are the KL divergence small enough to guarantee similar predictive rewards? \n- In Theorem 2, when does the exponential tail bound assumption hold? \n- In Table 1, the differences between RAML and SQDML do not seem to support the claim that SQDML is better than RAML.  Are the differences actually significant?  Are the differences between SQDML/RAML and ML significant?  In addition, how should \\tau be chosen in these experiments?\n",1,1,1,1,1,-1,1,1,-1,-1
B1mSWUxR--R3,"The authors claim three contributions in this paper. (1) They introduce the framework of softmax Q-distribution estimation, through which they are able to interpret the role the payoff distribution plays in RAML.  Specifically, the softmax Q-distribution serves as a smooth approximation to the Bayes decision boundary.  The RAML approximately estimates the softmax Q-distribution, and thus approximates the Bayes decision rule.  (2) Algorithmically, they further propose softmax Q-distribution maximum likelihood (SQDML) which improves RAML by achieving the exact Bayes decision boundary asymptotically.  (3) Through one experiment using synthetic data on multi-class classi\ufb01cation and one using real data on image captioning, they show that SQDML is consistently as good or better than RAML on the task-speci\ufb01c metrics that is desired to optimize.  \n\nI found the first contribution is sound, and it reasonably explains why RAML achieves better performance when measured by a specific metric.  Given a reward function, one can define the Bayes decision rule.  The softmax Q-distribution (Eqn. 12) is defined to be the softmax approximation of the deterministic Bayes rule.  The authors show that the RAML can be explained by moving the expectation out of the nonlinear function and replacing it with empirical expectation (Eqn. 17).  Of course, the moving-out is biased but the replacing is unbiased.  \n\nThe second contribution is partially valid,  although I doubt how much improvement one can get from SQDML.  The authors define the empirical Q-distribution by replacing the expectation in Eqn. 12 with empirical expectation (Eqn. 15).  In fact, this step can result in biased estimation because the replacement is inside the nonlinear function.  When x is repeated sufficiently in the data, this bias is small and improvement can be observed, like in the synthetic data example.  However, when x is not repeated frequently, both RAML and SQDML are biased.  Experiment in section 4.1.2 do not validate significant improvement, either. \n\nThe numerical results are relatively weak.  The synthetic experiment verifies the reward-maximizing property of RAML and SQDML.  However, from Figure 2, we can see that the result is quite sensitive to the temperature \\tau.  Is there any guidelines to choose \\tau?  For experiments in Section 4.2, all of them are to show the effectiveness of RAML, which are not very relevant to this paper.   These results are also lower than the state of the art performance.  \n\nA few questions:\n(1). The author may want to check whether (8) can be called a Bayes decision rule.  This is a direct result from definition of conditional probability.  No Bayesian elements, like prior or likelihood appears here. \n(2). In the implementation of SQDML, one can sample from (15) without exactly computing the summation in the denominator.  Compared with the n-gram replacement used in the paper, which one is better? \n(3). The authors may want to write Eqn. 17 in the same conditional form of Eqn. 12 and Eqn. 14.  This will make the comparison much more clear. \n(4). What is Theorem 2 trying to convey?  Although \\tau goes to 0, there is still a gap between Q and Q'.  This seems to suggest that for small \\tau, Q' is not a good approximation of Q.  Are the assumptions in Theorem 2 reasonable?  There are several typos in the proof of Theorem 2. \n(5). In section 4.2.2, the authors write \""the rewards we directly optimized in training (token-level accuracy for NER and UAS for dependency parsing) are more stable w.r.t. \u03c4 than the evaluation metrics (F1 in NER), illustrating that in practice, choosing a training reward that correlates well with the evaluation metric is important\"".  Could you explain it in more details?\n\n",1,1,1,1,1,-1,1,1,1,-1
B1n8LexRZ-R1,"In this work, the authors propose a procedure for tuning the parameters of an HMC algorithm (I guess, if I have understood correctly).\ n\nI think this paper has a good and strong point: this work points out the difficulties in choosing properly the parameters in a HMC method (such as the step and the number of iterations in the leapfrog integrator, for instance).  In the literature, specially in machine learning, there is ``fever\u2019\u2019 about HMC, in my opinion, partially unjustified. \n\nIf I have understood, your method is an adaptive HMC algorithm  where the parameters are updated online; or is the training  done in advance? Please, remark and clarify this point. \n\nHowever, I have other additional comments:\n\n- Eqs. (4) and (5) are quite complicated; I think a running toy example can help the interested reader. \n\n- I suggest to compare the proposed method to other efficient methods that do not use the gradient information (in some cases as multimodal posteriors, the use of the gradient information can be counter-productive for sampling purposes), such as Multiple Try Metropolis (MTM) schemes \n\nL. Martino, J. Read, On the flexibility of the design of Multiple Try Metropolis schemes, Computational Statistics, Volume 28, Issue 6, Pages: 2797-2823, 2013, \n\nadaptive techniques,  \n\nH. Haario, E. Saksman, and J. Tamminen. An adaptive Metropolis algorithm. Bernoulli, 7(2):223\u2013242, April 2001, \n\nand component-wise strategies as Gibbs Sampling, \n\nW. R. Gilks and P. Wild, Adaptive rejection sampling for Gibbs sampling, Appl. Statist., vol. 41, no. 2, pp. 337\u2013348, 199.\u2028 \n\nAt least, add a brief paragraph in the introduction citing and discussing this possible alternatives.",1,1,1,1,1,1,1,1,-1,-1
B1n8LexRZ-R2,"The paper proposed a generalized HMC by modifying the leapfrog integrator using neural networks to make the sampler to converge and mix quickly.  Mixing is one of the most challenge problems for a MCMC sampler, particularly when there are many modes in a distribution.  The derivations look correct to me.  In the experiments, the proposed algorithm was compared to other methods, e.g., A-NICE-MC and HMC.  It showed that the proposed method could mix between the modes in the posterior.  Although the method could mix well when applied to those particular experiments,  it lacks theoretical justifications why the method could mix well.",1,-1,1,1,1,1,1,1,-1,-1
B1n8LexRZ-R3,"The paper introduces a non-volume-preserving generalization of HMC whose transitions are determined by a set of neural network functions.  These functions are trained to maximize expected squared jump distance. \nThis works because each variable (of the state space) is modified in turn, so that the resulting update is invertible, with a tractable transformation inspired by Dinh et al 2016. \n\nOverall, I believe this paper is of good quality, clearly and carefully written, and potentially accelerates mixing in a state-of-the-art MCMC method, HMC, in many practical cases.  A few downsides are commented on below.\n\n The experimental section proves the usefulness of the method on a range of relevant test cases; in addition, an application to a latent variable model is provided sec5.2.  \nFig 1a presents results in terms of numbers of gradient evaluations, but I couldn't find much in the way of computational cost of L2HMC in the paper.  I can't see where the number \""124x\"" in sec 5.1 stems from.  As a user, I would be interested in the typical computational cost of both \""MCMC sampler training\"" and MCMC sampler usage (inference?), compared to competing methods.  This is admittedly hard to quantify objectively, but just an order of magnitude would be helpful for orientation.  \nWould it be relevant, in sec5.1, to compare to other methods than just HMC, eg LAHMC? \n\nI am missing an intuition for several things: eq7, the time encoding defined in Appendix C\n\nAppendix Fig5, I cannot quite see how the caption claim is supported by the figure (just hardly for VAE, but not for HMC). \n\nThe number \""124x ESS\"" in sec5.1 seems at odds with the number in the abstract, \""50x\"". \n\n# Minor errors\n- sec1: \""The sampler is trained to minimize a variation\"": should be maximize\n\""as well as on a the real-world\"" \n- sec3.2 \""and 1/2 v^T v the kinetic\"": \""energy\"" missing\n - sec4: the acronym L2HMC is not expanded anywhere in the paper\n The sentence \""We will denote the complete augmented...p(d)\"" might be moved to after \""from a uniform distribution\"" in the same paragraph.  \nIn paragraph starting \""We now update x\"":\n    - specify for clarity: \""the first update, which yields x' \""/ \""the second update, which yields x''  \""\n     - \""only affects $x_{\\bar{m}^t}$\"": should be $x'_{\\bar{m}^t}$  (prime missing)\n     - the syntax using subscript m^t is confusing to read; wouldn't it be clearer to write this as a function, eg \""mask(x',m^t)\""?\n     - inside zeta_2 and zeta_3, do you not mean $m^t\"" and $\\bar{m}^t$ ?\n - sec5: add reference for first mention of \""A NICE MC\""\n - Appendix A: \n    - \""Let's\"" -> \""Let\""\n    - eq12 should be x''=...\n-  Appendix C: space missing after \""Section 5.1\"" \n- Appendix D1: \""In this section is presented\"" : sounds odd\ n- Appendix D3: presumably this should consist of the figure 5 ? Maybe specify.",1,1,1,1,1,1,1,1,1,-1
B1NGT8xCZ-R1,"This is a very well-written paper that shows how to successfully use (generative) autoencoders together with the (discriminative) domain adversarial neural network (DANN) of Ganin et al. \nThe construction is simple but nicely backed by a probabilistic analysis of the domain adaptation problem. \n\nThe only criticism that I have towards this analysis is that the concept of shared parameter between the discriminative and predictive model (denoted by zeta in the paper) disappear when it comes to designing the learning model.   \n\nThe authors perform numerous empirical experiments on several types of problems.  They successfully show that using autoencoder can help to learn a good representation for discriminative domain adaptation tasks.   On the downside, all these experiments concern predictive (discriminative) problems.  Given the paper title, I would have expected some experiments in a generative context.  Also, a comparison with the Generative Adversarial Networks of Goodfellow et al. (2014) would be a plus. \nI would also like to see the results obtained using DANN stacked on mSDA representations, as it is done in Ganin et al. (2016). \n\nMinor comments:\n- Paragraph below Equation 6:  The meaning of $\\phi(\\psi)$ is unclear  \n- Equation (7): phi and psi seems inverted  \n- Section 4: The acronym MLP is used but never defined. \n\n=== update ===\nI lowered my score and confidence, see my new post below.",1,1,1,1,1,1,1,1,1,-1
B1NGT8xCZ-R2,"This paper proposed a probabilistic framework for domain adaptation that properly explains why maximizing both the marginal and the conditional log-likelihoods can achieve desirable performances.   \nHowever, I have the following concerns on novelty.  \n\n1. Although the paper gives some justiification why auto-encoder can work for domain adaptation from perspective of probalistics model, it does not give new formulation or algorithm to handle domain adaptation.   At this point, the novelty is weaken. \n2. In the introduction, the authors mentioned \u201climitations of mSDA is that it needs to explicitly form the covariance matrix of input features and then solves a linear system, which can be computationally expensive in high dimensional settings. \u201d However, mSDA cannot handle high dimension setting by performing the  reconstruction with a number of  random non-overlapping sub-sets of input features.  It is not clear why mSDA cannot handle time-series data but DAuto can.   DAuto does not consider the sequence/ordering of data either.  \n3. If my understanding is not wrong, the proposed DAuto is just a simple combination of three losses (i.e. prediction loss, reconstruction loss, domain difference loss).  As far as I know, this kind of loss is commonly used in most existing methods.",1,1,1,1,-1,1,1,1,1,-1
B1NGT8xCZ-R3,"The authors propose a probabilistic framework for semi-supervised learning and domain adaptation.  By varying the prior distribution, the framework can incorporate both generative and discriminative modeling.   The authors emphasize on one particular form of constraint on the prior distribution, that is weight (parameter) sharing, and come up with a concrete model named Dauto for domain adaptation.  A domain confusion loss is added to learn domain-invariant feature representations.  The authors compared Dauto with several baseline methods on several datasets and showed improvement. \n\nThe paper is well-organized and easy to follow.  The probabilistic framework itself is quite straight-forward.  The paper will be more interesting if the authors are able to extend the discussion on different forms of prior instead of the simple parameter sharing scheme.  \n\nThe proposed DAuto is essentially DANN+autoencoder.   The minimax loss employed in DANN and DAuto is known to be prone to degenerated gradient for the generator.  It would be interesting to see if the additional auto-encoder part help address the issue.  \n\nThe experiments miss some of the more recent baseline in domain adaptation, such as Adversarial Discriminative Domain Adaptation (Tzeng, Eric, et al. 2017).  \n\nIt could be more meaningful to organize the pairs in table by target domain instead of source, for example, grouping 9->9, 8->9, 7->9 and 3->9 in the same block.  DAuto does seem to offer more boost in domain pairs that are less similar.",1,1,1,1,1,1,1,1,1,-1
B1nLkl-0Z-R1,"I think I should understand the gist of the paper, which is very interesting, where the action of \\tilde Q(s,a) is drawn from a distribution.  The author also explains in detail the relation with PGQ/Soft Q learning, and the recent paper \""expected policy gradient\"" by Ciosek & Whiteson.  All these seems very sound and interesting. \n\nWeakness:\n1. The major weakness is that throughout the paper, I do not see an algorithm formulation of the Smoothie algorithm, which is the major algorithmic contribution of the paper (I think the major contribution of the paper is on the algorithmic side instead of theoretical).  Such representation style is highly discouraging and brings about un-necessary readability difficulties.  \n\n2. Sec. 3.3 and 3.4 is a little bit abbreviated from the major focus of the paper, and I guess they are not very important and novel (just educational guess, because I can only guess what the whole algorithm Smoothie is).  So I suggest moving them to the Appendix and make the major focus more narrowed down.",1,1,1,1,1,1,1,1,1,-1
B1nLkl-0Z-R2,"The paper introduces smoothed Q-values, defined as the value of drawing an action from a Gaussian distribution and following a given policy thereafter.   It demonstrates that this formulation can still be optimized with policy gradients, and in fact is able to dampen instability in this optimization using the KL-divergence from a previous policy, unlike preceding techniques.   Experiments are performed on an simple domain which nicely demonstrates its properties, as well as on continuous control problems, where the technique outperforms or is competitive with DDPG. \n\nThe paper is very clearly written and easy to read, and its contributions are easy to extract.   The appendix is quite necessary for the understanding of this paper, as all proofs do not fit in the main paper.   The inclusion of proof summaries in the main text would strengthen this aspect of the paper. \n\nOn the negative side, the paper fails to make a strong case for significant impact of this work; the solution to this, of course, is not overselling benefits, but instead having more to say about the approach or finding how to produce much better experimental results than the comparative techniques.   In other words, the slightly more stable optimization and slightly smaller hyperparameter search for this approach is unlikely to result in a large impact. \n\nOverall, however, I found the paper interesting, readable, and the technique worth thinking about, so I recommend its acceptance.",1,1,1,1,1,-1,1,1,1,-1
B1nLkl-0Z-R3,"This paper explores the idea of using policy gradients to learn a stochastic policy on complex control problems.   The central idea is to frame learning in terms of a new kind of Q-value that attempts to smooth out Q-values by framing them in terms of expectations over Gaussian policies. \n\nTo be honest, I didn't really \""get\"" this paper.\n*  As far I understand, all of the original work policy gradients involved stochastic policies.   Many are/were Gaussian.\n*  All Q-value estimators are designed to marginalize out the randomness in these stochastic policies.\n*  As far as I can tell, this is equivalent to a slightly different formulation, where the agent emits a deterministic action (\\mu,\\Sigma) and the environment samples an action from that distribution. \n\nUltimately, I couldn't discern /why/ this was a significant advance for RL, or even a meaningful new perspective on classic ideas. \n\nI thought the little 2-mode MOG was a nice example of the premise of the model. \n\nWhile I may or may not have understood the core technical contribution, I think the experiments can be critiqued: they didn't really seem to work out.   Figures 2&3 are unconvincing - the differences do not appear to be statistically significant.   Also, I was disappointed to see that the authors only compared to DDPG; they could have at least compared to TRPO, which they mention.   They dismiss it by saying that it takes 10 times as long, but gets a better answer - to which I respond, \""Very well, run your algorithm 10x longer and see where you end up!\""   I think we need to see a more compelling demonstration of why this is a useful idea before it's ready to be published. \n\nThe idea of penalizing a policy based on KL-divergence from a reference policy was explored at length by Bert Kappen's work on KL-MDPs.  Perhaps you should cite that?\n",1,1,1,1,1,1,-1,1,1,1
B1nxTzbRZ-R1,"The paper considers a problem of predicting hidden information in a poMDP with an application to Starcraft.  \n\nI find the problem of defogging quite interesting, even though it is a bit too Starcraft-specific some findings could perhaps be translated to other partially observed environments. \nAuthors use the dataset provided for Starcraft: Brood war by Lin et al, 2017. \n\nMy impression about the paper is that even though it touches a very interesting problem,  it neither is written well nor it contains much of a novelty in terms of algorithms, methods or network architectures. \n\nDetailed comments:\n* Authors should at very least cite (Vinyals et al, 2017) and explain why the environment and the dataset released for Starcraft 2 is less suited than the one provided by Lin et al. \n* Problem statement in section 3.1 should certainly be improved.  Authors introduce rather heavy notation which is then used in a confusing way.  For example, what is the top index in $s_t^{3-p}$ supposed to mean?  The notation is not much used after sec. 3.1, for example, figure 1 does not use it.  \n* A related issue, is that the definition of metrics is very informal and, again, does not use the already defined notation.  Including explicit formulas would be very helpful, because, for example, it looks like when reported in table 1 the metrics are spatially averaged, yet I could not find an explicit notion of that. \n* Authors seem to only consider deterministic defogging models.  However, to me it seems that even in 15 game steps the uncertainty over the hidden state is quite high and thus any deterministic model has a very limited potential in prediction it.  At least the concept of stochastic predictions should be discussed\n* The rule-based baselines are not described in detail.  What does \u201cusing game rules to infer the existence of unit types\u201d mean? \n* Another detail which I found missing is whether authors use just a screen, a mini-map or both.  In the game of Starcraft, only screen contains information about unit-types, but it\u2019s field of view is limited.  Hence, it\u2019s unclear to me whether a model should infer hidden information based on just a single screen + minimap observation (or a history of them) or due to how the dataset is constructed, all units are observed without spatial limitations of the screen. \n",1,1,1,1,1,1,1,1,-1,-1
B1nxTzbRZ-R2,"The authors introduce the task of \""defogging\"", by which they mean attempting to infer the contents of areas in the game StarCraft hidden by \""the fog of war\"". \n\nThe authors train a neural network to solve the defogging task, define several evaluation metrics, and argue that the neural network beats several naive baseline models.  \n\nOn the positive side, the task is a nice example of reasoning about a complex hidden state space, which is an important problem moving forwards in deep learning. \n\nOn the negative side, from what I can tell, the authors don't seem to have introduced any fundamentally new architectural choices in their neural network, so the contribution seems fairly specific to mastering StarCraft, but at the same time, the authors don't evaluate how much their defogger actually contributes to being able to win StarCraft games.  \n\nGranted, being able to infer hidden states is of course an important problem,  but the authors appear to mainly have applied existing techniques to a benchmark that has minimal practical significance outside of being able to win StarCraft competitions, meaning that, at least as the paper is currently framed, the critical evaluation metric would be showing that a defogger helps to win games.  \n\nTwo ways I could image the contribution being improved are either highlighting and generalizing novel insights gleaned from the process of building the neural network that could help people build \""defoggers\"" for other domains (and spelling out more explicitly what domains the authors expect their insights to generalize to), or doubling down on the StarCraft application specifically and showing that the defogger helps to win games.   A minimal version of the second modification would be having a bot that has access to a defogger play against a bot that does not have access to one. \n \nAll that said, as a paper on an application of deep learning, the paper appears to be solid, and if the area chairs are looking for that sort of contribution, then the work seems acceptable. \n\nMinor points:\n- Is there a benefit to having a model that jointly predicts unit presence and count, rather than having two separate models (e.g., one that feeds into the next)?   Could predicting presence or absence separately be a way to encourage sparsity, since absence of a unit is already representable as a count of zero?   The choice to have one model seems especially peculiar given the authors say they couldn't get one set of weights that works for both their classification and regression tasks \n- Notation: I believe the space U is never described in the main text.  What components precisely does an element of U have? \n- The authors say they use gameplay from no later than 11 minutes in the game to avoid the difficulties of increasing variance.  How long is a typical game?    Is this a substantial fraction of the time of the games studied?    If it is not, then perhaps the defogger would not help so much at winning.  \n- The F1 performance increases are somewhat small.   The L1 performance gains are bigger, but the authors only compare L1 on true positives.   This means they might have very bad error on false positives.   (The authors state they are favoring the baseline in this comparison, but it would be nice to have those numbers.)\n- I don't understand when the authors say the deep model has better memory than baselines (which includes a perfect memory baseline)",1,1,1,1,1,-1,1,1,1,-1
B1nxTzbRZ-R3,"# Summary\nThis paper introduces a new prediction problem where the model should predict the hidden opponent's state as well as the agent's state.  This paper presents a neural network architecture which takes the map information and several other features and reconstructs the unit occupancy and count information in the map.  The result shows that the proposed method performs better than several hand-designed baselines on two downstream prediction tasks in Starcraft. \n\n[Pros]\n- Interesting problem \n\n[Cons]\n- The proposed method is not much novel. \n- The evaluation is a bit limited to two specific downstream prediction tasks. \n\n# Novelty and Significance\n- The problem considered in this paper is interesting. \n- The proposed method is not much novel.  \n- Overall, this paper is too specific to Starcraft domain + particular downstream prediction tasks.  It would be much stronger to show the benefit of defogging objective on the actual gameplay rather than prediction tasks.  Alternatively, it could be also interesting to consider an RL problem where the agent should reveal the hidden state of the opponent as much/quickly as possible. \n\n# Quality\n- The experimental result is not much comprehensive.  The proposed method is expected to perform better than hand-designed methods on downstream prediction tasks.  It would be better to show an in-depth analysis of the learned model or show more results on different tasks (possibly RL tasks rather than prediction tasks). \n\n# Clarity\n- I did not fully understand the learning objective.  Does the model try to reconstruct the state of the current time-step or the future?  The learning objective is not clearly defined.  In Section 4.1, the target x and y have time steps from t1 to t2.  What is the range of t1 and t2?  If the proposed model is doing future prediction, it would be important to show and discuss long-term prediction results.",1,1,1,1,1,-1,1,1,1,-1
B1nZ1weCZ-R1,"The paper present online algorithms for learning multiple sequential problems.  The main contribution is to introduce active learning principles for sampling the sequential tasks in an online algorithm.  The contributions are interesting and experimental results seem promising.  But the paper is difficult to read due to many different ideas and because some algorithms and many important explanations must be found in the Appendix (ten sections in the Appendix and 28 pages).  Also, most of the paper is devoted to the study of algorithms for which the expected target scores are known.  This is a very strong assumption.  In my opinion, the authors should have put the focus on the DU4AC algorithm which get rids of this assumption.  Therefore, I am not convinced that the paper is ready for publication at ICLR'18. \n* Differences between BA3C and other algorithms are said to be a consequence of the probability distribution over tasks.  The gap is so large that I am not convinced on the fairness of the comparison . For instance, BA3C (Algorithm 2 in Appendix C) does not have the knowledge of the target scores while others heavily rely on this knowledge. \n* I do not see how the single output layer is defined. \n* As said in the general comments, in my opinion Section 6 should be developped and more experiments should be done with the DUA4C algorithm. \n* Section 7.1. It is not clear why degradation does not happen.  It seems to be only an experimental fact.",1,1,1,1,1,-1,1,1,1,-1
B1nZ1weCZ-R2,"\nThe authors show empirically that formulating multitask RL itself as an active learning and ultimately as an RL problem can be very fruitful.   They design and explore several approaches  to the active learning (or active sampling) problem, from a basic \nchange to the distribution to UCB to feature-based neural-network based RL.   The domain is video games.     All proposed approaches beat the uniform sampling baselines and the more sophisticated approaches do better in the scenarios with more tasks (one multitask  problem had 21 tasks).  \n\n\nPros:\n\n- very promising results with an interesting active learning approach to multitask RL \n\n- a number of approaches developed for the basic idea \n\n- a variety of experiments, on challenging multiple task problems (up to 21 tasks/games) \n\n- paper is overall well written/clear \n\nCons:\n\n- Comparison only to a very basic baseline (i.e. uniform sampling) \nCouldn't comparisons be made, in some way, to other multitask work? \n\n\n\nAdditional  comments:\n\n- The assumption of the availability of a target score goes against\nthe motivation that one need not learn individual networks  ..  authors\nsay instead one can use 'published' scores, but that only assumes\nsomeone else has done the work (and furthermore, published it!). \n\nThe authors do have a section on eliminating the need by doubling an\nestimate for each task) which makes this work more acceptable (shown\nfor 6 tasks or MT1, compared to baseline uniform sampling). \n\nClearly there is more to be done here for a future direction (could be\nmentioned in future work section). \n\n- The averaging metrics (geometric, harmonic vs arithmetic, whether\n  or not to clip max score achieved) are somewhat interesting, but in\n  the main paper, I think they are only used in section 6 (seems like\n  a waste of space).  Consider moving some of the results, on showing\n  drawbacks of arithmetic mean with no clipping (table 5 in appendix E), from the appendix to\n  the main paper. \n\n\n- The can be several benefits to multitask learning, in particular\n  time and/or space savings in learning new tasks via learning more\n  general features.  Sections 7.2 and 7.3 on specificity/generality of\n  features were interesting. \n\n\n\n--> Can the authors show that a trained network (via their multitask\n    approached) learns significantly faster on a brand new game    (that's similar to games already trained on), compared to learning from\n    scratch? \n\n--> How does the performance improve/degrade (or the variance), on the\n    same set of tasks, if the different multitask instances (MT_i)\n    formed a supersets hierarchy, ie if MT_2 contained all the\n    tasks/games in MT_1, could training on MT_2 help average\n    performance on the games in MT_1 ?  Could go either way since the network\n   has to allocate resources to learn other games too.   But is there a pattern? \n\n\n\n- 'Figure 7.2' in section 7.2 refers to Figure 5. \n\n\n- Can you motivate/discuss better why not providing the identity of a\n  game as an input is an advantage?  Why not explore both\n  possibilities?  what are the pros/cons? (section 3)\n\n\n\n\n",1,1,1,1,1,-1,1,1,1,-1
B1nZ1weCZ-R3,"In this paper active learning meets a challenging multitask domain: reinforcement learning in diverse Atari 2600 games.  A state of the art deep reinforcement learning algorithm (A3C) is used together with three active learning strategies to master multitask problem sets of increasing size, far beyond previously reported works. \n\nAlthough the choice of problem domain is particular to Atari and reinforcement learning, the empirical observations, especially the difficulty of learning many different policies together, go far beyond the problem instantiations in this paper.  Naive multitask learning with deep neural networks fails in many practical cases, as covered in the paper.  The one concern I have is perhaps the choice of distinct of Atari games to multitask learn may be almost adversarial, since naive multitask learning struggles in this case; but in practice, the observed interference can appear even with less visually diverse inputs. \n\nAlthough performance is still reduced compared to single task learning in some cases,  this paper delivers an important reference point for future work towards achieving generalist agents, which master diverse tasks and represent complementary behaviours compactly at scale. \n\nI wonder how efficient the approach would be on DM lab tasks, which have much more similar visual inputs, but optimal behaviours are still distinct.\",1,1,1,1,1,-1,1,1,1,-1
B1p461b0W-R1,"The paper makes a bold claim, that deep neural networks are robust to arbitrary level of noise.  It also implies that this would be true for any type of noise, and support this later claim using experiments on CIFAR and MNIST with three noise types: (1) uniform label noise (2) non-uniform but image-independent label noise, which is named \""structured noise\"", and (3) Samples from out-of-dataset classes.  The experiments show robustness to these types of noise.  \n\nReview: \nThe claim made by the paper is overly general, and in my own experience incorrect when considering real-world-noise.  This is supported by the literature on \""data cleaning\"" (partially by the authors), a procedure which is widely acknowledged as critical for good object recognition.   While it is true that some image-independent label noise can be alleviated in some datasets, incorrect labels in real world datasets can substantially harm classification accuracy. \n\nIt would be interesting to understand the source of the difference between the results in this paper and the more common results (where label noise damages recognition quality).  The paper did not get a chance to test these differences, and I can only raise a few hypotheses.  First, real-world noise depends on the image and classes in a more structured way. For instance, raters may confuse one bird species from a similar one, when the bird is photographed from a particular angle.  This could be tested experimentally, for example by adding incorrect labels for close species using the CUB data for fine-grained bird species recognition.    Another possible reason is that classes in MNIST and CIFAR10 are already very distinctive, so are more robust to noise.   Once again, it would be interesting for the paper to study why they achieve robustness to noise while the effect does not hold in general.   \n\nWithout such an analysis, I feel the paper should not be accepted to ICLR because the way it states its claim may mislead readers.   \n\nOther specific comments: \n-- Section 3.4 the experimental setup, should clearly state details of the optimization, architecture and hyper parameter search.   For example, for Conv4, how many channels at each layer?   how was the net initialized?  which hyper parameters were tuned and with which values?  were hyper parameters tuned on a separate validation set?  How was the train/val/test split done, etc.  These details are useful for judging technical correctness. \n-- Figure 8 failed to show for me.  \n-- Figure 9,10, need to specify which noise model was used.\n\n\n\n\n\n",1,1,1,1,1,1,1,1,1,-1
B1p461b0W-R2,"The authors study the effect of label noise on classification tasks.  They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  \n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.  Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015).  Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains.  \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules.  \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.  However, the previous settings can be reinterpreted in the authors setting.  I found the formulation of the \\alpha to be non-intuitive and confusing at times.  The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.  In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels).  This can be improved to help readers understand better.  \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels.  \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.  \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n",1,1,1,1,1,1,1,1,1,1
B1p461b0W-R3,"The authors study the effect of label noise on classification tasks.  They perform experiments of label noise in a uniform setting, structured setting as well provide some heuristics to mitigate the effect of label noise such as changing learning rate or batch size.  \n\nAlthough, the observations are interesting, especially the one on MNIST where the network performs well even with correct labels slightly above chance, the overall contributions are incremental.  Most of the observations of label noise such as training with structured noise, importance of larger datasets have already been archived in prior work such as in Sukhbataar et.al. (2014) and Van Horn et. al (2015).  Agreed that the authors do a more detailed study on simple MNIST classification, but these insights are not transferable to more challenging domains.  \n\nThe main limitation of the paper is proposing a principled way to mitigate noise as done in Sukhbataar et.al. (2014), or an actionable trade-off between data acquisition and training schedules.  \n\nThe authors contend that the way they deal with noise (keeping number of training samples constant) is different from previous setting which use label flips.  However, the previous settings can be reinterpreted in the authors setting.  I found the formulation of the \\alpha to be non-intuitive and confusing at times.  The graphs plot number of noisy labels per clean label so a alpha of 100 would imply 1 right label and 100 noisy labels for total 101 labels.  In fact, this depends on the task at hand (for MNIST it is 11 clean labels for 101 labels).  This can be improved to help readers understand better.  \n\nThere are several unanswered questions as to how this observation transfers to a semi-supervised or unsupervised setting, and also devise architectures depending on the level of expected noise in the labels.  \n\nOverall, I feel the paper is not up to mark and suggest the authors devote using these insights in a more actionable setting.  \nMissing citation: \""Training Deep Neural Networks on Noisy Labels with Bootstrapping\"", Reed et al.\n",1,1,1,1,1,1,1,1,1,1
B1QgVti6Z-R1,"This paper studies empirical risk in deep neural networks.  Results are provided in Section 4 for linear networks and in Section 5 for nonlinear networks. \nResults for deep linear neural networks are puzzling.  Whatever the number of layers, a deep linear NN is simply a matrix multiplication and minimizing the MSE is simply a linear regression.  So results in Section 4 are just results for linear regression and I do not understand why the number of layers come into play?  \nAlso this is never explicitly mentioned in the paper, I guess the authors make an assumption that the samples (x_i,y_i) are drawn i.i.d. from a given distribution D.  In such a case, I am sure results on the population risk minimization can be found for linear regression and should be compare to results in Section 4.\n\n",1,1,1,1,-1,-1,1,-1,-1,-1
B1QgVti6Z-R2,"This paper provides the analysis of empirical risk landscape for GENERAL deep neural networks (DNNs).  Assumptions are comparable to existing results for OVERSIMPLIFED shallow neural networks.  The main results analyzed: 1) Correspondence of non-degenerate stationary points between empirical risk and the population counterparts.  2) Uniform convergence of the empirical risk to population risk.  3) Generalization bound based on stability.  The theory is first developed for linear DNNs and then generalized to nonlinear DNNs with sigmoid activations. \n\nHere are two detailed comments:\n\n1) For deep linear networks with squared loss, Kawaguchi 2016 has shown that the global optima are the only non-degerenate stationary points.  Thus, the obtained non-degerenate stationary deep linear network should be equivalent to the linear regression model Y=XW.  Should the risk bound only depends on the dimensions of the matrix W? \n\n2) The comparison with Bartlett & Maass\u2019s (BM) work is a bit unfair, because their result holds for polynomial activations while this paper handles linear activations.  Thus, the authors need to refine BM's result for comparison.",1,1,1,1,1,1,1,-1,1,-1
B1QgVti6Z-R3,"Overall, this work seems like a reasonable attempt to answer the question of how the empirical loss landscape relates to the true population loss landscape.   The analysis answers:\n\n1) When empirical gradients are close to true gradients\ n2) When empirical isolated saddle points are close to true isolated saddle points \n3) When the empirical risk is close to the true risk. \n\nThe answers are all of the form that if the number of training examples exceeds a quantity that grows with the number of layers, width and the exponential of the norm of the weights with respect to depth, then empirical quantities will be close to true quantities.   I have not verified the proofs in this paper (given short notice to review) but the scaling laws in the upper bounds found seem reasonably correct.  \n\nAnother reviewer's worry about why depth plays a role in the convergence of empirical to true values in deep linear networks is a reasonable worry, but I suspect that depth will necessarily play a role even in deep linear nets because the backpropagation of gradients in linear nets can still lead to exponential propagation of errors between empirical and true quantities due to finite training data.   Moreover the loss surface of deep linear networks depends on depth even though the expressive capacity does not.    An analysis of dynamics on this loss surface was presented in Saxe et. al. ICLR 2014 which could be cited to address that reviewer's concern.   However, the reviewer's suggestion that the results be compared to what is known more exactly for simple linear regression is a nice one.  \n\nOverall, I believe this paper is a nice contribution to the deep learning theory literature.  However,  it would even better to help the reader with more intuitive statements about the implications of their results for practice, and the gap between their upper bounds and practice, especially given the intense interest in the generalization error problem.    Because their upper bounds look similar to those based on Rademacher complexity or VC dimension (although they claim theirs are a little tighter) - they should put numbers in to their upper bounds taken from trained neural networks, and see what the numerical evaluation of their upper bounds turn out to be in situations of practical interest where deep networks show good generalization performance despite having significantly less training data than number of parameters.   I suspect their upper bounds will be loose,  but still  - it would be an excellent contribution to the literature to quantitatively compare theory and practice with bounds that are claimed to be slightly tigher than previous bounds",1,1,1,1,1,1,1,1,1,-1
B1QRgziT--R1,"This paper borrows the classic idea of spectral regularization, recently applied to deep learning by Yoshida and Miyato (2017) and use it to normalize GAN objectives.  The ensuing GAN, coined SN-GAN, essentially ensures the Lipschitz property of the discriminator.  This Lipschitz property has already been proposed by recent methods and has showed some success.  However,  the authors here argue that spectral normalization is more powerful; it allows for models of higher rank (more non-zero singular values) which implies a more powerful discriminator and eventually more accurate generator.  This is demonstrated in comparison to weight normalization in Figure 4.  The experimental results are very good and give strong support for the proposed normalization. \n\n\nWhile the main idea is not new to machine learning (or deep learning), to the best of my knowledge it has not been applied on GANs.  The paper is overall well written (though check Comment 3 below), it covers the related work well and it includes an insightful discussion about the importance of high rank models.  I am recommending acceptance,  though I anticipate to see a more rounded evaluation of the exact mechanism under which SN improves over the state of the art.  More details in the comments below. \n\nComments:\n1. One concern about this paper is that it doesn\u2019t fully answer the reasons why this normalization works better.  I found the discussion about rank to be very intuitive,  The authors claim that other methods, like (Arjovsky et al. 2017) also suffer from the same rank deficiency.  I would like to see the same spectra included.  \n2. Continuing on the previous point: maybe there is another mechanism at play beyond just rank that give SN its apparent edge?  One way to test the rank hypothesis and better explain this method is to run a couple of truncated-SN experiments.  What happens if you run your SN but truncate its spectrum after every iteration in order to make it comparable to the rank of WN? Do you get comparable inception scores? Or does SN still win? \n3. Section 4 needs some careful editing for language and grammar.\n",1,1,1,1,1,1,1,1,1,-1
B1QRgziT--R2,"This paper proposes \""spectral normalization\"" -- constraining the spectral norm of the weights of each layer -- as a way to stabilize GAN training by in effect bounding the Lipschitz constant of the discriminator function.  The paper derives efficient approximations for the spectral norm, as well as an analysis of its gradient.  Experimental results on CIFAR-10 and STL-10 show improved Inception scores and FID scores using this method compared to other baselines and other weight normalization methods. \n\nOverall, this is a well-written paper that tackles an important open problem in training GANs using a well-motivated and relatively simple approach.  The experimental results seem solid and seem to support the authors' claims.  I agree with the anonymous reviewer that connections (and differences) to related work should be made clearer.  Like the anonymous commenter, I also initially thought that the proposed \""spectral normalization \"" is basically the same as \""spectral norm regularization\"", but given the authors' feedback on this I think the differences should be made more explicit in the paper. \n\nOverall this seems to represent a strong step forward in improving the training of GANs, and I strongly recommend this paper for publication. \n\nSmall Nits: \n\nSection 4: \""In order to evaluate the efficacy of our experiment\"": I think you mean \""approach\"". \n\nThere are a few colloquial English usages which made me smile, e.g. \n * Sec 4.1.1. \""As we prophesied ...\"", and in the paragraph below \n * \""... is a tad slower ...\"".",1,1,1,1,1,1,1,1,1,-1
B1QRgziT--R3,"The paper is motivated by the fact that in GAN training, it is beneficial to constrain the Lipschitz continuity of the discriminator.  The authors observe that the product of spectral norm of gradients per each layer serves as a good approximation of the overall Lipschitz continuity of the entire discriminating network, and propose gradient based methods to optimize a \""spectrally normalized\"" objective. \n\nI think the methodology presented in this paper is neat and the experimental results are encouraging.  However, I do have some comments on the presentation of the paper:\n\n1. Using power method to approximate matrix largest singular value is a very old idea, and I think the authors should cite some more classical references in addition to (Yoshida and Miyato).  For example,\n\nMatrix Analysis, book by Bhatia\nMatrix computation, book by Golub and Van Loan.\n\nSome recent work in theory of (noisy) power method might also be helpful and should be cited, for example,\nhttps://arxiv.org/abs/1311.2495\n\n2.  I think the matrix spectral norm is not really differentiable; hence the gradients the authors calculate in the paper should really be subgradients.  Please clarify this. \n\n3. It should be noted that even with the product of gradient norm, the resulting normalizer is still only an upper bound on the actual Lipschitz constant of the discriminator.  Can the authors give some empirical evidence showing that this approximation is much better than previous approximations, such as L2 norms of gradient rows which appear to be much easier to optimize?",1,1,1,1,1,1,1,-1,1,1
B1spAqUp--R1,"This paper is well written and easy to follow.  The authors propose pixel deconvolutional layers for convolutional neural networks.  The motivation of the proposed method, PixelDCL, is to remove the checkerboard effect of deconvolutoinal layers.  \nThe method consists of adding direct dependencies among the intermediate feature maps generated by the deconv layer.  PixelDCL is applied sequentially, therefore it is slower than the original deconvolutional layer.  The authors evaluate the model in two different problems: semantic segmentation (on PASCAL VOC and MSCOCO datasets) and in image generation VAE (with the CelebA dataset).  \n\nThe authors justify the proposed method as a way to alleviate the checkerboard effect (while introducing more complexity to the model and making it slower).  In the experimental section, however, they do not compare with other approaches to do so For example, the upsampling+conv approach, which has been shown to remove the checkerboard effect while being more efficient than the proposed method (as it does not require any sequential computation).  Moreover, the PixelDCL does not seem to bring substantial improvements on DeepLab (a state-of-the-art semantic segmentation algorithm).  More comments and further exploration on this results should be done.  Why no performance boost?  Is it because of the residual connection?  Or other component of DeepLab?  Is the proposed layer really useful once a powerful model is used? \n\nI also think the experiments on VAE are not conclusive.  The authors simply show set of generated images.  First, it is difficult to see the different of the image generated using deconv and PixelDCL.  Second, a set of 20 qualitative images does not (and cannot) validate any research idea.",1,1,1,1,1,1,1,1,1,-1
B1spAqUp--R2,"Paper summary:\nThis paper proposes a technique to generalize deconvolution operations used in standard CNN architectures.  Traditional deconvolution operation uses independent filter weights to compute output features at adjacent pixels.  This work proposes to do sequential prediction of adjacent pixel features (via intermediate feature maps) resulting in more spatially smooth outputs for deconvolution layer.  This new layer is referred to as \u2018pixel deconvolution layer\u2019 and it is demonstrated on two tasks of semantic segmentation and face generation. \n\n\nPaper Strengths:\n- Despite being simple technique, the proposed pixel deconvolution layer is novel and interesting",1,-1,-1,1,-1,-1,1,-1,1,-1
B1spAqUp--R3,"This paper proposed the new approach for feature upsampling called pixel deconvolution, which aims to resolve checkboard artifact of conventional deconvolution.  By sequentially applying a series of decomposed convolutions, the proposed method explicitly enforces the model to consider the relation between pixels thus effectively improve the deconvolution network with an increased computational cost to some extent. \n\nOverall, the paper is clearly written and easy to understand the main motivation and methods.  However, the checkboard artifact is a well-known problem of deconvolution network, and has been addressed by several approaches which are simpler than the proposed pixel deconvolution.  For example, it is well known that simple bilinear interpolation optionally followed by convolutions effectively removes checkboard artifact to some extent, and bilinear additive upsampling proposed in Wonja et al., 2017 also demonstrated its effectiveness as an alternative for deconvolution.   Comparisons against these approaches would make the paper stronger.   Besides, comparisons/discussions based on extensive analysis on various deconvolution architectures presented in Wonja et al., 2017 would also be interesting. \n\nWonja et al, The Devil is in the Decoder, In BMVC, 2017\n",1,1,1,1,1,1,1,1,1,-1
B1tC-LT6W-R1,"The problem considered in the paper is of compressing large networks (GRUs) for faster inference at test time.  \n\nThe proposed algorithm uses a two step approach: 1)  use trace norm regularization (expressed in variational form) on dense parameter matrices at training time without constraining the number of parameters,  b) initializing from the SVD of parameters trained in stage 1, learn a new network with reduced number of parameters. \n\nThe experiments on WSJ dataset are promising towards achieving a trade-off between number of parameters and accuracy.  \n\nI have the following questions regarding the experiments:\n1. Could the authors confirm that the reported CERS are on validation/test dataset and not on train/dev data?  It is not explicitly stated.  I hope it is indeed the former, else I have a major concern with the efficacy of the algorithm as ultimately, we care about the test performance of the compressed models in comparison to uncompressed model.  \n\n2. In B.1 the authors use an increasing number units in the hidden layers of the GRUs as opposed to a fixed size like in Deep Speech 2, an obvious baseline that is missing from the  experiments is the comparison with *exact* same GRU (with  768, 1024, 1280, 1536 hidden units) *without any compression*.  \n\n3.  What do different points in Fig 3 and 4 represent.  What are the values of lamdas that were used to train (the l2 and trace norm regularization) the Stage 1 of models shown in Fig 4.  I want to understand what is the difference in the  two types of  behavior of orange points (some of them seem to have good compression while other do not - it the difference arising from initialization or different choice of lambdas in stage 1.  \n\nIt is interesting that although L2 regularization does not lead to low \\nu parameters in Stage 1, the compression stage does have comparable performance to that of trace norm minimization.  The authors point it out, but a further investigation might be interesting.  \n\nWriting:\n1. The GRU model for which the algorithm is proposed is not introduced until the appendix.  While it is a standard network, I think the details should still be included in the main text to understand some of the notation referenced in the text like \u201c\\lambda_rec\u201d and \u201c\\lambda_norec\u201d",1,1,1,1,1,-1,1,-1,-1,-1
B1tC-LT6W-R2,"The authors propose a strategy for compressing RNN acoustic models in order to deploy them for embedded applications.  The technique consists of first training a model by constraining its trace norm, which allows it to be well-approximated by a truncated SVD in a second fine-tuning stage.  Overall, I think this is interesting work,  but I have a few concerns which I\u2019ve listed below:\n\n1. Section 4, which describes the experiments of compressing server sized acoustic models for embedded recognition seems a bit \u201cdisjoint\u201d from the rest of the paper.  I had a number of clarification questions spefically on this section:\n- Am I correct that the results in this section do not use the trace-norm regularization at all?  It would strengthen the paper significantly if the experiments presented on WSJ in the first section were also conducted on the \u201cinternal\u201d task with more data. \n- How large are the training/test sets used in these experiments (for test sets, number of words, for training sets, amount of data in hours (is this ~10,000hrs), whether any data augmentation such as multi-style training was done, etc.) \n- What are the \u201ctier-1\u201d and \u201ctier-2\u201d models in this section?  It would also aid readability if the various models were described more clearly in this section, with an emphasis on structure, output targets, what LMs are used, how are the LMs pruned for the embedded-size models, etc.  Also, particularly given that the focus is on embedded speech recognition, of which the acoustic model is one part, I would like a few more details on how decoding was done, etc. \n- The details in appendix B are interesting, and I think they should really be a part of the main paper.  That being said, the results in Section B.5, as the authors mention, are somewhat preliminary,  and I think the paper would be much stronger if the authors can re-run these experiments were models are trained to convergence. \n- The paper focuses fairly heavily on speech recognition tasks, and I wonder if it would be more suited to a conference on speech recognition.  \n\n2. Could the authors comment on the relative training time of the models with the trace-norm regularizer, L2-regularizer and the unconstrained model in terms of convergence time. \n\n3. Clarification question: For the WSJ experiments was the model decoded without an LM?  If no LM was used, then the choice of reporting results in terms of only CER is reasonable,  but I think it would be good to also report WERs on the WSJ set in either case. \n\n4. Could the authors indicate the range of values of \\lambda_{rec} and \\lambda_{nonrec} that were examined in the work?  Also, on a related note, in Figure 2, does each point correspond to a specific choice of these regularization parameters? \n\n5. Figure 4: For the models in Figure 4, it would be useful to indicate the starting CER of the stage-1 model before stage-2 training to get a sense of how stage-2 training impacts performance. \n\n6. Although the results on the WSJ set are interesting,  I would be curious if the same trends and conclusions can be drawn from a larger dataset -- e.g., the internal dataset that results are reported on later in the paper, or on a set like Switchboard.  I think these experiments would strengthen the paper. \n\n7. The experiments in Section 3.2.3 were interesting, since they demonstrate that the model can be warm-started from a model that hasn\u2019t fully converged.  Could the authors also indicate the CER of the model used for initialization in addition to the final CER after stage-2 training in Figure 5. \n\n8. In Section 4, the authors mention that quantization could be used to compress models further although this is usually degrades WER by 2--4% relative. I think the authors should consider citing previous works which have examined quantization for embedded speech recognition [1], [2]. In particular, note that [2] describes a technique for training with quantized forward passes which results in models that have smaller performance degradation relative to quantization after training. \nReferences:\n[1] Vincent Vanhoucke, Andrew Senior, and Mark Mao, \u201cImproving the speed of neural networks on cpus,\u201d in Deep Learning and Unsupervised Feature Learning Workshop, NIPS, 2011.\n[2] Raziel Alvarez, Rohit Prabhavalkar, Anton Bakhtin, \u201cOn the efficient representation and execution of deep acoustic models,\u201d Proc. of Interspeech, pp. 2746 -- 2750, 2016.\n\n9. Minor comment: The authors use the term \u201cwarmstarting\u201d to refer to the process of training NNs by initializing from a previous model.  It would be good to clarify this in the text.",1,1,1,1,1,1,1,1,1,1
B1tC-LT6W-R3,"Paper is well written and clearly explained.  The paper is a experimental paper as it has more content on the experimentation  and less content on problem definition and formulation.  The experimental section is strong and it has evaluated across different datasets and various scenarios.  However, I feel the contribution of the paper toward the topic is incremental and not significant enough to be accepted in this venue.  It only considers a slight modification into the loss function by adding a trace norm regularization.",1,1,1,1,-1,-1,1,1,1,-1
B1tExikAW-R1,"The idea is clearly stated (but lacks some details) and I enjoyed reading the paper.  \n\nI understand the difference between [Kos+17] and the proposed scheme but I could not understand in which situation the proposed scheme works better.  From the adversary's standpoint, it would be easier to manipulate inputs than latent variables.  On the other hand, I agree that sample-independent perturbation is much more practical than sample-dependent perturbation. \n\nIn Section 3.1, the attack methods #2 and #3 should be detailed more.  I could not imagine how VAE and T are trained simultaneously. \n\nIn Section 3.2, the authors listed a couple of loss functions.  How were these loss functions are combined?  The final optimization problem that is used for training of the propose VAE should be formally defined.  Also, the detailed specification of the VAE should be detailed. \n\nFrom figures in Figure 4 and Figure 5, I could see that the proposed scheme performs successfully in a qualitative manner,  however, it is difficult to evaluate the proposed scheme qualitatively without comparisons with baselines.  For example, can the proposed scheme can be compared with [Kos+17] or some other sample-dependent attacks?  Also, can you experimentally show that attacks on latent variables are more powerful than attacks on inputs?\n\n\n",1,1,1,1,1,1,1,1,1,-1
B1tExikAW-R2,"This paper misses the point of what VAEs (or GANs, in general) are used for.  The idea of using VAEs is not to encode and decode images (or in general any input), but to recover the generating process that created those images so we have an unlimited source of samples.  The use of these techniques for compressing is still unclear and their quality today is too low.  So the attack that the authors are proposing does not make sense and my take is that we should see significant changes before they can make sense.  \n\nBut let\u2019s assume that at some point they can be used as the authors propose.  In which one person encodes an image, send the latent variable to a friend, but a foe intercepts it on the way and tampers with it so the receiver recovers the wrong image without knowing.  Now if the sender believes the sample can be tampered with, if the sender codes z with his private key would not make the attack useless?  I think this will make the first attack useless.  \n\nThe other two attacks require that the foe is inserted in the middle of the training of the VAE.  This is even less doable, because the encoder and decoder are not train remotely.  They are train of the same machine or cluster in a controlled manner by the person that would use the system.  Once it is train it will give away the decoder and keep the encoder for sending information.\n\n",1,1,1,1,-1,-1,1,1,-1,-1
B1tExikAW-R3,"This paper is concerned with both security and machine learning.  \nAssuming that data is encoded, transmited, and decoded using a VAE,\nthe paper proposes a man-in-middle attack that alters the VAE encoding of the input data so that the decoded output will be misclassified. \nThe objectives are to: 1) fool the autoencoder; the classification output of the autoencoder is different from the actual class of the input;  2) make minimal change in the middle so that the attack is not detectable.  \n\nThis paper is concerned with both security and machine learning, but there is no clear contributions to either field.  From the machine learning perspective, the proposed \""attacking\"" method is standard without any technical novelty.  From the security perspective, the scenarios are too simplistic.  The encoding-decoding mechanism being attacked is too simple without any security enhancement.  This is an unrealistic scenario.  For applications with security concerns, there should have been methods to guard against man-in-the-middle attack, and the paper should have at least considered some of them.  Without considering the state-of-the-art security defending mechanism, it is difficult to judge the contribution of the paper to the security community.  \n\nI am not a security expert, but I doubt that the proposed method are formulated based on well founded security concepts and ideas.  For example, what are the necessary and sufficient conditions for an attacking method to be undetectable?  Are the criteria about the magnitude of epsilon given on Section 3.3. necessary and sufficient?  Is there any reference for them?  Why do we require the correspondence between the classification confidence of tranformed and original data?  Would it be enough to match the DISTRIBUTION of the confidence?",1,1,1,1,-1,-1,1,1,1,-1
B1twdMCab-R1,"This paper proposes a model for adding background knowledge to natural language understanding tasks.  The model reads the relevant text and then more assertions gathered from background knowledge before determining the final prediction.  The authors show this leads to some improvement on multiple tasks like question answering and natural language inference (they do not obtain state of the art but improve over a base model, which is fine in my opinion). \n\nI think the paper does a fairly good job at doing what it does,  it is just hard to get excited by it.  \nHere are my major comments:\n\n* The authors explains that the motivation for the work is that one cannot really capture all of the knowledge necessary for doing natural language understanding because the knowledge is very dynamic.  But then they just concept net to augment text.  This is quite a static strategy, I was assuming the authors are going to use some IR method over the web to back up their motivation.  As is, I don't really see how this motivation has anything to do with getting things out of a KB.  A KB is usually a pretty static entity, and things are added to it at a slow pace. \n\n* The author's main claim is that retrieving background knowledge and adding it when reading text can improve performance a little when doing QA and NLI.  Specifically they take text and add common sense knowledge from concept net.  The authors do a good job of showing that indeed the knowledge is important to gain this improvement through analysis.  However, is this statement enough to cross the acceptance threshold of ICLR?  Seems a bit marginal to me. \n\n* The author's propose a specific way of incorporating knowledge into a machine reading algorithm through re-embeddings that have some unique properties of sharing embeddings across lemmas and also having some residual connections that connect embeddings and some processed versions of them.  To me it is unclear why we should use this method for incorporating background knowledge and not some simpler way.  For example, have another RNN read the assertions and somehow integrate that.  The process of re-creating embeddings seems like one choice in a space of many, not the simplest, and not very well motivated.  There are no comparisons to other possibilities.  As a result, it is very hard for me to say anything about whether this particular architecture is interesting or is it just in general that background knowledge from concept net is useful.  As is, I would guess the second is more likely and so I am not convinced the architecture itself is a significant contribution. \n\nSo to conclude, the paper is well-written, clear, and has nice results and analysis.  The conclusion is that reading background knowledge from concept net boost performance using some architecture.  This is nice to know but I think does not cross the acceptance threshold.\n\n",1,1,1,1,-1,-1,1,1,1,-1
B1twdMCab-R2,"The main emphasis of this paper is how to add background knowledge so as to improve the performance of NLU (specifically QA and NLI) systems . They adopt the sensible perspective that background knowledge might most easily be added by providing it in text format.  However, in this paper, the way it is added is simply by updating word representations based on this extra text.  This seems too simple to really be the right way to add background knowledge.  \n\nIn practice, the biggest win of this paper turns out to be that you can get quite a lot of value by sharing contextualized word representations between all words with the same lemma (done by linguistic preprocessing;  the paper never says exactly how, not even if you read the supplementary material).  This seems a useful observation which it would be easy to apply everywhere and which shows fairly large utility from a bit of linguistically sensitive matching!   As the paper notes, this type of sharing is the main delta in this paper from simply using a standard deep LSTM (which the paper claims to not work on these data sets, though I'm not quite sure couldn't be made to work with more tuning). \n\npp. 6-7: The main thing of note seems to be that sharing of representations between words with the same lemma (which the tables refer to as \""reading\"" is worth a lot (3.5-6.0%), in every case rather more than use of background knowledge (typically 0.3-1.5%).  A note on the QA results: The QA results are certainly good enough to be in the range of \""good systems\"",  but none of the results really push the SOTA.  The best SQuAD (devset) results are shown as several percent below the SOTA.  In the table the TriviaQA results are shown as beating the SOTA, and that's fair wrt published work at the time of submission,  but other submissions show that all of these results are below what you get by running the DrQA (Chen et al. 2017) system off-the-shelf on TriviaQA, so the real picture is perhaps similar to SQuAD, especially since DrQA is itself now considerably below the SOTA on SQUAD.  Similar remarks perhaps apply to the NLI results. \n\np.7 In the additional NLI results, it is interesting and valuable to note that the lemmatization and knowledge help much more when amounts of data (and the covarying dimensionality of the word vectors) is much smaller,  but the fact that the ideas of this paper have quite little (or even negative) effects when run on the full data with full word vectors on top of the ESIM model again draws into question whether enough value is being achieved from the world knowledge. \n\nBiggest question:\n - Are word embeddings powerful enough as a form of memory to store the kind of relational facts that you are accessing as background knowledge? \n\nMinor notes:\n - The paper was very well written/edited.  The only real copyediting I noticed was in the conclusion: and be used \u2794 and can be used; that rely on \u2794 that relies on. \n - Should reference to (Manning et al. 1999) better be to (Manning et al. 2008) since the context here appears to be IR systems? \n - On p.3 above sec 3.1: What is u?  Was that meant to be z? \n - On p.8, I'm a bit suspicious of the \""Is additional knowledge used? \"" experiment which trains with knowledge and then tests without knowledge.  It's not surprising that this mismatch might hurt performance, even if the knowledge provided no incremental value over what could be gained from standard word vectors alone. \n - In the supplementary material the paper notes that the numbers are from the best result from 3 runs.  This seems to me a little less good experimental practice than reporting an average of k runs, preferably for k a bit bigger than 3.\n\n\n",1,1,1,1,1,1,1,1,1,-1
B1twdMCab-R3,"The quality of this paper is good.  The presentation is clear  but I find lack of description of a key topic.  The proposed model is not very innovative but works fine for the DQA task.  For the TE task, the proposed method does not perform better than the state-of-the-art systems.  \n\n- As ESIM is one of the key components in the experiments, you should briefly introduce ESIM and explain how you incorporated with your vector representations into ESIM. \n- The reference of ESIM is not correct. \n- Figure 1 is hard to understand.  \n- What corpus did you use to pre-train word vectors?  \n- As the proposed method was successful for the QA task,  you need to explain QA data sets and how the questions are solved. \n- I also expect performance and  error analysis of the task results.   \n- To claim \""task-agnostic\"", you need to try to apply your method to other NLP tasks as well. \n- Page 3. \\Sigma is not defined.",1,1,1,1,1,-1,1,1,1,1
B1uvH_gC--R1,"The paper describes a manifold learning method that adapts the old ideas of multidimensional scaling, with geodesic distances in particular, to neural networks.  The goal is to switch from a non-parametric to a parametric method and hence to have a straightforward out-of-sample extension. \n\nThe paper has several major shortcomings:\n* Any paper dealing with MDS and geodesic distances should test the proposed method on the Swiss roll, which has been the most emblematic benchmark since the Isomap paper in 2000.  Not showing the Swiss roll would possibly let the reader think that the method does not perform well on that example.  In particular, DR is one of the last fields where deep learning cannot outperform older methods like t-SNE.  Please add the Swiss roll example. \n* Distance preservation appears more and more like a dated DR paradigm.  Simple example from 3D to 2D are easily handled but beyond the curse of dimensionality makes things more complicated, in particular due to norm computation.  Computation accuracy of the geodesic distances in high-dimensional spaces can be poor.  This could be discussed and some experiments on very HD data should be reported. \n* Some key historical references are overlooked, like the SAMMANN.  There is also an over-emphasis on spectral methods, with the necessity to compute large matrices and to factorize them, probably owing to the popularity of spectral DR metods a decade ago.  Other methods might be computationally less expensive, like those relying on space-partitioning trees and fast multipole methods (subquadratic complexity).  Finally, auto-encoders could be mentioned as well; they have the advantage of providing the parametric inverse of the mapping too. \n* As a tool for unsupervised learning or exploratory data visualization, DR can hardly benefit from a parametric approach.  The motivation in the end of page 3 seems to be computational only. \n* Section 3 should be further detailed (step 2 in particular). \n* The experiments are rather limited, with only a few artifcial data sets and hardly any quantitative assessment except for some monitoring of the stress.  The running times are not in favor of the proposed method.  The data sets sizes are, however, quite limited, with N<10000 for point cloud data and N<2000 for the image manifold. \n* The conclusion sounds a bit vague and pompous ('by allowing a limited infusion of axiomatic computation...').  What is the take-home message of the paper?",1,1,1,1,1,1,1,1,1,-1
B1uvH_gC--R2,"The authors argue that the spectral dimensionality reduction techniques are too slow, due to the complexity of computing the eigenvalue decomposition, and that they are not suitable for out-of-sample extension.  They also note the limitation of neural networks, which require huge amounts of data to properly learn the data structure.  The authors therefore propose to first sub-sample the data and afterwards to learn an MDS-like cost function directly with a neural network, resulting in a parametric framework. \n\nThe paper should be checked for grammatical errors, such as e.g. consistent use of (no) hyphen in low-dimensional (or low dimensional). \n\nThe abbreviations should be written out on the first use, e.g. MLP, MDS, LLE, etc. \n\nIn the introduction the authors claim that the complexity of parametric techniques does not depend on the number of data points, or that moving to parametric techniques would reduce memory and computational complexities.  This is in general not true.  Even if the number of parameters is small, learning them might require complex computations on the whole data set.  On the other hand, even if the number of parameters is equal to the number of data points, the computations could be trivial, thus resulting in a complexity of O(N). \n\nIn section 2.1, the authors claim \""Spectral techniques are non-parametric in nature\""; this is wrong again.  E.g. PCA can be formulated as MDS (thus spectral), but can be seen as a parametric mapping which can be used to project new words. \n\nIn section 2.2, it says \""observation that the double centering...\"".  Can you provide a citation for this? \n\nIn section 3, the authors propose they technique, which should be faster and require less data than the previous methods, but to support their claim, they do not perform an analysis of computational complexity.  It is not quite clear from the text what the resulting complexity would be.  With N as number of data points and M as number of landmarks, from the description on page 4 it seems the complexity would be O(N + M^2), but the steps 1 and 2 on page 5 suggest it would be O(N^2 + M^2).  Unfortunately, it is also not clear what the complexity of previous techniques, e.g DrLim, is. \n\nFigure 3, contrary to text, does not provide a visualisation to the sampling mechanism. \n\nIn the experiments section, can you provide a citation for ADAM and explain how the parameters were selected?  Also, it is not meaningful to measure the quality of a visualisation via the MDS fit.  There are more useful approaches to this task, such as the quality framework [*]. \n\nIn figure 4a, x-axis should be \""number of landmarks\"". \n\nIt is not clear why the equation 6 holds.  Citation? \nIt is also not clear how exactly the equation 7 is evaluated.  It says \""By varying the number of layers and the number of nodes...\"", but the nodes and layer are not a part of the equation. \n\nThe notation for equation 8 is not explained.  Again, use [*].\n\n[*] Lee, John Aldo ; Verleysen, Michel. Scale-independent quality criteria for dimensionality reduction. In: Pattern Recognition Letters, Vol. 31, no. 14, p. 2248-2257 (2010). doi:10.1016/j.patrec.2010.04.013.\n",1,1,1,1,1,-1,1,1,-1,1
B1uvH_gC--R3,"The key contribution of the paper is a new method for nonlinear dimensionality reduction.  \n\nThe proposed method is (more or less) a modification of the DrLIM manifold learning algorithm (Hadsell, Chopra, LeCun 2006) with a slightly different loss function that is inspired by multidimensional scaling.  While DrLIM only preserves local geometry, the modified loss function presents the opportunity to preserve both local and global geometry.  The rest of the paper is devoted to an empirical validation of the proposed method on small-scale synthetic data (the familiar Swiss roll, as well as a couple of synthetic image datasets).  \n\nThe paper revisits mostly familiar ideas.  The importance of preserving both local and global information in manifold learning is well known, so unclear what the main conceptual novelty is.  This reviewer does not believe that modifying the loss function of a well established previous method that is over 10 years old (DrLIM) constitutes a significant enough contribution. \n\nMoreover, in this reviewer's experience, the major challenge is to obtain proper estimates of the geodesic distances between far-away points on the manifold, and such an estimation is simply too difficult for any reasonable dataset encountered in practice.  However, the authors do not address this, and instead simply use the Isomap approach for approximating geodesics by graph distances, which opens up a completely different set of challenges (how to construct the graph, how to deal with \""holes\"" in the manifold, how to avoid short circuiting in the all-pairs shortest path computations etc etc).  \n\nFinally, the experimental results are somewhat uninspiring.  It seems that the proposed method does roughly as well as Landmark Isomap (with slightly better generalization properties) but is slower by a factor of 1000x.  \n\nThe horizon articulation data, as well as the pose articulation data, are both far too synthetic to draw any practical conclusions. \n",1,1,1,1,-1,1,1,1,-1,-1
B1wN2f2-G-R1,"This paper surveys models for collaborative filtering with user/item covariate.  \n\nOverall the authors seems to have captured the essence of a large number of popular CF models and I found that the proposed model classification is reasonable.  The notation also make it easy to understand the differences between different models.  In that sense this paper could be useful to researchers wanting to better understand this field.  It may also be useful to develop further insights into current models (although the authors do not go that route).  \n\nThe impact of this paper may be limited in this community since it is a survey about a fairly niche topic (a subset of recommender systems) that may not be of central interest at ICLR.  Overall, I think this paper would be a better fit in a recsys, applied ML or information retrieval journal. \n\n\nA few comments: \n\nI find that there are several ways the paper could make a stronger contribution:  \n1) Use the unifying notation to discuss strengths and weaknesses of current approaches (ideally with insights about possible future approaches).\ n2) Report the results of a large study of many of the surveyed models on a large number of datasets.  Ideally further insights could be derived from these results.\ n3) Provide a common code framework with all methods\ n4) Add a discussion on more structured sources of covariates (e.g., social networks).  This could probably more or less easily be added as a subsection using the current classification. \n\n- A similar classification of collaborative filtering models with covariates is proposed in this thesis (p.41):\nhttps://tspace.library.utoronto.ca/bitstream/1807/68831/1/Charlin_Laurent_201406_PhD_thesis.pdf \n\n- The paper is well written overall  but the current version of the paper contains several typos.",1,1,1,1,1,1,1,1,-1,-1
B1wN2f2-G-R2,"This paper provides a survey of attribute-aware collaborative filtering.  In particular, it classifies existing methods into four different categories, according to the representation of the interactions of users, items and attributes.  Furthermore, the authors also provide the probabilistic interpretation of the models.  In addition, preliminary experiments comparing among different categories are also provided.  \n\nThere have existed several works which also provide surveys of attribute-aware collaborative filtering . Hence, the contribution of this paper is limited, although the authors claim two differences between their work and the existing ones.  In particular, the advantages and disadvantages of different categories are not systematically compared, and hence the readers cannot get insightful comments and suggestions from this survey.\ n\nIn general, survey papers are not very suitable for publication at conferences. \n",1,1,1,1,-1,1,1,1,1,-1
B1wN2f2-G-R3,"This paper reviews the existing literature on attribute-based collaborative filtering.  The author categories the existing works int four categories. \n\nWhile the categorization is reasonable , there is no proposed new work beyond the existing approaches.  No new insight is being discussed.  Such survey style paper is not appropriate to for ICLR.",1,1,-1,1,-1,1,1,1,1,-1
B1X0mzZCW-R1,"This paper suggests a simple yet effective approach for learning with weak supervision.  This learning scenario involves two datasets, one with clean data (i.e., labeled by the true function) and one with noisy data, collected using a weak source of supervision.    The suggested approach assumes a teacher and student networks, and builds the final representation incrementally, by taking into account the \""fidelity\"" of the weak label when training the student at the final step.   The fidelity score is given by the teacher, after being trained over the clean data, and it's used to build a cost-sensitive loss function for the students.   The suggested method seems to work well on several document classification tasks.   \n\nOverall, I liked the paper.   I would like the authors to consider the following questions - \n\n- Over the last 10 years or so, many different frameworks for learning with weak supervision were suggested (e.g., indirect supervision, distant supervision, response-based, constraint-based, to name a few).   First, I'd suggest acknowledging these works and discussing the differences to your work.  Second - Is your approach applicable to these frameworks?   It would be an interesting to compare to one of those methods  (e.g., distant supervision for relation extraction using a knowledge base), and see if by incorporating fidelity score, results improve.  \n\n- Can this approach be applied to semi-supervised learning?  Is there a reason to assume the fidelity scores computed by the teacher would not improve the student in a self-training framework? \n\n- The paper emphasizes that the teacher uses the student's initial representation, when trained over the clean data.   Is it clear that this step in needed?  Can you add an additional variant of your framework when the fidelity score are  computed by the teacher when trained from scratch?using different architecture than the student?  \n \n - I went over the authors comments and I appreciate their efforts to help clarify the issues raised.",1,1,1,1,1,1,1,1,1,-1
B1X0mzZCW-R2,"The problem of interest is to train deep neural network models with few labelled training samples.  The specific assumption is there is a large pool of unlabelled data, and a heuristic function that can provide label annotations, possibly with varying levels of noises, to those unlabelled data.  The adopted learning model is of a student/teacher framework as in privileged learning/knowledge distillation/model compression, and also machine teaching.  The student (deep neural network) model will learn from both labelled and unlabelled training data with the labels provided by the teacher (Gaussian process) model.  The teacher also supplies an uncertainty estimate to each predicted label.  How about the heuristic function?  This is used for learning initial feature representation of the student model.  Crucially, the teacher model will also rely on these learned features.  Labelled data and unlabelled data are therefore lie in the same dimensional space.  \n\nSpecific questions to be addressed:\n1)\tClustering of strongly-labelled data points.  Thinking about the statement \u201ceach an expert on this specific region of data space\u201d, if this is the case, I am expecting a clustering for both strongly-labelled data points and weakly-labelled data points.   Each teacher model is trained on a portion of strongly-labelled data, and will only predict similar weakly-labelled data.   On a related remark, the nice side-effect is not right as it was emphasized that data points with a high-quality label will be limited.   As well, GP models, are quite scalable nowadays (experiments with millions to billions of data points are available in recent NIPS/ICML papers, though, they are all rely on low dimensionality of the feature space for optimizing the inducing point locations).   It will be informative to provide results with a single GP model.   \n2)\tFrom modifying learning rates to weighting samples.   Rather than using uncertainty in label annotation as a multiplicative factor in the learning rate, it is more \u201cintuitive\u201d to use it to modify the sampling procedure of mini-batches (akin to baseline #4); sample with higher probability data points with higher certainty.   Here, experimental comparison with, for example, an SVM model that takes into account instance weighting will be informative, and a student model trained with logits (as in knowledge distillation/model compression). \n",1,1,1,1,1,-1,1,1,1,-1
B1X0mzZCW-R3,"The authors propose an approach for training deep learning models for situation where there is not enough reliable annotated data.   This algorithm can be useful because correct annotation of enough cases to train a deep model in many domains is not affordable.   The authors propose to combine a huge number of weakly annotated data with a small set of strongly annotated cases to train a model in a student-teacher framework.   The authors evaluate their proposed methods on one toy problem and two real-world problems.   The paper is well written, easy to follow, and have good experimental study.    My main problem with the paper is the lack of enough motivation and justification for the proposed method; the methodology seems pretty ad-hoc to me and there is a need for more experimental study to show how the methodology work  . Here are some questions that comes to my mind:  (1) Why first building a student model only using the weak data and why not all the data together to train the student model?   To me, it seems that the algorithm first tries to learn a good representation for which lots of data is needed and the weak training data can be useful but why not combing with the strong data?   (2) What are the sensitivity of the procedure to how weakly the weak data are annotated (this could be studied using both toy example and real-world examples)?   (3) The authors explicitly suggest using an unsupervised method (check Baseline no.1) to annotate data weakly?   Why not learning the representation using an unsupervised learning method (unsupervised pre training)?   This should be at least one of the baselines.  \n(4) the idea of using surrogate labels to learn representation is also not new.   One example work is \""Discriminative Unsupervised Feature Learning with Exemplar Convolutional Neural Networks\"". The authors didn't compare their method with this one.",1,1,1,1,1,1,1,1,-1,-1
B1X4DWWRb-R1,"The paper proposes a novel way of causal inference in situations where in causal SEM notation the outcome Y = f(T,X) is a function of a treatment T and covariates X.  The goal is to infer the treatment effect E(Y|T=1,X=x) - E(Y|T=0,X=x) for binary treatments at every location x.  If the treatment effect can be learned, then forecasts of Y under new policies that assign treatment conditional on X will still \""work\"" and the distribution of X can also change without affecting the accuracy of the predictions.  \n\nWhat is proposed seems to be twofold:\n- instead of using a standard inverse probability weighting, the authors construct a bound for the prediction performance under new distributions of X and new policies and learn the weights by optimizing this bound.  The goal is to avoid issues that arise if the ratio between source and target densities become very large or small and the weights in a standard approach would become very sparse, thus leading to a small effective sample size. \n- as an additional ingredient the authors also propose \""representation learning\"" by mapping x to some representation Phi(x).  \nThe goal is to learn the mapping Phi (and its inverse) and the weighting function simultaneously by optimizing the derived bound on the prediction performance.  \n\nPros: \n- The problem is relevant and also appears in similar form in domain adaptation and transfer learning.  \n- The derived bounds and procedures are interesting and nontrivial, even if there is some overlap with earlier work of Shalit et al.  \n\nCons:\n- I am not sure if ICLR is the optimal venue for this manuscript but will leave this decision to others.  \n- The manuscript is written in a very compact style and I wish some passages would have been explained in more depth and detail.  Especially the second half of page 5 is at times very hard to understand as it is so dense.  \n- The implications of the assumptions in Theorem 1 are not easy to understand, especially relating to the quantities B_\\Phi, C^\\mathcal{F}_{n,\\delta} and D^{\\Phi,\\mathcal{H}}_\\delta.  Why would we expect these quantities to be small or bounded?  How does that compare to the assumptions needed for standard inverse probability weighting?   \n- I appreciate that it is difficult to find good test datasets for evaluating causal estimator.    The experiment on the semi-synthetic IHDP dataset is ok, even though there is very little information about its structure in the manuscript (even basic information like number of instances or dimensions seems missing?).  The example does not provide much insight into the main ideas and when we would expect the procedure to work more generally.\n\n\n\n\n\n\n\n\n\n",1,1,1,1,1,1,1,1,1,-1
B1X4DWWRb-R2,"This paper proposes a deep learning architecture for joint learning of feature representation, a target-task mapping function, and a sample re-weighting function.  Specifically, the method tries to discover feature representations, which are invariance in different domains, by minimizing the re-weighted empirical risk and distributional shift between designs. \nOverall, the paper is well written and organized with good description on the related work, research background, and theoretic proofs.  \n\nThe main contribution can be the idea of learning a sample re-weighting function, which is highly important in domain shift.  However, as stated in the paper, since the causal effect of an intervention T on Y conditioned on X is one of main interests, it is expected to add the related analysis in the experiment section.",1,1,1,1,1,1,1,1,1,-1
B1X4DWWRb-R3,"Summary:\nThis paper proposes a new approach to tackle the problem of prediction under\nthe shift in design, which consists of the shift in policy (conditional\ndistribution of treatment given features) and the shift in domain (marginal \ndistribution of features). \n\nGiven labeled samples from a source domain and unlabeled samples from a target\ndomain, this paper proposes to minimize the risk on the target domain by \njointly learning the shift-invariant representation and the re-weighting \nfunction for the induced representations.  According to Lemma 1 and its finite\nsample version in Theorem 1, the risk on the target domain can be upper bounded\nby the combination of 1) the re-weighted empirical risk on the source domain;  \nand 2) the distributional discrepancy between the re-weighted source domain and\nthe target domain.  These theoretical results justify the objective function\nshown in Equation 8.  \n\nExperiments on the IHDP dataset demonstrates the advantage of the proposed\napproach compared to its competing alternatives. \n\nComments:\n1) This paper is well motivated.  For the task of prediction under the shift in\ndesign, shift-invariant representation learning (Shalit 2017) is biased even in\nthe inifite data limit.  On the other hand, although re-weighting methods are\nunbiased, they suffer from the drawbacks of high variance and unknown optimal\nweights.  The proposed approach aims to overcome these drawbacks. \n\n2) The theoretical results justify the optimization procedures presented in\nsection 5.  Experimental results on the IHDP dataset confirm the advantage of\nthe proposed approach.  \n\n3) I have some questions on the details.  In order to make sure the second \nequality in Equation 2 holds, p_mu (y|x,t) = p_pi (y|x,t) should hold as well. \nIs this a standard assumption in the literature? \n\n4) Two drawbacks of previous methods motivate this work, including the bias of\nrepresentation learning and the high variance of re-weighting.  According to\nLemma 1, the proposed method is unbiased for the optimal weights in the large\ndata limit.  However, is there any theoretical guarantee or empirical evidence\nto show the proposed method does not suffer from the drawback of high variance? \n\n5) Experiments on synthetic datasets, where both the shift in policy and the\nshift in domain are simulated and therefore can be controlled, would better \ndemonstrate how the performance of the proposed approach (and thsoe baseline \nmethods) changes as the degree of design shift varies.  \n\n6) Besides IHDP, did the authors run experiments on other real-world datasets, \nsuch as Jobs, Twins, etc?",1,1,1,1,1,1,1,1,1,-1
B1ydPgTpW-R1,"The author(s) proposed to use a deep bidirectional recurrent neural network to estimate the auction price of license plates based on the sequence of letters and digits.  The method uses a learnable character embedding to transform the data, but is an end-to-end approach . The analysis of squared error for the price regression shows a clear advantage of the method over previous models that used hand crafted features.  \nHere are my concerns:\n1) As the price shows a high skewness in Fig. 1, it may make more sense to use relative difference instead of absolute difference of predicted and actual auction price in evaluating/training each model.  That is, making an error of $100 for a plate that is priced $1000 has a huge difference in meaning to that for a plate priced as $10,000.  \n\n2) The time-series data seems to have a temporal trend which makes retraining beneficial as suggested by authors in section 7.2.  If so, the evaluation setting of dividing data into three *random* sets of training, validation, and test, in 5.3 doesn't seem to be the right and most appropriate choice.  It should however, be divided into sets corresponding to non-overlapping time intervals to avoid the model use of temporal information in making the prediction.",1,1,1,1,1,-1,1,1,-1,-1
B1ydPgTpW-R2,"Summary: The authors take two pages to describe the data they eventually analyze - Chinese license plates (sections 1,2), with the aim of predicting auction price based on the \""luckiness\"" of the license plate number.   The authors mentions other papers that use NN's to predict prices, contrasting them with the proposed model by saying they are usually shallow not deep, and only focus on numerical data not strings.  Then the paper goes on to present the model which is just a vanilla RNN, with standard practices like batch normalization and dropout.   The proposed pipeline converts each character to an embedding with the only sentence of description being \""Each character is converted by a lookup table to a vector representation, known as character embedding.\""    Specifics of the data,  RNN training, and the results as well as the stability of the network to hyperparameters is also examined.  Finally they find a \""a feature vector for each plate by summing up the output of the last recurrent layer overtime. \ and the use knn on these features to find other plates that are grouped together to try to explain how the RNN predicts the prices of the plates.  In section 7,  the RNN is combined with a handcrafted feature model he criticized in a earlier section for being too simple to create an ensemble model that predicts the prices marginally better.  \n\nSpecific Comments on Sections: \nComments: Sec 1,2\nIn these sections the author has somewhat odd references to specific economists that seem a little off topic, and spends a little too much time in my opinion setting up this specific data. \n\nSec 3\nThe author does not mention the following reference: \""Deep learning for stock prediction using numerical and textual information\"" by Akita et al. that does incorporate non-numerical info to predict stock prices with deep networks. \n\nSec 4\nWhat are the characters embedded with? This is important to specify.  Is it Word2vec or something else?  What does the lookup table consist of?  References should be added to the relevant methods.  \n\nSec 5\nI feel like there are many regression models that could have been tried here with word2vec embeddings that would have been an interesting comparison.  LSTMs as well could have been a point of comparison.  \n\nSec 6\n Nothing too insightful is said about the RNN Model.  \n\nSec 7\nThe ensembling was a strange extension especially with the Woo model given that the other MLP architecture gave way better results in their table. \n\nOverall: This is a unique NLP problem, and it seems to make a lot of sense to apply an RNN here, considering that word2vec is an RNN.  However comparisons are lacking and the paper is not presented very scientifically.   The lack of comparisons made it feel like the author cherry picked the RNN to outperform other approaches that obviously would not do well.\n",1,1,1,1,1,1,1,1,1,-1
B1ydPgTpW-R3,"The authors present a deep neural network that evaluates plate numbers.  The relevance of this problem is that there are auctions for plate numbers in Hong Kong, and predicting their value is a sensible activity in that context.  I find that the description of the applied problem is quite interesting; in fact overall the paper is well written and very easy to follow.  There are some typos and grammatical problems (indicated below),  but nothing really serious. \n\nSo, the paper is relevant and well presented.  However, I find that the proposed solution is an application of existing techniques, so it lacks on novelty and originality.  Even though the significance of the work is apparent given the good results of the proposed neural network,  I believe that such material is more appropriate to a focused applied meeting.  However, even for that sort of setting I think the paper requires some additional work, as some final parts of the paper have not been tested yet (the interesting part of explanations).  Hence I don't think the submission is ready for publication at this moment. \n\nConcerning the text, some questions/suggestions:\n- Abstract, line 1: I suppose \""In the Chinese society...\""--- are there many Chinese societies? \n- The references are not properly formatted; they should appear at (XXX YYY) but appear as XXX (YYY) in many cases, mixed with the main text.  \n- Footnote 1, line 2: \""an exchange\"". \n- Page 2, line 12: \""prices.  Among\"".\n- Please add commas/periods at the end of equations. \n- There are problems with capitalization in the references.",1,1,1,1,1,-1,1,1,1,-1
B1Z3W-b0W-R1,"This paper proposes an iterative inference scheme for latent variable models that use inference networks.  Instead of using a fixed-form inference network, the paper proposes to use the learning to learn approach of Andrychowicz et. al.  The parameter of the inference network is still a fixed quantity but the function mapping is based on a deep network (e.g. it could be RNN but the experiments uses a feed-forward network). \n\nMy main issue with the paper is that it does not do a good job justifying the main advantages of the proposed approach.  It appears that the iterative method should result in \""direct improvement with additional samples and inference iterations\"".  I am supposing this is at the test time.  It is not clear exactly when this will be useful.  \n\nI believe an iterative approach is also possible to perform with the standard VAE, e.g., by bootstrapping over the input data and then using the iterative scheme of Rezende et. al.  2014 (they used this method to perform data imputation). \n\nThe paper should also discuss the additional difficulty that arises when training the proposed model and compare them to training of standard inference networks in VAE. \n\nIn summary, the paper needs to do a better job in justifying the advantages obtained by the proposed method.",1,1,1,1,1,1,1,1,-1,-1
B1Z3W-b0W-R2,"This paper proposes a learning-to-learn approach to training inference networks in VAEs that make explicit use of the gradient of the log-likelihood with respect to the latent variables to iteratively optimize the variational distribution.  The basic approach follows Andrychowicz et al. (2016), but there are some extra considerations in the context of learning an inference algorithm.  \n\nThis approach can significantly reduce the amount of slack in the variational bound due to a too-weak inference network (above and beyond the limitations imposed by the variational family).   This source of error is often ignored in the literature,  although there are some exceptions that may be worth mentioning:\n* Hjelm et al. (2015; https://arxiv.org/pdf/1511.06382.pdf) observe it for directed belief networks (admittedly a different model class). \n* The ladder VAE paper by Sonderby et al. (2016, https://arxiv.org/pdf/1602.02282.pdf) uses an architecture that reduces the work that the encoder network needs to do, without increasing the expressiveness of the variational approximation. \n* The structured VAE paper by Johnson et al. (2016, https://arxiv.org/abs/1603.06277) also proposes an architecture that reduces the load on the inference network.\n* A very recent paper by Krishnan et al. (https://arxiv.org/pdf/1710.06085.pdf, posted to arXiv days before the ICLR deadline) is probably closest; it also examines using iterative optimization (but no learning-to-learn) to improve training of VAEs.  They remark that the benefits on binarized MNIST are pretty minimal compared to the benefits on sparse, high-dimensional data like text and recommendations; this suggests that the learning-to-learn approach in this paper may shine more if applied to non-image datasets and larger numbers of latent variables. \n\nI think this is good and potentially important work,  although I do have some questions/concerns about the results in Table 1 (see below). \n\n(Dempster et al. 1977) is not the best reference for this section; that paper only considers the case where the E and M steps can be done in closed form on the whole dataset.  A more relevant reference would be Stochastic Variational Inference by Hoffman et al. (2013), which proposes using iterative optimization of variational parameters in the inner loop of a stochastic optimization algorithm. \n\nSection 4: The statement p(z)=N(z;mu_p,Sigma_p) doesn\u2019t quite match the formulation of Rezende&Mohamed (2014).  First, in the case where there is only one layer of latent variables, there is almost never any reason to use anything but a normal(0, I) prior, since the first weight matrix of the decoder can reproduce the effects of any mean or covariance.  Second, in the case where there are two or more layers, the joint distribution of all z need not be Gaussian (or even unimodal) since the means and variances at layer n can depend nonlinearly on the value of z at layer n+1.  An added bonus of eliminating the mu_p, Sigma_p: you could get rid of one subscript in mu_q and sigma_q, which would reduce notational clutter. \n\nWhy not have mu_{q,t+1} depend on sigma_{q,t} as well as mu_{q,t}? \n\nTable 1: These results are strange in a few ways:\n* The gap between the standard and iterative inference network seems very small (0.3 nats at most).  This is much smaller than the gap in Figure 5(a). \n* The MNIST results are suspiciously good overall, given that it\u2019s ultimately a Gaussian approximation and simple fully connected architecture.  I\u2019ve read a lot of papers evaluating that sort of model/variational distribution as a baseline, and I don\u2019t think I\u2019ve ever seen a number better than ~87 nats.",1,1,1,1,1,1,1,1,1,-1
B1Z3W-b0W-R3,"Instead of either optimization-based variational EM or an amortized inference scheme implemented via a neural network as in standard VAE models, this paper proposes a hybrid approach that essentially combines the two.   In particular, the VAE inference step, i.e., estimation of q(z|x), is conducted via application of a recent learning-to-learn paradigm (Andrychowicz et al., 2016), whereby direct gradient ascent on the ELBO criteria with respect to moments of q(z|x) is replaced with a neural network that iteratively outputs new parameter estimates using these gradients.   The resulting iterative inference framework is applied to a couple of small datasets and shown to produce both faster convergence and a better likelihood estimate. \n\nAlthough probably difficult for someone to understand that is not already familiar with VAE models, I felt that this paper was nonetheless clear and well-presented, with a fair amount of useful background information and context.   From a novelty standpoint though, the paper is not especially strong given that it represents a fairly straightforward application of (Andrychowicz et al., 2016).   Indeed the paper perhaps anticipates this perspective and preemptively offers that \""variational inference is a qualitatively different optimization problem\"" than that considered in (Andrychowicz et al., 2016), and also that non-recurrent optimization models are being used for the inference task, unlike prior work.   But to me, these are rather minor differentiating factors, since learning-to-learn is a quite general concept already, and the exact model structure is not the key novel ingredient.   That being said, the present use for variational inference nonetheless seems like a nice application, and the paper presents some useful insights such as Section 4.1 about approximating posterior gradients. \n\nBeyond background and model development, the paper presents a few experiments comparing the proposed iterative inference scheme against both variational EM, and pure amortized inference as in the original, standard VAE.   While these results are enlightening,  most of the conclusions are not entirely unexpected.   For example, given that the model is directly trained with the iterative inference criteria in place, the reconstructions from Fig. 4 seem like exactly what we would anticipate, with the last iteration producing the best result.   It would certainly seem strange if this were not the case.   And there is no demonstration of reconstruction quality relative to existing models, which could be helpful for evaluating relative performance.   Likewise for Fig. 6, where faster convergence over traditional first-order methods is demonstrated; but again, these results are entirely expected as this phenomena has already been well-documented in (Andrychowicz et al., 2016). \n\nIn terms of Fig. 5(b) and Table 1, the proposed approach does produce significantly better values of the ELBO critera; however, is this really an apples-to-apples comparison?   For example, does the standard VAE have the same number of parameters/degrees-of-freedom as the iterative inference model, or might eq. (4) involve fewer parameters than eq. (5) since there are fewer inputs?  Overall, I wonder whether iterative inference is better than standard inference with eq. (4), or whether the recurrent structure from eq. (5) just happens to implicitly create a better neural network architecture for the few examples under consideration.   In other words, if one plays around with the standard inference architecture a bit, perhaps similar results could be obtained. \n\n\nOther minor comment:\n* In Fig. 5(a), it seems like the performance of the standard inference model is still improving  but the iterative inference model has mostly saturated. \n* A downside of the iterative inference model not discussed in the paper is that it requires computing gradients of the objective even at test time, whereas the standard VAE model would not.",1,1,1,1,1,1,1,1,1,-1
B1ZvaaeAZ-R1,"The paper studies the effect of reduced precision weights and activations on the performance, memory and computation cost of deep networks and proposes a quantization scheme and wide filters to offset the accuracy lost due to the reduced precision.  The study is performed on AlexNet, ResNet and Inception on the Imagenet datasets and results show that accuracy matching the full precision baselines can be obtained by widening the filters on the networks.  \n\nPositives\n- Using lower precision activations to save memory and compute seems new and widening the filter sizes seems to recover the accuracy lost due to the lower precision. \n\nNegatives\n- While the exhaustive analysis is extremely useful  the overall technical contribution of the paper that of widening the networks is fairly small.  \n- The paper motivates the need for reduced precision weights from the perspective of saving memory footprint when using large batches.  However, the results are more focused on compute cost.  Also large batches are used mainly during training where memory is generally not a huge issue.  Memory critical situations such as inference on mobile phones can be largely mitigated by using smaller batch sizes.  It might help to emphasize the speed-up in compute more in the contributions.",1,1,1,1,1,-1,1,1,1,-1
B1ZvaaeAZ-R2,"This is a well-written paper with good comparisons to a number of earlier approaches.  It focuses on an approach to get similar accuracy at lower precision, in addition to cutting down the compute costs.  Results with 2-bit activations and 4-bit weights seem to match baseline accuracy across the models listed in the paper. \n\nOriginality\nThis seems to be first paper that consistently matches baseline results below int-8 accuracy, and shows a promising future direction. \n\nSignificance\nGoing down to below 8-bits and potentially all the way down to binary (1-bit weights and activations) is a promising direction for future hardware design.  It has the potential to give good results at lower compute and more significantly in providing a lower power option, which is the biggest constraint for higher compute today.  \n\nPros:\n- Positive results with low precision (4-bit, 2-bit and even 1-bit) \n- Moving the state of the art in low precision forward \n- Strong potential impact, especially on constrained power environments (but not limited to them) \n- Uses same hyperparameters as original training, making the process of using this much simpler. \n\nCons/Questions\n- They mention not quantizing the first and last layer of every network.  How much does that impact the overall compute?  \n- Is there a certain width where 1-bit activation and weights would match the accuracy of the baseline model?  This could be interesting for low power case, even if the \""effective compute\"" is larger than the baseline.\n",1,1,1,1,1,1,1,1,1,-1
B1ZvaaeAZ-R3,"This paper presents an simple and interesting idea to improve the performance for neural nets.  The idea is we can reduce the precision for activations and increase the number of filters, and is able to achieve better memory usage (reduced).  The paper is aiming to solve a practical problem, and has done some solid research work to validate that.   In particular, this paper has also presented a indepth study on AlexNet with very comprehensive results and has validated the usefulness of this approach.    \n\nIn addition, in their experiments, they have demonstrated pretty solid experimental results, on AlexNet and even deeper nets such as the state of the art Resnet.  The results are convincing to me.  \n\nOn the other side, the idea of this paper does not seem extremely interesting to me, especially many decisions are quite natural to me, and it looks more like a very empirical practical study.  So the novelty is limited. \n\nSo overall given limited novelty  but the paper presents useful results,  I would recommend borderline leaning towards reject.",1,1,1,1,-1,-1,1,1,1,-1
BJ0hF1Z0b-R1,"Summary: The paper provides the first evidence of effectively training large RNN based language models under the constraint of differential privacy.  The paper focuses on the user-level privacy setting, where the complete contribution of a single user is protected as opposed to protecting a single training example.  The algorithm is based on the Federated Averaging and Federated Stochastic gradient framework. \n\nPositive aspects of the paper: The paper is a very strong empirical paper, with experiments comparable to industrial scale.  The paper uses the right composition tools like moments accountant to get strong privacy guarantees.  The main technical ideas in the paper seem to be i) bounding the sensitivity for weighted average queries, and ii) clipping strategies for the gradient parameters, in order to control the norm.  Both these contributions are important in the effectiveness of the overall algorithm. \n\nConcern: The paper seems to be focused on demonstrating the effectiveness of previous approaches to the setting of language models.  I did not find strong algorithmic ideas in the paper.  I found the paper to be lacking in that respect",1,1,1,1,-1,-1,1,1,1,-1
BJ0hF1Z0b-R2,"\nSummary of the paper\n-------------------------------\n\nThe authors propose to add 4 elements to the 'FederatedAveraging' algorithm to provide a user-level differential privacy guarantee.  The impact of those 4 elements on the model'a accuracy and privacy is then carefully analysed. \n\nClarity, Significance and Correctness\n--------------------------------------------------\n\nClarity: Excellent \n\nSignificance: I'm not familiar with the literature of differential privacy, so I'll let more knowledgeable reviewers evaluate this point. \n\nCorrectness: The paper is technically correct. \n\nQuestions\n--------------\n\n1. Figure 1: Adding some noise to the updates could be view as some form of regularization, so I have trouble understand why the models with noise are less efficient than the baseline. \n2. Clipping is supposed to help with the exploding gradients problem.  Do you have an idea why a low threshold hurts the performances?  Is it because it reduces the amplitude of the updates (and thus simply slows down the training)? \n3. Is your method compatible with other optimizers, such as RMSprop or ADAM (which are commonly used to train RNNs)? \n\nPros\n------\n\n1. Nice extensions to FederatedAveraging to provide privacy guarantee. \n2. Strong experimental setup that analyses in details the proposed extensions. \n3. Experiments performed on public datasets. \n\nCons\n-------\n\nNone \n\nTypos\n--------\n\n1. Section 2, paragraph 3 : \""is given in Figure 1\"" -> \""is given in Algorithm 1\""\n\n Note\n-------\n\nSince I'm not familiar with the differential privacy literature, I'm flexible with my evaluation based on what other reviewers with more expertise have to say.",1,1,1,1,1,-1,1,1,-1,-1
BJ0hF1Z0b-R3,"This paper extends the previous results on differentially private SGD to user-level differentially private recurrent language models.  It experimentally shows that the proposed differentially private LSTM achieves comparable utility compared to the non-private model. \n\nThe idea of training differentially private neural network is interesting and very important to the machine learning + differential privacy community.  This work makes a pretty significant contribution to such topic.  It adapts techniques from some previous work to address the difficulties in training language model and providing user-level privacy.  The experiment shows good privacy and utility. \n\nThe presentation of the paper can be improved a bit.  For example, it might be better to have a preliminary section before Section2 introducing the original differentially private SGD algorithm with clipping, the original FedAvg and FedSGD, and moments accountant as well as privacy amplification; otherwise, it can be pretty difficult for readers who are not familiar with those concepts to fully understand the paper.  Such introduction can also help readers understand the difficulty of adapting the original algorithms and appreciate the contributions of this work.\n",1,1,1,1,1,1,1,1,1,-1
BJ4prNx0W-R1,"Quality\nThe paper is well-written and clear, and includes relevant comparisons to previous work (NPI and recursive NPI). \n\nClarity\nThe paper is clearly written. \n\nOriginality\nTo my knowledge the method proposed in this work is novel.  It is the first to study constructing minimal training sets for NPI given a black-box oracle.  However, as pointed out by the authors, there is a lot of similar prior work in software testing. \n\nSignificance\nThe work could be potentially significant,  but there are some very strong assumptions made in the paper that could limit the impact.  If the NPI has access to a black-box oracle, it is not clear what is the use of training an NPI in the first place.  It would be very helpful to describe a potential scenario where the proposed approach could be useful.  Also, it is assumed that the number of possible inputs is finite (also true for the recursive NPI paper), and it is not clear what techniques or lessons of this paper might transfer to tasks with perceptual inputs.  The main technical contribution is the search procedure to find minimal training sets and pare down the observation size, and the empirical validation of the idea on several algorithmic tasks. \n\nPros\n- Greatly improves the data efficiency of recursive NPI. \n- Training and verification sets are automatically generated by the proposed method. \n\nCons\n- Requires access to a black-box oracle to construct the dataset. \n- Not clear that the idea will be useful in more complex domains with unbounded inputs.\n",1,1,1,1,1,1,1,1,1,-1
BJ4prNx0W-R2,"In this paper, the authors consider the problem of generating a training data set for the neural programmer-interpreter from an executable oracle.  In particular, they aim at generating a complete set that fully specifies the behavior of the oracle.  The authors propose a technique that achieves this aim by borrowing ideas from programming language and abstract interpretation.  The technique systematically interacts with the oracle using observations, which are abstractions of environment states, and it is guaranteed to produce a data set that completely specifies the oracle.  The authors later describes how to improve this technique by further equating certain observations and exploring only one in each equivalence class.  Their experiments show that this improve technique can produce complete training sets for three programs. \n\nIt is nice to see the application of ideas from different areas for learning-related questions.  However, there is one thing that bothers me again and again. Why do we need a data-generation technique in the paper at all?  Typically, we are given a set of data, not an oracle that can generate such data, and our task is to learn something from the data.  If we have an executable oracle, it is now clear to me why we want to replicate this oracle by an instance of the neural programmer-interpreter.  One thing that I can see is that the technique in the paper can be used when we do research on the neural programmer-interpreter.  During research, we have multiple executable oracles and need to produce good training data from them.  The authors' technique may let us do this data-generation easily.  But this benefit to the researchers does not seem to be strong enough for the acceptance at ICLR'18.\n\n",1,1,1,1,-1,-1,1,1,1,-1
BJ4prNx0W-R3,"Previous work by Cai et al. (2017) shows how to use Neural Programmer-Interpreter (NPI) framework to prove correctness of a learned neural network program by introducing recursion.  It requires generation of a diverse training set consisting of execution traces which describe in detail the role of each function in solving a given input problem.  Moreover, the traces need to be recursive: each function only takes a finite, bounded number of actions.  In this paper, the authors show how training set can be generated automatically satisfying the conditions of Cai et al.'s paper.  They iteratively explore all\npossible behaviors of the oracle in a breadth-first manner, and the bounded nature of the recursive\noracle ensures that the procedure converges.  As a running example, they show how this can be be done for bubblesort.  The training set generated in this Fprocess may have a lot of duplicates, and the authors show how these duplicates can possibly be removed.  It indeeds shows a dramatic reduction in the number of training samples for the three experiments that have been shown in the paper.  \n\nI am not an expert in this area, so it is difficult for me to judge the technical merit of the work.  My feeling from reading the paper is that it is rather incremental over Cai et al.  I am impressed by the results of the three experiments that have been shown here, specifically, the reduction in the training samples once they have been generated is significant.  But these are also the same set of experiments performed by Cai et al.  \n\nGiven the original number of traces generated is huge, I do not understand, why this method is at all practical.  This also explains why the authors have just tested the performance on extremely small sized data. It will not scale.  So, I am hesitant accepting the paper.  I would have been more enthusiastic if the authors had proposed a way to combine the training space exploration as well as removing redundant traces together to make the whole process more scalable and done experiments on reasonably sized data.",1,1,1,1,1,1,1,1,1,-1
BJ78bJZCZ-R1,"The authors present RDA, the Recurrent Discounted Attention unit, that improves upon RWA, the earlier introduced Recurrent Weighted Average unit, by adding a discount factor.  While the RWA was an interesting idea  with bad results (far worse than the standard GRU or LSTM with standard attention except for hand-picked tasks), the RDA brings it more on-par with the standard methods. \n\nOn the positive side, the paper is clearly written and adding discount to RWA, while a small change, is original.  On the negative side, in almost all tasks the RDA is on par or worse than the standard GRU  - except for MultiCopy where it trains faster,  but not to better results and it looks like the difference is between few and very-few training steps anyway.  The most interesting result is language modeling on Hutter Prize Wikipedia, where RDA very significantly improves upon RWA - but again, only matches a standard GRU or LSTM.  So the results are not strongly convincing, and the paper lacks any mention of newer work on attention.  This year strong improvements over state-of-the-art have been achieved using attention for translation (\""Attention is All You Need\"") and image classification (e.g., Non-local Neural Networks, but also others in ImageNet competition).  To make the evaluation convincing enough for acceptance, RDA should be combined with those models and evaluated more competitively on multiple widely-studied tasks.",1,1,1,1,1,1,1,1,1,-1
BJ78bJZCZ-R2,"This paper extends the recurrent weight average (RWA, Ostmeyer and Cowell, 2017) in order to overcome the limitation of the original method while maintaining its advantage.  The motivation of the paper and the approach taken by the authors are sensible, such as adding discounting was applied to introduce forget mechanism to the RWA and manipulating the attention and squash functions. \n\nThe proposed method is using Elman nets as the base RNN.  I think the same method can be applied to GRUs or LSTMs . Some parameters might be redundant,  however, assuming that this kind of attention mechanism is helpful for learning long-term dependencies and can be computed efficiently, it would be nice to see the outcomes of this combination. \n\nIs there any explanation why LSTMs perform so badly compared to GRUs, the RWA and the RDA? \nOverall, the proposed method seems to be very useful for the RWA.",1,1,1,1,1,-1,1,1,1,-1
BJ78bJZCZ-R3,"Summary:\nThis paper proposes an extension to the RWA model by introducing the discount gates to computed discounted averages instead of the undiscounted attention.  The problem with the RWA is that the averaging mechanism can be numerically unstable due to the accumulation operations when computing d_t. \n\nPros:\n- Addresses an issue of RWAs. \n\nCons:\n-The paper addresses a problem with an issue with RWAs.  But it is not clear to me why would that be an important contribution. \n-The writing needs more work. \n-The experiments are lacking and the results are not good enough. \n\nGeneral Comments:\n\nThis paper addresses an issue regarding to RWA which is not really widely adopted and well-known architecture, because it seems to have some have some issues that this paper is trying to address.  I would still like to have a better justification on why should we care about RWA and fixing that model.  \n\nThe writing of this paper seriously needs more work.   The Lemma 1 doesn't make sense to me, I think it has a typo in it, it should have been (-1)^t c instead of -1^t c. \n\nThe experiments are only on toyish and small scale tasks.  According to the results the model doesn't really do better than a simple LSTM or GRU.",1,1,1,1,-1,-1,1,1,1,-1
BJ8vJebC--R1,"This paper investigates the impact of character-level noise on various flavours of neural machine translation.  It tests 4 different NMT systems with varying degrees and types of character awareness, including a novel meanChar system that uses averaged unigram character embeddings as word representations on the source side.  The authors test these systems under a variety of noise conditions, including synthetic scrambling and keyboard replacements, as well as natural (human-made) errors found in other corpora and transplanted to the training and/or testing bitext via replacement tables.  They show that all NMT systems, whether BPE or character-based, degrade drastically in quality in the presence of both synthetic and natural noise, and that it is possible to train a system to be resistant to these types of noise by including them in the training data.  Unfortunately, they are not able to show any types of synthetic noise helping address natural noise.  However, they are able to show that a system trained on a mixture of error types is able to perform adequately on all types of noise. \n\nThis is a thorough exploration of a mostly under-studied problem.  The paper is well-written and easy to follow.  The authors do a good job of positioning their study with respect to related work on black-box adversarial techniques, but overall, by working on the topic of noisy input data at all, they are guaranteed novelty.  The inclusion of so many character-based systems is very nice, but it is the inclusion of natural sources of noise that really makes the paper work.  Their transplanting of errors from other corpora is a good solution to the problem, and one likely to be built upon by others.  In terms of negatives, it feels like this work is just starting to scratch the surface of noise in NMT.  The proposed meanChar architecture doesn\u2019t look like a particularly good approach to producing noise-resistant translation systems, and the alternative solution of training on data where noise has been introduced through replacement tables isn\u2019t extremely satisfying.  Furthermore, the use of these replacement tables means that even when the noise is natural, it\u2019s still kind of artificial.  Finally, this paper doesn\u2019t seem to be a perfect fit for ICLR, as it is mostly experimental with few technical contributions that are likely to be impactful; it feels like it might be more at home and have greater impact in a *ACL conference. \n\nRegarding the artificialness of their natural noise - obviously the only solution here is to find genuinely noisy parallel data, but even granting that such a resource does not yet exist, what is described here feels unnaturally artificial.  First of all, errors learned from the noisy data sources are constrained to exist within a word.  This tilts the comparison in favour of architectures that retain word boundaries (such as the charCNN system here), while those systems may struggle with other sources of errors such as missing spaces between words.  Second, if I understand correctly, once an error is learned from the noisy data, it is applied uniformly and consistently throughout the training and/or test data.  This seems worse than estimating the frequency of the error and applying them stochastically (or trying to learn when an error is likely to occur).  I feel like these issues should at least be mentioned in the paper, so it is clear to the reader that there is work left to be done in evaluating the system on truly natural noise.\n\nAlso, it is somewhat jarring that only the charCNN approach is included in the experiments with noisy training data (Table 6).  I realize that this is likely due to computational or time constraints, but it is worth providing some explanation in the text for why the experiments were conducted in this manner.  On a related note, the line in the abstract stating that \u201c... a character convolutional neural network  is able to simultaneously learn representations robust to multiple kinds of noise\u201d implies that the other (non-charCNN) architectures could not learn these representations, when in reality, they simply weren\u2019t given the chance. \n\nSection 7.2 on the richness of natural noise is extremely interesting,  but maybe less so to an ICLR audience.  From my perspective, it would be interesting to see that section expanded, or used as the basis for future work on improve architectures or training strategies. \n\nI have only one small, specific suggestion: at the end of Section 3, consider deleting the last paragraph break, so there is one paragraph for each system (charCNN currently has two paragraphs).\n\n[edited for typos]",1,1,1,1,1,1,1,1,1,-1
BJ8vJebC--R2,"This paper empirically investigates the performance of character-level NMT systems in the face of character-level noise, both synthesized and natural.  The results are not surprising:\n\n* NMT is terrible with noise. \n\n* But it improves on each noise type when it is trained on that noise type. \n\nWhat I like about this paper is that:\n\n1) The experiments are very carefully designed and thorough. \n\n2) This problem might actually matter.  Out of curiosity, I ran the example (Table 4) through Google Translate, and the result was gibberish.  But as the paper shows, it\u2019s easy to make NMT robust to this kind of noise, and Google (and other NMT providers) could do this tomorrow.  So this paper could have real-world impact. \n\n3) Most importantly, it shows that NMT\u2019s handling of natural noise does *not* improve when trained with synthetic noise; that is, the character of natural noise is very different.  So solving the problem of natural noise is not so simple\u2026 it\u2019s a *real* problem.  Speculating, again: commercial MT providers have access to exactly the kind of natural spelling correction data that the researchers use in this paper, but at much larger scale.  So these methods could be applied in the real world.  (It would be excellent if an outcome of this paper was that commercial MT providers answered it\u2019s call to provide more realistic noise by actually providing examples.) \n\nThere are no fancy new methods or state-of-the-art numbers in this paper.  But it\u2019s careful, curiosity-driven empirical research of the type that matters, and it should be in ICLR.",1,1,1,1,-1,-1,1,1,1,-1
BJ8vJebC--R3,"This paper investigates the impact of noisy input on Machine Translation, and tests simple ways to make NMT models more robust. \n\nOverall the paper is a clearly written, well described report of several experiments.  It shows convincingly that standard NMT models completely break down on both natural \""noise\"" and various types of input perturbations.  It then tests how the addition of noise in the input helps robustify the charCNN model somewhat.  The extent of the experiments is quite impressive: three different NMT models are tried, and one is used in extensive experiments with various noise combinations. \n\nThis study clearly addresses an important issue in NMT and will be of interest to many in the NLP community.  The outcome is not entirely surprising (noise hurts and training and the right kind of noise helps)  but the impact may be.  I wonder if you could put this in the context of \""training with input noise\"", which has been studied in Neural Network for a while (at least since the 1990s).  I.e. it could be that each type of noise has a different regularizing effect, and clarifying what these regularizers are may help understand the impact of the various types of noise.  Also, the bit of analysis in Sections 6.1 and 7.1 is promising, if maybe not so conclusive yet. \n\nA few constructive criticisms:\n\nThe way noise is included in training (sec. 6.2) could be clarified (unless I missed it) e.g. are you generating a fixed \""noisy\"" training set and adding that to clean data?  Or introducing noise \""on-line\"" as part of the training?  If fixed, what sizes were tried?  More information on the experimental design would help. \n\nTable 6 is highly suspect: Some numbers seem to have been copy-pasted in the wrong cells, eg. the \""Rand\"" line for German, or the Swap/Mid/Rand lines for Czech.  It's highly unlikely that training on noisy Swap data would yield a boost of +18 BLEU points on Czech -- or you have clearly found a magical way to improve performance. \n\nAlthough the amount of experiment is already important, it may be interesting to check whether all se2seq models react similarly to training with noise: it could be that some architecture are easier/harder to robustify in this basic way. \n\n[Response read -- thanks]\nI agree with authors that this paper is suitable for ICLR, although it will clearly be of interest to ACL/MT-minded folks.",1,1,1,1,1,1,1,1,1,-1
BJaU__eCZ-R1,"Quality\n\nThis is a very clear contribution which elegantly demonstrates the use of extensions of GAN variants in the context of neuroimaging. \n\nClarity\n\nThe paper is well-written.  Methods and results are clearly described.  The authors state significant improvements in classification using generated data.  These claims should be substantiated with significance tests comparing classification on standard versus augmented datasets. \n\nOriginality\n\nThis is one of the first uses of GANs in the context of neuroimaging.""  \n\nSignificance \n\nThe approach outlined in this paper may spawn a new research direction. \n\nPros\n\nWell-written and original contribution demonstrating the use of GANs in the context of neuroimaging. \n\nCons\n\nThe focus on neuroimaging might be less relevant to the broader AI community",1,1,1,1,1,-1,1,1,1,-1
BJaU__eCZ-R2,"This paper proposes to use 3D conditional GAN models to generate\nfMRI scans.  Using the generated images, paper reports improvement\nin classification accuracy on various tasks. \n\nOne claim of the paper is that a generative model of fMRI\ndata can help to caracterize and understand variability of scans\nacross subjects. \n\nArticle is based on recent works such as Wasserstein GANs and AC-GANs\nby (Odena et al., 2016). \n\nDespite the rich literature of this recent topic the related work\nsection is rather convincing. \n\nModel presented extends IW-GAN by using 3D convolution and also\nby supervising the generator using sample labels. 1\n\nMajor:\n\n- The size of the generated images is up to 26x31x22 which is limited\n(about half the size of the actual resolution of fMRI data).  As a\nconsequence results on decoding learning task using low resolution\nimages can end up worse than with the actual data (as pointed out). \nWhat it means is that the actual impact of the work is probably limited. \n\n- Generating high resolution images with GANs even on faces for which\nthere is almost infinite data is still a challenge.  Here a few thousand\ndata points are used.  So it raises too concerns: First is it enough? \nUsing so-called learning curves is a good way to answer this.  Second\nis what are the contributions to the state-of-the-art of the 2\nmethods introduced?  Said differently, as there\nis no classification results using images produced by an another\nGAN architecture it is hard to say that the extra complexity\nproposed here (which is a bit contribution of the work) is actually\nnecessary. \n\nMinor:\n\n- Fonts in figure 4 are too small.\n",1,1,1,1,1,1,1,1,1,-1
BJaU__eCZ-R3,"The work is motivated by a real challenge of neuroimaging analysis: how to increase the amount of data to support the learning of brain decoding. \nThe contribution seems to mix two objectives: on one hand to prove that it is possible to do data augmentation for fMRI brain decoding, on the other hand to design (or better to extend) a new model (to be more precise two models). \nConcerning the first objective the empirical results do not provide meaningful support that the generative model is really effective.  The improvement is really tiny and a statistical test (not included in the analysis) probably wouldn't pass a significant threshold.   This analysis is missing a straw man.  It is not clear whether the difference in the evaluation measures is related to the greater number of examples or by the specific generative model. \nConcerning the contribution of the model, one novelty is the conditional formulation of the discriminator.  The design of the empirical evaluation doesn't address the analysis of the impact of this new formulation.  It is not clear whether the supposed improvement is related to the conditional formulation.  \nFigure 3 and Figure 5 illustrate the brain maps generated for Collection 1952 with ICW-GAN and for collection 503 with ACD-GAN.  It is not clear how the authors operated the choices of these figures.  From the perspective of neuroscience a reader,  would expect to look at the brain maps for the same collection with different methods.  The pairwise brain maps would support the interpretation of the generated data.  It is worthwhile to remember that the location of brain activations is crucial to detect whether the brain decoding (classification) relies on artifacts or confounds. \n\nMinor comments\n- typos: \""a first application or this\"" => \""a first application of this\"" (p.2)\n- \""qualitative quality\"" (p.2)",1,1,1,1,1,-1,1,1,1,-1
BJB7fkWR--R1,"In this paper, the authors propose a new approach for learning underlying structure of visually distinct games. \n\nThe proposed approach combines convolutional layers for processing input images, Asynchronous Advantage Actor Critic for deep reinforcement learning task and adversarial approach to force the embedding representation to be independent of the visual representation of games.  \n\nThe network architecture is suitably described and seems reasonable to learn simultaneously similar games, which are visually distinct.  However, the authors do not explain how this architecture can be used to do the domain adaptation.  \nIndeed, if some games have been learnt by the proposed algorithm, the authors do not precise what modules have to be retrained to learn a new game.  This is a critical issue, because the experiments show that there is no gain in terms of performance to learn a shared embedding manifold (see DA-DRL versus baseline in figure 5). \nIf there is a gain to learn a shared embedding manifold, which is plausible, this gain should be evaluated between a baseline, that learns separately the games, and an algorithm, that learns incrementally the games.  \nMoreover, in the experimental setting, the games are not similar but simply the same. \n\nMy opinion is that this paper is not ready for publication.  The interesting issues are referred to future works.\n",1,1,1,1,1,-1,1,1,1,-1
BJB7fkWR--R2,"This paper introduces a method to learn a policy on visually different but otherwise identical games.  While the idea would be interesting in general,  unfortunately the experiment section is very much toy example so that it is hard to know the applicability of the proposed approach to any more reasonable scenario.  Any sort of remotely convincing experiment is left to 'future work'. \n\nThe experimental setup is 4x4 grid world with different basic shape or grey level rendering.  I am quite convinced that any somewhat correctly setup vanilla deep RL algorithm would solve these sort of tasks/ ensemble of tasks almost instantly out of the box. \n\nFigure 5: Looks to me like the baseline is actually doing much better than the proposed methods? \n\nFigure 6: Looking at those 2D PCAs, I am not sure any of those method really abstracts the rendering away.  Anyway, it would be good to have a quantified metric on this, which is not just eyeballing PCA scatter plots.",1,1,1,1,-1,-1,1,1,1,-1
BJB7fkWR--R3,"- This paper discusses an agent architecture which uses a shared representation to train multiple tasks with different sprite level visual statistics.  The key idea is that the agent learns a shared representations for tasks with different visual statistics \n\n- A lot of important references  touching on very similar ideas are missing.  For e.g. \""Unsupervised Pixel-level Domain Adaptation with Generative Adversarial Networks\"", \""Using Simulation and Domain Adaptation to Improve Efficiency of Deep Robotic Grasping\"", \""Schema Networks: Zero-shot Transfer with a Generative Causal Model of Intuitive Physics\"".  \n\n- This paper has a lot of orthogonal details.  For instance sec 2.1 reviews the history of games and AI, which is besides the key point and does not provide any literary context.  \n\n- Only single runs for the results are shown in plots.  How statistically valid are the results? \n\n- In the last section authors mention the intent to do future work on atari and other env.  Given that this general idea has been discussed in the literature several times, it seems imperative to at least scale up the experiments before the paper is ready for publication",1,1,1,1,1,1,1,1,1,1
BJcAWaeCW-R1,"The authors try to combine the power of GANs with hierarchical community structure detections.  While the idea is sound,  many design choices of the system is questionable.  The problem is particularly aggravated by the poor presentation of the paper, creating countless confusions for readers.  I do not recommend the acceptance of this draft. \n\nCompared with GAN, traditional graph analytics is model-specific and non-adaptive to training data.  This is also the case for hierarchical community structures.  By building the whole architecture on the Louvain method, the proposed method is by no means truly model-agnostic.  In fact, if the layers are fine enough, a significant portion of the network structure will be captured by the sum-up module instead of the GAN modules, rendering the overall behavior dominated by the community detection algorithm.  \n\nThe evaluation remains superficial with minimal quantitative comparisons.  Treating degree distribution and clustering coefficient (appeared as cluster coefficient in draft) as global features is problematic.   They are merely global average of local topological features which is incapable of capturing true long-distance structures in graphs.   \n\nThe writing of the draft leaves much to be desired.  The description of the architecture is confusing with design choices never clearly explained.  Multiple concepts needs better introduction, including the very name of their model GTI and the idea of stage identification.  Not to mention numerous grammatical errors, I suggest the authors seek professional English writing services.",1,1,1,1,1,-1,1,1,-1,-1
BJcAWaeCW-R2,"Quality: The work has too many gaps for the reader to fill in.  The generator (reconstructed matrix) is supposed to generate a 0-1 matrix (adjacency matrix) and allow backpropagation of the gradients to the generator.  I am not sure how this is achieved in this work.  The matrix is not isomorphic invariant and the different clusters don\u2019t share a common model.  Even implicit models should be trained with some way to leverage graph isomorphisms and pattern similarities between clusters.  How can such a limited technique be generalizing?  There is no metric in the results showing how the model generalizes, it may be just overfitting the data.\ n\nClarity: The paper organization needs work; there are also some missing pieces to put the NN training together.  It is only in Section 2.3 that the nature of G_i^\\prime becomes clear,  although it is used in Section 2.2. Equation (3) is rather vague for a mathematical equation.  From what I understood from the text, equation (3) creates a binary matrix from the softmax output using an indicator function.  If the output is binary, how can the gradients backpropagate? Is it backpropagating with a trick like the Gumbel-Softmax trick of Jang, Gu, and Poole 2017 or Bengio\u2019s path derivative estimator?  This is a key point not discussed in the manuscript.  \nAnd if I misunderstood the sentence \u201cturn re_G into a binary matrix\u201d and the values are continuous, wouldn\u2019t the discriminator have an easy time distinguishing the generated data from the real data.  And wouldn\u2019t the generator start working towards vanishing gradients in its quest to saturate the re_G output? \n\nOriginality: The work proposes an interesting approach: first cluster the network, then learning distinct GANs over each cluster.  There are many such ideas now on ArXiv but it would be unfair to contrast this approach with unpublished work.  There is no contribution in the GAN / neural network aspect.  It is also unclear whether the model generalizes.  I don\u2019t think this is a good fit for ICLR. \n\nSignificance: Generating graphs is an important task in in relational learning tasks, drug discovery, and in learning to generate new relationships from knowledge bases.  The work itself, however, falls short of the goal.  At best the generator seems to be working but I fear it is overfitting.  The contribution for ICLR is rather minimal, unfortunately",1,1,1,1,-1,1,1,-1,1,-1
BJcAWaeCW-R3,"The proposed approach, GTI, has many free parameters: number of layers L, number of communities in each layer, number of non-overlapping subgraphs M, number of nodes in each subgraph k, etc.   No analysis is reported on how these affect the performance of GTI. \n\nGTI uses the Louvain hierarchical community detection method to identify the hierarchy in the graph and METIS to partition the communities.   How important are these two methods to the success of GTI? \n\nWhy is it reasonable to restore a k-by-k adjacency matrix from the standard uniform distribution (as stated in Section 2.1)? \n\nWhy is the stride for the convolutional/deconvoluational layers set to 2 (as stated in Section 2.1)? \n\nEquation 1 has a symbol E in it.   E is defined (in Section 2.2) to be \""all the inter-subgraph (community) edges identified by the Louvain method for each hierarchy. \""  However, E can be intra-community because communities are partitioned by METIS.   More discussion is needed about the role of edges in E.  \n\nEquation 3 sparsifies (i.e. prunes the edges) of a graph -- namely $re_{G}$.   However, it is not clear how one selects a $re^{i}{G}$ from among the various i values.   The symbol i is an index into $CV_{i}$, the cut-value of the i-th largest unique weight-value. \n\nWas the edge-importance reported in Section 2.3 checked against various measures of edge importance such as edge betweenness? \n\nTable 1 needs more discussion in terms of retained edge percentage for ordered stages.   Should one expect a certain trend in these sequences? \n\nAlmost all of the experiments are qualitative and can be easily made quantitive by comparing PageRank or degree of nodes. \n\nThe discussion on graph sampling does not include how much of the graph was sampled.   Thus, the comparisons in Tables 2 and 3 are not fair.\n\nThe most realistic graph generator is the BTER model.  See http://www.sandia.gov/~tgkolda/bter_supplement/ and http://www.sandia.gov/~tgkolda/feastpack/doc_bter_match.html.\n\nA minor point: The acronym GTI is never defined.",1,1,1,1,1,-1,1,1,-1,-1
BJDEbngCZ-R1,"The work investigates convergence guarantees of gradient-type policies for reinforcement learning and continuous control\nproblems, both in deterministic and randomized case, whiling coping with non-convexity of the objective.  I found that the paper suffers many shortcomings that must be addressed:\n\n1) The writing and organization is quite cumbersome and should be improved. \n2) The authors state in the abstract (and elsewhere): \""... showing that (model free) policy gradient methods globally converge to the optimal solution ...\"". This is misleading and NOT true.  The authors show the convergence of the objective but not of the iterates sequence.  This should be rephrased elsewhere. \n3) An important literature on convergence of descent-type methods for semialgebraic objectives is available but not discussed.",1,1,1,1,1,1,1,1,-1,-1
BJDEbngCZ-R2,"I find this paper not suitable for ICLR.  All the results are more or less direct applications of existing optimization techniques, and not provide fundamental new understandings of the learning REPRESENTATION.",1,1,-1,1,-1,-1,1,1,1,-1
BJDEbngCZ-R3,"The paper studies the global convergence for policy gradient methods for linear control problems.  \n(1) The topic of this paper seems to have minimal connection with ICRL.  It might be more appropriate for this paper to be reviewed at a control/optimization conference, so that all the technical analysis can be evaluated carefully.  \n\n(2) I am not convinced if the main results are novel.  The convergence of policy gradient does not rely on the convexity of the loss function, which is known in the community of control and dynamic programming.  The convergence of policy gradient is related to the convergence of actor-critic, which is essentially a form of policy iteration.  \n\n(3) The main results of this paper seem technical sound.  However, the results seem a bit limited because it does not apply to neural-network function approximator.  It does not apply to the more general control problem rather than quadratic cost function, which is quite restricted.  I might have missed something here.  I strongly suggest that these results be submitted to a more suitable venue.\n\n",1,1,1,1,-1,1,1,1,1,-1
BJDH5M-AW-R1,"Summary: This work proposes a way to create 3D objects to fool the classification of their pictures from different view points by a neural network. \nRather than optimizing the log-likelihood of a single example, the optimization if performed over a the expectation of a set of transformations of sample images.  Using an inception v3 net, they create adversarial attacks on a subset of the imagenet validation set transformed by translations, lightening conditions, rotations, and scalings among others, and observe a drop of the classifier accuracy performance from 70% to less than 1%.  They also create two 3D printed objects which most pictures taken from random viewpoints are fooling the network in its class prediction. \n \n\nMain comments:\n- The idea of building 3D adversarial objects is novel so the study is interesting.  However, the paper is incomplete, with a very low number of references, only 2 conference papers if we assume the list is up to date.  \nSee for instance Cisse et al. Houdini: fooling Deep Structured Prediction Models, NIPS 2017 for a recent list of related work in this research area.  \n- The presentation of the results is not very clear.   See specific comments below.\n- It would be nice to include insights to improve neural nets to become less sensitive to these attacks.  \n\n\nMinor comments:\nFig1 : a bug with color seems to have been fixed\nModel section: be consistent with the notations.   Bold everywhere or nowhere \nResults: The tables are difficult to read and should be clarified: \nWhat does the l2 metric stands for ?  \nHow about min, max ? \nAccuracy -> classification accuracy \nModels -> 3D models\nDescribe each metric (Adversarial, Miss-classified, Correct)\n",1,1,1,1,1,1,1,1,1,-1
BJDH5M-AW-R2,"The authors present a method to enable robust generation of adversarial visual\ninputs for image classification. \n\nThey develop on the theme that 'real-world' transformations typically provide a\ncountermeasure against adversarial attacks in the visual domain, to show that\ncontextualising the adversarial exemplar generation by those very\ntransformations can still enable effective adversarial example generation. \n\nThey adapt an existing method for deriving adversarial examples to act under a\nprojection space (effectively a latent-variable model) which is defined through\na transformations distribution. \n\nThey demonstrate the effectiveness of their approach in the 2D and 3D\n(simulated and real) domains. \n\nThe paper is clear to follow and the objective employed appears to be sound.  I\nlike the idea of using 3D generation, and particularly, 3D printing, as a means\nof generating adversarial examples -- there is definite novelty in that\nparticular exploration for adversarial examples. \n\nI did however have some concerns:\n\n1. What precisely is the distribution of transformations used for each \n   experiment?  Is it a PCFG?  Are the different components quantised such that\n   they are discrete rvs, or are there still continuous rvs? (For example, is\n   lighting discretised to particular locations or taken to be (say) a 3D\n   Gaussian?)  And on a related note, how were the number of sampled \n   transformations chosen? \n\n   Knowing the distribution (and the extent of it's support) can help situate\n   the effectiveness of the number of samples taken to derive the adversarial\n   input. \n\n2. While choosing the distance metric in transformed space, LAB is used, but\n   for the experimental results, l_2 is measured in RGB space -- showing the\n   RGB distance is perhaps not all that useful given it's not actually being\n   used in the objective.  I would perhaps suggest showing LAB, maybe in\n   addition to RGB if required. \n\n3. Quantitative analysis: I would suggest reporting confidence intervals;\n   perhaps just the 1st standard deviation over the accuracies for the true and\n   'adversarial' labels -- the min and max don't help too much in understanding\ n   what effect the monte-carlo approximation of the objective has on things. \n\n   Moreover, the min and max are only reported for the 2D and rendered 3D\n   experiments -- it's missing for the 3D printing experiment. \n\n4. Experiment power: While the experimental setup seems well thought out and\n   structured, the sample size (i.e, the number of entities considered) seems a\n   bit too small to draw any real conclusions from.  There are 5 exemplar\n   objects for the 3D rendering experiment and only 2 for the 3D printing one. \n\n   While I understand that 3D printing is perhaps not all that scalable to be\n   able to rattle off many models, the 3D rendering experiment surely can be\n   extended to include more models?  Were the turtle and baseball models chosen\n   randomly, or chosen for some particular reason?  Similar questions for the 5\n   models in the 3D rendering experiment. \n\n5. 3D printing experiment transformations: While the 2D and 3D rendering\n   experiments explicitly state that the sampled transformations were random,\n   the 3D printing one says \""over a variety of viewpoints\"".  Were these\n   viewpoints chosen randomly? \n\nMost of these concerns are potentially quirks in the exposition rather than any\nissues with the experiments conducted themselves.  For now, I think the\nsubmission is good for a weak accept  \u2013- if the authors address my concerns, and/or\ncorrect my potential misunderstanding of the issues, I'd be happy to upgrade my\nreview to an accept.",1,1,1,1,1,-1,1,1,1,-1
BJDH5M-AW-R3,"The paper proposes a method to synthesize adversarial examples that remain robust to different 2D and 3D perturbations.  The paper shows this is effective by transferring the examples to 3D objects that are color 3D-printed and show some nice results. \n\nThe experimental results and video showing that the perturbation is effective for different camera angles, lighting conditions and background is quite impressive.  This work convincingly shows that adversarial examples are a real-world problem for production deep-learning systems rather than something that is only academically interesting. \n\nHowever, the authors claim that standard techniques require complete control and careful setups (e.g. in the camera case) is quite misleading, especially with regards to the work by Kurakin et. al.  This paper also seems to have some problems of its own (for example the turtle is at relatively the same distance from the camera in all the examples, I expect the perturbation wouldn't work well if it was far enough away that the camera could not resolve the HD texture of the turtle). \n\nOne interesting point this work raises is whether the algorithm is essentially learning universal perturbations (Moosavi-Dezfooli et. al).  If that's the case then complicated transformation sampling and 3D mapping setup would be unnecessary.  This may already be the case since the training set already consists of multiple lighting, rotation and camera type transformations so I would expect universal perturbations to already produce similar results in the real-world. \n\nMinor comments:\nSection 1.1: \""a affine\"" -> \""an affine\""\nTypo in section 3.4: \""of a of a \""\nIt's interesting in figure 9 that the crossword puzzle appears in the image of the lighthouse. \n\nMoosavi-Dezfooli, S. M., Fawzi, A., Fawzi, O., & Frossard, P. Universal adversarial perturbations. CVPR 2017.",1,1,1,1,1,1,1,1,1,-1
BJehNfW0--R1,"The paper adds to the discussion on the question whether Generative Adversarial Nets (GANs) learn the target distribution.  Recent theoretical analysis of GANs by Arora et al. show that of the discriminator capacity of is bounded, then there is a solution the closely meets the objective but the output distribution has a small support.  The paper attempts to estimate the size of the support for solutions produced by typical GANs experimentally.  The main idea used to estimate the support is the Birthday theorem that says that with probability at least 1/2, a uniform sample (with replacement) of size S from a set of  N elements will have a duplicate given S > \\sqrt{N}.  The suggested plan is to manually check for duplicates in a sample of size s and if duplicate exists, then estimate the size of the support to be s^2.  One should note that the birthday theorem assumes uniform sampling.   In the revised versions, it has been clarified that the tested distribution is not assumed to be uniform but the distribution has \""effectively\"" small support size using an indistinguishability notion.  Given this method to estimate the size of the support, the paper also tries to study the behaviour of estimated support size with the discriminator capacity.  Arora et al. showed that the output support size has nearly linear dependence on the discriminator capacity.  Experiments are conducted in this paper to study this behaviour by varying the discriminator capacity and then estimating the support size using the idea described above.  A result similar to that of Arora et al. is also given for the special case of Encoder-Decoder GAN. \n\nEvaluation: \nSignificance: The question whether GANs learn the target distribution is important and any  significant contribution to this discussion is of value.  \n\nClarity: The paper is written well and the issues raised are well motivated and proper background is given.  \n\nOriginality: The main idea of trying to estimate the size of the support using a few samples by using birthday theorem seems new.  \n\nQuality: The main idea of this work is to give a estimation technique for the support size for the output distribution of GANs. \n",1,1,1,1,-1,1,1,-1,1,-1
BJehNfW0--R2,"This paper proposes a clever new test based on the birthday paradox for measuring diversity in generated samples.  The main goal is to quantify mode collapse in state-of-the-art generative models.  The authors also provide a specific theoretical construction that shows bidirectional GANs cannot escape specific cases of mode collapse. \nUsing the birthday paradox test, the experiments show that GANs can learn and consistently reproduce the same examples, which are not necessarily exactly the same as training data (eg. the triplets in Figure 1). \nThe results are interpreted to mean that mode collapse is strong in a number of state-of-the-art generative models. \nBidirectional models (ALI, BiGANs) however demonstrate significantly higher diversity that DCGANs and MIX+DCGANs. \nFinally, the authors verify empirically the hypothesis that diversity grows linearly with the size of the discriminator. \n\nThis is a very interesting area and exciting work.  The main idea behind the proposed test is very insightful.  The main theoretical contribution stimulates and motivates much needed further research in the area.  In my opinion both contributions suffer from some significant limitations.  However, given how little we know about the behavior of modern generative models, it is a good step in the right direction. \n\n\n1. The biggest issue with the proposed test is that it conflates mode collapse with non-uniformity.  The authors do mention this issue, but do not put much effort into evaluating its implications in practice, or parsing Theorems 1 and 2.  My current understanding is that, in practice, when the birthday paradox test gives a collision I have no way of knowing whether it happened because my data distribution is modal, or because my generative model has bad diversity.  Anecdotally, real-life distributions are far from uniform, so this should be a common issue.  I would still use the test as a part of a suite of measurements, but I would not solely rely on it.  I feel that the authors should give a more prominent disclaimer to potential users of the test. \n\n2. Also, given how mode collapse is the main concern, it seems to me that a discussion on coverage is missing.  The proposed test is a measure of diversity, not coverage, so it does not discriminate between a generator that produces all of its samples near some mode and another that draws samples from all modes of the true data distribution.  As long as they yield collisions at the same rate, these two generative models are \u2018equally diverse\u2019. Isn\u2019t coverage of equal importance? \n\n3. The other main contribution of the paper is Theorem 3, which shows\u2014via a very particular construction on the generator and encoder\u2014that bidirectional GANs can also suffer from serious mode collapse.  I welcome and are grateful for any theory in the area.  This theorem might very well capture the underlying behavior of bidirectional GANs, however, being constructive, it guarantees nothing in practice.  In light of this, the statement in the introduction that \u201cencoder-decoder training objectives cannot avoid mode collapse\u201d might need to be qualified.  In particular, the current statement seems to obfuscate the understanding that training such an objective would typically not result into the construction of Theorem 3.",1,1,1,1,1,-1,1,1,1,-1
BJehNfW0--R3,"The article \""Do GANs Learn the Distribution?  Some Theory and Empirics\"" considers the important problem of quantifying whether the distributions obtained from generative adversarial networks come close to the actual distribution of images.  The authors argue that GANs in fact generate the distributions with fairly low support. \n\nThe proposed approach relies on so-called birthday paradox which allows to estimate the number of objects in the support by counting number of matching (or very similar) pairs in the generated sample.  This test is expected to experimentally support the previous theoretical analysis by Arora et al. (2017).  The further theoretical analysis is also performed showing that for encoder-decoder GAN architectures the distributions with low support can be very close to the optimum of the specific (BiGAN) objective. \n\nThe experimental part of the paper considers the CelebA and CIFAR-10 datasets.  We definitely see many very similar images in fairly small sample generated.  So, the general claim is supported.  However, if you look closely at some pictures, you can see that they are very different though reported as similar.  For example, some deer or truck pictures.  That's why I would recommend to reevaluate the results visually, which may lead to some change in the number of near duplicates and consequently the final support estimates. \n\nTo sum up, I think that the general idea looks very natural and the results are supportive.  On theoretical side, the results seem fair (though I didn't check the proofs) and, being partly based on the previous results of Arora et al. (2017), clearly make a step further.",1,1,1,1,1,1,1,1,1,-1
BJgd7m0xRZ-R1,"The authors propose a defense against attacks on the security of one-class SVM based anonaly detectors.  The core idea is to perform a random projection of the data (which is supposed to decrease the impact from adversarial distortions).  The approach is empirically tested on the following data: MNIST, CIFAR, and SVHN. \n\nThe paper is moderately well written and structured.  Command of related work is ok,  but some relevant refs are missing (e.g., Kloft and Laskov, JMLR 2012).   The empirical results actually confirm that indeed the strategy of reducing the dimensionality using random projections reduces the impact from adversarial distortions.   This is encouraging.  Right now, there is no theoretical justification for the approach, nor even a (in my opinion) convincing movitation/Intuition behind the approach.  Also, the attack model should formally introduced. \n\nIn summary, I d like to encourage the authors to further investigate into their approach, but I am not convinced by the manuscript in the current form.  It lacks both in sound theoretical justification and intuitive motivation of the approach.  The experiments, however, show clearly advantages of the approach  (again, here further experiments are necessary, e.g., varying the dose of adversarial points).",1,1,1,1,1,1,1,1,1,-1
BJgd7m0xRZ-R2,"In this paper, the authors explore how using random projections can be used to make OCSVM robust to adversarially perturbed training data.   While the intuition is nice and interesting,  the paper is not very clear in describing the attack and the experiments do not appropriately test whether this method actually provides robustness. \n\nDetails:\nhave been successfully in anomaly detection --> have been successfully used in anomaly detectionP \n\n\""The adversary would select a random subset of anomalies, push them towards the normal data cloud and inject these perturbed points into the training set\"" -- This seems backwards.   As in the example that follows, if the adversary wants to make anomalies seem normal at test time, it should move normal points outward from the normal point cloud (eg making a 9 look like a weird 7). \n\nAs s_attack increases, the anomaly data points are moved farther away from the normal data cloud, altering the position of the separating hyperplane. -- This seems backwards from Fig 2.    From (a) to (b) the red points move closer to the center while in (c) they move further away (why?).    The blue points seem to consistently become more dense from (a) to (c).  \n\nThe attack model is too rough.    It seems that without bounding D, we can make the model arbitrarily bad, no?    Assumption 1 alludes to this but doesn't specify what is \""small\""?    Also the attack model is described without considering if the adversary knows the learner's algorithm.    Even if there is randomness, can the adversary take actions that account for that randomness?  \n\nDoes selecting a projection based on compactness remove the randomness?  \n\nExperiments -- why/how would you have distorted test data?    Making an anomaly seem normal by distorting it is easy.  \n\nI don't see experiments comparing having random projections and not.    This seems to be the fundamental question -- do random projects help in the train_D | test_C case?  \n\nExperiments don't vary the attack much to understand how robust the method is.",1,1,1,1,1,-1,1,1,1,-1
BJgd7m0xRZ-R3,"Although the problem addressed in the paper seems interesting,  but there lacks of evidence to support some of the arguments that the authors make.  And the paper does not contribute novelty to representation learning, therefore, it is not a good fit for the conference.  Detailed critiques are as following:1. The idea proposed by the authors seems too quite simple.  It is just performing random projections for 1000 times and choose the set of projection parameters that results in the highest compactness as the dimensionality reduction model parameter before one-class SVM. \n2. It says in the experiments part that the authors have used 3 different S_{attack} values, but they only present results for S_{attack} = 0.5.  It would be nicer if they include results for all S_{attack} values that they have used in their experiments, which would also give the reader insights on how the anomaly detection performance degrades when the S_attack value change. \n3. The paper claims that the nonlinear random projection is a defence against adversary due to the randomness, but there is no results in the paper proving that other non-random projections are susceptible to adversary that is designed to target that projection mechanism and nonlinear random projection is able to get away with that.  And PCA as a non-random projection would a nice baseline to compare against. \n4. The paper seems to misuse the term \u201cFalse positive rate\u201d as the y label of figure 3(d/e/f).  The definition of false positive rate is FP/(FP+TN), so if the FPR=1 it means that all negative samples are labeled as positive.  So it is surprising to see FPR=1 in Figure 3(d) when feature dimension=784 while the f1 score is still high in Figure 3(a).  From what I understand, the paper means to present the percentage of adversarial examples that are misclassified instead of all the anomaly examples that get misclassified.  The paper should come up with a better term for that evaluation. \n5. The conclusion, that robustness of the learned model increases wrt the integrity attacks increases when the projection dimension becomes lower, cannot be drawn from Figure 3(d).  Need more experiment on more dimensionality to prove that. \n6. In the appendix B results part, sometimes the word \u2019S_attack\u2019 is typed wrong. And the values in  \u201cdistorted/distorted\u201d columns in Table 5 do not match up with the ones in Figure 3(c).",1,1,1,1,1,-1,1,1,1,-1
BJgPCveAW-R1,"This paper examines sparse connection patterns in upper layers of convolutional image classification networks.   Networks with very few connections in the upper layers are experimentally determined to perform almost as well as those with full connection masks. \n\nWhile it seems clear in general that many of the connections are not needed and can be made sparse (Figures 1 and 2), I found many parts of this paper fairly confusing, both in how it achieves its objectives, as well as much of the notation and method descriptions.   I've described many of the points I was confused by in more detailed comments below. \n\n\nDetailed comments and questions:\n\n\nThe distribution of connections in \""windows\"" are first described to correspond to a sort of semi-random spatial downsampling, to get different views distributed over the full image.   But in the upper layers, the spatial extent can be very small compared to the image size, sometimes even 1x1 depending on the network downsampling structure.   So are do the \""windows\"" correspond to spatial windows, and if so, how?   Or are they different (maybe arbitrary) groupings over the feature maps? \n\nAlso a bit confusing is the notation \""conv2\"", \""conv3\"", etc.   These names usually indicate the name of a single layer within the network (conv2 for the second convolutional layer or series of layers in the second spatial size after downsampling, for example).   But here it seems just to indicate the number of \""CL\"" layers: 2.  And p.1 says that the \""CL\"" layers are those often referred to as \""FC\"" layers, not \""conv\"" (though they may be convolutionally applied with spatial 1x1 kernels). \n\nThe heuristic for spacing connections in windows across the spatial extent of an image makes intuitive sense, but I'm not convinced this will work well in all situations, and may even be sub-optimal for the examined datasets.   For example, to distinguish MNIST 1 vs 7 vs 9, it is most important to see the top-left:  whether it is empty, has a horizontal line, or a loop.   So some regions are more important than others, and the top half may be more important than an equally spaced global view.   So the description of how to space connections between windows makes some intuitive sense, but I'm unclear on whether other more general connections might be even better, including some that might not be as easily analyzed with the \""scatter\"" metric described. \n\nAnother broader question I have is in the distinction between lower and upper layers (those referred to as \""feature extracting\"" and \""classification\"" in this paper).   It's not clear to me that there is a crisply defined difference here (though some layers may tend to do more of one or the other function, such as we might interpret).   So it seems that expanding the investigation to include all layers, or at least more layers, would be good:  It might be that more of the \""classification\"" function is pushed down to lower layers, as the upper layers are reduced in size.   How would they respond to similar reductions? \n\nI'm also unsure why on p.6 MNIST uses 2d windows, while CIFAR uses 3d --- The paper mentions the extra dimension is for features, but MNIST would have a features dimension as well at this stage, I think?   I'm also unsure whether the windows are over spatial extent only, or over features.",1,1,1,1,1,-1,1,1,-1,-1
BJgPCveAW-R2,"The authors propose reducing the number of parameters learned by a deep network by setting up sparse connection weights in classification layers.  Numerical experiments show that such sparse networks can have similar performance to fully connected ones.  They introduce a concept of \u201cscatter\u201d that correlates with network performance.  Although  I found the results useful and potentially promising,  I did not find much insight in this paper. \nIt was not clear to me why scatter (the way it is defined in the paper) would be a useful performance proxy anywhere but the first classification layer.  Once the signals from different windows are intermixed, how do you even define the windows?   \nMinor\nSecond line of Section 2.1: \u201clesser\u201d -> less or fewer\n",1,1,1,1,1,-1,1,1,1,-1
BJgPCveAW-R3,"The paper seems to claims that\n1) certain ConvNet architectures, particularly AlexNet and VGG, have too many parameters, \n 2) the sensible solution is leave the trunk of the ConvNet unchanged, and to randomly sparsify the top-most weight matrices. \nI have two problems with these claims:\n1) Modern ConvNet architectures (Inception, ResNeXt, SqueezeNet, BottleNeck-DenseNets and ShuffleNets) don't have large fully connected layers. \n2) The authors reject the technique of 'Deep compression' as being impractical.  I suspect it is actually much easier to use in practice as you don't have to a-priori know the correct level of sparsity for every level of the network. \n\np3. What does 'normalized' mean?  Batch-norm? \np3. Are you using an L2 weight penalty?  If not, your fully-connected baseline may be unnecessarily overfitting the training data. \np3. Table 1. Where do the choice of CL Junction densities come from?  Did you do a grid search to find the optimal level of sparsity at each level? \np7-8. I had trouble following the left/right & front/back notation. \np8. Figure 7. How did you decide which data points to include in the plots?",1,1,1,1,1,-1,1,1,-1,-1
BJhxcGZCW-R1,"SUMMARY.\n\nThe paper presents a variational autoencoder for generating entity pairs given a relation in a medical setting. \nThe model strictly follows the standard VAE architecture with an encoder that takes as input an entity pair and a relation between the entities. \nThe encoder maps the input to a probabilistic latent space. \nFinally, a generator is used to generate entity pairs give a relation. \n\n----------\n\nOVERALL JUDGMENT\nThe paper presents a clever use of VAEs for generating entity pairs conditioning on relations. \nMy main concern about the paper is that it seems that the authors have tuned the hyperparameters and tested on the same validation set. \nIf this is the case, all the analysis and results obtained are almost meaningless. \nI suggest the authors make clear if they used the split training, validation, test. \nUntil then it is not possible to draw any conclusion from this work. \n\nAssuming the experimental setting is correct, it is not clear to me the reason of having the representation of r (one-hot-vector of the relation) also in the decoding/generation part. \nThe hidden representation obtained by the encoder should already capture information about the relation. \nIs there a specific reason for doing so?\n\n",1,1,1,1,1,-1,1,1,-1,1
BJhxcGZCW-R2,"The authors suggest using a variational autoencoder to infer binary relationships between medical entities.  The model is quite simple and intuitive and the authors demonstrate that it can generate meaningful relationships between pairs of entities that were not observed before.   \nWhile the paper is very well-written  I have certain concerns regarding the motivation, model, and evaluation methodology followed: \n\n1) A stronger motivation for this model is required.  Having a generative model for causal relationships between symptoms and diseases is \""intriguing\"" yet I am really struggling with the motivation of getting such a model from word co-occurences in a medical corpus.  I can totally buy the use of the proposed model as means to generate additional training data for a discriminative model used for information extraction but the authors need to do a better job at explaining the downstream applications of their model.  \n\n2) The word embeddings used seem to be sufficient to capture the \""knowledge\"" included in the corpus.  An ablation study of the impact of word embeddings on this model is required.  \n\n3) The authors do not describe how the data from xywy.com were annotated.  Were they annotated by experts in the medical domain or random users? \n\n4) The metric of quality is particularly ad-hoc",1,1,1,1,1,-1,1,1,1,-1
BJhxcGZCW-R3,"In the medical context, this paper describes the classic problem of \""knowledge base completion\"" from structured data only (no text).   The authors argue for the advantages of a generative VAE approach (but without being convincing).   They do not cite the extensive literature on KB completion.   They present experimental results on their own data set, evaluating only against simpler baselines of their own VAE approach, not the pre-existing KB methods. \n\nThe authors seem unaware of a large literature on \""knowledge base completion.\""  E.g. [Bordes, Weston, Collobert, Bengio, AAAI, 2011],  [Socher et al 2013 NIPS], [Wang, Wang, Guo 2015 IJCAI], [Gardner, Mitchell 2015 EMNLP], [Lin, Liu, Sun, Liu, Zhu AAAI 2015], [Neelakantan, Roth, McCallum 2015],  \n\nThe paper claims that operating on pre-structured data only (without using text) is an advantage.   I don't find the argument convincing.   There are many methods that can operate on pre-structured data only, but also have the ability to incorporate text data when available, e.g. \""universal schema\"" [Riedel et al, 2014]. \n\nThe paper claims that \""discriminative approaches\"" need to iterate over all possible entity pairs to make predictions.   In their generative approach they say they find outputs by \""nearest neighbor search.\""   But the same efficient search is possible in many of the classic \""discriminatively-trained\"" KB completion models also. \n\nIt is admirable that the authors use an interesting (and to my knowledge novel) data set.   But the method should also be evaluated on multiple now-standard data sets, such as FB15K-237 or NELL-995.   The method is evaluated only against their own VAE-based alternatives.   It should be evaluated against multiple other standard KB completion methods from the literature, such as Jason Weston's Trans-E, Richard Socher's Tensor Neural Nets, and Neelakantan's RNNs.\n",1,1,1,1,1,1,1,1,1,-1
BJIgi_eCZ-R1,"The paper first analyzes recent works in machine reading comprehension (largely centered around SQuAD), and mentions their common trait that the attention is not \""fully-aware\"" of all levels of abstraction, e.g. word-level, phrase-level, etc.  In turn, the paper proposes a model that performs attention at all levels of abstraction, which achieves the state of the art in SQuAD.  They also propose an attention mechanism that works better than others (Symmetric + ReLU). \n\nStrengths:\n- The paper is well-written and clear. \n- I really liked Table 1 and Figure 2; it nicely summarizes recent work in the field. \n- The multi-level attention is novel and indeed seems to work, with convincing ablations. \n- Nice engineering achievement, reaching the top of the leaderboard (in early October). \n\n\nWeaknesses:\n- The paper is long (10 pages) but relatively lacks substances.  Ideally, I would want to see the visualization of the attention at each level (i.e. how they differ across the levels) and also possibly this model tested on another dataset (e.g. TriviaQA). \n- The authors claim that the symmetric + ReLU is novel, but  I think this is basically equivalent to bilinear attention [1] after fully connected layer with activation, which seems quite standard.  Still useful to know that this works better, so would recommend to tone down a bit regarding the paper's contribution. \n\n\nMinor:\n- Probably figure 4 can be drawn better.  Not easy to understand nor concrete. \n- Section 3.2 GRU citation should be Cho et al. [2]. \n\n\nQuestions:\n- Contextualized embedding seems to give a lot of improvement in other works too.  Could you perform ablation without contextualized embedding (CoVe)? \n\n\nReference:\n[1] Luong et al. Effective Approaches to Attention-based Neural Machine Translation.  EMNLP 2015.\n[2] Cho et al. Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. EMNLP 2014.",1,1,1,1,1,1,1,1,1,-1
BJIgi_eCZ-R2,"The primary intellectual point the authors make is that previous networks for machine comprehension are not fully attentive.  That is, they do not provide attention on all possible layers on abstraction such as the word-level and the phrase-level . The network proposed here, FusionHet, fixes problem.  Importantly, the model achieves state-of-the-art performance of the SQuAD dataset. \n\nThe paper is very well-written and easy to follow.  I found the architecture very intuitively laid out, even though this is not my area of expertise.  Moreover, I found the figures very helpful -- the authors clearly took a lot of time into clearly depicting their work!  What most impressed me, however, was the literature review.  Perhaps this is facilitated by the SQuAD leaderboard, which makes it simple to list related work.  Nevertheless, I am not used to seeing comparison to as many recent systems as are presented in Table 2.  \n\nAll in all, it is difficult not to highly recommend an architecture that achieves state-of-the-art results on such a popular dataset.",1,1,-1,1,-1,1,1,1,1,-1
BJIgi_eCZ-R3,"(Score before author revision: 4)\n(Score after author revision: 7)\n\nI think the authors have taken both the feedback of reviewers as well as anonymous commenters thoroughly into account, running several ablations as well as reporting nice results on an entirely new dataset (MultiNLI) where they show how their multi level fusion mechanism improves a baseline significantly.  I think this is nice since it shows how their mechanism helps on two different tasks (question answering and natural language inference). \n\nTherefore I would now support accepting this paper. \n\n------------(Original review below) -----------------------\n\nThe authors present an enhancement to the attention mechanism called \""multi-level fusion\"" that they then incorporate into a reading comprehension system.  It basically takes into account a richer context of the word at different levels in the neural net to compute various attention scores. \n\ni.e. the authors form a vector \""HoW\"" (called history of the word), that is defined as a concatenation of several vectors:\n\nHoW_i = [g_i, c_i, h_i^l, h_i^h]\n\nwhere g_i = glove embeddings, c_i = COVE embeddings (McCann et al. 2017), and h_i^l and h_i^h are different LSTM states for that word. \n\nThe attention score is then a function of these concatenated vectors i.e. \\alpha_{ij} = \\exp(S(HoW_i^C, HoW_j^Q)) \n\nResults on SQuAD show a small gain in accuracy (75.7->76.0 Exact Match).  The gains on the adversarial set are larger but that is because some of the higher performing, more recent baselines don't seem to have adversarial numbers. \n\nThe authors also compare various attention functions (Table 5) showing a particularone (Symmetric + ReLU) works the best.  \n\nComments:\n\n-I feel overall the contribution is not very novel.   The general neural architecture that the authors propose in Section 3 is generally quite similar to the large number of neural architectures developed for this dataset (e.g. some combination of attention between question/context and LSTMs over question/context).  The only novelty is these \""HoW\"" inputs to the extra attention mechanism that takes a richer word representation into account. \n\n-I feel the model is seems overly complicated for the small gain (i.e. 75.7->76.0 Exact Match), especially on a relatively exhausted dataset (SQuAD) that is known to have lots of pecularities (see anonymous comment below).  It is possible the gains just come from having more parameters. \n\n-The authors (on page 6) claim that that by running attention multiple times with different parameters but different inputs (i.e. \\alpha_{ij}^l, \\alpha_{ij}^h, \\alpha_{ij}^u) it will learn to attend to \""different regions for different level\"".  However, there is nothing enforcing this and the gains just probably come from having more parameters/complexity.",1,1,1,1,-1,1,1,1,1,-1
BJInEZsTb-R1,"This paper introduces a generative approach for 3D point clouds.  More specifically, two Generative Adversarial approaches are introduced: Raw point cloud GAN, and Latent-space GAN (r-GAN and l-GAN as referred to in the paper).  In addition, a GMM sampling + GAN decoder approach to generation is also among the experimented variations.  \n\nThe results look convincing for the generation experiments in the paper, both from class-specific (Figure 1) and multi-class generators (Figure 6).  \n\nOne question that arises is whether the point cloud approaches to generation is any more valuable compared to voxel-grid based approaches.  Especially Octree based approaches [1-below] show very convincing and high-resolution shape generation results,  whereas the details seem to be washed out for the point cloud results presented in this paper.  \n\nI would like to see comparison experiments with voxel based approaches in the next update for the paper.  \n\n[1]\n@article{tatarchenko2017octree,\n  title={Octree Generating Networks: Efficient Convolutional Architectures for High-resolution 3D Outputs},\n  author={Tatarchenko, Maxim and Dosovitskiy, Alexey and Brox, Thomas},\n  journal={arXiv preprint arXiv:1703.09438},\n  year={2017}\n}\n\nIn light of the authors' octree updates score is updated.  I expect these updates to be reflected in the final version of the paper itself as well.",1,1,1,1,1,1,1,1,1,-1
BJInEZsTb-R2,"3D data processing is very important topic nowadays, since it has a lot of applications: robotics, AR/VR, etc. \n\nCurrent approaches to 2D image processing based on Deep Neural Networks provide very accurate results and a wide variety of different architectures for image modelling, generation, classification, retrieval. \n\nThe lack of DL architectures for 3D data is due to complexity of representation of 3D data, especially when using 3D point clouds. \n\nConsidered paper is one of the first approaches to learn GAN-type generative models. \nUsing PointNet architecture and latent-space GAN, the authors obtained rather accurate generative model. \n\nThe paper is well written, results of experiments are convincing, the authors provided the code on the github, realizing their architectures.  \n\nThus I think that the paper should be published.",1,1,-1,1,-1,1,1,1,1,1
BJInEZsTb-R3,"Summary:\n\nThis paper proposes generative models for point clouds.  First, they train an auto-encoder for 3D point clouds,  somewhat similar to PointNet (by Qi et al.).  Then, they train generative models over the auto-encoder's latent space, both using a \""latent-space GAN\"" (l-GAN) that outputs latent codes, and a Gaussian Mixture Model.  To generate point clouds, they sample a latent code and pass it to the decoder.  They also introduce a \""raw point cloud GAN\"" (r-GAN) that, instead of generating a latent code, directly produces a point cloud.\ n\nThey evaluate the methods on several metrics.  First, they show that the autoencoder's latent space is a good representation for classification problems, using the ModelNet dataset.  Second, they evaluate the generative model on several metrics (such as Jensen-Shannon Divergence) and study the benefits and drawbacks of these metrics, and suggest that one-to-one mapping metrics such as earth mover's distance are desirable over Chamfer distance.  Methods such as the r-GAN score well on the latter by over-representing parts of an object that are likely to be filled. \n\nPros:\n\n- It is interesting that the latent space models are most successful, including the relatively simple GMM-based model.  Is there a reason that these models have not been as successful in other domains?\ n\n- The comparison of the evaluation metrics could be useful for future work on evaluating point cloud GANs.  Due to the simplicity of the method, this paper could be a useful baseline for future work. \n\n- The part-editing and shape analogies results are interesting, and it would be nice to see these expanded in the main paper. \n\nCons:\n\n- How does a model that simply memorizes (and randomly samples) the training set compare to the auto-encoder-based models on the proposed metrics? How does the diversity of these two models differ? \n\n- The paper simultaneously proposes methods for generating point clouds, and for evaluating them.  The paper could therefore be improved by expanding the section comparing to prior, voxel-based 3D methods, particularly in terms of the diversity of the outputs.  Although the performance on automated metrics is encouraging,  it is hard to conclude much about under what circumstances one representation or model is better than another. \n\n- The technical approach is not particularly novel.  The auto-encoder performs fairly well,  but it is just a series of MLP layers that output a Nx3 matrix representing the point cloud, trained to optimize EMD or Chamfer distance.  The most successful generative models are based on sampling values in the auto-encoder's latent space using simple models (a two-layer MLP or a GMM). \n\n- While it is interesting that the latent space models seem to outperform the r-GAN, this may be due to the relatively poor performance of r-GAN than to good performance of the latent space models, and directly training a GAN on point clouds remains an important problem.\ n\n- The paper could possibly be clearer by integrating more of the \""background\"" section into later sections.  Some of the GAN figures could also benefit from having captions. \n\nOverall, I think that this paper could serve as a useful baseline for generating point clouds,  but I am not sure that the contribution is significant enough for acceptance.\n",1,1,1,1,1,1,1,1,1,-1
BJjBnN9a--R1,"The paper introduces the notion of continuous convolutional neural networks.  \nThe main idea of the paper is to project examples into an RK Hilbert space\nand performs convolution and filtering into that space.  Interestingly, the\nfilters defined in the Hilbert space  have parameters that are learnable. \n\nWhile the idea may be novel and interesting,  \nMost data that are available for learning are in discrete forms and hopefully,\nthey have been digitalized according to Shannon theory.  This means that they bring\nall necessary information for rebuilding their continuous counterpart.  Hence, it is\nnot clear why projecting them back into continuous functions is of interest.  \n\nAnother point that is not clear or at least misleading is the so-called Hilbert Maps. \nAs far as I understand, Equation (4) is not an embedding into an Hilbert space but\nis more a proximity space representation [1].  Hence, the learning framework of the\nauthors can be casted more as a learning with similarity function than learning\ninto a RKHS [2].  A proper embedding would have mapped $x$ into a function\nbelonging to $\\mH$.  In addition, it seems that all computations are done\ninto a \\ell^2 space instead of in the RKHS (equations 5 and 11).  \nLearning good similarity functions is also not novel [3] and Equations\n(6) and (7) corresponds to learning these similarity functions. \nAs far as I remember, there exists also some paper from the nineties that\nlearn the parameters of RBF networks but unfortunately I have not been able to\ngoogle some of them. \n\n\nPart 3 is the most interesting part of the paper,  however it would have been\ngreat if the authors provide other kernel functions with closed-form convolution \nformula that may be relevant for learning. \nThe proposed methodology is evaluated on some standard benchmarks in vision.  While\nresults are pretty good,  it is not clear how the various cluster sets have been obtained\nand what are their influence on the performances (if they are randomly initialized, it \nwould be great to see standard deviation of performances with respect to initializations). \nI would also be great to have intuitions on why a single continuous filter works betters\nthan 20 discrete ones (if this behaviour is consistent accross initialization). \n\nOn the overall, while the idea may be of interested,  the paper lacks in motivations\nin connecting to relevant previous works and in providing insights on why it works. \nHowever, performance results seem to be competitive and that's the reader may\nbe eager for insights. \n\n\nminor comments\n---------------\n\n* the paper employs vocabulary that is not common in ML.  eg. I am not sure what\noccupancy values, or inducing points are.  \n\n* Supposingly that the authors properly consider computation in RKHS, then \\Sigma_i\nshould be definite positive right?  how update in (7) is guaranteed to be DP?  \nThis constraints may not be necessary if instead they used proximity space representation. \n\n\n\n\n\n[1] https://alex.smola.org/papers/1999/GraHerSchSmo99.pdf\n[2] https://www.cs.cmu.edu/~avrim/Papers/similarity-bbs.pdf\n[3] A. Bellet, A. Habrard and M. Sebban. Similarity Learning for Provably Accurate Sparse Linear Classification.",1,1,1,1,1,1,1,1,1,-1
BJjBnN9a--R2,"This paper aims to provide a continuous variant of CNN.   The main idea is to apply CNN on Hilbert maps of the data.  The data is mapped to a continuous Hilbert space via a reproducing kernel and a convolution layer is defined using the kernel matrix.  A convolutional Hilbert layer algorithm is introduced and evaluated on image classification data sets. \n\nThe paper is well written and provides some new insights on incorporating kernels in CNN. \n\nThe kernel matrix in Eq. 5 is not symmetric and the kernel function in Eq. 3 is not defined over a pair of inputs.  In this case, the projections of the data via the kernel are not necessarily in a RKHS.  The connection between Hilbert maps and RKHS in that sense is not clear in the paper. \n\nThe size of a kernel matrix depends on the sample size.  In large scale situations, working with the kernel matrix can be computational expensive.  It is not clear how this issue is addressed in this paper. \n\nIn section 2.2, how \\mu_i and \\sigma_i are computed? \n\nHow the proposed approach can be compared to convolutional kernel networks (NIPS paper) of Mairal et al. (2014)?",1,1,1,1,1,1,1,1,1,-1
BJjBnN9a--R3,"This paper formulates a variant of convolutional neural networks which models both activations and filters as continuous functions composed from kernel bases.  A closed-form representation for convolution of such functions is used to compute in a manner than maintains continuous representations, without making discrete approximations as in standard CNNs. \n\nThe proposed continuous convolutional neural networks (CCNNs) project input data into a RKHS with a Gaussian kernel function evaluated at a set of inducing points; the parameters defining the inducing points are optimized via backprop.  Filters in convolutional layers are represented in a similar manner, yielding a closed-form expression for convolution between input and filters.  Experiments train CCNNs on several standard small-scale image classification datasets: MNIST, CIFAR-10, STL-10, and SVHN. \n\nWhile the idea is interesting and might be a good alternative to standard CNNs,  the paper falls short in terms of providing experimental validation that would demonstrate the latter point.  It unfortunately only experiments with CCNN architectures with a small number (eg 3) layers.  They do well on MNIST, but MNIST performance is hardly informative as many supervised techniques achieve near perfect results.  The CIFAR-10, STL-10, and SVHN results are disappointing.  CCNNs do not outperform the prior CNN results listed in Table 2,3,4.  Moreover, these tables do not even cite more recent higher-performing CNNs.  See results table in (*) for CIFAR-10 and SVHN results on recent ResNet and DenseNet CNN designs which far outperform the methods listed in this paper. \n\nThe problem appears to be that CCNNs are not tested in a regime competitive with the state-of-the-art CNNs on the datasets used.Why not?   To be competitive, deeper CCNNs would likely need to be trained.   I would like to see results for CCNNs with many layers (eg 16+ layers) rather than just 3 layers.   Do such CCNNs achieve performance compatible with ResNet/DenseNet on CIFAR or SVHN?   Given that CIFAR and SVHN are relatively small datasets, training and testing larger networks on them should not be computationally prohibitive.  \n\nIn addition, for such experiments, a clear report of parameters and FLOPs for each network should be included in the results table.   This would assist in understanding tradeoffs in the design space.  \n\nAdditional questions:\n\nWhat is the receptive field of the CCNNs vs those of the standard CNNs to which they are compared?   If the CCNNs have effectively larger receptive field, does this create a cost in FLOPs compared to standard CNNs?  \n\nFor CCNNs, why does the CCAE initialization appear to be essential to achieving high performance on CIFAR-10 and SVHN?   Standard CNNs, trained on supervised image classification tasks do not appear to be dependent on initialization schemes that do unsupervised pre-training.   Such dependence for CCNNs appears to be a weakness in comparison.",1,1,1,1,1,1,1,1,-1,-1
BJJLHbb0--R1,"1. This is a good paper, makes an interesting algorithmic contribution in the sense of joint clustering-dimension reduction for unsupervised anomaly detection\n2.  It demonstrates clear performance improvement via comprehensive comparison with state-of-the-art methods\n3.  Is the number of Gaussian Mixtures 'K' a hyper-parameter in the training process?  can it be a trainable parameter? \n4. Also, it will be interesting to get some insights or anecdotal evidence on how the joint learning helps beyond the decoupled learning framework, such as what kind of data points (normal and anomalous) are moving apart due to the joint learning",1,1,1,1,1,-1,1,1,1,-1
BJJLHbb0--R2,"The paper presents a new technique for anomaly detection where the dimension reduction and the density estimation steps are jointly optimized.  The paper is rigorous and ideas are clearly stated.  The idea to constraint the dimension reduction to fit a certain model, here a GMM, is relevant, and the paper provides a thorough comparison with recent state-of-the-art methods.  My main concern is that the method is called unsupervised, but it uses the class information in the training, and also evaluation.  I'm also not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n\n1. The framework uses the class information, i.e., \u201conly data samples from the normal class are used for training\u201d, but it is still considered unsupervised.  Also, the anomaly detection in the evaluation step is based on a threshold which depends on the percentage of known anomalies, i.e., a priori information.  I would like to see a plot of the sample energy as a function of the number of data points.  Is there an elbow that indicates the threshold cut?  Better yet it would be to use methods like Local Outlier Factor (LOF) (Breunig et al., 2000 \u2013 LOF:Identifying Density-based local outliers) to detect the outliers (these methods also have parameters to tune, sure, but using the known percentage of anomalies to find the threshold is not relevant in a purely unsupervised context when we don't know how many anomalies are in the data). \n2. Is there a theoretical justification for computing the mixture memberships for the GMM using a neural network?  \n3. How do the regularization parameters \\lambda_1 and \\lambda_2 influence the results? \n4. The idea to jointly optimize the dimension reduction and the clustering steps was used before neural nets (e.g., Yang et al., 2014 -  Unsupervised dimensionality reduction for Gaussian mixture model).  Those approaches should at least be discussed in the related work, if not compared against. \n5. The authors state that estimating the mixture memberships with a neural network for GMM in the estimation network instead of the standard EM algorithm works better.  Could you provide a comparison with EM? \n6. In the newly constructed space that consists of both the extracted features and the representation error, is a Gaussian model truly relevant?  Does it well describe the new space?  Do you normalize the features (the output of the dimension reduction and the representation error are quite different)?  Fig. 3a doesn't seem to show that the output is a clear mixture of Gaussians. \n7. The setup of the KDDCup seems a little bit weird, where the normal samples and anomalies are reversed (because of percentage), where the model is trained only on anomalies, and it detects normal samples as anomalies  ... I'm not convinced that it is the best example, especially that is it the one having significantly better results, i.e. scores ~ 0.9 vs. scores ~0.4/0.5 score for the other datasets. \n8. The authors mention that \u201cwe can clearly see from Fig. 3a that DAGMM is able to well separate  ...\u201d - it is not clear to me, it does look better than the other ones, but not clear.   If there is a clear separation from a different view, show that one instead.   We don't need the same view for all methods.   \n9. In the experiments the reduced dimension used is equal to 1 for two of the experiments and 2 for one of them.  This seems very drastic! \n\nMinor comments:\n\n1. Fig.1: what dimension reduction did you use? Add axis labels.\n2.  \u201cDAGMM preserves the key information of an input sample\u201d - what does key information mean? \n3. In Fig. 3 when plotting the results for KDDCup, I would have liked to see results for the best 4 methods from Table 1, OC-SVM performs better than PAE.  Also DSEBM-e and DSEBM-r seems to perform very well when looking at the three measures combined.  They are the best in terms of precision. \n4. Is the error in Table 2 averaged over multiple runs? If yes, how many? \n\nQuality \u2013 The paper is thoroughly written, and the ideas are clearly presented.  It can be further improved as mentioned in the comments. \n\nClarity \u2013 The paper is very well written with clear statements, a pleasure to read.  \n\nOriginality \u2013 Fairly original, but it still needs some work to justify it better.  \n\nSignificance \u2013 Constraining the dimension reduction to fit a certain model is a relevant topic, but I'm not convinced of how well the Gaussian model fits the low-dimensional representation and how well can a neural network compute the GMM mixture memberships. \n",1,1,1,1,1,1,1,1,1,-1
BJJLHbb0--R3,"Summary\n\nThis applications paper proposes using a deep neural architecture to do unsupervised anomaly detection by learning the parameters of a GMM end-to-end with reconstruction in a low-dimensional latent space.  The algorithm employs a tailored loss function that involves reconstruction error on the latent space, penalties on degenerate parameters of the GMM, and an energy term to model the probability of observing the input samples. \n\nThe algorithm replaces the membership probabilities found in the E-step of EM for a GMM with the outputs of a subnetwork in the end-to-end architecture.  The GMM parameters are updated with these estimated responsibilities as usual in the M-step during training. \n\nThe paper demonstrates improvements in a number of public datasets.  Careful reporting of the tuning and hyperparameter choices renders these experiments repeatable, and hence a suitable improvement in the field.  Well-designed ablation studies demonstrate the importance of the architectural choices made, which are generally well-motivated in intuitions about the nature of anomaly detection. \n\nCriticisms\n\nBased on the performance of GMM-EN, the reconstruction error features are crucial to the success of this method.  Little to no detail about these features is included.  Intuitively, the estimation network is given the latent code conditioned and some (probably highly redundant) information about the residual structure remaining to be modeled.  Why did the choices that were made in the paper yield this success?  How do you recommend other researchers or practitioners selected from the large possible space of reconstruction features to get the best results? \n\nQuality\n\nThis paper does not set out to produce a novel network architecture.  Perhaps the biggest innovation is the use of reconstruction error features as input to a subnetwork that predicts the E-step output in EM for a GMM.  This is interesting and novel enough in my opinion to warrant publication at ICLR, along with the strong performance and careful reporting of experimental design.\n\n",1,1,1,1,1,-1,1,1,1,1
BJjquybCW-R1,"This paper presents several theoretical results on the loss functions of CNNs and fully-connected neural networks.  I summarize the results as follows:\n\n(1) Under certain assumptions, if the network contains a \""wide\u201c hidden layer, such that the layer width is larger than the number of training examples, then (with random weights) this layer almost surely extracts linearly independent features for the training examples. \n\n(2) If the wide layer is at the top of all hidden layers, then the neural network can perfectly fit the training data. \n\n(3) Under similar assumptions and within a restricted parameter set S_k, all critical points are the global minimum.  These solutions achieve zero squared-loss. \n\nI would consider result (1) as the main result of this paper, because (2) is a direct consequence of (1).  Intuitively, (1) is an easy result.  Under the assumptions of Theorem 3.5, it is clear that any tiny random perturbation on the weights will make the output linearly independent.  The result will be more interesting if the authors can show that the smallest eigenvalue of the output matrix is relatively large, or at least not exponentially small. \n\nResult (3) has severe limitations, because: (a) there can be infinitely many critical point not in S_k that are spurious local minima;  (b) Even though these spurious local minima have zero Lebesgue measure, the union of their basins of attraction can have substantial Lebesgue measure;  (c) inside S_k, Theorem 4.4 doesn't exclude the solutions with exponentially small gradients, but whose loss function values are bounded away above zero.  If an optimization algorithm falls onto these solutions, it will be hard to escape. \n\nOverall, the paper presents several incremental improvement over existing theories.  However, the novelty and the technical contribution are not sufficient for securing an acceptance.\n\n",1,1,1,1,-1,-1,1,1,1,-1
BJjquybCW-R2,"This paper analyzes the expressiveness and loss surface of deep CNN.  I think the paper is clearly written, and has some interesting insights.",1,-1,-1,1,-1,-1,1,-1,1,-1
BJjquybCW-R3,"This paper analyzes the loss function and properties of CNNs with one \""wide\"" layer, i.e., a layer with number of neurons greater than the train sample size.  Under this and some additional technique conditions, the paper shows that this layer can extract linearly independent features and all critical points are local minimums.  I like the presentation and writing of this paper.  However, I find it uneasy to fully evaluate the merit of this paper, mainly because the \""wide\""-layer assumption seems somewhat artificial and makes the corresponding results somewhat expected.  The mathematical intuition is that the severe overfitting induced by the wide layer essentially lifts the loss surface to be extremely flat so training to zero/small error becomes easy.  This is not surprising.  It would be interesting to make the results more quantitive, e.g., to quantify the tradeoff between having local minimums and having nonzero training error.",1,1,1,1,1,-1,1,1,1,-1
BJk59JZ0b-R1,"\n\nThe authors devise and explore use of the hessian of the\n(approximate/learned) value function (the critic) to update the policy\n(actor) in the actor-critic  approach to RL.   They connect their\ntechnique, 'guide actor-critic' or GAC, to existing actor-critic\nmethods (authors claim only two published work use 1st order\ninformation on the critic).  They show that the 2nd order information\ncan be useful (in several of the 9 tasks, their GAC techniques were\nbest or competitive, and in only one, performed poorly compared to best). \n\nThe paper has a technical focus. \n\npros:\n\n- Strict generalization of an existing (up to 1st order) actor-critic approaches \n\n- Compared to many existing techniques, on 9 tasks \n\ncons:\n\n- no mention of time costs, except that for more samples, S > 1, for\n taylor approximation, it can be very expensive. \n\n- one would expect more information to strictly improve performance,\n  but the results are a bit mixed (perhaps due to convergence to local optima and both actor and critic being learned at same time, \n  or the Gaussian assumptions, etc.). \n\n- relevance: the work presents a new approach to actor-critique strategy for\n  reinforcement learning, remotely related to 'representation\n  learning' (unless value and policies are deemed a form of\n  representation). \n\n\nOther comments/questions:\n\n- Why does the performance start high on Ant (1000), then goes to 0\n(all approaches)? \n\n- How were the tasks selected?  Are they all the continuous control tasks available in open ai?\n\n\n \n\n\n\n\n",1,1,1,1,-1,1,1,-1,1,-1
BJk59JZ0b-R2,"The experimental results are similar to previously proposed methods.  \n\nThe paper is fairly well-written, provides proofs of detailed properties of the algorithm, and has decent experimental results.  However, the method is not properly motivated.  As far as I can tell, the paper never answers the questions: Why do we need a guide actor?  What problem does the guide actor solve?  \n\nThe paper argues that the guide actor allows to introduce second order methods, but (1) there are other ways of doing so and  (2) it\u2019s not clear why we should want to use second-order methods in reinforcement learning in the first place.  Using second order methods is not an end in itself.  The experimental results show the authors have found a way to use second order methods without making performance *worse*.  Given the high variability of deep RL, they have not convincingly shown it performs better. \n\nThe paper does not discuss the computational cost of the method.  How does it compare to other methods?  My worry is that the method is more complicated and slower than existing methods, without significantly improved performance. \n\nI recommend the authors take the time to make a much stronger conceptual and empirical case for their algorithm. \n",1,1,1,1,1,1,1,1,-1,-1
BJk59JZ0b-R3,"The paper presents a clever trick for updating the actor in an actor-critic setting: computing a guide actor that diverges from the actor to improve critic value, then updating the actor parameters towards the guide actor.  This can be done since, when the parametrized actor is Gaussian and the critic value can be well-approximated as quadratic in the action, the guide actor can be optimized in closed form. \n\nThe paper is mostly clear and well-presented,  except for two issues: 1) there is virtually nothing novel presented in the first half of the paper (before Section 3.3);  and 2) the actual learning step is only presented on page 6, making it hard to understand the motivation behind the guide actor until very late through the paper.\n\nThe presented method itself seems to be an important contribution, even if the results are not overwhelmingly positive.  It'd be interesting to see a more elaborate analysis of why it works well in some domains but not in others.  More trials are also needed to alleviate any suspicion of lucky trials.  \n\nThere are some other issues with the presentation of the method, but these don't affect the merit of the method:  \n\n1. Returns are defined from an initial distribution that is stationary for the policy.   While this makes sense in well-mixing domains, the experiment domains are not well-mixing for most policies during training, for example a fallen humanoid will not get up on its own, and must be reset.  \n\n2. The definition of beta(a|s) as a mixture of past actors is inconsistent with the sampling method, which seems to be a mixture of past trajectories.  \n\n3. In the first paragraph of Section 3.3: \""[...] the quality of a guide actor mostly depends on the accuracy of Taylor's approximation.  \"" What else does it depend on?  Then: \""[...] the action a_0 should be in a local vicinity of a. \""; and \""[...] the action a_0 should be similar to actions sampled from pi_theta(a|s).\"" What do you mean \""should\""?  In order for the Taylor approximation to be good? \n\n4. The line before (19) is confusing, since (19) is exact and not an approximation.  For the approximation (20), it isn't clear if this is a good approximation.  Why/when is the 2nd term in (19) small? \n\n5. The parametrization nu of \\hat{Q} is never specified in Section 3.6.  This is important in order to evaluate the complexities involved in computing its Hessian.\n",1,1,1,1,1,-1,1,1,1,1
BJk7Gf-CZ-R1,"The paper gives sufficient and necessary conditions for the global optimality of the loss function of deep linear neural networks.  The paper is an extension of Kawaguchi'16.  It also provides some sufficient conditions for the non-linear cases.  \n\nI think the main technical concerns with the paper is that the technique only applies to a linear model, and it doesn't sound the techniques are much beyond Kawaguchi'16.  I am happy to see more papers on linear models, but I would expect there are more conceptual or technical ingredients in it.  As far as I can see, the same technique here will fail for non-linear models for the same reason as Kawaguchi's technique.  Also, I think a more interesting question might be turning the landscape results into an algorithmic result --- have an algorithm that can guarantee to converge a global minimum.  This won't be trivial because the deep linear networks do have a lot of very flat saddle points and therefore it's unclear whether one can avoid those saddle points.",1,1,1,1,1,1,1,1,1,-1
BJk7Gf-CZ-R2,"Summary:\nThe paper gives theoretical results regarding the existence of local minima in the objective function of deep neural networks.  In particular:\n- in the case of deep linear networks, they characterize whether a critical point is a global optimum or a saddle point by a simple criterion.  This improves over recent work by Kawaguchi who showed that each critical point is either a global minimum or a saddle point (i.e., none is a local minimum), by relaxing some hypotheses and adding a simple criterion to know in which case we are .\n- in the case of nonlinear network, they provide a sufficient condition for a solution to be a global optimum, using a function space approach. \n\nQuality:\nThe quality is very good.  The paper is technically correct and nontrivial.  All proofs are provided and easy to follow. \n\nClarity:\nThe paper is very clear.  Related work is clearly cited, and the novelty of the paper well explained.  The technical proofs of the paper are in appendices, making the main text very smooth. \n\nOriginality:\nThe originality is weak.  It extends a series of recent papers correctly cited.  There is some originality in the proof which differs from recent related papers. \n\nSignificance:\nThe result is not completely surprising,  but it is significant given the lack of theory and understanding of deep learning.  Although the model is not really relevant for deep networks used in practice, the main result closes a question about characterization of critical points in simplified models if neural network, which is certainly interesting for many people.",1,1,1,1,-1,1,1,1,1,-1
BJk7Gf-CZ-R3,"\n-I think title is misleading, as the more concise results in this paper is about linear networks I recommend adding linear in the title i.e. changing the title to \u2026 deep LINEAR networks \n\n- Theorems 2.1, 2.2 and the observation (2) are nice! \n \n- Theorem 2.2 there is no discussion about the nature of the saddle point is it strict?  Does this theorem imply that the global optima can be reached from a random initialization?  Regardless of if this theorem can deal with these issues, a discussion of the computational implications of this theorem is necessary. \n\n- I\u2019m a bit puzzled by Theorems 4.1 and 4.2 and why they are useful.  Since these results do not seem to have any computational implications about training the neural nets what insights do we gain about the problem by knowing this result?  Further discussion would be helpful.\n",1,1,1,1,1,-1,1,1,-1,-1
BJLmN8xRW-R1,"\nSUMMARY\n\nThis paper addresses the cybersecurity problem of domain generation algorithm (DGA)  detection.  A class of malware uses algorithms to automatically generate artificial domain names for various purposes, e.g. to generate large numbers of rendezvous points.  DGA detection concerns the (automatic) distinction of actual and artificially generated domain names.  In this paper, a basic problem formulation and general solution approach is investigated, namely that of treating the detection as a text classification task and to let domain names arrive to the classifier as strings of characters.  A set of five deep learning architectures (both CNNs and RNNs) are compared empirical on the text classification task.  A domain name data set with two million instances is used for the experiments.  The main conclusion is that the different architectures are almost equally accurate and that this prompts a preference of simpler architectures over more complex architectures, since training time and the likelihood for overfitting can potentially be reduced. \n\nCOMMENTS\n\nThe introduction is well-written, clear, and concise.  It describes the studied real-world problem and clarifies the relevance and challenge involved in solving the problem.  The introduction provides a clear overview of deep learning architectures that have already been proposed for solving the problem as well as some architectures that could potentially be used. One suggestion for the introduction is that the authors take some of the description of the domain problem and put it into a separate background section to reduce the text the reader has to consume before arriving at the research problem and proposed solution. \n\nThe methods section (Section 2) provides a clear description of each of the five architectures along with brief code listings and details about whether any changes or parameter choices were made for the experiment.  In the beginning of the section, it is not clarified why, if a 75 character string is encoded as a 128 byte ASCII sequence, the content has to be stored in a 75 x 128 matrix instead of a vector of size 128.  This is clarified later but should perhaps be discussed earlier to allow readers from outside the subarea to grasp the approach. \n\nSection 3 describes the experiment settings, the results, and discusses the learned representations and the possible implications of using either the deep architectures or the \u201cbaseline\u201d Random Forest classifier.  Perhaps, the authors could elaborate a little bit more on why Random Forests were trained on a completely different set of features than the deep architectures?  The data is stated to be randomly divided into training (80%), validation (10%), and testing (10%).  How many times is this procedure repeated?  (That is, how many experimental runs were averaged or was the experiment run once?). \n\nIn summary, this is an interesting and well-written paper on a timely topic.  The main conclusion is intuitive.  Perhaps the conclusion is even regarded as obvious by some but, in my opinion, the result is important since it was obtained from new, rather extensive experiments on a large data set and through the comparison of several existing (earlier proposed) architectures.  Since the main conclusion is that simple models should be prioritised over complex ones (due to that their accuracy is very similar), it would have been interesting to get some brief comments on a simplicity comparison of the candidates at the conclusion. \n\nMINOR COMMENTS\n\nAbstract: \u201cLittle studies\u201d -> \u201cFew studies\u201d \n\nTable 1: \u201capproach\u201d -> \u201capproaches\u201d \n\nFigure 1: Use the same y-axis scale for all subplots (if possible) to simplify comparison.  Also, try to move Figure 1 so that it appears closer to its inline reference in the text. \n\nSection 3: \u201cbased their on popularity\u201d -> \u201cbased on their popularity\u201d\n\n",1,1,1,1,1,-1,1,1,1,1
BJLmN8xRW-R2,"This paper applies several NN architectures to classify url\u2019s between benign and malware related URLs. \nThe baseline is random forests and feature engineering. \n\nThis is clearly an application paper.  \nNo new method is being proposed, only existing methods are applied directly to the task. \n\nI am not familiar with the task at hand so I cannot properly judge the quality/accuracy of the results obtained but it seems ok. \nFor evaluation data was split randomly in 80% train, 10% test and 10% validation.  Given the amount of data 2*10**6 samples, this seems sufficient. \nI think the evaluation could be improved by using malware URLs that were obtained during a larger time window. \nSpecifically, it would be nice if train, test and validation URLs would be operated chronologically. I.e. all train url precede the validation and test urls. \nIdeally, the train and test urls would also be different in time.  This would enable a better test of the generalization capabilities in what is essentially a continuously changing environment.  \n\nThis paper is a very difficult for me to assign a final rating. \nThere is no obvious technical mistake  and the paper is written reasonably well. \nThere is however a lack of technical novelty or insight in the models themselves.  \nI think that the paper should be submitted to a journal or conference in the application domain where it would be a better fit. \n\nFor this reason, I will give the score marginally below the acceptance threshold now. \nBut if the other reviewers argue that the paper should be accepted I will change my score.\n\n",1,1,-1,1,1,-1,1,1,1,-1
BJLmN8xRW-R3,"This paper proposes to automatically recognize domain names as malicious or benign by deep networks (convnets and RNNs) trained to directly classify the character sequence as such. \n\n\nPros\n\nThe paper addresses an important application of deep networks, comparing the performance of a variety of different types of model architectures. \n\nThe tested networks seem to perform reasonably well on the task. \n\n\nCons\n\nThere is little novelty in the proposed method/models -- the paper is primarily focused on comparing existing models on a new task. \n\nThe descriptions of the different architectures compared are overly verbose -- they are all simple standard convnet / RNN architectures.   The code specifying the models is also excessive for the main text -- it should be moved to an appendix or even left for a code release. \n\nThe comparisons between various architectures are not very enlightening as they aren\u2019t done in a controlled way -- there are a large number of differences between any pair of models so it\u2019s hard to tell where the performance differences come from.  It\u2019s also difficult to compare the learning curves among the different models (Fig 1) as they are in separate plots with differently scaled axes. \n\nThe proposed problem is an explicitly adversarial setting and adversarial examples are a well-known issue with deep networks and other models, but this issue is not addressed or analyzed in the paper.  (In fact, the intro claims this is an advantage of not using hand-engineered features for malicious domain detection, seemingly ignoring the literature on adversarial examples for deep nets.) For example, in this case an attacker could start with a legitimate domain name and use black box adversarial attacks (or white box attacks, given access to the model weights) to derive a similar domain name that the models proposed here would classify as benign. \n\n\nWhile this paper addresses an important problem, in its current form the novelty and analysis are limited and the paper has some presentation issues.",1,1,1,1,1,-1,1,1,1,1
BJlrSmbAZ-R1,"This paper proposes an approximate method to construct Bayesian uncertainty estimates in networks trained with batch normalization. \n\nThere is a lot going on in this paper.  Although the overall presentation is clean,  there are few key shortfalls (see below).  Overall, the reported functionality is nice,  although the experimental results are difficult to intepret (despite laudable effort by the authors to make them intuitive). \n\nSome open questions that I find crucial:\n\n* How exactly is the \u201cstochastic forward-pass\u201d performed that gives rise to the moment estimates?  This step is the real meat of the paper, yet I struggle to find a concrete definition in the text.  Is this really just an average over a few recent weights during optimization?  If so, how is this method specific to batch normalization?   Maybe I\u2019m showing my own lack of understanding here, but it\u2019s worrying that the actual sampling technique is not explained anywhere.   This relates to a larger point about the paper's main point: What, exactly, is the Bayesian interpretation of batch normalization proposed here?   In Bayesian Dropout, there is an explicit variational objective.   Here, this is replaced by an implicit regularizer.    The argument in Section 3.3 seems rather weak to me.    To paraphrase it: If the prior vanishes, so does the regularizer. Fine.  But what's the regularizer that's vanishing?  The sentence that \""the influence of the prior diminishes as the size of the training data increases\"" is debatable for something as over-parametrized as a DNN.  I wouldn't be surprised that there are many directions in the weight-space of a trained DNN along which the posterior is dominated by the prior. \n\n* I\u2019m confused about the statements made about the \u201cconstant uncertainty\u201d baseline.  First off, how is this (constant) width of the predictive region chosen?  Did I miss this, or is it not explained anywhere?  Unless I misunderstand the definition of CRPS and PLL, that width should matter, no?  Then, the paragraph at the end of page 8 is worrying: The authors essentially say that the constant baseline is quite close to the estimate constructed in their work because constant uncertainty is \u201cquite a reasonable baseline\u201d.  That can hardly be true (if it is, then it puts the entire paper into question! If trivial uncertainty is almost as good as this method, isn't the method trivial, too?).  \nOn a related point: What would Figure 2 look like for the constand uncertainty setting?  Just a horizontal line in blue and red?  But at which level? \n\nI like this paper.  It is presented well (modulo the above problems), and it makes some strong points.  But I\u2019m worried about the empirical evaluation, and the omission of crucial algorithmic details.  They may hide serious problems.",1,1,1,1,1,-1,1,1,1,-1
BJlrSmbAZ-R2,"*Summary*\n\nThe paper proposes using batch normalisation at test time to get the predictive uncertainty.  The stochasticity of the prediction comes from different minibatches of training data that were used to normalise the activity/pre-activation values at each layer.  This is justified by an argument that using batch norm is doing variational inference, so one should use the approximate posterior provided by batch norm at prediction time.  Several experiments show Monte Carlo prediction at test time using batch norm is better than dropout. \n\n*Originality and significance*\n\nAs far as I understand, almost learning algorithms similar to equation 2 can be recast as variational inference under equation 1.  However, the critical questions are what is the corresponding prior, what is the approximating density, what are the additional approximations to obtain 2, and whether the approximation is a good approximation for getting closer to the posterior/obtain better prediction.  \n\nIt is not clear to me from the presentation what the q(w) density is -- whether this is explicit (as in vanilla Gaussian VI or MC dropout), or implicit (the stochasticity on the activity h due to batch norm induces an equivalence q on w). \n\nFrom a Bayesian perspective, it is also not satisfying to ignore the regularisation term by an empirical heuristic provided in the batch norm paper [small \\lambda] -- what is the rationale of this?  Can this be explained by comparing the variational free-energy.  \n\nThe experiments also do not compare to modern variational inference methods using the reparameterisation trick with Gaussian variational approximations (see Blundell et al 2016) or richer variational families (see e.g. Louizos and Welling, 2016, 2017).  The VI method included in the PBP paper (Hernandez-Lobato and Adams, 2015) does not use the reparameterisation trick, which has been found to reduce variance and improve over Graves' VI method. \n\n*Clarity*\nThe paper is in general well written and easy to understand.  \n\n*Additional comments*\n\nPage 2: Monte Carlo Droput --> Dropout\nPage 3 related work: (Adams, 2015) should be (Hernandez-Lobato and Adams, 2015)",1,1,1,1,1,1,1,1,1,-1
BJlrSmbAZ-R3,"The authors show how the regularization procedure called batch normalization,\ncurrently being used by most deep learning systems, can be understood as\nperforming approximate Bayesian inference.  The authors compare this approach to\nMonte Carlo dropout (another regularization technique which can also be\nconsidered to perform approximate Bayesian inference).  The experiments\nperformed show that the Bayesian view of batch normalization performs similarly\nas MC dropout in terms of the estimates of uncertainty that it produces. \n\nQuality:\n\nI found the quality to be low in some aspects.  First, the description of what\nis the prior used by batch normalization in section 3.3 is unsatisfactory.  The\nauthors basically refer to Appendix 6.4 for the case in which the weight decay\npenalty is not zero.  The details in that Appendix are almost none, they just\nsay \""it is thus possible to derive the prior...\"".  \n\nThe results in Table 2 are a bit confusing.   The authors should highlight in\nbold face the results of the best performing method.  \n\nThe authors indicate that they do not need to compare to variational methods\nbecause Gal and Ghahramani 2015 compare already to those methods.   However, Gal\nand Ghahramani's code used Bayesian optimization methods to tune\nhyper-parameters and this code contains a bug that optimizes hyper-parameters\nby maximizing performance on the test data.   In particular for hyperparameter\nselection, they average performance across (subsets of) 5 of the training sets\nfrom the 20x train/test split, and then using the tau which got the best\naverage performance for all of 20x train/test splits to evaluate performance:\n\nhttps://github.com/yaringal/DropoutUncertaintyExps/blob/master/bostonHousing/net/experiment_BO.py#L54 \n\nTherefore, the claim that \n\n\""Since we have established that MCBN performs on par with MCDO, by proxy we\nmight conclude that MCBN outperforms those VI methods as well.\""\n\nis not valid. \n\nAt the beginning of section 4.3 the authors indicate that they follow in their\nexperiments the setup of Gal and Ghahramani (2015).  However, Gal and Ghahramani\n(2015) actually follow Hern\u00e1ndez-Lobato and Adams, 2015 so the correct\nreference should be the latter one. \n\nClarity:\n\nThe paper is clearly written and easy to follow and understand. \n\nI found confusing how to use the proposed method to obtain estimates of\nuncertainty for a particular test data point x_star.  The paragraph just above\nsection 4 says that the authors sample a batch of training data for this, but\nassume that the test point x_star has to be included in this batch. \nHow is this actually done in practice? \n\nOriginality:\n\nThe proposed contribution is original.  This is the first time that a Bayesian\ninterpretation has been given to the batch normalization regularization\nproposal. \n\nSignificance:\n\nThe paper's contributions are significant.  Batch normalization is a very\npopular regularization technique and showing that it can be used to obtain\nestimates of uncertainty is relevant and significant.  Many existing deep\nlearning systems can use this to produce estimates of uncertainty in their\npredictions.\n",1,1,1,1,1,1,1,1,1,1
BJMuY-gRW-R1,"This paper proposes to jointly learning a semantic objective and inducing a binary tree structure for word composition, which is similar to (Yogatama et al, 2017).  Differently from (Yogatama et al, 2017), this paper doesn\u2019t use reinforcement learning to induce a hard structure, but adopts a chart parser manner and basically learns all the possible binary parse trees in a soft way.  \n\nOverall, I think it is really an interesting direction and the proposed method sounds reasonable.  However, I am concerned about the following points:  \n\n- The improvements are really limited on both the SNLI and the Reverse Dictionary tasks.  (Yogatama et al, 2017) demonstrate results on 5 tasks and I think it\u2019d be helpful to present results on a diverse set of tasks and see if conclusions can generally hold.  Also, it would be much better to have a direct comparison to (Yogatama et al, 2017), including the performance and also the induced tree structures. \n\n- The computational complexity of this model shouldn\u2019t be neglected.  If I understand it correctly, the model needs to compute O(N^3) LSTM compositions.  This should be at least discussed in the paper.  And I am not also sure how hard this model is being converged in all experiments (compared to LSTM or supervised tree-LSTM).\ n\n- I am wondering about the effects of the temperature parameter t. Is that important for training? \n\nMinor:\n- What is the difference between LSTM and left-branching LSTM? \n- I am not sure if the attention overt chart is a highlight of the paper or not.  If so, better move that part to the models section instead of mention it briefly in the experiments section.  Also, if any visualization (over the chart) can be provided, that\u2019d be helpful to understand what is going on. \n",1,1,1,1,1,1,1,1,1,-1
BJMuY-gRW-R2,"Summary: The paper proposes to use the CYK chart-based mechanism to compute vector representations for sentences in a bottom-up manner as in recursive NNs . The key idea is to maintain a chart to take into account all possible spans.  The paper also introduces an attention method over chart cells.  The experimental results show that the propped model outperforms tree-lstm using external parsers. \n\nComment: I kinda like the idea of using chart, and the attention over chart cells.  The paper is very well written. \n- My only concern about the novelty of the paper is that the idea of using CYK chart-based mechanism is already explored in Le and Zuidema (2015). \n- Le and Zudema use pooling and this paper uses weighted sum.  Any differences in terms of theory and experiment? \n- I like the new attention over chart cells.  But I was surprised that the authors didn\u2019t use it in the second experiment (reverse dictionary). \n- In table 2, it is difficult for me to see if the difference between unsupervised tree-lstm and right-branching tree-lstm (0.3%) is \u201cgood enough\u201d.  In which cases the former did correctly but the latter didn\u2019t? \n- In table 3, what if we use the right-branching tree-lstm with attention? \n- In table 4, why do Hill et al lstm and bow perform much better than the others?\n",1,1,1,1,1,1,1,1,1,-1
BJMuY-gRW-R3,"The paper presents a model titled the \""unsupervised tree-LSTM,\"" in which the authors mash up a dynamic-programming chart and a recurrent neural network.  As far as I can glean, the topology of the neural network is constructed using the chart of a CKY parser.  When combining different constituents, an energy function is computed (equation 6) and the resulting energies are passed through a softmax.  The architecture achieves impressive results on two tasks: SNLI and the reverse dictionary of Hill et al. (2016). \n\nOverall, I found the paper deeply uninspired.  The authors downplay the similarity of their paper to that of Le and Zuidema (2015), which  I did not appreciate.  It's true that Le and Zuidema take a parse forest from an existing parser, but it still contains an exponential number of trees, as does the work in here.  Note that exposition in Le and Zuidema (2015) discusses the pruned case as well, i.e., a compete parse forest.  The authors of this paper simply write \""Le and Zuidema (2015) propose a model that takes as input a parse forest from an external parser, in order to deal with uncertainty. \"" I would encourage the authors to revisit Le and Zuidema (2015), especially section 3.2, and consider the technical innovations over the existing work.  I believe the primary difference (other using an LSTM instead of a convnet) is to replace max-pooling with softmax-pooling. Do these two architectural changes matter?  The experiments offer no empirical comparison.  In short, the insight of having an end-to-end differentiable function based on a dynamic-programming chart is pretty common -- the idea is in the air.  The authors provide yet another instantiation of such an approach, but this time with an LSTM.  \n\nThe technical exposition is also relatively poor.  The authors could have expressed their network using a clean recursion, following the parse chart, but opted not to, and, instead,  provided a round-about explanation in English.  Thus, despite the strong results,  I would not like to see this work in the proceedings, due to the lack of originality and poor technical discussion.  If the paper were substantially cleaned-up, I would be willing to increase my rating.",1,1,1,-1,1,1,1,1,1,-1
BJNRFNlRW-R1,"This paper formulates GAN as a Lagrangian of a primal convex constrained optimization problem.  They then suggest to modify the updates used in the standard GAN training to be similar to the primal-dual updates typically used by primal-dual subgradient methods.  I think this is a nice contribution that does yield to some interesting insights.  However I do have some concerns about the way the paper is currently written and I find some claims misleading. \n\nPrior convergence proofs: I think the way the paper is currently written is misleading.  The authors quote the paper from Ian Goodfellow: \u201cFor GANs, there is no theoretical prediction as to\nwhether simultaneous gradient descent should converge or not. \u201d. However, the f-GAN paper gave a proof of convergence, see Theorem 2 here: https://arxiv.org/pdf/1606.00709.pdf.  A recent NIPS paper by (Nagarajan and Kolter, 2017) also study the convergence properties of simultaneous gradient descent.  Another problem is of course the assumptions required for the proof that typically don\u2019t hold in practice (see comment below). \n\nConvex-concave assumption: In practice the GAN objective is optimized over the parameters of the neural network rather than the generative distribution.  This typically yields a non-convex non-concave optimization problem.  This should be mentioned in the paper and I would like to see a discussion concerning the gap between the theory and the practical algorithm. \n\nRelation to existing regularization techniques: Combining Equations 11 and 13, the second terms acts as a regularizer that minimizes [\\lapha f_1(D(x_i))]^2.  This looks rather similar to some of the recent regularization techniques such as\nImproved Training of Wasserstein GANs, https://arxiv.org/pdf/1704.00028.pdf \nStabilizing Training of Generative Adversarial Networks through Regularization, https://arxiv.org/pdf/1705.09367.pdf\nCan the authors comment on this?  I think this would also shed some light as to why this approach alleviates the problem of mode collapse. \n\nCurse of dimensionality: Nonparametric density estimators such as the KDE technique used in this paper suffer from the well-known curse of dimensionality.  For the synthetic data, the empirical evidence seem to indicate that the technique proposed by the authors does work  but I\u2019m not sure the empirical evidence provided for the MNIST and CIFAR-10 datasets is sufficient to judge whether or not the method does help with mode collapse.  The inception score fails to capture this property.  Could the authors explore other quantitative measure?  Have you considered trying your approach on the augmented version of the MNIST dataset used in Metz et al. (2016) and Che et al. (2016)? \n\nExperiments\nTypo: Should say \u201cThe data distribution is p_d(x) = 1{x=1}\u201d.\n",1,1,1,1,1,1,1,1,1,-1
BJNRFNlRW-R2,"This paper proposed a framework to connect the solving of GAN with finding the saddle point of a minimax problem. \nAs a result, the primal-dual subgradient methods can be directly introduced to calculate the saddle point. \nAdditionally, this idea not only fill the relatviely lacking of theoretical results for GAN or WGAN, but also provide a new perspective to modify the GAN-type models. \nBut this saddle point model reformulation  in section 2 is quite standard, with limited theoretical analysis in Theorem 1. \nAs follows, the resulting algorithm 1 is also standard primal-dual method for a saddle point problem. \nMost important I think, the advantage of considering GAN-type model as a saddle point model is that first--order methods can be designed to solve it.  But the numerical experiments part seems to be a bit weak, because the MINST or CIFAR-10 dataset is not large enough to test the extensibility for large-scale cases.",1,1,1,1,1,-1,1,1,1,-1
BJNRFNlRW-R3,"In this paper, the authors study the relationship between training GANs and primal-dual subgradient methods for convex optimization.  Their technique can be applied on top of existing GANs and can address issues such as mode collapse.  The authors also derive a GAN variant similar to WGAN which is called the Approximate WGAN.  Experiments on synthetic datasets demonstrate that the proposed formulation can avoid mode collapse.  This is a strong contribution \n\nIn Table 2 the difference between inception scores for DCGAN and this approach seems significant to ignore.  The authors should explain more possibly. \nThere is a typo in Page 2 \u2013 For all these varaints, -variants.\n",1,1,1,1,1,-1,1,1,1,-1
BJOFETxR--R1,"Summary:  The paper applies graph convolutions with deep neural networks to the problem of \""variable misuse\"" (putting the wrong variable name in a program statement) in graphs created deterministically from source code.   Graph structure is determined by program abstract syntax tree (AST) and next-token edges, as well as variable/function name identity, assignment and other deterministic semantic relations.   Initial node embedding comes from both type and tokenized name information.   Gated Graph Neural Networks (GGNNs, trained by maximum likelihood objective) are then run for 8 iterations at test time. \n\nThe evaluation is extensive and mostly very good.   Substantial data set of 29m lines of code.   Reasonable baselines.   Nice ablation studies.   I would have liked to see separate precision and recall rather than accuracy.   The current 82.1% accuracy is nice to see,   but if 18% of my program variables were erroneously flagged as errors, the tool would be useless.    I'd like to know if you can tune the threshold to get a precision/recall tradeoff that has very few false warnings, but still catches some errors. \n\nNice work creating an implementation of fast GGNNs with large diverse graphs.   Glad to see that the code will be released.   Great to see that the method is fast---it seems fast enough to use in practice in a real IDE. \n\nThe model (GGNN) is not particularly novel, but I'm not much bothered by that.    I'm very happy to see good application papers at ICLR.   I agree with your pair of sentences in the conclusion: \""Although source code is well understood and studied within other disciplines such as programming language research, it is a relatively new domain for deep learning.  It presents novel opportunities compared to textual or perceptual data, as its (local) semantics are well-defined and rich additional information can be extracted using well-known, efficient program analyses. \""  I'd like to see work in this area encouraged.   So I recommend acceptance.   If it had better (e.g. ROC curve) evaluation and some modeling novelty, I would rate it higher still. \n\nSmall notes:\nThe paper uses the term \""data flow structure\"" without defining it. \nYour data set consisted of C# code.   Perhaps future work will see if the results are much different in other languages.\n",1,1,1,1,1,-1,1,1,1,1
BJOFETxR--R2,"The paper introduces an application of Graph Neural Networks (Li's Gated Graph Neural Nets, GGNNs, specifically) for reasoning about programs and programming.  The core idea is to represent a program as a graph that a GGNN can take as input, and train the GGNN to make token-level predictions that depend on the semantic context.  The two experimental tasks were: 1) identifying variable (mis)use, ie. identifying bugs in programs where the wrong variable is used,  and 2) predicting a variable's name by consider its semantic context. \n\nThe paper is generally well written, easy to read and understand, and the results are compelling.  The proposed GGNN approach outperforms (bi-)LSTMs on both tasks.  Because the tasks are not widely explored in the literature, it could be difficult to know how crucial exploiting graphically structured information is, so the authors performed several ablation studies to analyze  this out.  Those results show that as structural information is removed, the GGNN's performance diminishes, as expected.  As a demonstration of the usefulness of their approach, the authors ran their model on an unnamed open-source project and claimed to find several bugs, at least one of which potentially reduced memory performance. \n\nOverall the work is important, original, well-executed, and should open new directions for deep learning in program analysis.  I recommend it be accepted.",1,1,1,1,-1,1,1,1,1,-1
BJOFETxR--R3,"This paper presents a novel application of machine learning using Graph NN's on ASTs to identify incorrect variable usage and predict variable names in context.  It is evaluated on a corpus of 29M SLOC, which is a substantial strength of the paper. \n\nThe paper is to be commended for the following aspects:\n1) Detailed description of GGNNs and their comparison to LSTMs \n2) The inclusion of ablation studies to strengthen the analysis of the proposed technique \n3) Validation on real-world software data \n4) The performance of the technique is reasonable enough to actually be used. \n\nIn reviewing the paper the following questions come to mind:\n1) Is the false positive rate too high to be practical?   How should this be tuned so developers would want to use the tool? \n2) How does the approach generalize to other languages? (Presumably well, but something to consider for future work.) \n\nDespite these questions, though, this paper is a nice addition to deep learning applications on software data and I believe it should be accepted.\n\n",1,1,1,1,1,-1,1,1,1,-1
BJRxfZbAW-R1,"The authors propose an extension to the Neural Statistician which can model contexts with multiple partially overlapping features.  This model can explain datasets by taking into account covariate structure needed to explain away factors of variation and it can also share this structure partially between datasets. \n\nA particularly interesting aspect of this model is the fact that it can learn these context c as features conditioned on meta-context a, which leads to a disentangled representation. \nThis is also not dissimilar to ideas used in 'Bayesian Representation Learning With Oracle Constraints' Karaletsos et al 2016 where similar contextual features c are learned to disentangle representations over observations and implicit supervision. \n\nThe authors provide a clean variational inference algorithm to learn their model.  However, a key problem is the following: the nature of the discrete variables being used makes them hard to be inferred with variational inference.  The authors mention categorical reparametrization as their trick of choice, but do not go into empirical details int heir experiments regarding the success of this approach.  In fact, it would be interesting to study which level of these variables could be analytically collapsed (such as done in the Semi-Supervised learning work by Kingma et al 2014) and which ones can be sampled effectively using a form of reparametrization. \n\nThis also touches on the main criticism of the paper: While the model technically makes sense and is cleanly described and derived,   the empirical evaluation is on the weak side and the rich properties of the model are not really shown off.  It would be interesting if the authors could consider adding a more illustrative experiment and some more empirical results regarding inference in this model and the marginal structures that can be learned with this model in controlled toy settings. \nCan the model recover richer structure that was imposed during data generation?  How limiting is the learning of a? \nHow does the likelihood of the model behave under the circumstances? \nThe experiments do not really convey how well this all will work in practice",1,1,1,1,1,1,1,1,1,-1
BJRxfZbAW-R2,"This paper introduces a conditional variant of the model defined in the Neural Statistician (https://arxiv.org/abs/1606.02185).  The generative model defines the process that produces the dataset.  This model is first a mixture over contexts followed by i.i.d. generation of the dataset with possibly some unobserved random variable.  This corresponds to a mixture of Neural Statisicians.  The authors suggest that such a model could help with disentangling factors of variation in data.  In the experiments they only consider training the model with the context selection variable and the data variables observed. \n\nUnfortunately there is minimal quantitative evaluation (visualizing 264 MNIST samples is not enough).  The only quantitative evaluation is in Table 1, and it seems the model is not able to generalize reliably to all rotations and all digits.  Clearly, we can't expect perfect performance, but there are some troubling results: 5.2 accuracy on non-rotated 0s, 0.0 accuracy on non-rotated 6s.  Every digit has at least one rotation that is not well classified, so this section could use more discussion and analysis.  For example, how would this metric classify VAE samples with contexts corresponding only to digit type (no rotations)? How would this metric classify vanilla VAE samples that are hand labeled?  Moreover, the context selection variable \""a\"" should be considered part of the dataset, and as such the paper should report how \""a\"" was selected. \n\nThis model is a relatively simple extension of the Neural Statistician, so the novelty of the idea is not enough to counterbalance the lack of quantitative evaluation.  I do think the idea is well-motivated, and represents a promising way to incorporate prior knowledge of concepts into our training of VAEs.  Still, the paper as it stands is not complete, and I encourage the authors to followup with more thorough quantitative empirical evaluations.\n",1,1,1,1,1,-1,1,1,1,-1
BJRxfZbAW-R3,"This paper proposes a model for learning to generate data conditional on attributes.  Demonstrations show that the model is capable of learning to generate data with attribute combinations that were not present in conjunction at training time. \n\nThe model is interesting, and the results, while preliminary, suggest that the model is capable of making quite interesting generalizations (in particular, it can synthesize images that consist of settings of features that have not been seen before). \n\nHowever, this paper is mercilessly difficult to read.  The most serious problems are the extensive discussion of the fully unsupervised variant (rather than the semisupervised variant that is evaluated), poor use of examples when describing the model, nonstandard terminology (\u201cconcepts\u201d and \u201ccontext\u201d are extremely vague terms that are not defined precisely) and discussions to vaguely related work that does not clarify but rather obscures what is going on in the paper. \n\nFor the evaluation, since this paper proposes a technique for learning a posterior recognition model, it would be extremely interesting to see if the model is capable of recognizing images appropriately that combine \u201ccontexts\u201d that were not observed during training.  The experiments show that the generation component is quite effective,  but this is an obvious missing step. \n\nAnyway, some other related work:\nLample et al. (2017 NIPS). Fader Networks. I realize this work is more ambitious since it seeks to be a fully generative model including of the contexts/attributes.  But I mostly bring it up because it is an impressively clear presentation of a model and experimental set up.",1,1,1,1,1,1,1,1,1,-1
BJRZzFlRb-R1,"This paper proposed a new method to compress the space complexity of word embedding vectors by introducing summation composition over a limited number of basis vectors, and representing each embedding as a list of the basis indices.  The proposed method can reduce more than 90% memory consumption while keeping original model accuracy in both the sentiment analysis task and the machine translation tasks. \n\nOverall, the paper is well-written.  The motivation is clear, the idea and approaches look suitable and the results clearly follow the motivation. \n\nI think it is better to clarify in the paper that the proposed method can reduce only the complexity of the input embedding layer.  For example, the model does not guarantee to be able to convert resulting \""indices\"" to actual words (i.e., there are multiple words that have completely same indices, such as rows 4 and 6 in Table 5), and also there is no trivial method to restore the original indices from the composite vector.  As a result, the model couldn't be used also as the proxy of the word prediction (softmax) layer, which is another but usually more critical bottleneck of the machine translation task. \nFor reader's comprehension, it would like to add results about whole memory consumption of each model as well. \nAlso, although this paper is focused on only the input embeddings, authors should refer some recent papers that tackle to reduce the complexity of the softmax layer.  There are also many studies, and citing similar approaches may help readers to comprehend overall region of these studies. \n\nFurthermore, I would like to see two additional analysis.  First, if we trained the proposed model with starting from \""zero\"" (e.g., randomly settling each index value), what results are obtained? Second, What kind of information is distributed in each trained basis vector? Are there any common/different things between bases trained by different tasks?",1,1,1,1,1,1,1,1,1,-1
BJRZzFlRb-R2,"This paper presents an interesting idea to word embeddings that it combines a few base vectors to generate new word embeddings.  It also adopts an interesting multicodebook approach for encoding than binary embeddings.  \n\nThe paper presents the proposed approach to a few NLP problems and have shown that this is able to significant reduce the size, increase compression ratio, and still achieved good accuracy. \n\nThe experiments are convincing and solid.  Overall I am weakly inclined to accept this paper.",1,1,-1,1,-1,-1,1,1,1,-1
BJRZzFlRb-R3,"The authors proposed to compress word embeddings by approximate matrix factorization, and to solve the problem with the Gumbel-soft trick.  The proposed method achieved compression rate 98% in a sentiment analysis task, and compression rate over 94% in machine translation tasks, without a performance loss.  \n\nThis paper is well-written and easy to follow.   The motivation is clear and the idea is simple and effective.\ n\nIt would be better to provide deeper analysis in Subsection 6.1.  The current analysis is too simple.  It may be interesting to explain the meanings of individual components.  Does each component is related to a certain topic?  Is it meaningful to perform ADD or SUBSTRACT on the leaned code?  \n\nIt may also be interesting to provide suitable theoretical analysis, e.g., relationships with the SVD of the embedding matrix.\n",1,1,1,1,1,-1,1,1,1,-1
BJubPWZRW-R1,"This paper presents a so-called cross-view training for semi-supervised deep models.  Experiments were conducted on various data sets and experimental results were reported.\n\nPros:\n* Studying semi-supervised learning techniques for deep models is of practical significance. \n\nCons:\n* The novelty of this paper is marginal.  The use of unlabeled data is in fact a self-training process.  Leveraging the sub-regions of the image to improve performance is not new and has been widely-studied in image classification and retrieval.  \n* The proposed approach suffers from a technical weakness or flaw.  For the self-labeled data, the prediction of each view is enforced to be same as the assigned self-labeling.  However, since each view related to a sub-region of the image (especially when the model is not so deep), it is less likely for this region to contain the representation of the concepts (e.g., some local region of an image with a horse may exhibit only grass); enforcing the prediction of this view to be the same self-labeled concepts (e.g,\u201chorse\u201d) may drive the prediction away from what it should be ( e..g, it will make the network to predict grass as horse).  Such a flaw may affect the final performance of the proposed approach. \n* The word \u201cview\u201d in this paper is misleading.  The \u201cview\u201d in this paper is corresponding to actually sub-regions in the images \n* The experimental results indicate that the proposed approach fails to perform better than the compared baselines in table 2, which reduces the practical significance of the proposed approach. \n",1,1,1,1,-1,1,1,1,1,-1
BJubPWZRW-R2,"The paper proposes a \u2019Cross View training\u2019 approach to semi-supervised learning.  In the teacher-student framework for semi-supervised learning, it introduces a new cross view consistency loss that includes auxiliary softmax layers (linear layers followed by softmax) on lower levels of the student model.  The auxiliary softmax layers take different views of the input for prediction. \n\nPros:\n1. A simple approach to encourage better representations learned from unlabeled examples.  \n\n2. Experiments are comprehensive. \n\nCons:\n\n0. The whole paper just presented strategies and empirical results.  There are no discussions of insights and why the proposed strategy work, for what cases it will work, and for what cases it will not work? Why?   \n\n1. The addition of auxiliary layers improves Sequence Tagging results marginally.  \n\n2. The claim of cross-view for sequence tagging setting is problematic.  Because the task is per-position tagging, those added signals are essentially not part of the examples, but the signals of its neighbors.  \n\n3. Adding n^2 linear layers for image classification essentially makes the model much larger.  It is unfair to compare to the baseline models with much fewer parameters.  \n\n4. The \""CVT, no noise\"" should be compared to \""CVT, random noise\"", then to \""CVT, adversarial noise\"". The current results show that the improvements are mostly from VAT, instead of CVT. \n\n\n",1,1,1,1,1,-1,1,1,1,-1
BJubPWZRW-R3,"This paper proposes a multi-view semi-supervised method.  For the unlabelled data, a single input (e.g., a picture) is partitioned into k new inputs permitting overlap.  Then a new objective is to obtain k predictions as close as possible to the prediction from the model learned from mere labeled data. \n\nTo be more precise, as seen from the last formula in section 3.1, the most important factor is the D function (or KL distance used here).  As the author said, we could set the noisy parameter in the first part to zero, but have to leave this parameter non-zero in the second term.  Otherwise, the model can't learn anything. \n\nMy understanding is that the key factor is not the so called k views (as in the first sight, this method resembles conventional ensemble learning very much), but the smoothing distribution around some input x (consistency related loss).  In another word, we set the k for unlabeled data as 1, but use unlabeled data k times in the scale (assuming no duplicate unlabeled data), keeping the same training (consistency objective) method, would this new method obtain a similar performance?  If my understanding is correct, the authors should further discuss the key novelty compared to the previous work stated in the second paragraph of section 1.  One obvious merit is that the unlabeled data is utilized more efficiently, k times better.\n\n\n",1,1,1,1,1,1,1,1,1,-1
BJuWrGW0Z-R1,"This paper considers the task of learning program embeddings with neural networks with the ultimate goal of bug detection program repair in the context of students learning to program.  Three NN architectures are explored, which leverage program semantics rather than pure syntax.   The approach is validated using programming assignments from an online course, and compared against syntax based approaches as a baseline. \n\nThe problem considered by the paper is interesting,  though it's not clear from the paper that the approach is a substantial improvement over previous work.  This is in part due to the fact that the paper is relatively short, and would benefit from more detail.   I noticed the following issues:\n\n1) The learning task is based on error patterns, but it's not clear to me what exactly that means from a software development standpoint. \n2) Terms used in the paper are not defined/explained.  For example, I assume GRU is gated recurrent unit, but this isn't stated. \n3) Treatment of related work is lacking.   For example, the Cai et al. paper from ICLR 2017 is not considered \n4) If I understand dependency reinforcement embedding correctly, a RNN is trained for every trace. If so, is this scalable? \n\nI believe the work is very promising,  but this manuscript should be improved prior to publication.",1,1,1,1,1,1,1,1,1,-1
BJuWrGW0Z-R2,"Summary of paper: The paper proposes an RNN-based neural network architecture for embedding programs, focusing on the semantics of the program rather than the syntax.  The application is to predict errors made by students on programming tasks.  This is achieved by creating training data based on program traces obtained by instrumenting the program by adding print statements.  The neural network is trained using this program traces with an objective for classifying the student error pattern (e.g. list indexing, branching conditions, looping bounds).\n\n---\n\nQuality: The experiments compare the three proposed neural network architectures with two syntax-based architectures.  It would be good to see a comparison with some techniques from Reed & De Freitas (2015) as this work also focuses on semantics-based embeddings. \nClarity: The paper is clearly written. \nOriginality: This work doesn't seem that original from an algorithmic point of view since Reed & De Freitas (2015) and Cai et. al (2017) among others have considered using execution traces.  However the application to program repair is novel (as far as I know). \nSignificance: This work can be very useful for an educational platform though a limitation is the need for adding instrumentation print statements by hand. \n\n---\n\nSome questions/comments:\n- Do we need to add the print statements for any new programs that the students submit?  What if the structure of the submitted program doesn't match the structure of the intended solution and hence adding print statements cannot be automated? \n\n---\n\nReferences \n\nCai, J., Shin, R., & Song, D. (2017).  Making Neural Programming Architectures Generalize via Recursion.  In International Conference on Learning Representations (ICLR).",1,1,1,1,1,1,1,1,1,-1
BJuWrGW0Z-R3,"The authors present 3 architectures for learning representations of programs from execution traces.  In the variable trace embedding, the input to the model is given by a sequence of variable values.  The state trace embedding combines embeddings for variable traces using a second recurrent encoder.  The dependency enforcement embedding performs element-wise multiplication of embeddings for parent variables to compute the input of the GRU to compute the new hidden state of a variable.  The authors evaluate their architectures on the task of predicting error patterns for programming assignments from Microsoft DEV204.1X (an introduction to C# offered on edx) and problems on the Microsoft CodeHunt platform.  They additionally use their embeddings to decrease the search time for the Sarfgen program repair system. \n\nThis is a fairly strong paper.  The proposed models make sense and the writing is for the most part clear, though there are a few places where ambiguity arises: \n\n- The variable \""Evidence\"" in equation (4) is never defined.  \n\n- The authors refer to \""predicting the error patterns\"", but again don't define what an error pattern is.  The appendix seems to suggest that the authors are simply performing multilabel classification based on a predefined set of classes of errors, is this correct?  \n\n- It is not immediately clear from Figures 3 and 4 that the architectures employed are in fact recurrent. \n\n- Figure 5 seems to suggest that dependencies are only enforced at points in a program where assignment is performed for a variable, is this correct?\ n\nAssuming that the authors can address these clarity issues, I would in principle be happy for the paper to appear.",1,1,1,1,1,-1,1,1,-1,-1
BJvWjcgAZ-R1,"This paper proposes a new variant of DQN where the DQN targets are computed on a full episode by a \u00ab backward \u00bb update (i.e. from end to start of episode).  The targets\u2019 update rule is similar to a regular tabular Q-learning update with high learning rate beta: this allows faster propagation of rewards obtained at the end of the episode (while beta=0 corresponds to regular DQN with no such reward propagation).  This mechanism is shown to improve on Q-learning in a toy 2D maze environment (with MNIST-based pixel states providing cell coordinates) with beta=1, and on DQN and its optimality tightening variant on Atari games with beta=0.5. \n\nThe intuition behind the algorithm (that one should try to speed up the propagation of rewards across multiple steps) is not new, in fact it has inspired other approaches like n-step Q-learning, eligibility traces or more recently Retrace(lambda) in deep RL.  Actually the idea of replaying experiences in backward order can be traced back to the origins of experience replay (\u00ab  Programming Robots Using Reinforcement Learning and Teaching \u00bb, Lin, 1991), something that is not mentioned here.  That being said, to the best of my knowledge the specific algorithm proposed in this submission (Alg. 2) is novel, even if Alg. 1 is not (Alg. 1 can be seen as a specific instance of Lin\u2019s algorithm with a very high learning rate, and clearly only makes sense in toy deterministic environments). \n\nIn the absence of any theoretical analysis of the proposed approach, I would have expected an in-depth empirical validation.  Unfortunately this is not the case here. In the toy environment (4.1) I am surprised by the really poor quality of the results (paths 5-10 times longer than the shortest path on average): have algorithms been run for a long enough time?  Or maybe the average is a bad performance measure due to outliers?  I would have also appreciated a comparison to Retrace(lambda), which is a more principled way to use multi-step rewards than n-step Q-learning (which is technically an on-policy method).  Similar remarks can be made on the Atari experiments (4.2), where 10M frames is really low (the original DQN paper had results on 50M frames, and Rainbow reports 200M frames in only ~2x the training time reported here).  The comparison also should have included prioritized experience replay, which has been shown to provide a significant boost in DQN, but may be tricky to combine with the proposed algorithm.  Overall comparing only to vanilla DQN and its optimality tightening variant is too limited when there have been so many other meaningful improvements over DQN. This makes it really hard to tell whether the proposed algorithm would actually help when combined with a state-of-the-art method like Rainbow for instance. \n\nA few additional small remarks and questions:\n- \u00ab Second, there is no point in updating a one-step transition unless the future transitions have not been updated yet.  \u00bb: should \u00ab unless \u00bb be replaced by \u00ab if \u00bb? \n- In 4.1 is there a maximum number of steps per episode and can you please confirm that training is done independently for each maze? \n- Typo in eq. 3: the - in the max should be a comma \n- There is a good amount of typos and grammar errors,  though they do not harm the readability of the paper \n- Citations for \u00ab Deep Reinforcement Learning with Double Q-learning \u00bb and \u00ab Dueling Network Architectures for Deep Reinforcement Learning \u00bb could refer to their conference versions\n- \u00ab epsilon starts from 1 and is annealed to 0 at 200,000 steps in a quadratic manner \u00bb: please specify the exact formula \n- Fig. 7 is really confusing, there seem to be typos and it is not clear why the beta updates appear in these specific cells, please revise it if you want to keep it",1,1,1,1,1,1,1,1,1,1
BJvWjcgAZ-R2,"The authors propose a simple modification to the DQN algorithm they call Episodic Backward Update.  The algorithm selects transitions in a backward order fashion from end of episode to be more effective in propagating learning of new rewards.   This issue of fast propagation of updates is a common theme in RL (cf eligibility traces, prioritised sweeping, and more recently DQN with prioritised replay etc.).   Here the proposed update applies the max Bellman operator recursively on a trajectory (unsure whether this is novel), with some decay to prevent accumulating errors with the nested max.  \n\nThe paper is written in a clear way.    The proposed approach seems reasonable, but I would have guessed that prioritized replay would also naturally sample transitions in roughly that order - given that TD-errors would at first be higher towards the end of an episode and progress backwards from there.   I think this should have been one of the baselines to compare to for that reason.  \n\nThe experimental results seem promising in the illustrative MNIST domain.   Atari results seem decent, especially given that experiments are limited to 10M frames, though the advantage compared to the related approach of optimality tightening is not obvious. \n",1,1,1,1,1,1,1,1,1,-1
BJvWjcgAZ-R3,"This paper proposes a new way of sampling data for updates in deep-Q networks.  The basic principle is to update Q values starting from the end of the episode in order to facility quick propagation of rewards back along the episode. \n\nThe paper is interesting, but it lacks the proper comparisons to previously published techniques. \n\nThe results presented by this paper shows improvement over the baseline.  But the Atari results is still significantly worse than the current SOTA.  Some (theoretical) analysis would be nice.  It is hard to judge whether the objective defined in the non-tabular defines a contraction operator at all in the tabular case. \n\nThere has been a number of highly relevant papers.  Prioritized replay, for example, could have a very similar effect to proposed approach in the tabular case. \n\nIn the non-tabular case, the Retrace algorithm, tree backup, Watkin's Q learning all bear significant resemblance to the proposed method.  Although the proposed algorithm is different from all 3, the authors should still have compared to at least one of them as a baseline.  The Retrace algorithm specifically has also been shown to help significantly in the Atari case, and it defines a convergent update rule.",1,1,1,1,1,1,1,1,1,-1
BJy0fcgRZ-R1,Quality\n\nThis paper demonstrates that human category representations can be inferred by sampling deep feature spaces.  The idea is an extension of the earlier developed MCMC with people approach where samples are drawn in the latent space of a DCGAN and a BiGAN.  The approach is thoroughly validated using two online behavioural experiments. \n\nClarity\n\nThe rationale is clear and the results are straightforward to interpret.  In Section 4.2.1 statements on resemblance and closeness to mean faces could be tested.  Last sentences on page 7 are hard to parse.  The final sentence probably relates back to the CI approach.  A few typos.\n\nOriginality\n\nThe approach is a straightforward extension of the MCMCP approach using generative models. \n\nSignificance \n\nThe approach improves on previous category estimation approaches by embracing the expressiveness of recent generative models.  Extensive experiments demonstrate the usefulness of the approach. \n\nPros\n\nUseful extension of an important technique backed up by behavioural experiments. \n\nCons\n\nDoes not provide new theory but combines existing ideas in a new manner.,1,1,1,1,1,-1,1,1,1,-1
BJy0fcgRZ-R2,"This paper presents a method based on GANs for visualizing how humans represent visual categories.  Authors perform experiments on two datasets: Asian Faces Dataset and ImageNet Large Scale Recognition Challenge dataset. \n\nPositive aspects:\n+ The idea of using GANs for this goal is smart and interesting \n+ The results seem interesting too \n\nWeaknesses:\n- Some aspects of the paper are not clear and presentation needs improvement. \n- I miss a clearer results comparison with previous methods, like Vondrick et al. 2015. \n\nSpecific comments and questions:\n\n-  Figure 1 is not clear.  Authors should clarify how they use the inference network and what the two arrows from this inference network represent. \n- Figure 2 is also not clear.  Just the FLD projections of the MCMCP chains are difficult to interpret.  The legend of the figure is too tiny.  The right part of the figure should be better described in the text or in the caption, I don't understand well what this illustrates. \n- Regarding to the human experiments with AMT: how do the authors deal with noise on the workers performance?  Is any qualification task used?  What are the instructions given to the workers? \n- In section 4.2. the authors state \""We also simultaneously learn a corresponding inference network, .... granular human biases captured\"".  This seems interesting  but I didn't find any result on that in the paper.  Can you give more details or refer to where in the paper it is discussed/tested? \n- Figure 4 shows \""most interpretable mixture components\"".   How this \""most interpretable\"" were selected? \n- In second paragraph Section 4.3, it should be Table 1 instead of Figure 1.  \n- It would be interesting to see a discussion on why MCMCP Density is better for group 1 and MCMCP Mean is better for group 2.  To see the confusion matrixes could be useful. \n\nI like this paper.  The addressed problem is challenging and the proposed idea seems interesting.   However, the aspects mentioned make me think the paper needs some improvements to be published.\n",1,1,1,1,1,1,1,1,1,-1
BJy0fcgRZ-R3,"The idea of using MCMCP with GANs is well-motivated and well-presented \nin the paper, and the approach is new as far as I know.   Figures 3 and 5 are\nconvincing evidence that MCMCP compares favorably to direct sampling of\nthe GAN feature space using the classification images approach. \n\nHowever, as discussed in the introduction, the reason an efficient\nsampling method might be interesting would be to provide insight\non the components of perception.   On these insights, the paper felt\nincomplete. \n\nFor example, it was not investigated whether the method identifies\nclassification features that generalize.   The faces experiment is\nsimilar to previous work done by Martin (2011) and Kontsevich\n(2004) but unlike that previous work does not investgiate whether\nclassification features have been identified that can be added to an\narbitrary image to change the attribute \""happy vs sad\"" or \""male vs female\"". \n\nSimilarly, the second experiment in Table 1 compares classification\naccuracy between different sampling methods, but it does not provide\nany comparison as done in Vondrick (2015) to a classifier trained\nin a conventional way (such as an SVM), so it is difficult to discern\nwhether the learned distributions are informative. \n\nFinally, the effect of choosing GAN features vs a more \""naive\"" feature\nspace is not explored in detail.   For example, the GAN is trained\non an image data set with many birds and cars but not many\nfire hydrants.   Is the method giving us a picture of this data set?",1,1,1,1,1,1,1,1,1,-1
BJyy3a0Ez-R1,"Pros:\n* asynchronous model-parallel training of deep learning models would potentially help in further scaling of deep learning models \n* the paper is clearly written and easy to understand \n\nCons:\n* weak experiments: performance of algorithms are not analyzed in terms of wall-clock, and important baselines are not compared against, making it difficult to judge the practical usefulness of the proposed algorithm \n* weak theory: although the algorithm is claimed to be motivated by continuous-time formulation of gradient descent, neither convergence proof nor algorithm design really use the continuous-time formulation and discrete-time formulation seems to suffice; the proof is straightforward corollary of Lin et al. \n\nSummary: This paper proposes to update parameters of each layer in deep neural network asynchronously, instead of updating all layers simultaneously and synchronously.  Authors derive this algorithm by first formulating gradient descent in continuous-time form and then modifying time dependencies between layers.  While asynchronous updates of parameters in stochastic gradient descent has been explored (dating back to [1] in 1986, and authors should also be referring to [2]), to my knowledge application of these ideas to layer-by-layer model parallelism for deep neural networks has not been studied.  Since model-parallel training across machines has not been very successful, and model-parallelism has been only exploited within machines, asynchronous model-parallel optimization is an important topic of research which has the promise of scaling deep learning models beyond the memory capacity of a single machine. \n\nUnfortunately, the practical usefulness of the algorithm has not been demonstrated.  It remains unanswered whether this algorithm can be implemented efficiently in modern hardware architectures, or in which situations this algorithm will be more useful than existing algorithms.  Experiments are all reported in terms of the number of updates (epochs), but this is not useful in judging the practical advantage of the proposed algorithm.  What matters in practice is how fast the algorithm is in improving the performance as a function of _wall-clock time_, and I would expect that synchronous algorithms would be much faster than the proposed algorithm in terms of wall-clock time, as they can better exploit optimized tensor arithmetic on CPUs and GPUs.  Also, authors should compare against mini-batch gradient descent, because this is the most popular way of training deep neural networks; authors has the burden of proof that the proposed algorithm is practically more useful than the existing standard method. \n\nAuthors argue their algorithm is motivated by continuous-time formulation of stochastic gradient descent, but it is unclear to me whether the continuous-time formulation was really necessary to derive the proposed algorithm.  The algorithm operates in discrete time horizon, and continuous time is not used anywhere.  Authors rely mostly on Lin et al for the convergence proof, which is also based on discrete time horizon. \n\nAuthors argue in page 1 that Continuous Propagation is statistically superior to mini-batch gradient descent, but I cannot find statistical superiority of the method.  Also, the upper bound of the time-delay T slows down the convergence rate (Proposition in the appendix), so it is unclear whether asynchronous update is theoretically faster than synchronous mini-batch gradient descent.  I think which algorithm is faster depends on values of L, T and M. \n\nAuthors do not provide enough citations.  Continuous-time characterization of gradient descent has a long history, and authors should provide citation of it, for example when (5) is introduced.  Authors should provide more discussion of the history of model-parallel asynchronous SGD (such as [1] and [2]), and when mentioning alternatives like Czarnecki et al (2017), authors should discuss what advantages and disadvantages the proposed algorithm has against these alternatives. \n\n\n[1] Distributed Asynchronous Deterministic and Stochastic Gradient Optimization Algorithms (Tsitsiklis, Bertsekas and Athans, 1986)\n[2] Hogwild!: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent (Niu et al, 2011)\n",1,1,1,1,1,1,1,1,1,1
BJyy3a0Ez-R2,"The authors propose a decoupled backpropagation method, called continuous propagation, through the interpretation of backpropagation as a continuous differential system.  Because the layer-wise decoupling, it can easily be applied for distributed training of the model.  The authors provide a convergence proof on the proposed algorithm and also provides some empirical experiment results. \n\nAlthough I found the proposed method is interesting enough to investigate more thoroughly,  it is a shame to see the overall quality of the paper very weak.  The writing requires a significant improvement: in addition to the overall unclarity of the exposition, it sometimes use unexplained abbreviation (e.g., CPGD, CP).  The experiments are also very weak.  Important information on the experiment settings are missing, e.g., how the model is parallelized. \n\n- Mini-batch gradient descent (MBGD) is unfamiliar concept compared to SGD.  It needs to be better defined.",1,1,1,1,1,-1,1,1,1,-1
BJyy3a0Ez-R3,"# Summary of paper\nThe authors propose a parallel algorithm for training deep neural networks. Unlike other parallel variants of SGD, this parallelizes across the layers and not across samples. \n\n# Summary of review\nThe idea of model-parallelism (as opposed to data parallelism) is appealing and an important open problem.  However, this contribution is far from correctly addressing the problem.  The Algorithm is poorly described and crucial parts of the algorithm are very confusing.  Mathematical rigor in the proof and discussion is lacking. Proof has mathematical errors.  \n\n# Detailed comments\n* Key definitions are scattered across the paper, making it very difficult to understand and forcing the reader to continuously go back and forth looking for the definition of  a variable.  To make things worse, some variables are simply not defined.  For example, I can't find the definition of D.  From the context it seems to be the number of layers in the network (I shouldn't need to guess).  \n\n* From Algorithm 1, the bracket notation is used for both indexing and specifying the size of the variables?  This is nonstandard and confusing. \n\n* Again, from Algorithm 1, it is not clear which parts can be performed asynchronously.  It is even not clear to me if the algorithm can be run asynchronously (as some of the other reviewers seem to imply) or if its a synchronous algorithm but analyzed asynchronously to accomodate for delay in the information coming from their \""continuous-propagation\"" factorization?   \n\n* Eq. (3) and (4): I doubt this is true without some assumptions on the distribution of the data generating process.   \n\n* The proof, despite being a trivial application of existing work, has obvious flaws.   After equation (17) it is stated that \""the left-hand side is independent of x_{k, m, l}\"" which is not true since Theta_{k+1} is computed **precisely** using x_{k, m, l} and so is not independent (this is actually done correctly in Lian 2015, where the expectation is correctly carried on that term).   \n\n* The proof relies on an inequality (16) in which key quantities are not defined (what is L? is L = L_d?) and which is impossible to verify in practice (T is not known).   This crucial detail is only mentioned in the appendix, giving the impression in the main text that the algorithm is always convergent.   It should clearly be stated in the main text that convergence depends on a step-size that needs to be defined from unknown quantities. \n\n* As mentioned in the other reviews, key references are lacking, e.g., for ODE interpretation, Eq. (3) and (4). \n\nIn appendix:\n\n * Assumption 3, 4: Why is upper superindex d?  In any case, be consistent, most of the time these are used but then its stated \""for all Theta\"" (whithout superindex)\n * Proposition: what is L? is L = L_d? \n\n\nOther\n\n  * Assumption 5: decay -> delay?\n\n",1,1,1,1,-1,1,1,1,-1,-1
BJ_wN01C--R1,"In this paper, the authors present an approach to implement deep learning directly on sparsely connected graphs.  Previous approaches have focused on transferring trained deep networks to a sparse graph for fast or efficient utilization; using this approach, sparse networks can be trained efficiently online, allowing for fast and flexible learning.  Further investigation is necessary to understand the full implications of the two main conceptual changes introduced here (signed connections that can disappear and random walk in parameter space),  but the initial results are quite promising.\n\n It would also be interesting to understand more fully how performance scales to larger networks.  If the target connectivity could be pushed to a very sparse limit, where only a fixed number of connections were added with each additional neuron, then this could significantly shape how these networks are trained at very large scales.  Perhaps the heuristics for initializing the connectivity matrices will be insufficient, but could these be improved in further work?\n\n As a last minor comment, the authors should specify explicitly what the shaded areas are in Fig. 4b,c.",1,1,1,1,1,-1,1,1,1,-1
BJ_wN01C--R2,"The authors provide a novel, interesting, and simple algorithm capable of training with limited memory.   The algorithm is well-motivated and clearly explained, and empirical evidence suggests that the algorithm works well.   However, the paper needs additional examination in how the algorithm can deal with larger data inputs and outputs.   Second, the relationship to existing work needs to be explained better.\n\n Pro:\nThe algorithm is clearly explained, well-motivated, and empirically supported.\n\n Con:\nThe relationship to stochastic gradient markov chain monte carlo needs to be explained better.   In particular, the update form was first introduced in [1], the annealing scheme was analyzed in [2], and the reflection step was introduced in [3].  These relationships need to be explained clearly. \nThe evidence is presented on very small input data.   With something like natural images, the parameterization is much larger and with more data, the number of total parameters is much larger.   Is there any evidence that the proposed algorithm could continue performing comparatively as the total number of parameters in state-of-the-art networks increases?  This would require a smaller ratio of included parameters. \n\n[1] Welling, M. and Teh, Y.W., 2011. Bayesian learning via stochastic gradient Langevin dynamics.  In Proceedings of the 28th International Conference on Machine Learning (ICML-11)(pp. 681-688). \n\n[2] Chen, C., Carlson, D., Gan, Z., Li, C. and Carin, L., 2016, May.  Bridging the gap between stochastic gradient MCMC and stochastic optimization.  In Artificial Intelligence and Statistics(pp. 1051-1060).\n\n  [3] Patterson, S. and Teh, Y.W., 2013.  Stochastic gradient Riemannian Langevin dynamics on the probability simplex.  In Advances in Neural Information Processing Systems (pp. 3102-3110).\n \n""",1,1,1,1,1,1,1,1,1,-1
BJ_wN01C--R3,"This paper presents an iterative approach to sparsify a network already during training.  During the training process, the amount of connections in the network is guaranteed to stay under a specific threshold.  This is a big advantage when training is performed on hardware with computational limitations, in comparison to \""post-hoc\"" sparsification methods, that compress the network after training. \nThe method is derived by considering the \""rewiring\"" of an (artificial) neural network as a stochastic process.  This perspective is based on a recent model in computational biology but also can be interpreted as a (sequential) monte carlo sampling based stochastic gradient descent approach.  References to previous work in this area are missing, e.g.\n\n[1] de Freitas et al., Sequential Monte Carlo Methods to Train Neural Network\nModels, Neural Computation 2000\n[2] Welling et al., Bayesian Learning via Stochastic Gradient Langevin Dynamics, ICML 2011\n\n Especially the stochastic gradient method in [2] is strongly related to the existing approach. \n\nPositive aspects\n\n- The presented approach is well grounded in the theory of stochastic processes.  The authors provide proofs of convergence by showing that the iterative updates converge to a fixpoint of the stochastic process\n\n-  By keeping the temperature parameter of the stochastic process high, it can be directly applied to online transfer learning.\n\n - The method is specifically designed for online learning with limited hardware ressources.\n\n Negative aspects\n\n- The presented approach is outperformed for moderate compression levels (by Han's pruning method for >5% connectivity on MNIST, Fig. 3 A, and by l1-shrinkage for >40% connectivity on CIFAR-10 and TIMIT, Fig. 3 B&C).  Especially the results on MNIST suggest that this method is most advantageous for very high compression levels.  However in these cases the overall classification accuracy has already dropped significantly which could limit the practical applicability.\n\n - A detailled discussion of the relation to previously existing very similar work is missing (see above) \n\n\nTechnical Remarks\n\nFig. 1, 2 and 3 are referenced on the pages following the page containing the figure.  Readibility could be slightly increased by putting the figures on the respective pages.\n",1,1,1,1,1,1,1,1,1,1
Bk-ofQZRb-R1,"Summary: This paper tackles the issue of combining TD learning methods with function approximation.  The proposed algorithm constrains the gradient update to deal with the fact that canonical TD with function approximation ignores the impact of changing the weights on the target of the TD learning rule.  Results with linear and non-linear function approximation highlight the attributes of the method. \n\nQuality: The quality of the writing, notation, motivation, and results analysis is low . I will give a few examples to highlight the point.  The paper motivates that TD is divergent with function approximation, and then goes on to discuss MSPBE methods that have strong convergence results, without addressing why a new approach is needed.  There are many missing references: ETD, HTD, mirror-prox methods, retrace, ABQ. Q-sigma.  This is a very active area of research and the paper needs to justify their approach.  The paper has straightforward technical errors and naive statements: e.g. the equation for the loss of TD takes the norm of a scalar.  The paper claims that it is not well-known that TD with function approximation ignores part of the gradient of the MSVE. There are many others. \n\nThe experiments have serious issues.  Exp1 seems to indicate that the new method does not converge to the correct solution.  The grid world experiment is not conclusive as important details like the number of episodes and how parameters were chosen was not discussed.  Again exp3 provides little information about the experimental setup. \n\nClarity: The clarity of the text is fine, though errors make things difficult sometimes.  For example The Bhatnagar 2009 reference should be Maei. \n \nOriginality: As mentioned above this is a very active research area, and the paper makes little effort to explain why the multitude of existing algorithms are not suitable.  \n\nSignificance: Because of all the things outlined above, the significance is below the bar for this round",1,1,1,1,-1,1,1,1,1,-1
Bk-ofQZRb-R2,"This paper proposes adding a constraint to the temporal difference update to minimize the effect of the update on the next state\u2019s value.  The constraint is added by projecting the original gradient to the orthogonal of the maximal direction of change of the next state\u2019s value.  It is shown empirically that the constrained update does not diverge on Baird\u2019s counter example and improves performance in a grid world domain and cart pole over DQN. \n\nThis paper is reasonably readable.  The derivation for the constraint is easy to understand and seems to be an interesting line of inquiry that might show potential. \n\nThe key issue is that the justification for the constrained gradients is lacking.  What is the effect, in terms of convergence, in modifying the gradient in this way?  It seems highly problematic to simply remove a whole part of the gradient, to reduce effect on the next state.  For example, if we are minimizing the changes our update will make to the value of the next state, what would happen if the next state is equivalent to the current state (or equivalent in our feature space)?  In general, when we project our update to be orthogonal to the maximal change of the next states value, how do we know it is a valid direction in which to update?  \n\nI would have liked some analysis of the convergence results for TD learning with this constraint, or some better intuition in how this effects learning.  At the very least a mention of how the convergence proof would follow other common proofs in RL.  This is particularly important, since GTD provides convergent TD updates under nonlinear function approximation; the role for a heuristic constrained TD algorithm given convergent alternatives is not clear.  \n \nFor the experiments, other baselines should be included, particularly just regular Q-learning.  The primary motivation comes from the use of a separate target network in DQN, which seems to be needed in Atari (though I am not aware of any clear result that demonstrates why, rather just from informal discussions).  Since you are not running experiments on Atari here, it is invalid to simply assume that such a second network is needed.  A baseline of regular Q-learning should be included for these simpler domains.  \n\nThe results in Baird\u2019s counter example are discouraging for the new constraints.  Because we already have algorithms which better solve this domain, why is your method advantageous?  The point of showing your algorithm not solve Baird\u2019s counter example is unclear. \n\nThere are also quite a few correctness errors in the paper, and the polish of the plots and language needs work, as outlined below.  \n\nThere are several mistakes in the notation and background section.  \n1. \u201cIf we consider TD-learning using function approximation, the loss that is minimized is the squared TD error. \u201c This is not true; rather, TD minimizes the mean-squared project Bellman error.  Further, L_TD is strangely defined: why a squared norm, for a scalar value?  \n2. The definition of v and delta_TD w.r.t. to v seems unnecessary, since you only use Q.  As an additional (somewhat unimportant) point, the TD-error is usually defined as the negative of what you have.  \n3. In the function approximation case the value function and q functions parameterized by \\theta are only approximations of the expected return. \n4. Defining the loss w.r.t. the state, and taking the derivative of the state w.r.t. to theta is a bit odd.  Likely what you meant is the q function, at state s_t?  Also, are ignoring the gradient of the value at the next step?  If so, this further means that this is not a true gradient.   \n\nThere is a lot of white space around the plots, which could be used for larger more clear figures.  The lack of labels on the plots makes them hard to understand at a glance, and the overlapping lines make finding certain algorithm\u2019s performance much more difficult.  I would recommend combining the plots into one figure with a drawing program so you have more control over the size and position of the plots. \n\nExamples of odd language choices:\n\t-\t\u201cThe idea also does not immediately scale to nonlinear function approximation.  Bhatnagar et al. (2009) propose a solution by projecting the error on the tangent plane to the function at the point at which it is evaluated.  \u201c - The paper you give exactly solves for the nonlinear function approximation case.  What do you mean does not scale to nonlinear function approximation?  Also Maei is the first author on this paper. \n\t-\t\u201cThough they do not point out this insight as we have\u201d - This seems to be a bit overreaching. \n- \u201cthe gradient at s_{t+1} that will change the value the most\u201d  - This is too colloquial.  I think you simply mean the gradient of the value function, for the given s_t, but its not clear.",1,1,1,1,1,1,1,1,1,-1
Bk-ofQZRb-R3,"This is an interesting idea, and written clearly.  The experiments with Baird's and CartPole were both convincing as preliminary evidence that this could be effective.  However, it is very hard to generalize from these toy problems.  First, we really need a more thorough analysis of what this does to the learning dynamics itself.  Baring theoretical results, you could analyze the changes to the value function at the current and next state with and without the constraint to illustrate the effects more directly.  I think ideally, I would want to see this on Atari or some of the continuous control domains often used.  If this allows the removing of the target network for instance, in those more difficult tasks, then this would be a huge deal. \n\nAdditionally, I do not think the current gridworld task adds anything to the experiments, I would rather actually see this on a more interesting linear function approximation on some other simple task like Mountain Car than a neural network on gridworld.  The reason this might be interesting is that when the parameter space is lower dimensional (not an issue for neural nets, but could be problematic for linear FA) the constraint might be too much leading to significantly poorer performance.  I suspect this is the actual cause for it not converging to zero for Baird's, although please correct me if I'm wrong on that. \n\nAs is, I cannot recommend acceptance given the current experiments and lack of theoretical results.  But I do think this is a very interesting direction and hope to see more thorough experiments or analysis to support it. \n\nPros:\nSimple, interesting idea\nWorks well on toy problems, and able to prevent divergence in Baird's counter-example \n\nCons:\nLacking in theoretical analysis or significant experimental results",1,1,1,1,1,-1,1,1,1,-1
Bk346Ok0W-R1,"This paper proposes sensor transformation attention network (STAN), which dynamically select appropriate sequential sensor inputs based on an attention mechanism.  \n\nPros:\nOne of the main focuses of this paper is to apply this method to a real task, multichannel speech recognition based on CHiME-3, by providing its reasonable sensor selection function in real data especially to avoid audio data corruptions.  This analysis is quite intuitive, and also shows the effectiveness of the proposed method in this practical setup.  \n\nCons:\nThe idea seems to be simple and does not have significant originality.  Also, the paper does not clearly mention the attention mechanism part, and needs some improvement.  \n\nComments:\n-\tThe paper mainly focuses on the soft sensor selection.  However, in an array signal processing context (and its application to multichannel speech recognition), it would be better to mention beamforming techniques, where the compensation of the delays of sensors is quite important. \n-\tIn addition, there is a related study of using multichannel speech recognition based on sequence-to-sequence modeling and attention mechanism by Ochiai et al, \""A Unified Architecture for Multichannel End-to-End Speech Recognition with Neural Beamforming,\"" IEEE Journal of Selected Topics in Signal Processing.  This paper uses the same CHiME-3 database, and also showing a similar analysis of channel selection.  It\u2019s better to discuss about this paper as well as a reference. \n-\tSection 2: better to explain about how to obtain attention scores z in more details. \n-\tFigure 3, experiments of Double audio/video clean conditions: I cannot understand why they are improved from single audio/video clean conditions.Need some explanations.  \n-\tSection 3.1: 39-dimensional Mel-frequency cepstral coefficients (MFCCs) -> 13 -dimensional Mel-frequency cepstral coefficients (MFCCs) with 1st and 2nd order delta features. \n-\tSection 3.2 Dataset \u201cAs for TIDIGIT\u201d: \u201cAs for GRID\u201d(?) \n-\tSection 4 Models \u201cThe parameters of the attention modules are either shared across sensors (STAN-shared) or not shared across sensors (STAN- default). \u201d: It\u2019s better to explain this part in more details, possibly with some equations. It is hard to understand the difference.\n\n",1,1,1,1,1,1,1,1,1,-1
Bk346Ok0W-R2,"The manuscript introduces the sensor transformation attention networks, a generic neural architecture able to learn the attention that must be payed to different input channels (sensors) depending on the relative quality of each sensor with respect to the others.  Speech recognition experiments on synthetic noise on audio and video, as well as real data are shown. \n\nFirst of all, I was surprised on the short length of the discussion on the state-of-the-art.  Attention models are well known and methods to merge information from multiple sensors also (very easily, Multiple Kernel Learning, but many others). \n\nSecond, from a purely methodological point of view, STANs boil down to learn the optimal linear combination of the input sensors.  There is nothing wrong about this, but perhaps other more complex (non-linear) models to combine data could lead to more robust learning. \n\nThird, the experiments with synthetic noise are significant to a reduced extend.  Indeed, adding Gaussian noise to a replicated input is too artificial to be meaningful.  The network is basically learning to discard the sensor when the local standard deviation is high.  But this is not the kind of noise found in many applications, and this is clearly shown in the performances on real data (not always improving w.r.t state of the art).  The interesting part of these experiments is that the noise is not stationary, and this is quite characteristic of real-world applications.  Also, to be fair when discussion the results, the authors should say that simple concatenation outperforms the single sensor paradigm. \n\nI am also surprised about the baseline choice.  The authors propose a way to merge/discard sensors, and there is no comparison with other ways of doing it (apart from the trivial sensor concatenation).  It is difficult to understand the benefit of this technique if no other baseline is benchmarked.  This mitigates the impact of the manuscript. \n\nI am not sure that the discussion in page corresponds to the actual number on Table 3, I did not understand what the authors wrote.",1,1,1,1,1,1,1,1,1,-1
Bk346Ok0W-R3,"Summary: \n\nThe authors consider the use of attention for sensor, or channel, selection.  The idea is tested on several speech recognition datasets, including TIDIGITS and CHiME3, where the attention is over audio channels, and GRID, where the attention is over video channels.  Results on TIDIGITS and GRID show a clear benefit of attention (called STAN here) over concatenation of features.  The results on CHiME3 show gain over the CHiME3 baseline in channel-corrupted data. \n\nReview:\n\nThe paper reads well,  but as a standard application of attention lacks novelty.  The authors mention that related work is generalized but fail to differentiate their work relative to even the cited references (Kim & Lane, 2016; Hori et al., 2017).  Furthermore, while their approach is sold as a general sensor fusion technique,  most of their experimentation is on microphone arrays with attention directly over magnitude-based input features, which cannot utilize the most important feature for signal separation using microphone arrays---signal phase.  Their results on CHiME3 are terrible: the baseline CHiME3 system is very weak, and their system is only slightly better!  The winning system has a WER of only 5.8%(vs. 33.4% for the baseline system), while more than half of the submissions to the challenge were able to cut the WER of the baseline system in half or better!  http://spandh.dcs.shef.ac.uk/chime_challenge/chime2015/results.html. Their results wrt channel corruption on CHiME3, on the other hand, are reasonable, because the model matches the problem being addressed\u2026\n\nOverall Assessment: \n\n In summary, the paper lacks novelty wrt technique, and as an \u201capplication-of-attention\u201d paper fails to be even close to competitive with the state-of-the-art approaches on the problems being addressed.  As such, I recommend that the paper be rejected. \n\n\nAdditional comments: \n\n-\tThe experiments in general lack sufficient detail: Were the attention masks trained supervised or unsupervised?  Were the baselines with concatenated features optimized independently?  Why is there no multi-channel baseline for the GRID results?  \n-\tIssue with noise bursts plot (Input 1+2 attention does not sum to 1) \n-\tA concatenation based model can handle a variable #inputs: it just needs to be trained/normalized properly during test (i.e. like dropout)\u2026\n",1,1,1,1,-1,1,1,1,1,-1
Bk9nkMa4G-R1,"The main goal of this paper is to learn a ConvNet classifier which performs better for classes in the tail of the class occurrence distribution, ie for classes with relatively few annotated examples.  In order to do so, they constrain the final softmax layer, using weights and biases based on the class means, in a nearest-class-mean style layer.  In practice the class means are \""learned\"", yet regularised towards the batch class means. \n\nMy main concern with the paper is in the theoretical underpinning of the work.  From the title a Bayesian approach is suggested, while in practice a rather standard softmax classifier is learned, albeit with a different regulariser (last layer is regularised towards batch class means).  Also the Gaussian Mixture Model, is not a true mixture model, in the sense that normally GMMs are used for describing a distribution of unlabelled data, in this case, each class is described with a \""Gaussian\"", and thus the class probabilities are the reseponsibilities proportional to the class Gaussian.  To take this one further, it is assumed that there is equal class probabilities and each class has a the same Identity matrix as covariance matrix.  Taking away a large part of the Gaussian distribution.  The relation (Eq 6) with Softmax is insightful, yet already discussed in eg Mensink et al 2013 (already cited for the Nearest Class Mean classifier).  \n\nA second concern is the experimental exploration.  First of all, it is unclear if the method works much better for the tail than the standard softmax.  That is not apparent from the results.  For example, Fig 4 shows -except for CIFAR10- not a clear relation between class index and proposed relative improvement, it is also unclear if there is just a difficult class (eg at index 150), or that the experiment has been repeated several times.  Moreover, when the performance becomes more stable for the classes in the tail, I'd have expected that the standard deviation of the mean class accuracy would decrease, from the results there is no difference between Softmax and the proposed method: 44 +/-1 for Softmax (miniImageNet) to 41 +/-1 for the proposed NCM approach.  In the final experiment is the regularised version  compared to an unregularised one, which shows that the first performs better.  However, I'm a little unsure about these conclusions, what is the unregularized version exactly doing, how is it different from a standard softmax? \n\nRemaining (minor) remarks:\n- It is unclear how iCaRL has been used - it has been proposed as an iterative classification method. \n- Eq 2: how would this perform on a learned Softmax representation? Preferably including the (co)variance and class priors? \n- Figure 4: Gain -> Relative performance\n- The batch size must have a great influence on the functioning of the regularisation (especially when there are many classes, in that case just a single example counts for the class mean). This is not explored in the paper.",1,1,1,1,1,1,1,1,1,-1
Bk9nkMa4G-R2,"The paper aims to address a common issue in many classification applications: that the number of training data from different classes is much unbalanced.  The paper proposes a Bayesian framework to address it with a Gaussian mixture model. \n\nOverall the math looks reasonable.  I am not sure about the novelty of the paper, as it is a relatively standard definition of Bayesian math.  Essentially, instead of computing a softmax prediction which is the discrimination probability of each class given the input, one uses a logistic regression type interpretation (equation 1).  This has been used in multi-class classification before.  For example, many early SVM papers deal with multi-class classification by training 1-vs-all classifiers on each class and then choose the one having the highest score (possibly with a class-prior adjustment).\ n\nNote that this actually changes the underlying assumption a bit: softmax basically assumes the classes are mutually exclusive, while this interpretation implicitly assumes that the classes are not related to each other - an image could belong to multiple classes.  This probably does not match the assumption of many of the datasets being tested upon (CIFAR, MNIST) but I don't consider that a fundamental issue. \n\nI am quite a bit concerned about the experimentation protocol as well.  The datasets are relatively smaller scale, and datasets such as MNIST and CIFAR are known to overfit.  As a result, although there are approaches taken to generate unbalanced datasets out of them (e.g. MNIST).  Regardless, the results seem to suggest that the proposed method is similar to softmax performance - which is expected as they are similar - but I am not sure if it accurately evaluated / analyzed the possible application and performance gain of the proposed method.",1,1,1,1,-1,1,1,1,1,-1
Bk9nkMa4G-R3,"This paper presents a method based on a Bayesian classifier that improves classification of rare classes in datasets with long tail class distributions.  The method is based on balance the class-priors to generalize well for rare classes.  By using a Gaussian Mixture Model (GMM), authors are able to obtain a factorization of class-likelihoods and class-priors leading to a closed-form maximum likelihood estimation that can be integrated to differente classification models, such as current deep learning classifiers.  Authors, also propose an evaluation approach that addresses the bias towards the head and intra-class-variation of classes in the tail. \n\nThey face class imbalance problems, particular long tail distributions, by fixing: i) The covariance matrices of all the classes to be the identity, and ii) The priors over each class to be uniform.  So all classes, popular and rare, have equal weight for Bayesian classification.  To me, this is not a fundamental way to solve the long-tail problem, in the sense that by fixing isotropic likelihoods and flat priors, authors are also ignoring information that can be relevant in some classification problems, where a good prior can be useful to disambiguate confusing situations.  On other hand, using a unimodal function to model each class is an over-simplification that ignores intra-class complexity.  \n\nThe datasets used by the authors are balanced, so they artificially transform them into long-tailed,  it will be good to test directly on real long-tailed datasets.  The experimentation is only performed using small to medium datasets (< 80K instances), it will be good to show if the benefits of the proposed approach can also be present in the case of large datasets.  In this sense, I agree with the authors that the evaluation protocol for long-tailed datasets can't be just based on average accuracy, however, the protocol proposed requires to train the model several times, therefore, it does not scale properly to large datasets that are the common rule in the deep learning world.  Results respect to similar state-of-the-art techniques shows a reasonable improvement (depends of the dataset, approx. 1-3%).",1,1,1,1,1,-1,1,1,1,-1
Bk9zbyZCZ-R1,"The paper introduces a new memory mechanism specifically tailored for agent navigation in 2D environments.  The memory consists of a 2D array and includes trainable read/write mechanisms.  The RL agent's policy is a function of the context read, read, and next step write vectors (which are functions of the observation).  The effectiveness of the proposed architecture is evaluated via reinforcement learning (% of mazes solved).  The evaluation included 1000 test mazes--which sets a good precedent for evaluation in this subfield.  \n\nMy main concern is the lack of experiments to test whether the agent really learned to localize and plan routes using it's memory architecture.  The downsampling experiment in Section 5.1 seems to indicate the contrary: downsampling the memory should lead to position aliasing which seems to indicate that the agent is not using its memory to store the map and its own location.  I'm concerned whether the proposed agent is actually employing a navigation strategy, as seems to be suggested, or is simply a good agent architecture for this task (e.g. for optimization reasons).  The short experiment in Appendix E seems to try and answer this question, but it's results are anecdotal at best.  \n\nIf good RL performance on navigation tasks is the ultimate goal then one can imagine an agent that directly copies the raw map observation (world centric) into memory and use something like a value iteration network or shortest path planning to plan routes.  My point is that there are classical algorithms to solve navigation even in partially observable 2D grid worlds, why bother with deep RL here?",1,1,1,1,1,-1,1,1,1,-1
Bk9zbyZCZ-R2,"This paper presents a fully differentiable neural architecture for mapping and path planning for navigation in previously unseen environments, assuming near perfect* relative localization provided by velocity.  The model is more general than the cognitive maps (Gupta et al, 2017) and builds on the NTM/DNC or related architectures (Graves et al, 2014, 2016, Rae et al, 2017) thanks to the 2D spatial structure of the associative memory.  Basically, it consists of a 2D-indexed grid of features (the map) M_t that can be summarized at each time point into read vector r_t, and used for extracting a context c_t for the current agent state s_t, compute (thanks to an LSTM/GRU) an updated write vector w_{t+1}^{x,y} at the current position and update the map using that write vector.  The position {x,y} is a binned representation of discrete or continuous coordinates.  The absolute coordinate map can be replaced by a relative ego-centric map that is shifted (just like in Gupta et al, 2017) as the agent moves. \n\nThe experiments are exhaustive and include remembering the goal location with or without cues (similarly to Mirowski et al, 2017, not cited) in simple mazes of size 4x4 up to 8x8 in the 3D Doom environment.  The most important aspect is the capability to build a feature map of previously unseen environments. \n\nThis paper, showing excellent and important work, has already been published on arXiv 9 months ago and widely cited.  It has been improved since, through different sets of experiments and apparently a clearer presentation,  but the ideas are the same.  I wonder how it is possible that the paper has not been accepted at ICML or NIPS (assuming that it was actually submitted there).  What are the motivations of the reviewers who rejected the paper - are they trying to slow down competing research, or are they ignorant, and is the peer review system broken?  I quite like the formulation of the NIPS ratings: \""if this paper does not get accepted, I am considering boycotting the conference\"". \n\n* The noise model experiment in Appendix D is commendable,  but the noise model is somewhat unrealistic (very small variance, zero mean Gaussian) and assumes only drift in x and y, not along the orientation.  While this makes sense in grid world environments or rectilinear mazes, it does not correspond to realistic robotic navigation scenarios with wheel skid, missing measurements, etc...  Perhaps showing examples of trajectories with drift added would help convince the reader (there is no space restriction in the appendix).",1,1,1,-1,1,1,1,1,1,1
Bk9zbyZCZ-R3,"# Summary\nThis paper presents a new external-memory-based neural network (Neural Map) for handling partial observability in reinforcement learning.  The proposed memory architecture is spatially-structured so that the agent can read/write from/to specific positions in the memory.  The results on several memory-related tasks in 2D and 3D environments show that the proposed method outperforms existing baselines such as LSTM and MQN/FRMQN.  \n\n[Pros]\n- The overall direction toward more flexible/scalable memory is an important research direction in RL. \n- The proposed memory architecture is new.  \n- The paper is well-written. \n\n[Cons]\n- The proposed memory architecture is new but a bit limited to 2D/3D navigation tasks. \n- Lack of analysis of the learned memory behavior. \n\n# Novelty and Significance\nThe proposed idea is novel in general.  Though [Gupta et al.] proposed an ego-centric neural memory in the RL context, the proposed memory architecture is still new in that read/write operations are flexible enough for the agent to write any information to the memory, whereas [Gupta et al.] designed the memory specifically for predicting free space.  On the other hand, the proposed method is also specific to navigation tasks in 2D or 3D environment, which is hard to apply to more general memory-related tasks in non-spatial environments.  But, it is still interesting to see that the ego-centric neural memory works well on challenging tasks in a 3D environment. \n\n# Quality\nThe experiment does not show any analysis of the learned memory read/write behavior especially for ego-centric neural map and the 3D environment.  It is hard to understand how the agent utilizes the external memory without such an analysis.  \n\n# Clarity\nThe paper is overall clear and easy-to-follow except for the following.  In the introduction section, the paper claims that \""the expert must set M to a value that is larger than the time horizon of the currently considered task\"" when mentioning the limitation of the previous work.  In some sense, however, Neural Map also requires an expert to specify the proper size of the memory based on prior knowledge about the task.",1,1,1,1,1,1,1,1,1,-1
BkbOsNeSM-R1,"This authors proposed to use an implicit weight normalization approach to replace the explicit weight normalization used in training of neural networks.  The authors claimed to obtain efficiency improvement and better numerical stability. \n\nThis is a short paper that contains five pages.  The idea of the proposed implicit weight normalization is to apply the normalization to scaling the input rather than the rows of the matrices.  In terms of the overall time complexity, the improvement seems quite limited considering that the normalization is not the bottleneck operations in the training.  In addition, it is not very clear how the proposed approach benefits the mini-batch training of the network.  In terms of numerical stability, though experimental results were reported, there is no theoretical analysis.  The experiments are quite limited.\n",1,1,1,1,-1,-1,1,1,1,-1
BkbOsNeSM-R2,"This paper proposes a computationally fast method to train neural networks with normalized weights.  Experiments demonstrate that their method is promising compared to the competitor \u201cNormProp\u201d which explicitly normalizes the weights of neural networks. \n\nPros: \n(1) The paper is easy to follow. \n\n(2) Authors use figures that are easy to understand to explain their core idea, i.e., maintaining a vector which estimates the row norm of weight matrix and implicitly normalizing weights. \n\nCons:\n(1) If we count the matrix multiplication operation in fc layer along with normalization (in common cases normalization should follow a weighted layer), the whole computation complexity becomes O(mn) rather than O(n+m), so I doubt how fast it could be in the common case. \n\n(2) Authors did a MNIST experiment with a 2-fc layer neural network for comparing their FastNorm to NormProp.  It is a bit strange that they do not show the difference of speed, but show that FastNorm can outperform NormProp in terms of classification accuracy with a higher learning rate.  Since the efficiency is one of the main contributions, I suggest authors add this comparison.\n\n (3) The proposed FastNorm improves the stability by observing the standard deviation of validation accuracies in training phase.  The authors attribute this to the reduction of accumulated rounding error in training process, which is somewhat against the community\u2019s consensus, i.e., float precision is not that important so we can use float32 or even float16 to train/do inference for neural networks.  I\u2019m curious if this phenomenon still holds if authors use float64 in the experiments. \n\nSome typos:\nFirst line in page 3: \u201cbrining\u201d should be \u201cbringing\u201d\ n\nOverall, I think the current version of the paper is not ready for ICLR conference.  Authors need more experiments to show their approach\u2019s effectiveness.  For example, batching and convolution as mentioned by authors would be more significant. \n",1,1,1,1,1,-1,1,1,1,-1
BkbOsNeSM-R3,"The paper consider a method for \""weight normalization\"" of layers of a neural network.   The weight matrix is maintained normalized, which helps accuracy.   However, the simplest way to normalize on a fully connected layer is quadratic (adding squares of weights and taking square root). \n\n The paper proposes \""FastNorm\"", which is a way to implicitly maintain the normalized weight matrix using much less computation.   Essentially, a normalization vector is maintained an updated separately.  \n\n  Pros:   Natural method to do weight normalization efficeintly\n\n   Cons:   A very natural and simple solution that is fairly obvious. \n\n          Limited experiments \n\n",1,1,1,1,-1,-1,1,1,1,-1
BkCV_W-AZ-R1,"This paper presents Advantage-based Regret Minimization, somewhat similar to advantage actor-critic with REINFORCE. \nThe main focus of the paper seems to be the motivation/justification of this algorithm with connection to the regret minimization literature (and without Markov assumptions). \nThe claim that ARM is more robust to partially observable domains is supported by experiments where it outperforms DQN. \n\nThere are several things to like about this paper:\n- The authors do a good job of reviewing/referencing several papers in the field of \""regret minimization\"" that would probably be of interest to the ICLR community + provide non-obvious connections / summaries of these perspectives. \n- The issue of partial observability is good to bring up, rather than simply relying on the MDP framework that is often taken as a given in \""deep reinforcement learning\"". \n- The experimental results show that ARM outperforms DQN on a suite of deep RL tasks. \n\nHowever, there are also some negatives:\n- Reviewing so much of the CFR-literature in a short paper means that it ends up feeling a little rushed and confused. \n- The ultimate algorithm *seems* like it is really quite similar to other policy gradient methods such as A3C, TRPO etc.  At a high enough level, these algorithms can be written the same way... there are undoubtedly some key differences in how they behave, but it's not spelled out to the reader and I think the connections can be missed. \n- The experiment/motivation I found most compelling was 4.1 (since it clearly matches the issue of partial observability)  but we only see results compared to DQN ... it feels like you don't put a compelling case for the non-Markovian benefits of ARM vs other policy gradient methods.  Yes A3C and TRPO seem like they perform very poorly compared to ARM ... but I'm left wondering how/why? \n\nI feel like this paper is in a difficult position of trying to cover a lot of material/experiments in too short a paper. \nA lot of the cited literature was also new to me, so it could be that I'm missing something about why this is so interesting. \nHowever, I came away from this paper quite uncertain about the real benefits/differences of ARM versus other similar policy gradient methods ... I also didn't feel the experimental evaluations drove a clear message except \""ARM did better than all other methods on these experiments\"" ... I'd want to understand how/why and whether we should expect this universally. \nThe focus on \""regret minimization perspectives\"" didn't really get me too excited ...\n\nOverall I would vote against acceptance for this version.\n",1,1,1,1,1,1,1,1,1,-1
BkCV_W-AZ-R2,"This paper introduces the concepts of counterfactual regret minimization in the field of Deep RL.  Specifically, the authors introduce an algorithm called ARM which can deal with partial observability better.   The results is interesting and novel.   This paper should be accepted.  \n\nThe presentation of the paper can be improved a bit.   Much of the notation introduced in section 3.1 is not used later on.   There seems to be a bit of a disconnect before and after section 3.3.  The algorithm in deep RL could be explained a bit better. \n\nThere are some papers that could be connected.  Notably the distributional RL work that was recently published could be very interesting to compare against in partially observed environments. \n\nIt could also be interesting if the authors were to run the proposed algorithm on environments where long-term memory is required to achieve the goals. \n\nThe argument the authors made against recurrent value functions is that recurrent value could be hard to train.  An experiment illustrating this effect could be illuminating. \n\nCan the proposed approach help when we have recurrent value functions?  Since recurrence does not guarantee that all information needed is captured. \n\n\nFinally some miscellaneous points:\n\nOne interesting reference: Memory-based control with recurrent neural\nnetworks by Heess et al. \n\nPotential typos: in the 4th bullet point in section 3.1, should it be \\rho^{\\pi}(h, s')?",1,1,1,1,1,1,1,1,1,-1
BkCV_W-AZ-R3,"Quality and clarity:\n\nThe paper provides a game-theoretic inspired variant of policy-gradient algorithm based on the idea of counter-factual regret minimization.  The paper claims that the approach can deal with the partial observable domain better than the standard methods.  However the results only show that the algorithm converges, in some cases, faster than the previous work  reaching asymptotically to a same or worse performance.  Whereas one would expect that the algorithm achieve a better asymptotic performance in compare to methods which are designed for fully observable domains and thus performs sub-optimally in the POMDPs.  \n\nThe paper dives into the literature of counter-factual regret minimization without providing much intuition on why this type of ideas should provide improvement in the case of partial observable domain.  To me it is not clear at all why this idea should help in the partial observable domains beside the argument that this method is designed in the game-theoretic settings   which makes no Markov assumption .   The way that I interpret this algorithm is that by adding A+ to the return the algorithm  introduces some bias for actions which are likely to be optimal so it is in some sense implements the optimism in the face of uncertainty principle.   This may explains why this algorithm converges faster than the baseline as it produces better exploration strategy.   To me it is not clear that the boost comes from the fact that the algorithm deals with partial observability more efficiently.  \n\n\nOriginality and Significance:\n\nThe proposed algorithm seems original.   However,  as it is acknowledged by the authors this type of optimistic policy gradient algorithms have been previously used in RL (though maybe not with the game theoretic justification).   I believe the algorithm introduced  in this paper, if it is presented well, can be  an interesting addition to the literature of Deep RL, e.g.,  in terms of improving the rate of convergence.   However, the current version of paper  does not provide conclusive evidence for that as in most of the domains the algorithm only converge marginally faster than the standard ones.   Given the fact that algorithms like dueling DQN and DDPG are   for the best asymptotic results and not  for the best convergence rate, this improvement  can be due to the choice of hyper parameter such as step size or epsilon decay scheduling.   More experiments over a range of hyper parameter is needed before one can conclude that this algorithm improves the rate of convergence.\n",1,1,1,1,1,1,1,1,1,-1
BkDB51WR--R1,"Interesting ideas that extend LSTM to produce probabilistic forecasts for univariate time series, experiments are okay.  Unclear if this would work at all in higher-dimensional time series.  It is also unclear to me what are the sources of the uncertainties captured.\ n\n\nThe author proposed to incorporate 2 different discretisation techniques into LSTM, in order to produce probabilistic forecasts of univariate time series.  The proposed approach deviates from the Bayesian framework where there are well-defined priors on the model, and the parameter uncertainties are subsequently updated to incorporate information from the observed data, and propagated to the forecasts.  Instead, the conditional density p(y_t|y_{1:t-1|, \\theta}) was discretised by 1 of the 2 proposed schemes and parameterised by a LSTM.  The LSTM was trained using discretised data and cross-entropy loss with regularisations to account for ordering of the discretised labels.  Therefore, the uncertainties produced by the model appear to be a black-box.  It is probably unlikely that the discretisation method can be generalised to high-dimensional setting? \n\nQuality: The experiments with synthetic data sufficiently showed that the model can produce good forecasts and predictive standard deviations that agree with the ground truth.  In the experiments with real data, it's unclear how good the uncertainties produced by the model are.  It may be useful to compare to the uncertainty produced by a GP with suitable kernels.  In Fig 6c, the 95pct CI looks more or less constant over time. Is there an explanation for that? \n\nClarity: The paper is well-written.  The presentations of the ideas are pretty clear. \n\nOriginality: Above average.  I think the regularisation techniques proposed to preserve the ordering of the discretised class label are quite clever. \n\nSignificance: Average.  It would be excellent if the authors can extend this to higher dimensional time series. \n\nI'm unsure about the correctness of Algorithm 1 as I don't have knowledge in SMC.",1,1,1,1,1,-1,1,1,1,-1
BkDB51WR--R2,"The papers proposes a recurrent neural network-based model to learn the temporal evolution of a probability density function.  A Monte Carlo method is suggested for approximating the high dimensional integration required for multi-step-ahead prediction. \n\nThe approach is tested on two artificially generated datasets and on two real-world datasets, and compared with standard approaches such as the autoregressive model, the Kalman filter, and a regression LSTM.\ n\nThe paper is quite dense and quite difficult to follow, also due to the complex notation used by the authors.\ n\nThe comparison with other methods is very week, the authors compare their approach with two very simple alternatives, namely a first-order autoregressive mode and the Kalman filter.   More sophisticated should have been employed.",1,1,1,1,1,1,1,1,-1,-1
BkDB51WR--R3,"This work proposes an LSTM based model for time-evolving probability densities.  The model does not assume an explicit prior over the underlying dynamical systems, instead only uncertainty over observation noise is explicitly considered.  Experiments results are good for given synthetic scenarios  but less convincing for real data.   \n\nClarity: The paper is well-written.  Some notations in the LSTM section could be better explained for readers who are unfamiliar with LSTMs.  Otherwise, the paper is well-structured and easy to follow.\ n\nOriginality: I'm not familiar with LSTMs, it is hard for me to judge the originality here. \n\nSignificance: Average.  The work would be stronger if the authors can extend this to higher dimensional time series.  There are also many papers on this topic using Gaussian process state-space (GP-SSM) models where an explicit prior is assumed over the underlying dynamical systems.  The authors might want to comment on the relative merits between GP-SSMs and DE-RNNs. \n\nThe SMC algorithm used is a sequential-importance-sampling (SIS) method.  I think it's correct but may not scale well with dimensions.",1,1,1,1,1,1,1,1,-1,-1
BkeC_J-R--R1,"This paper proposes leveraging labelled controlled data to accelerate reinforcement-based learning of a control policy.   It provides two main contributions: pre-training the policy network of a DDPG agent in a supervised manner so that it begins in reasonable state-action distribution and regalurizing the Q-updates of the q-network to be biased towards existing actions.   The authors use the TORCS enviroment to demonstrate the performance of their method both in final cumulative return of the policy and speed of learning. \n\nThis paper is easy to understand  but has a couple shortcomings and some fatal (but reparable) flaws:. \n\n1) When using RL please try to standardize your notation to that used by the community, it makes things much easier to read.   I would strongly suggest avoiding your notation a(x|\\Theta) and using \\pi(x) (subscripting theta or making conditional is somewhat less important).   Your a(.) function seems to be the policy here, which is invariable denoted \\pi in the RL literature.   There has been recent effort to clean up RL notation which is presented here: https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf.  You have no obligation to use this notation but it does make reading of your paper much easier on others in the community.   This is more of a shortcoming than a fundamental issue. \n\n2) More fatally, you have failed to compare your algorithm's performance against benchline implementations of similar algorithms.   It is almost trivial to run DDPG on Torcs using the openAI baselines package [https://github.com/openai/baselines].   I would have loved, for example, to see the effects of simply pre-training the DDPG actor on supervised data, vs. adding your mixture loss on the critic.   Using the baselines would have (maybe) made a very compelling graph showing DDPG, DDPG + actor pre-training, and then your complete method.\n\n 3) And finally, perhaps complementary to point 2), you really need to provide examples on more than one environment.   Each of these simulated environments has its own pathologies linked to determenism, reward structure, and other environment particularities.   Almost every algorithm I've seen published will often beat baselines on one environment and then fail to improve or even be wors on others, so it is important to at least run on a series of these.   Mujoco + AI Gym should make this really easy to do (for reference, I have no relatinship with OpenAI).   Running at least cartpole (which is a very well understood control task), and then perhaps reacher, swimmer, half-cheetah etc. using a known contoller as your behavior policy (behavior policy is a good term for your data-generating policy.)\n\n 4) In terms of state of the art you are very close to Todd Hester et. al's paper on imitation learning, and although you cite it, you should contrast your approach more clearly with the one in that paper.   Please also have a look at some more recent work my Matej Vecerik, Todd Hester & Jon Scholz: 'Leveraging Demonstrations for Deep Reinforcement Learning on Robotics Problems with Sparse Rewards' for an approach that is pretty similar to yours.\n\n Overall I think your intuitions and ideas are good,  but the paper does not do a good enough job justifying empirically that your approach provides any advantages over existing methods.   The idea of pre-training the policy net has been tried before (although I can't find a published reference) and in my experience will help on certain problems, and hinder on others, primarily because the policy network is already 'overfit' somewhat to the expert, and may have a hard time moving to a more optimal space.   Because of this experience I would need more supporting evidence that your method actually generalizes to more than one RL environment.",1,1,1,1,1,1,1,1,1,1
BkeC_J-R--R2,"This paper proposes to combine reinforcement learning with supervised learning to speed up learning.  Unlike their claim in the paper, the idea of combining supervised and RL is not new.  A good example of this is a supervised actor-critic by Barto (2004).  I think even alphaGo uses some form of supervision.  However, if I understand correctly, it seems that combining supervision of RL at a later fine-tuning phase by considering supervision as a regularization term is an interesting idea that seems novel. \n\nHaving the luxury of some supervised episodes is of course useful.  The first step of building a supervised initial model looks straight forward.  The next step of the algorithm is less easy to follow, and presentation of the ideas could be much better.  This part of the paper leaves me already with many questions such as why is it essential to consider only a deterministic case and also to consider greedy optimization? Doesn\u2019t this prevent exploration? What are the network parameters (e.g. size of layers) etc.  I am not sure I could redo the work from the provided information.\n\n Overall, it is unclear to me what the advantage of the algorithm is over pure supervised learning, and I don\u2019t think a compelling case has been made.  Since the influence of the supervision is increased by increasing alpha, it can be expected that results should be better for increasing alpha.  The results seem to indicate that an intermediate level of alpha is best, though I would even question the statistical significance by looking at the curves in Figure 3.  Also, what is the epoch number, and why is this 1 for alpha=0?  If the combination of supervised learning with RL is better, than this should be clearly stated.  Some argument is made that pure supervision is overfitting, but would one then not simply add some other regularizer?  \n\nThe presentation could also be improved with some language edits.  Several articles are wrongly placed and even some meaning is unclear.  For example, the phrase \u201ccontinuous input sequence\u201d does not make sense; maybe you mean \u201cinput sequence of real valued quantities\u201d.\n\n In summary, while the paper contains some good ideas, I certainly think it needs more work to make a clear case for this method. \n",1,1,1,1,1,1,1,1,1,-1
BkeC_J-R--R3,"\nThe paper was fairly easy to follow,  but I would not say it was well written.  These are minor annoyances; there were some typos and a strange citation format.  There is nothing wrong with the fundamental idea itself,  but given the experimental results it just is not clear that it is working.\ n\nThe bot performance significantly better than the fully trained agent.  This leads to a few questions:\n\n1. What was the performance of the \""regression policy\"", that was learned during the supervised pretraining phase?\ n2. Given enough time would the basic RL agent reach similar performance? (Guessing no...) Why not?\n3.  Considering the results of Figure 3 (right) shouldn't the conclusion be that the RL portion is essentially contributing nothing? \n\nPros:\nThe regularization of the Q-values w.r.t. the policy of another agent is interesting\n\n Cons:\nNot very well setup experiments\nPerformance is lower than you would expect just using supervised training\nNot clear what parts are working and what parts are not\n\n\n",1,1,1,1,1,-1,1,1,1,-1
Bki1Ct1AW-R1,"This study proposes the use of non-negative matrix factorization accounting for baseline by subtracting the pre-stimulus baseline from each trial and subsequently decompose the data using a 3-way factorization thereby identifying spatial and temporal modules as well as their signed activation.  The method is used on data recorded from mouse and pig retinal ganglion cells of time binned spike trains providing improved performance over non-baseline corrected data.  \n\nPros:\nThe paper is well written, the analysis interesting and the application of the Tucker2 framework sound.  Removing baseline is a reasonable step and the paper includes analysis of several spike-train datasets.  The analysis of the approaches in terms of their ability to decode is also sound and interesting. \n\nCons:\nI find the novelty of the paper limited: \nThe authors extend the work by (Onken et al. 2016) to subtract baseline (a rather marginal innovation) of this approach.  To use a semi-NMF type of update rule (as proposed by Ding et al .2010) and apply the approach to new spike-train datasets evaluating performance by their decoding ability (decoding also considered in Onken et al. 2016).\n\n Multiplicative update-rules are known to suffer from slow-convergence and I would suspect this also to be an issue for the semi-NMF update rules.  It would therefore be relevant and quite easy to consider other approaches such as active set or column wise updating also denoted HALS which admit negative values in the optimization, see also the review by N. Giles\nhttps://arxiv.org/abs/1401.5226\nas well as for instance:\nNielsen, S\u00f8ren F\u00f8ns Vind, and Morten M\u00f8rup.  \""Non-negative tensor factorization with missing data for the modeling of gene expressions in the human brain.\""  Machine Learning for Signal Processing (MLSP), 2014 IEEE International Workshop on. IEEE, 2014. \n\nIt would improve the paper to also discuss that the non-negativity constrained Tucker2 model may be subject to local minima solutions and have issues of non-uniqueness (i.e. rotational ambiguity).  At least local minima issues could be assessed using multiple random initializations. \n\nThe results are in general only marginally improved by the baseline corrected non-negativity constrained approach.  For comparison the existing methods ICA, Tucker2 should also be evaluated for the baseline corrected data, to see if it is the constrained representation or the preprocessing influencing the performance.  Finally, how performance is influenced by dimensionality P and L should also be clarified.\n\n It seems that it would be naturally to model the baseline by including mean values in the model rather than treating the baseline as a preprocessing step.  This would bridge the entire framework as one model and make it potentially possible to avoid structure well represented by the Tucker2 representation to be removed by the preprocessing.\n\n\n\n Minor: \nThe approach corresponds to a Tucker2 decomposition with non-negativity constrained factor matrices and unconstrained core - please clarify this as you also compare to Tucker2 in the paper with orthogonal factor matrices.\n\nDing et al. in their semi-NMF work provide elaborate derivation with convergence guarantees.   In the present paper these details are omitted and it is unclear how the update rules are derived from the KKT conditions and the Lagrange multiplier and how they differ from standard semi-NMF, this should be better clarified. \n\n",1,1,1,1,1,1,1,1,1,-1
Bki1Ct1AW-R2,"In this paper, the authors present an adaptation of space-by-time non-negative matrix factorization (SbT-NMF) that can rigorously account for the pre-stimulus baseline activity.  The authors go on to compare their baseline-corrected (BC) method with several established methods for dimensionality reduction of spike train data.\n\n Overall, the results are a bit mixed.  The BC method often performs similarly to or is outperformed by non-BC SbT-NMF.  The authors provide a possible mechanism to explain these results, by analyzing classification performance as a function of baseline firing rate.  The authors posit that their method can be useful when sensory responses are on the order of magnitude of baseline activity; however, this doesn't fully address why non-BC SbT-NMF can strongly outperform the BC method in certain tasks (e.g. the step of light, Fig. 3b).  Finally, while this method introduces a principled way to remove mean baseline activity from the sensory-driven response, this may also discount the effect that baseline firing rate and fast temporal fluctuations can have on the response (Destexhe et al., Nature Reviews Neuroscience 4, 2003; Gutnisky DA et al., Cerebral Cortex 27, 2017).",1,1,1,1,1,1,1,1,1,-1
Bki1Ct1AW-R3,"In this contribution, the authors propose an improvement of a tensor decomposition method for decoding spike train.  Relying on a non-negative matrix factorization, the authors tackle the influence of the baseline activity on the decomposition.  The main consequence is that the retrieved components are not necessarily non-negative and the proposed decomposition rely on signed activation coefficients.  An experimental validation shows that for high frequency baseline (> 0.7 Hz), the baseline corrected algorithm yields better classification results than non-corrected version (and other common factorization techniques). \n\n The objective function is defined with a Frobenius norm, which has an important influence on the obtained solutions, as it could be seen on Figure 2.  The proposed method seems to provide a more discriminant factorization than the NMF one, at the expense of the sparsity of spatial and temporal components, impeding the biological interpretability.   A possible solution is to add a regularization term to the objective function to ensure the sparsity of the factorization.",1,1,1,1,1,-1,1,1,1,-1
BkIkkseAZ-R1,"The paper studies the theoretical properties of the two-layer neural networks.  \n\nTo summarize the result, let's use the theta to denote the layer closer to the label, and W to denote the layer closer to the data.  \n\nThe paper shows that \na) if W is fixed, then with respect to the randomness of the data, with prob. 1, the Jacobian matrix of the model is full rank \nb) suppose that we run an algorithm with fresh samples, then with respect to the randomness of the k-th sample, we have that with prob. 1, W_k is full rank, and the Jacobian of the model is full rank.  \n\nIt's know (essentially from the proof of Carmon and Soudry) that if the Jacobian of the model is full rank for any matrix W w.r.t the randomness of the data, then all stationary points are global.  But the paper cannot establish such a result.  \n\nThe paper is not very clear, and after figuring out what it's doing, I don't feel it really provides many new things beyond C-S and Xie et al. \n\nThe paper argues that it works for activation beyond relu but result a) is much much weaker than the one with for all quantifier for W.  result b) is very sensitive to the exactness of the events (such as W is exactly full rank) --- the events that the paper talks just naturally never happen as long as the density of the random variables doesn't degenerate.   \n\nAs the author admitted, the results don't provide any formal guarantees for the convergence to a global minimum.  It's also a bit hard for me to find the techniques here provide new ideas that would potentially lead to resolving this question.  \n\n--------------------\n\nadditional review after seeing the author's response: \n\nThe author's response pointed out some of the limitation of Soudry and Carmon, and Xie et al's which I agree.  However, none of this limitation is addressed by this paper (or addressed in a misleading way to some extent.)   The key technical limitation is the dependency of the local minima on the weight parameters . Soudry and Carmon addresses this in a partial way by using the random dropout, which is a super cool idea.  Xie et al couldn't address this globally but show that the Jacobian is well conditioned for a class of weights.  The paper here doesn't have either and only shows that for a single fixed weight matrix, the Jacobian is well-conditioned.  \n\nI don't also see the value of extension to other activation function . To some extent this is not consistent with the empirical observation that relu is very important for deep learning.  \n\nRegarding the effect of randomness, since the paper only shows the convergence to a first-order optimal solution, I don't see why randomness is necessary.  Gradient descent can converge to a first order optimal solution.  (Indeed I have a typo in my previous review regarding \""w.r.t. k-th sample\"", which should be \""w.r.t. k-th update\"". )  Moreover, to justify the effect of the randomness, the paper should have empirical experiments.  \n\nI think the writing of the paper is also misleading in several places. \n",1,1,1,1,-1,1,1,1,1,-1
BkIkkseAZ-R2,"This paper aims to study some of the theoretical properties of the global optima of single-hidden layer neural networks and also the convergence to such a solution.  I think there are some interesting arguments made in the paper e.g. Lemmas 4.1, 5.1, 5.2, and 5.3.  However, as I started reading beyond intro I increasingly got the sense that this paper is somewhat incomplete e.g. while certain claims are made (abstract/intro) the theoretical justification are rather far from these claims.  Of course there is a chance that I might be misunderstanding some things and happy to adjust my score based on the discussions here. \n\nDetailed comments:\n1) My main concern is that the abstract and intro claims things that are never proven (or even stated) in the rest of the paper \nExample 1 from abstract: \n\u201cWe show that for a wide class of differentiable activation functions (this class involved \u201calmost\u201d all functions which are not piecewise linear), we have that first-order optimal solutions satisfy global optimality provided the hidden layer is non-singular.\u201d\n\nThis is certainly not proven and in fact not formally stated anywhere in the paper.  Closest result to this is Lemma 4.1 however, because the optimal solution is data dependent this lemma can not be used to conclude this.  \n\nExample 2 from intro when comparing with other results on page 2:\nThe authors essentially state that they have less restrictive assumptions in the form of the network or assumptions on the data (e.g. do not require Gaussianity).  However as explained above the final conclusions are also significantly weaker than this prior literature so it\u2019s a bit of apples vs oranges comparison. \n\n2) Page 2 minor typos\nWe study training problem -->we study the training problem\nIn the regime training objective--> in the regime the training objective \n\n3) the basic idea argument and derivative calculations in section 3 is identical to section 4 of Soltan...et al \n\n4) Lemma 4.1 is nice, well done! That being said it does not seem easy to make it (1) quantifiable (2) apply to all W.  It would also be nice to compare with Soudry et. al. \n\n5) Argument on top of page 6 is incorrect as the global optima is data dependent and hence lemma 4.1 (which is for a fixed matrix) does not apply \n\n6) Section 5 on page 6. Again the stated conclusion here that the iterates do not lead to singular W is much weaker than the claims made early on. \n \n7) I haven\u2019t had time yet to verify correctness of Lemmas 5.1, 5.2, and Lemma 5.3 in detail but if this holds is a neat argument to side step invertibility w.r.t. W, Nicely done! \n\n8) What is the difference between Lemma 5.4 and Lemma 6.12 of Soltan...et al  \n\n9) Theorem 5.9. Given that the arguments in this paper do not show asymptotic convergence to a point where gradient vanishes and W is invertible why is the proposed algorithm better than a simple approach in which gradient descent is applied but a small amount of independent Gaussian noise is injected in every iteration over W.  By adjusting the noise variance across time one can ensure a result of the kind in Theorem 5.9 (Of course in the absence of a quantifiable version of Lemma 4.1 which can apply to all W that result will also suffer from the same issues).\n",1,1,1,1,1,1,1,1,1,-1
BkIkkseAZ-R3,"I only got access to the paper after the review deadline; and did not have a chance to read it until now. Hence the lateness and brevity. \n\nThe paper tackles an important theoretical question; and it offers results that are complementary to existing results (e.g., Soudry et al).  However, the paper does not properly relate their results, assumptions in the context of the existing literature.  Much explanation is needed in the author reply in order to clear these questions. \n\nThe work should not be evaluated from a practical perspective as it is of a theoretical nature. \n\nI agree with most of the criticism raised by other reviewers . However, I also believe the authors managed to clear essentially of the criticism in they reply.  The paper lacks in clarity as currently written.  \n\nThe results are interesting, but more explanation is needed for the main message to be conveyed more clearly.  I suggest 7, but the paper has a potential to become 8 in my eyes in a future resubmission.\n",1,1,1,1,1,1,1,1,1,-1
BkJ3ibb0--R1,"This paper presents Defense-GAN: a GAN that used at test time to map the input generate an image (G(z)) close (in MSE(G(z), x)) to the input image (x), by applying several steps of gradient descent of this MSE.  The GAN is a WGAN trained on the train set (only to keep the generator).  The goal of the whole approach is to be robust to adversarial examples, without having to change the (downstream task) classifier, only swapping in the G(z) for the x. \n\n+ The paper is easy to follow. \n+ It seems (but I am not an expert in adversarial examples) to cite the relevant litterature (that I know of) and compare to reasonably established attacks and defenses. \n+ Simple/directly applicable approach that seems to work experimentally;  but\n- A missing baseline is to take the nearest neighbour of the (perturbed) x from the training set. \n- Only MNIST-sized images, and MNIST-like (60k train set, 10 labels) datasets: MNIST and F-MNIST. \n- Between 0.043sec and 0.825 sec to reconstruct an MNIST-sized image. \n? MagNet results were very often worse than no defense in Table 4, could you comment on that? \n- In white-box attacks, it seems to me like L steps of gradient descent on MSE(G(z), x) should be directly extended to L steps of (at least) FGSM-based attacks, at least as a control.",1,1,1,1,1,1,1,1,1,-1
BkJ3ibb0--R2,"This paper presents a method to cope with adversarial examples in classification tasks, leveraging a generative model of the inputs.   Given an accurate generative model of the input, this approach first projects the input onto the manifold learned by the generative model (the idea being that inputs on this manifold reflect the non-adversarial input distribution).   This projected input is then used to produce the classification probabilities.   The authors test their method on various adversarially constructed inputs (with varying degrees of noise).  \n\nQuestions/Comments:\n\n- I am interested in unpacking the improvement of Defense-GAN over the MagNet auto-encoder based method.   Is the MagNet auto-encoder suffering lower accuracy because the projection of an adversarial image is based on an encoding function that is learned only on true data?   If the decoder from the MagNet approach were treated purely as a generative model, and the same optimization-based projection approach (proposed in this work) was followed, would the results be comparable?   \n\n- Is there anything special about the GAN approach, versus other generative approaches?  \n\n- In the black-box vs. white-box scenarios, can the attacker know the GAN parameters?   Is that what is meant by the \""defense network\"" (in experiments bullet 2)? \n\n- How computationally expensive is this approach take compared to MagNet or other adversarial approaches?  \n\nQuality: The method appears to be technically correct. \n\nClarity: This paper clearly written;  both method and experiments are presented well.  \n\nOriginality: I am not familiar enough with adversarial learning to assess the novelty of this approach.  \n\nSignificance: I believe the main contribution of this method is the optimization-based approach to project onto a generative model's manifold.   I think this kernel has the potential to be explored further (e.g. computational speed-up, projection metrics)",1,1,1,1,1,1,1,1,-1,-1
BkJ3ibb0--R3,"The authors describe a new defense mechanism against adversarial attacks on classifiers (e.g., FGSM).  They propose utilizing Generative Adversarial Networks (GAN), which are usually used for training generative models for an unknown distribution, but have a natural adversarial interpretation.  In particular, a GAN consists of a generator NN G which maps a random vector z to an example x, and a discriminator NN D which seeks to discriminate between an examples produced by G and examples drawn from the true distribution.  The GAN is trained to minimize the max min loss of D on this discrimination task, thereby producing a G (in the limit) whose outputs are indistinguishable from the true distribution by the best discriminator.  \n\nUtilizing a trained GAN, the authors propose the following defense at inference time.  Given a sample x (which has been adversarially perturbed), first project x onto the range of G by solving the minimization problem z* = argmin_z ||G(z) - x||_2. This is done by SGD.  Then apply any classifier trained on the true distribution on the resulting x* = G(z*).  \n\nIn the case of existing black-box attacks, the authors argue (convincingly) that the method is both flexible and empirically effective.  In particular, the defense can be applied in conjunction with any classifier (including already hardened classifiers), and does not assume any specific attack model.  Nevertheless, it appears to be effective against FGSM attacks, and competitive with adversarial training specifically to defend against FGSM.  \n\nThe authors provide less-convincing evidence that the defense is effective against white-box attacks.  In particular, the method is shown to be robust against FGSM, RAND+FGSM, and CW white-box attacks.  However, it is not clear to me that the method is invulnerable to novel white-box attacks.  In particular, it seems that the attacker can design an x which projects onto some desired x* (using some other method entirely), which then fools the classifier downstream. \n\nNevertheless, the method is shown to be an effective tool for hardening any classifier against existing black-box attacks \n(which is arguably of great practical value).  It is novel and should generate further research with respect to understanding its vulnerabilities more completely.  \n\nMinor Comments:\nThe sentence starting \u201cUnless otherwise specified\u2026\u201d at the top of page 7 is confusing given the actual contents of Tables 1 and 2, which are clarified only by looking at Table 5 in the appendix. This should be fixed",1,1,1,1,1,-1,1,1,1,-1
Bkl1uWb0Z-R1,"This paper adds source side dependency syntax trees to an NMT model without explicit supervision.  Exploring the use of syntax in neural translation is interesting but I am not convinced that this approach actually works based on the experimental results. \n\nThe paper distinguishes between syntactic and semantic objectives (4th paragraph in section 1), attention, and heads.  Please define what semantic attention is.  You just introduce this concept without any explanation.  I believe you mean standard attention, if so, please explain why standard attention is semantic. \n\nClarity. What is shared attention exactly?  Section 3.2 says that you share attention weights from the decoder with encoder. Please explain this a bit more.  Also the example in Figure 3 is not very clear and did not help me in understanding this concept. \n\nResults. A good baseline would be to have two identical attention mechanisms to figure out if improvements come from more capacity or better model structure.  Flat attention seems to add a self-attention model and is somewhat comparable to two mechanisms.  The results show hardly any improvement over the flat attention baseline (at most 0.2 BLEU which is well within the variation of different random initializations).  It looks as if the improvement comes from adding additional capacity to the model.  \n\nEquation 3: please define H",1,1,1,1,1,-1,1,1,-1,-1
Bkl1uWb0Z-R2,"This paper describes a method to induce source-side dependency structures in service to neural machine translation.  The idea of learning soft dependency arcs in tandem with an NMT objective is very similar to recent notions of self-attention (Vaswani et al., 2017, cited) or previous work on latent graph parsing for NMT (Hashimoto and Tsuruoka, 2017, cited).  This paper introduces three innovations: (1) they pass the self-attention scores through a matrix-tree theorem transformation to produce marginals over tree-constrained head probabilities; (2) they explicitly specify how the dependencies are to be used, meaning that rather than simply attending over dependency representations with a separate attention, they select a soft word to attend to through the traditional method, and then attend to that word\u2019s soft head (called Shared Attention in the paper); and (3) they gate when attention is used.  I feel that the first two ideas are particularly interesting.  Unfortunately, the results of the NMT experiments are not particularly compelling, with overall gains over baseline NMT being between 0.6 and 0.8 BLEU.  However, they include a useful ablation study that shows fairly clearly that both ideas (1) and (2) contribute equally to their modest gains, and that without them (FA-NMT Shared=No in Table 2), there would be almost no gains at all.  Interesting side-experiments investigate their accuracy as a dependency parser, with and without a hard constraint on the system\u2019s latent dependency decisions. \n\nThis paper has some very good ideas, and asks questions that are very much worth asking.  In particular, the question of whether a tree constraint is useful in self-attention is very worthwhile.  Unfortunately, this is mostly a negative result, with gains over \u201cflat attention\u201d being relatively small.  I also like the \u201cShared Attention\u201d - it makes a lot of sense to say that if the \u201csemantic\u201d attention mechanism has picked a particular word, one should also attend to that word\u2019s head; it is not something I would have thought of on my own.  The paper is also marred by somewhat weak writing, with a number of disfluencies and awkward phrasings making it somewhat difficult to follow. \n\nIn terms of specific criticisms:\n\nI found the motivation section to be somewhat weak.  We need a better reason than morphology to want to do source-side dependency parsing.  All published error analyses of strong NMT systems (Bentivogli et al, EMNLP 2016; Toral and Sanchez-Cartagena, EACL 2017; Isabelle et al, EMNLP 2017) have shown that morphology is a strength, not a weakness of these systems, and the sorts of head selection problems shown in Figure 1 are, in my experience, handled capably by existing LSTM-based systems. \n\nThe paper mentions \u201csignificant improvements\u201d in only two places: the introduction and the conclusion.  With BLEU score differences being so low, the authors should specify how statistical significance is measured;  ideally using a technique that accounts for the variance of random restarts (i.e.: Clark et al, ACL 2011). \nEquation (3): I couldn\u2019t find the definition for H anywhere. \n\nSentence before Equation (5): I believe there is a typo here, \u201cf takes z_i\u201d should be \u201cf takes u_t\u201d. \n\nFirst section of Section 3: please cite the previous work you are talking about in this sentence. \n\nMy understanding was that the dependency marginals in p(z_{i,j}=1|x,\\phi) in Equation (11) are directly used as \\beta_{i,j}.  If I\u2019m correct, that\u2019s probably worth spelling out explicitly in Equation (11): \\beta_{i,j} = p(z_{i,j}=1|x,\\phi) = \u2026. \n\nI don\u2019t don\u2019t feel like the clause between equations (17) and (18), \u201cwhen sharing attention weights from the decoder with the encoder\u201d is a good description of your clever \u201cshared attention\u201d idea.  In general, I found this region of the paper, including these two equations and the text between them, very difficult to follow. \n\nSection 4.4: It\u2019s very very good that you compared to \u201cflat attention\u201d,  but it\u2019s too bad for everyone cheering for linguistically-informed syntax that the results weren\u2019t better. \n\nTable 5: I had a hard time understanding Table 5 and the corresponding discussion.  What are \u201cproduction percentages\u201d? \n\nFinally, it would have been interesting to include the FA system in the dependency accuracy experiment (Table 4), to see if it made a big difference there",1,1,1,1,1,1,1,1,1,-1
Bkl1uWb0Z-R3,"This paper induces latent dependency syntax in the source side for NMT.  Experiments are made in En-De and En-Ru. \n\nThe idea of imposing a non-projective dependency tree structure was proposed previously by Liu and Lapata (2017) and the structured attention model by Kim and Rush (2017).  In light of this, I see very little novelty in this paper.  The only novelty seems to be the gate that controls the amount of syntax needed for generating each target word.  Seems thin for a ICLR paper. \n\nCaption of Fig 1: \""subject/object\"" are syntactic functions, not semantic roles. \n\nI don't see how the German verb \""orders\"" inflects with gender...  Can you post the gold German sentence? \n\nSec 2 is poorly explained. What is z_t? Do you mean u_t instead? This is confusing. \n \nExpressions (12) to (15) are essentially the same as in Liu and Lapata (2017), not original contributions of this paper. \n\nWhy is hard attention (sec 3.3) necessary?  It's not differentiable and requires sampling for training.  This basically spoils the main advantage of structured attention mechanisms as proposed by Kim and Rush (2017). \n\nExperimentally, the gains are quite small compared to flat attention, which is disappiointing. \n\nIn table 3, it would be very helpful to display the English source. \n\nTable 4 is confusing.  The DA numbers (rightmost three columns) are for the 2016 or 2017 dataset? \n\nComparison with predicted parses by Spacy are by no means \""gold\"" parses... \n\nMinor comments:\n- Sec 1: \""... optimization techniques like Adam, Attention, ...\"" -> Attention is not an optimization technique, but part of a model; \n- Sec 1: \""abilities not its representation\"" -> comma before \""not\""",1,1,1,1,1,1,1,1,1,-1
BkLhaGZRW-R1,"The paper proposes a regularizer that encourages a GAN discriminator to focus its capacity in the region around the manifolds of real and generated data points, even when it would be easy to discriminate between these manifolds using only a fraction of its capacity, so that the discriminator provides a more informative signal to the generator.  The regularizer rewards high entropy in the signs of discriminator activations.  Experiments show that this helps to prevent mode collapse on synthetic Gaussian mixture data and improves Inception scores on CIFAR10.  \n\nThe high-level idea of guiding model capacity by rewarding high-entropy activations  is interesting and novel to my knowledge (though I am not an expert in this space).  Figure `1 is a fantastic illustration that presents the core idea very clearly.  That said I found the intuitive story a little bit difficult to follow;  -- it's true that in Figure 1b the discriminator won't communicate the detailed structure of the data manifold to the generator, but it's not clear why this would be a problem;  -- the gradients should still pull the generator *towards* the manifold of real data, and as this happens and the manifolds begin to overlap, the discriminator will naturally be forced to allocate its capacity towards finer-grained details.  Is the implicit assumption that for real, high-dimensional data the generator and data manifolds will *never* overlap?  But in that case much of the theoretical story goes out the window.  I'd also appreciate further discussion of the relationship of this approach to Wasserstein GANs, which also attempt to provide a clearer training gradient when the data and generator manifolds do not overlap. \n\nMore generally I'd like to better understand what effect we'd expect this regularizer to have.  It appears to be motivated by improving training dynamics, which is understandably a significant concern.  Does it also change the location of the Nash equilibria?  (or equivalently, the optimal generator under the density-ratio-estimator interpretation of discriminators proposed by https://arxiv.org/abs/1610.03483).  I'd expect that it would but the effects of this changed objective are not discussed in the paper.  \n\n The experimental results seem promising, although not earthshattering.  I would have appreciated a comparison to other methods for guiding discriminator representation capacity, e.g. autoencoding (I'd also imagine that learning an inference network (e.g. BiGAN) might serve as a useful auxiliary task?).  \n\nOverall this feels like an cute hack, supported by plausible intuition but without deep theory or compelling results on real tasks (yet).  As such I'd rate it as borderline; though perhaps interesting enough to be worth presenting and discussing. \n\nA final note: this paper was difficult to read due to many grammatical errors and unclear or misleading constructions, as well as missing citations (e.g. sec 2.1).  From the second paragraph alone:\n\""impede their wider applications in new data domain\"" -> domains\n\""extreme collapse and heavily oscillation\"" -> heavy oscillation\n\""modes of real data distribution\"" -> modes of the real data distribution\n\""while D fails to exploit the failure to provide better training signal to G\"" -> should be \""this failure\"" to refer to the previously-described generator mode collapse, or rewrite entirely\n\""even when they are their Jensen-Shannon divergence\"" -> even when their Jensen-Shannon divergence\n I'm sympathetic to the authors who are presumably non-native English speakers;  many good papers contain mistakes, but in my opinion the level in this paper goes beyond what is appropriate for published work.  I encourage the authors to have the work proofread by a native speaker; clearer writing will ultimately increase the reach and impact of the paper",1,1,1,1,1,1,1,1,1,1
BkLhaGZRW-R2,"The paper proposed a novel regularizer that is to be applied to the (rectifier) discriminators in GAN in order to encourage a better allocation of the \""model capacity\"" of the discriminators over the (potentially multi-modal) generated / real data points, which might in turn helps with learning a more faithful generator. \n\nThe paper is in general very well written, with intuitions and technical details well explained and empirical studies carefully designed and executed. \n\nSome detailed comments / questions: \n\n1. It seems the concept of \""binarized activation patterns\"", which the proposed regularizer is designed upon, is closely coupled with rectifier nets.  I would therefore suggest the authors to highlight this assumption / constraint more clearly e.g. in the abstract. \n\n2. In order for the paper to be more self-contained, maybe list at least once the formula for \""rectifier net\"" (sth. like \""a^T max(0, wx + b) + c\"") ?  This might also help the readers better understand where the polytopes in Figure 1 come from. \n\n3. In section 3.1, when presenting random variables (U_1, ..., U_d), I find the word \""Bernourlli\"" a bit misleading because typically people would expect U_i to take values from {0, 1} whereas here you assume {-1, +1}.  This can be made clear with just one sentence yet would greatly help with clearing away confusions for subsequent derivations. \nAlso, \""K\"" is already used to denote the mini-batch size, so it's a slight abuse to reuse \""k\"" to denote the \""kth marginal\"". \n\n4. In section 3.2, it may be clearer to explicitly point out the use of the \""3-sigma\"" rule for Gaussian distributions here.  But I don't find it justified anywhere why \""leave 99.7% of i, j pairs unpenalized\"" is sth.  to be sought for here? \n\n5. In section 3.3, when presenting Corollary 3.3 of Gavinsky & Pudlak (2015), \""n\"" abruptly appears without proper introduction / context. \n\n6. For the empirical study with 2D MoG, would an imbalanced mixture make it harder for the BRE-regularized GAN to escape from modal collapse? \n\n7. Figure 3 is missing the sub-labels (a), (b), (c), (d)",1,1,1,1,1,-1,1,1,1,-1
BkLhaGZRW-R3,"The paper presents a method for improving the diversity of Generative Adversarial Network (GAN) by promoting the Gnet's weights to be as informative as possible.  This is achieved by penalizing the correlation between responses of hidden nodes and promoting low entropy intra node.  Numerical experiments that demonstrate the diversity increment on the generated samples are shown. \n\nConcerns.\n\nThe paper is hard do tear and it is deficit to identify the precise contribution of the authors.  Such contribution can, in my opinion, be summarized  in a potential of the form\n\nwith\n\n$$\nR_BRE = a R_ME+ b R_AC = a \\sum_k  \\sum_i s_{ki}^2   +  b \\sum_{<k,l>} \\sum_i \\{ s_{ki} s_{li} \\}   \n$$\n(Note that my version of R_ME is different to the one proposed by the authors, but it could have the same effect)\n\nWhere a and b are parameters that weight the relative contribution of each term  (maybe computed as suggested in the paper). \n\nIn this formulation:\n\nThen R_ME has a high response if the node has saturated responses -1\u2019s or 1``s, as one desire such saturated responses, a should be negative. \n\nThe R_AC, penalizes correlation between responses of different nodes. \n\nThe point is, \n\na) The second term will introduce  low correlation in saturated vectors, then the will be informative.  \n \nb) why the authors use the softsign instead the tanh:  $tahnh \\in C^2 $! Meanwhile the derivative id softsign is discontinuous. \n\nc)  It is not clear is the softsign is used besides the activation function: In page 5 is said \u201cR_BRE can be applied on ant rectified layer before the nolinearity\u201d  . This seems tt the authors propose to add a second activation function (the softsign), why not use the one is in teh layer? \n\nd) The authors found hard to regularize the gradient $\\nabla_x D(x)$, even they tray tanh and cosine based activations.  It seems that effectively, the  introduce their additional softsign in the process. \n\ne) En the definition of R_AC, I denoted by <k,l> the pair of nodes (k \\ne l).  However, I think that it should be for pair in the same layer. It is not clear in the paper. \n\nf) It is supposed that the L_1 regularization motes the weights to be informative, this work is doing something similar.  How is it compared  the L_1 regularization vs. the proposal? \n\nRecommendation\nI tried to read the paper several times and I accept that it was very hard to me.  The most difficult part is the lack of precision on the maths, it is hard to figure out what the authors contribution indeed are.  I think there is some merit in the work.  However, it is not very well organized and many points are not defined.  In my opinion, the paper is in a preliminary stage and should be refined.  I recommend a \u201cSOFT\u201d REJECT",1,1,1,1,1,-1,1,1,1,-1
BkM27IxR--R1,"[Main comments]\n\n* I would advice the authors to explain in more details in the intro\nwhat's new compared to Li & Malik (2016) and Andrychowicz et al.  (2016).\nIt took me until section 3.5 to figure it out. \n\n* If I understand correctly, the only new part compared to Li & Malik (2016) is\nsection 3.5, where block-diagonal structure is imposed on the learned matrices. \nIs that correct? \n\n* In the experiments, why not comparing with Li & Malik (2016)? (i.e., without\n  block-diagonal structure) \n\n* Please clarify whether the objective value shown in the plots is wrt the training\n  set or the test set.  Reporting the training objective value makes little\nsense to me, unless the time taken to train on MNIST is taken into account in\nthe comparison.  \n\n* Please clarify what are the hyper-parameters of your meta-training algorithm\n  and how you chose them. \n\nI will adjust my score based on the answer to these questions. \n\n[Other comments]\n\n* \""Given this state of affairs, perhaps it is time for us to start practicing\n  what we preach and learn how to learn\""\n\nThis is in my opinion too casual for a scientific publication... \n\n* \""aim to learn what parameter values of the base-level learner are useful\n  across a family of related tasks\""\n\nIf this is essentially multi-task learning, why not calling it so?  \""Learning\nwhat to learn\"" does not mean anything.   I understand that the authors wanted to\nhave \""what\"", \""which\"" and \""how\"" sections but this is not clear at all. \n\nWhat is a \""base-level learner\""? I think it would be useful to define it more\nprecisely early on. \n\n* I don't see the difference between what is described in Section 2.2\n  (\""learning which model to learn\"") and usual machine learning (searching for\nthe best hypothesis in a hypothesis class). \n\n* Typo: p captures the how -> p captures how \n\n* The L-BFGS results reported in all Figures looked suspicious to me.   How do you\n  explain that it converges to a an objective value that is so much worse? \nMoreover, the fact that there are huge oscillations makes me think that the\nauthors are measuring the function value during the line search rather than\nthat at the end of each iteration",1,1,1,1,1,1,1,1,1,-1
BkM27IxR--R2,"This paper proposed a reinforcement learning (RL) based method to learn an optimal optimization algorithm for training shallow neural networks.  This work is an extended version of [1], aiming to address the high-dimensional problem. \n\n\n\nStrengths:\n\nThe proposed method has achieved a better convergence rate in different tasks than all other hand-engineered algorithms. \nThe proposed method has better robustess in different tasks and different batch size setting. \nThe invariant of coordinate permutation and the use of block-diagonal structure improve the efficiency of LQG. \n\n\nWeaknesses:\n\n1. Since the batch size is small in each experiment, it is hard to compare convergence rate within one epoch.  More iterations should be taken and the log-scale style figure is suggested.  \n\n2. In Figure 1b, L2LBGDBGD converges to a lower objective value, while the other figures are difficult to compare, the convergence value should be reported in all experiments. \n\n3. \u201cThe average recent iterate\u201c described in section 3.6 uses recent 3 iterations to compute the average, the reason to choose \u201c3\u201d, and the effectiveness of different choices should be discussed, as well as the \u201c24\u201d used in state features. \n\n4. Since the block-diagonal structure imposed on A_t, B_t, and F_t, how to choose a proper block size?  Or how to figure out a coordinate group? \n\n5. The caption in Figure 1,3, \u201cwith 48 input and hidden units\u201d should clarify clearly. \nThe curves of different methods are suggested to use different lines (e.g., dashed lines) to denote different algorithms rather than colors only. \n\n6. typo: sec 1 parg 5, \u201ccurrent iterate\u201d -> \u201ccurrent iteration\u201d. \n\n\nConclusion:\n\nSince RL based framework has been proposed in [1] by Li & Malik, this paper tends to solve the high-dimensional problem.  With the new observation of invariant in coordinates permutation in neural networks, this paper imposes the block-diagonal structure in the model to reduce the complexity of LQG algorithm.  Sufficient experiment results show that the proposed method has better convergence rate than [1].  But comparing to [1], this paper has limited contribution. \n\n[1]: Ke Li and Jitendra Malik. Learning to optimize. CoRR, abs/1606.01885, 2016.",1,1,1,1,1,1,1,1,1,-1
BkM27IxR--R3,"Summary of the paper\n---------------------------\nThe paper derives a scheme for learning optimization algorithm for high-dimensional stochastic problems as the one involved in shallow neural nets training.  The main motivation is to learn to optimize with the goal to design a meta-learner able to generalize across optimization problems (related to machine learning applications as learning a neural network) sharing the same properties.  For this sake, the paper casts the problem into reinforcement learning framework and relies on guided policy search (GPS) to explore the space of states and actions.  The states are represented by the iterates, the gradients, the objective function values, derived statistics and features, the actions are the update directions of parameters to be learned.  To make the formulated problem tractable, some simplifications are introduced (the policies are restricted to gaussian distributions family, block diagonal structure is imposed on the involved parameters).  The mean of the stationary non-linear policy of GPS is modeled as a recurrent network with parameters to be learned.  A hatch of how to learn the overall process is presented.  Finally experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach .\n\nComments\n-------------\n- The overall idea of the paper, learning how to optimize, is very seducing and the experimental evaluations (comparison to normal optimizers and other meta-learners) tend to conclude the proposed method is able to learn the behavior of an optimizer and to generalize to unseen problems. \n- Materials of the paper sometimes appear tedious to follow, mainly in sub-sections 3.4 and 3.5.  It would be desirable to sum up the overall procedure in an algorithm.  Page 5, the term $\\omega$ intervening in the definition of the policy $\\pi$ is not defined. \n- The definitions of the statistics and features (state and observation features) look highly elaborated.  Can authors provide more intuition on these precise definitions?  How do they impact for instance changing the time range in the definition of $\\Phi$) in the performance of the meta-learner? \n- Figures 3 and 4 illustrate some oscillations of the proposed approach.  Which guarantees do we have that the algorithm will not diverge as L2LBGDBGD does?  How long should be the training to ensure a good and stable convergence of the method? \n- An interesting experience to be conducted and shown is to train the meta-learner on another dataset (CIFAR for example) and to evaluate its generalization ability on the other sets to emphasize the effectiveness of the method",1,1,1,1,1,-1,1,1,1,-1
BkN_r2lR--R1,"This paper presents an image-to-image cross domain translation framework based on generative adversarial networks.  The contribution is the addition of an explicit exemplar constraint into the formulation which allows best matches from the other domain to be retrieved.  The results show that the proposed method is superior for the task of exact correspondence identification and that AN-GAN rivals the performance of pix2pix with strong supervision. \n\n\nNegatives:\n1.) The task of exact correspondence identification seems contrived.  It is not clear which real-world problems have this property of having both all inputs and all outputs in the dataset, with just the correspondence information between inputs and outputs missing. \n2.) The supervised vs unsupervised experiment on Facades->Labels (Table 3) is only one scenario where applying a supervised method on top of AN-GAN\u2019s matches is better than an unsupervised method.   More transfer experiments of this kind would greatly benefit the paper and support the conclusion that \u201cour self-supervised method performs similarly to the fully supervised method. \u201d \n\nPositives:\n1.) The paper does a good job motivating the need for an explicit image matching term inside a GAN framework. \n2.) The paper shows promising results on applying a supervised method on top of AN-GAN\u2019s matches. \n\nMinor comments:\n1. The paper sometimes uses L1 and sometimes L_1, it should be L_1 in all cases. \n2. DiscoGAN should have the Kim et al citation, right after the first time it is used. I had to look up DiscoGAN to realize it is just Kim et al",1,1,1,1,1,1,1,1,1,1
BkN_r2lR--R2,"The paper presents a method for finding related images (analogies) from different domains based on matching-by-synthesis.  The general idea is interesting and the results show improvements over previous approaches, such as CycleGAN (with different initializations, pre-learned or not).  The algorithm is tested on three datasets. \n\nWhile the approach has some strong positive points, such as good experiments and theoretical insights (the idea to match by synthesis and the proposed loss which is novel, and combines the proposed concepts),;  the paper lacks clarity and sufficient details. \n\nInstead of the longer intro and related work discussion,;  I would prefer to see a Figure with the architecture and more illustrative examples to show that the insights are reflected in the experiments.  Also, the matching part, which is discussed at the theoretical level, could be better explained and presented at a more visual level.  It is hard to understand sufficiently well what the formalism means without more insigh t.\n\nAlso, the experiments need more details. For example, it is not clear what the numbers in Table 2 mean",1,1,1,1,1,1,1,1,1,-1
BkN_r2lR--R3,"This paper adds an interesting twist on top of recent unpaired image translation work.  A domain-level translation function is jointly optimized with an instance-level matching objective.  This yields the ability to extract corresponding image pairs out of two unpaired datasets, and also to potentially refine unpaired translation by subsequently training a paired translation function on the discovered matches.  I think this is a promising direction, but the current paper has unconvincing results, and it\u2019s not clear if the method is really solving an important problem yet. \n\nMy main criticism is with the experiments and results.  The experiments focus almost entirely on the setting where there actually exist exact matches between the two image sets.  Even the partial matching experiments in Section 4.1.2 only quantify performance on the images that have exact matches.  This is a major limitation since the compelling use cases of the method are in scenarios where we do not have exact matches.  It feels rather contrived to focus so much on the datasets with exact matches since,;  1) these datasets actually come as paired data and, in actual practice, supervised translation can be run directly,;  2) it\u2019s hard to imagine datasets that have exact but unknown matches (I welcome the authors to put forward some such scenarios); 3) when exact matches exist, simpler methods may be sufficient, such as matching edges.  There is no comparison to any such simple baselines. \n\nI think finding analogies that are not exact matches is much more compelling.  Quantifying performance in this case may be hard, and the current paper only offers a few qualitative results.  I\u2019d like to see far more results, and some attempt at a metric.  One option would be to run user studies where humans judge the quality of the matches.  The results shown in Figure 2 don\u2019t convince me, not just because they are qualitative and few, but also because I\u2019m not sure I even agree that the proposed method is producing better results:  for example, the DiscoGAN results have some artifacts but capture the texture better in row 3. \n\nI was also not convinced by the supervised second step in Section 4.3. Given that the first step achieves 97% alignment accuracy, it\u2019s no surprised that running an off-the-shelf supervised method on top of this will match the performance of running on 100% correct data.  In other words, this section does not really add much new information beyond what we could already infer given that the first stage alignment was so successful. \n\nWhat I think would be really interesting is if the method can improve performance on datasets that actually do not have ground truth exact matches.  For example, the shoes and handbags dataset or even better, domain adaptation datasets like sim to real. \n\nI\u2019d like to see more discussion of why the second stage supervised problem is beneficial.  Would it not be sufficient to iterate alpha and T iterations enough times until alpha is one-hot and T is simply training against a supervised objective (Equation 7)? \n\nMinor comments:\n1. In the intro, it would be useful to have a clear definition of \u201canalogy\u201d for the present context. \n2. Page 2: a link should be provided for the Putin example, as it is not actually in Zhu et al. 2017.\n3.  Page 3: \u201cWeakly Supervised Mapping\u201d \u2014 I wouldn\u2019t call this weakly supervised. Rather, I\u2019d say it\u2019s just another constraint / prior, similar to cycle-consistency, which was referred to under the \u201cUnsupervised\u201d section. \n4. Page 4 and throughout: It\u2019s hard to follow which variables are being optimized over when.  For example, in Eqn. 7, it would be clearer to write out the min over optimization variables. \n5. Page 6: The Maps dataset was introduced in Isola et al. 2017, not Zhu et al. 2017. \n6. Page 7: The following sentence is confusing and should be clarified:  \u201cThis shows that the distribution matching is able to map source images that are semantically similar in the target domain. \u201d\n7. Page 7: \u201cThis shows that a good initialization is important for this task. \u201d \u2014 Isn\u2019t this more than initialization?  Rather, removing the distributional and cycle constraints changes the overall objective being optimized. \n8. In Figure 2, are the outputs the matched training images, or are they outputs of the translation function? \n9. Throughout the paper, some citations are missing enclosing parentheses",1,1,1,1,1,-1,1,1,1,-1
BkoCeqgR--R1,"The paper proposes and evaluates a method to make neural networks for image recognition color invariant. \n\nThe contribution of the paper is: \n - some proposed methods to extract a color-invariant representation \n - an experimental evaluation of the methods on the cifar 10 dataset \n - a new dataset \""crashed cars \""\n - evaluation of the best method from the cifar10 experiments on the new dataset \n\nPros: \n - the crashed cars dataset is interesting.  The authors have definitely found an interesting untapped source of interesting images. \n\n\nCons: \n- The authors name their method order network but the method they propose is not really parts of the network but simple preprocessing steps to the input of the network.  \n- The paper is incomplete without the appendices.  In fact the paper is referring to specific figures in the appendix in the main text. \n - the authors define color invariance as a being invariant to which specific color an object in an image does have, e.g. whether a car is red or green, but they don't think about color invariance in the broader context - color changes because of lighting, shades, .....  Also, the proposed methods aim to preserve the \""colorfullness\"" of a color.  This is also problematic, because while the proposed method works for a car that is green or a car that is red, it will fail for a car that is black (or white) - because in both cases the \""colorfulness\"" is not relevant.  Note that this is specifically interesting in the context of the task at hand (cars) and many cars being, white, grey (silver), or black.  \n- the difference in the results in table 1 could well come from the fact that in all of the invariant methods except for \""ord\"" the input is a WxHx1 matrix, but for \""ord\"" and \""cifar\"" the input is a \""WxHx3\"" matrix.  This probably leads to more parameters in the convolutions.  \n- the results in the  figure 4: it's very unlikely that the differences reported are actually significant.  It appears that all methods perform approximately the same - and the authors pick a specific line (25k steps) as the relevant one in which the RGB-input space performs best.  The proposed method does not lead to any relevant improvement. \nFigure 6/7: are very hard to read. I am still not sure what exactly they are trying to say. \n\nMinor comments: \n - section 1: \""called for is network\"" -> called for is a network \n - section 1.1: And and -> And \n - section 1.1: Appendix -> Appendix C\n - section 2: Their exists many -> There exist many\n - section 2: these transformation -> these transformations\n - section 2: what does \""the wallpaper groups\"" refer to? \n - section 2: are a groups -> are groups\n - section 3.2: reference to a non-existing figure\n - section 3.2/Training: 2499999 iterations = steps? \n - section 3.2/Training: longer as suggested -> longer than suggeste",1,1,1,1,-1,-1,1,1,1,-1
BkoCeqgR--R2,"The authors investigate a modified input layer that results in color invariant networks.  The proposed methods are evaluated on two car datasets.  It is shown that certain color invariant \""input\"" layers can improve accuracy for test-images from a different color distribution than the training images. \n\n\nThe proposed assumptions are not well motivated and seem arbitrary.  Why is using a permutation of each pixels' color a good idea? \n\nThe paper is very hard to read.  The message is unclear and the experiments to prove it are of very limited scope,;  i.e. one small dataset with the only experiment purportedly showing generalization to red cars. \n\nSome examples of specific issues:\n- the abstract is almost incomprehensible and it is not clear what the contributions are \n- Some references to Figures are missing the figure number, eg. 3.2 first paragraph,  \n- It is not clear how many input channels the color invariant functions use, eg. p1 does it use only one channel and hence has fewer parameters? \n- are the training and testing sets all disjoint (sec 4.3)?\n- at random points figures are put in the appendix, even though they are described in the paper and seem to show key results (eg \""tested on nored-test\"") \n- Sec 4.6: The explanation for why the accuracy drops for all models is not clear.  Is it because the total number of training images drops?  If that's the case the whole experimental setup seems flawed. \n- Sec 4.6: the authors refer to the \""order net\"" beating the baseline, however, from Fig 8 (right most) it appears as if all models beat the baseline.  In the conclusion they say that weighted order net beats the baseline on all three test sets w/o red cars in the training set.  Is that Fig 8 @0%?  The baseline seems to be best performing on \""all cars\"" and \""non-red cars\""\n\nIn order to be at an appropriate level for any publication the experiments need to be much more general in scope.\n",1,1,1,1,-1,-1,1,1,1,-1
BkoCeqgR--R3,"The authors test a CNN on images with color channels modified (such that the values of the three channels, after modification, are invariant to permutations). \n\nThe main positive point is that the performance does not degrade too much.  However, there are several important negative points which should prevent this work, as it is, from being published. \n\n1. Why is this type of color channel modification relevant for real life vision?  The invariance introduced here does not seem to be related to any real world phenomenon.  The nets, in principle, could learn to recognize objects based on shape only, and the shape remains stable when the color channels are changed. \n\n2. Why is the crash car dataset used in this scenario?  It is not clear to me why this types of theoretical invariance is tested on such as specific dataset.  Is there a real reason for that? \n\n3. The writing could be significantly improved, both at the grammatical level and the level of high level organization and presentation.  I think the authors should spend time on better motivating the choice of invariance used, as well as on testing with different (potentially new) architectures, color change cases, and datasets. \n\n4. There is no theoretical novelty and the empirical one seems to be very limited, with less convincing results.",1,1,1,1,1,-1,1,1,1,-1
BkoXnkWAb-R1,"The paper proposed a new activation function that tries to alleviate the use of  other form of normalization methods for RNNs.  The activation function keeps the activation roughly zero-centered.  \n\nIn general, this is an interesting direction to explore, the idea is interesting,;  however, I would like to see more experiments. \n\n1. The authors tested out this new activation function on RNNs.  It would be interesting to see the results of the new activation function on LSTM. \n\n2. The experimental results are fairly weak compared to the other methods that also uses many layers.  For PTB and Text8, the results are comparable to recurrent batchnorm with similar number of parameters, however the recurrent batchnorm model has only 1 layer, whereas the proposed architecture has 36 layers.   \n\n3.  It would also be nice to show results on tasks that involve long term dependencies, such as speech modeling. \n\n4. If the authors could test out the new activation function on LSTMs, it would be interesting to perform a comparison between LSTM baseline, LSTM + new activation function, LSTM + recurrent batch norm. \n\n5. It would be nice to see the gradient flow with the new activation function compared to the ones without. \n\n6. The theorems and proofs are rather preliminary, they may not necessarily have to be presented as theorems.",1,1,1,1,1,-1,1,1,1,-1
BkoXnkWAb-R2,"This paper proposes a self-normalizing bipolar extension for the ReLU activation family.  For every neuron out of two, authors propose to preserve the negative inputs.  Such activation function allows to shift the mean of i.i.d. variables to zeros in the case of ReLU or to a given saturation value in the case of ELU. \n\nCombined with variance preserving initialization scheme, authors empirically observe that the bipolar ReLU allows to better preserve the mean and variance of the activations through training compared to regular ReLU for a deep stacked RNN. \n\nAuthors evaluate their bipolar activation on PTB and Text8 using a deep stacked RNN.   They show that bipolar activations allow to train deeper RNN (up to some limit) and leads to better generalization performances compared to the ReLU /ELU activation functions.  They also show that they can train deep residual network architecture on CIFAR without the use of BN. \n\nQuestion:\n- Which layer mean and variance are reported in Figure 2?  What is the difference between the left and right plots? \n- In Table 1, we observe that ReLU-RNN (and BELU-RNN for very deep stacked RNN) leads to worst validation performances.  It would be nice to report the training loss to see if this is an optimization or a generalization problem. \n- How does bipolar activation compare to model train with BN on CIFAR10? \n- Did you try bipolar activation function for gated recurrent neural networks for LSTM or GRU? \n- As stated in the text, BELU-RNN outperforms BN-LSTM for PTB.  However, BN-LSTM outperforms BELU-RNN on Text8.  Do you know why the trend is not consistent across datasets? \n\n-Clarity/Quality\nThe paper is well written and pleasant to read. \n\n\n- Originality:\nSelf-normalizing function have been explored also in scaled ELU, however the application of self-normalizing function to RNN seems novel. \n\n- Significance:\nActivation function is still a very active research topic and self-normalizing function could potentially be impactful for RNN given that the normalization approaches (batch norm, layer norm) add a significant computational cost.  In this paper, bipolar activations are used to train very deep stacked RNN.  However, the stacked RNN with bipolar activation are not competitive regarding to other recurrent architectures.  It is not clear what are the advantage of deep stacked RNN in that context.",1,1,1,1,1,-1,1,1,1,-1
BkoXnkWAb-R3,"Summary:\nThis paper proposes a simple recipe to preserve proximity to zero mean for activations in deep neural networks.  The proposal is to replace the non-linearity in half of the units in each layer with its \""bipolar\"" version -- one that is obtained by flipping the function on both axes. \nThe technique is tested on deep stacks of recurrent layers, and on convolutional networks with depth of 28, showing that improved results over the baseline networks are obtained.  \n\nClarity:\nThe paper is easy to read.  The plots in Fig. 2 and the appendix are quite helpful in improving presentation.  The experimental setups are explained in detail.  \n\nQuality and significance:\nThe main idea from this paper is simple and intuitive.  However, the experiments to support the idea do not seem to match the motivation of the paper.  As stated in the beginning of the paper, the motivation behind having close to zero mean activations is that this is expected to speed up training using gradient descent.  However, the presented results focus on the performance on held-out data instead of improvements in training speed.  This is especially the case for the RNN experiments. \n\nFor the CIFAR-10 experiment, the training loss curves do show faster initial progress in learning.  However, it is unclear that overall training time can be reduced with the help of this technique.  To evaluate this speed up effect, the dependence on the choice of learning rate and other hyperparameters should also be considered. \n\nNevertheless, it is interesting to note the result that the proposed approach converts a deep network that does not train into one which does in many cases.  The method appears to improve the training for moderately deep convolutional networks without batch normalization (although this is tested on a single dataset),;  but is not practically useful yet since the regularization benefits of Batch Normalization are also taken away",1,1,1,1,1,-1,1,1,1,-1
BkpiPMbA--R2,"Compared to previous studies, this paper mainly claims that the information from larger neighborhoods (more directions or larger distances) will better characterize the relationship between adversarial examples and the DNN model. \n\nThe idea of employing ensemble of classifiers is smart and effective.  I am curious about the efficiency of the method. \n\nThe experimental study is extensive.  Results are well discussed with reasonable observations.  In addition to examining the effectiveness, authors also performed experiments to explain why OPTMARGIN is superior.  Authors are suggested to involve more datasets to validate the effectiveness of the proposed method. \n\nTable 5 is not very clear.  Authors are suggested to discuss in more detail",1,1,1,1,1,-1,1,1,1,-1
BkpiPMbA--R3,"The paper presents a new approach to generate adversarial attacks to a neural network, and subsequently present a method to defend a neural network from those attacks.  I am not familiar with other adversarial attack strategies aside from the ones mentioned in this paper, and therefore I cannot properly assess how innovative the method is. \n\nMy comments are the following:\n\n1- I would like to know if benign examples are just regular examples or some short of simple way of computing adversarial attacks. \n\n2- I think the authors should provide a more detailed and formal description of the OPTMARGIN method.  In section 3.2 they explain that \""Our attack uses existing optimization attack techniques to...\"", but one should be able to understand the method without reading further references.  Specially a formal representation of the method should be included. \n\n3- Authors mention that OPTSTRONG attack does not succeed in finding adversarial examples (\""it succeeds on 28% of the samples on MNIST;73% on CIFAR-10\"").  What is the meaning of success rate in here?  Is it the % of times that the classifier is confused? \n\n4- OPTSTRONG produces images that are notably more distorted than OPTBRITTLE (by RMS and also visually in the case of MNIST).  So I actually cannot tell which method is better, at least in the MNIST experiment.  One could do a method that completely distort the image and therefore will be classified with as a class.  But adversarial images should be visually undistinguishable from original images.  Generated CIFAR images seem similar than the originals, although CIFAR images are very low resolution, so judging this is hard. \n\n4- As a side note, it would be interesting to have an explanation about why region classification is providing a worse accuracy than point classification for CIFAR-10 benign samples. \n\nAs a summary, the authors presented a method that successfully attacks other existing defense methods, and present a method that can successfully defend this attack.  I would like to see more formal definitions of the methods presented.  Also, just by looking at RMS it is expected that this method works better than OPTBRITTLE, since the images are more distorted.  It would be needed to have a way of visually evaluate the similarity between original images and generated images",1,1,1,1,1,-1,1,1,-1,-1
BkPrDFgR--R1,"The paper studies methods for verifying neural nets through their piecewise\nlinear structure.  The authors survey different methods from the literature,\npropose a novel one, and evaluate them on a set of benchmarks. \n\nA major drawback of the evaluation of the different approaches is that\neverything was used with its default parameters.  It is very unlikely that these\ndefaults are optimal across the different benchmarks.  To get a better impression\nof what approaches perform well, their parameters should be tuned to the\nparticular benchmark.  This may significantly change the conclusions drawn from\nthe experiments. \n\nFigures 4-7 are hard to interpret and do not convey a clear message.  There is no\nclear trend in many of them and a lot of noise.  It may be better to relate the\nstructure of the network to other measures of the hardness of a problem, e.g.\nthe phase transition.  Again parameter tuning would potentially change all of\nthese figures significantly, as would e.g. a change in hardware.  Given the kind\nof general trend the authors seem to want to show here, I feel that a more\ntheoretic measure of problem hardness would be more appropriate here. \n\nThe authors say of the proposed TwinStream dataset that it \""may not be\nrepresentative of real use-cases\"". It seems odd to propose something that is\nentirely artificial. \n\nThe description of the empirical setup could be more detailed.  Are the\nproperties that are being verified different properties, or the same property on\ndifferent networks? \n\nThe tables look ugly.  It seems that the header \""data set\"" should be \""approach\""\nor something similar. \n\nIn summary, I feel that while there are some issues with the paper, it presents\ninteresting results and can be accepted.",1,1,1,1,1,-1,1,1,1,-1
BkPrDFgR--R2,"Summary:\n\nThis paper:\n- provides a compehensive review of existing techniques for verifying properties of neural networks. \n- introduces a simple branch-and-bound approach. \n- provides fairly extensive experimental comparison of their method and 3 others (Reluplex, Planet, MIP) on 2 existing benchmarks and a new synthetic one. \n\nRelevance: Although there isn't any learning going on, the paper is relevant to the conference. \n\nClarity: Writing is excellent, the content is well presented and the paper is enjoyable read. \n\nSoundness: As far as I can tell, the work is sound. \n\nNovelty: This is in my opinion the weakest point of the paper.  There isn't really much novelty in the work.  The branch&bound method is fairly standard, two benchmarks were already existing and the third one is synthetic with weights that are not even trained (so not clear how relevant it is).  The main novel result is the experimental comparison, which does indeed show some surprising results (like the fact that BaB works so well). \n\nSignificance: There is some value in the experimental results, and it's great to see you were able to find bugs in existing methods.  Unfortunately, there isn't much insight to be gained from them.  I couldn't see any emerging trend/useful recommendations (like \""if your problem looks like X, then use algorithm B\"").  This is unfortunately often the case when dealing with combinatorial search/optimization",1,1,1,1,-1,1,1,1,1,-1
BkPrDFgR--R3,"The paper compares some recently proposed method for validation of properties\nof piece-wise linear neural networks and claims to propose a novel method for\nthe same.  Unfortunately, the proposed \""branch and bound method\"" does not explain\nhow to implement the \""bound\"" part (\""compute lower bound\"") -- and has been used \nseveral times in the same application,;  incl.:\n\nRuediger Ehlers. Planet. https://github.com/progirep/planet,\nChih-Hong Cheng, Georg Nuhrenberg, and Harald Ruess.  Maximum resilience of artificial neural networks. Automated Technology for Verification and Analysis\nAlessio Lomuscio and Lalit Maganti.  An approach to reachability analysis for feed-forward relu neural networks. arXiv:1706.07351 \n\nSpecifically, the authors say: \""In our experiments, we use the result of \nminimising the variable corresponding to the output of the network, subject \nto the constraints of the linear approximation introduced by Ehlers (2017a)\""\nwhich sounds a bit like using linear programming relaxations, which is what\nthe approaches using branch and bound cited above use.  If that is the case,\nthe paper does not have any original contribution.  If that is not the case,\nthe authors may have some contribution to make, but have not made it in this\npaper, as it does not explain the lower bound computation other than the one\nbased on LPs. \n\nGenerally, I find a jarring mis-fit between the motivation (deep learning\nfor driving, presumably involving millions or billions of parameters) and\nthe actual reach of the methods proposed (hundreds of parameters). \nThis reach is NOT inherent in integer programming, per se.  Modern solvers\nroutinely solve instances with tens of millions of non-zeros in the constraint\nmatrix, but require a strong relaxation.  The authors may hence consider\nimproving the LP relaxation, noting that the big-M constraint are notorious\nfor producing weak relaxations.",1,1,1,1,1,1,1,1,1,-1
BkpXqwUTZ-R1,"- This paper is not well written and incomplete.  There is no clear explanation of what exactly the authors want to achieve in the paper, what exactly is their approach/contribution, experimental setup, and analysis of their results.  \n\n- The paper is hard to read due to many abbreviations, e.g., the last paragraph in page 2.  \n\n- The format is inconsistent. Section 1 is numbered, but not the other sections.  \n\n- in page 2, what do the numbers mean at the end of each sentence? Probably the figures?  \n\n- in page 2, \""in this figure\"": which figure is this referring to? \n\n\nComments on prior work:\n\np 1: authors write: \""vanilla backpropagation (VBP)\"" \""was proposed around 1987 Rumelhart et al. (1985). \"" \n\nNot true. A main problem with the 1985 paper is that it does not cite the inventors of backpropagation.  The VBP that everybody is using now is the one published by  Linnainmaa in 1970, extending Kelley's work of 1960.  The first to publish the application of VBP to NNs was Werbos in 1982. Please correct.  \n\np 1: authors write: \""Almost at the same time, biologically inspired convolutional networks was also introduced as well using VBP LeCun et al. (1989). \""\n\nHere one must cite the person who really invented this biologically inspired convolutional architecture (but did not apply backprop to it): Fukushima (1979). He is cited later, but in a misleading way. Please correct. \n\np 1: authors write: \""Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012). \"" \n\nNot true. Deep Learning was introduced by Ivakhnenko and Lapa in 1965: the first working method for learning in multilayer perceptrons of arbitrary depth. Please correct.(The term \""deep learning\"" was introduced to ML in 1986 by Dechter for something else.)  \n\np1: authors write: \""Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015). \"" \n\nHighway networks were published half a year earlier than resnets, and reached many hundreds of layers before resnets. Please correct. \n\n\nGeneral recommendation: Clear rejection for now.  But perhaps the author want to resubmit this to another conference, taking into account the reviewer comments.\n\n",1,1,1,1,-1,1,1,1,-1,1
BkpXqwUTZ-R2,"The paper falls far short of the standard expected of an ICLR submission.  \n\nThe paper has little to no content.  There are large sections of blank page throughout.  The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description.  The experiments are only performed on MNIST.  The subfigures are not labeled.  The paper over-uses acronyms; sentences like \u201cIn this figure, VBP, VBP with FBA, and ITD using FBA for VBP\u2026\u201d are painful to read",1,1,-1,1,-1,-1,1,1,-1,-1
BkpXqwUTZ-R3,"The paper is incomplete and nowhere near finished, it should have been withdrawn . \n\nThe theoretical results are presented in a bitmap figure and only referred to in the text (not explained),  and  the results on datasets are not explained either (and pretty bad). A waste of my time.",1,-1,-1,1,-1,-1,-1,1,-1,-1
BkQCGzZ0--R1,"The topic is interesting however the description in the paper is lacking clarity.  The paper is written in a procedural fashion - I first did that, then I did that and after that I did third.  Having proper mathematical description and good diagrams of what you doing would have immensely helped.  Another big issue is the lack of proper validation in Section 3.4.  Even if you do not know what metric to use to objectively compare your approach versus baseline there are plenty of fields suffering from a similar problem yet  doing subjective evaluations, such as listening tests in speech synthesis.  Given that I see only one example I can not objectively know if your model produces examples like that 'each' time so having just one example is as good as having none.",1,1,1,1,1,-1,1,1,-1,-1
BkQCGzZ0--R2,"This is an interesting paper focusing on building discrete reprentations of sequence by autoencoder.  \nHowever, the experiments are too weak to demonstrate the effectiveness of using discrete representations. \nThe design of the experiments on language model is problematic. \nThere are a few interesting points about discretizing the represenations by saturating sigmoid and gumbel-softmax, but the lack of comparisons to benchmarks is a critical defect of this paper.  \n\n\nGenerally, continuous vector representations are more powerful than discrete ones, but discreteness corresponds to some inductive biases that might help the learning of deep neural networks, which is the appealing part of discrete representations, especially the stochastic discrete representations.  \nHowever, I didn't see the intuitions behind the model that would result in its superiority to the continuous counterpart.  \nThe proposal of DSAE might help evaluate the usage of the 'autoencoding function' c(s), but it is certainly not enough to convince people.  \nHow is the performance if c(s) is replaced with the representations achieved from autoencoder, variational autoencoder or simply the sentence vectors produced by language model? \nThe qualitative evaluation on 'Deciperhing the Latent Code' is not enough either.  \nIn addition, the language model part doesn't sound correct, because the model cheated on seeing the further before predicting the words autoregressively. \nOne suggestion is to change the framework to variational auto-encoder, otherwise anything related to perplexity is not correct in this case. \n\nOverall, this paper is more suitable for the workshop track . It also needs a lot of more studies on related work.",1,1,1,1,1,-1,1,1,-1,-1
BkQCGzZ0--R3,"The authors describe a method for encoding text into a discrete representation / latent space . On a measure that they propose, they should it outperforms an alternative Gumbel-Softmax method for both language modeling and NMT. \n\nThe proposed method seems effective, and the proposed DSAE metric is nice, though it\u2019s surprising if previous papers have not used metrics similar to normalized reduction in log-ppl . The datasets considered in the experiments are also large, another plus.  However, overall, the paper is difficult to read and parse, especially since low-level details are weaved together with higher-level points throughout, and are often not motivated. \n\nThe major critique would be the qualitative nature of results in the sections on \u201cDecipering the latent code\u201d and (to a lesser extent) \u201cMixed sample-beam decoding. \u201d These two sections are simply too anecdotal, although it is nice being stepped through the reasoning for the single example considered in Section 3.3.  Some quantitative or aggregate results are needed, and it should at least be straightforward to do so using human evaluation for a subset of examples for diverse decoding",1,1,1,1,1,-1,1,1,1,-1
BkQqq0gRb-R1,"Overall, the idea of this paper is simple but interesting.  Via performing variational inference in a kind of online manner, one can address continual learning for deep discriminative or generative networks with considerations of model uncertainty. \n\nThe paper is written well,  and literature review is sufficient.  My comment is mainly about its importance for large-scale computer vision applications.  The neural networks in the experiments are shallow. \n",1,1,-1,1,-1,1,1,-1,1,-1
BkQqq0gRb-R2,"This paper proposes a new method, called VCL, for continual learning.  This method is a combination of the online variational inference for streaming environment with Monte Carlo method.  The authors further propose to maintain a coreset which consists of representative data points from the past tasks.  Such a coreset is used for the main aim of avoiding the catastrophic forgetting problem in continual learning.  Extensive experiments shows that VCL performs very well, compared with some state-of-the-art methods.  \n\nThe authors present two ideas for continual learning in this paper: (1) Combination of online variational inference and sampling method, (2) Use of coreset to deal with the catastrophic forgetting problem.  Both ideas have been investigated in Bayesian literature, while (2) has been recently investigated in continual learning.  Therefore, the authors seems to be the first to investigate the effectiveness of (1) for continual learning.  From extensive experiments, the authors find that the first idea results in VCL which can outperform other state-of-the-art approaches, while the second idea plays little role.  \n\nThe finding of the effectiveness of idea (1) seems to be significant.  The authors did a good job when providing a clear presentation, a detailed analysis about related work, an employment to deep discriminative models and deep generative models, and a thorough investigation of empirical performance. \n\nThere are some concerns the authors should consider:\n- Since the coreset plays little role in the superior performance of VCL, it might be better if the authors rephrase the title of the paper.  When the coreset is empty, VCL turns out to be online variational inference [Broderich et al., 2013; Ghahramani & Attias, 2000].  Their finding of the effectiveness of online variational inference for continual learning should be reflected in the writing of the paper as well. \n- It is unclear about the sensitivity of VCL with respect to the size of the coreset. The authors should investigate this aspect. \n- What is the trade-off when the size of the coreset increases?\n",1,1,1,1,1,1,1,1,1,-1
BkQqq0gRb-R3,"The paper describes the problem of continual learning, the non-iid nature of most real-life data and point out to the catastrophic forgetting phenomena in deep learning.  The work defends the point of view that Bayesian inference is the right approach to attack this problem and address difficulties in past implementations.  \n\nThe paper is well written,  the problem is described neatly in conjunction with the past work,  and the proposed algorithm is supported by experiments.  The work is a useful addition to the communit y.\n\nMy main concern focus on the validity of the proposed model in harder tasks such as the Atari experiments in Kirkpatrick et. al. (2017) or the split CIFAR experiments in Zenke et. al. (2017).  Even though the experiments carried out in the paper are important, they fall short of justifying a major step in the direction of the solution for the continual learning problem.",1,1,1,1,1,1,1,1,1,-1
BkrsAzWAb-R1,"SUMMARY:\n\nThe authors reinvent a 20 years old technique for adapting a global or component-wise learning rate for gradient descent.  The technique can be derived as a gradient step for the learning rate hyperparameter, or it can be understood as a simple and efficient adaptation technique. \n\n\nGENERAL IMPRESSION:\n\nOne central problem of the paper is missing novelty.  The authors are well aware of this. They still manage to provide added value. \nDespite its limited novelty, this is a very interesting and potentially impactful paper.  I like in particular the detailed discussion of related work, which includes some frequently overlooked precursors of modern methods. \n\n\nCRITICISM:\n\nThe experimental evaluation is rather solid, but not perfect.  It considers three different problems: logistic regression (a convex problem), and dense as well as convolutional networks. That's a solid spectrum.  However, it is not clear why the method is tested only on a single data set: MNIST.  Since it is entirely general, I would rather expect a test on a dozen different data sets.  That would also tell us more about a possible sensitivity w.r.t. the hyperparameters \\alpha_0 and \\beta. \n\nThe extensions in section 5 don't seem to be very useful.  In particular, I cannot get rid of the impression that section 5.1 exists for the sole purpose of introducing a convergence theorem.  Analyzing the actual adaptive algorithm would be very interesting.  In contrast, the present result is trivial and of no interest at all, since it requires knowing a good parameter setting, which defeats a large part of the value of the method. \n\n\nMINOR POINTS:\n\npage 4, bottom: use \\citep for Duchi et al. (2011). \n\nNone of the figures is legible on a grayscale printout of the paper.  Please do not use color as the only cue to identify a curve. \n\nIn figure 2, top row, please display the learning rate on a log scale. \n\npage 8, line 7 in section 4.3: \""the the\"" (unintended repetition) \n\nEnd of section 4: an increase from 0.001 to 0.001002 is hardly worth reporting - or am I missing something",1,1,1,1,1,1,1,1,1,-1
BkrsAzWAb-R2,"The authors consider a method (which they trace back to 1998, but may have a longer history) of learning the learning rate of a first-order algorithm at the same time as the underlying model is being optimized, using a stochastic multiplicative update.  The basic observation (for SGD) is that if \\theta_{t+1} = \\theta_t - \\alpha \\nabla f(\\theta_t), then \\partial/\\partial\\alpha f(\\theta_{t+1}) = -<\\nabla f(\\theta_t), \\nabla f(\\theta_{t+1})>, i.e. that the negative inner product of two successive stochastic gradients is equal in expectation to the derivative of the tth update w.r.t. the learning rate \\alpha. \n\nI have seen this before for SGD (the authors do not claim that the basic idea is novel), but I believe that the application to other algorithms (the authors explicitly consider Nesterov momentum and ADAM) are novel, as is the use of the multiplicative and normalized update of equation 8 (particularly the normalization). \n\nThe experiments are well-presented, and appear to convincingly show a benefit.  Figure 3, which explores the robustness of the algorithms to the choice of \\alpha_0 and \\beta, is particularly nicely-done, and addresses the most natural criticism of this approach (that it replaces one hyperparameter with two). \n\nThe authors highlight theoretical convergence guarantees as an important future work item, and the lack of them here (aside from Theorem 5.1, which just shows asymptotic convergence if the learning rates become sufficiently small) is a weakness, but not, I think, a critical one.  This appears to be a promising approach, and bringing it back to the attention of the machine learning community is valuable.",1,1,1,1,1,1,1,1,1,-1
BkrsAzWAb-R3,"\nThis paper revisits an interesting and important trick to automatically adapt the stepsize.  They consider the stepsize as a parameter to be optimized and apply stochastic gradient update for the stepsize.  Such simple trick alleviates the effort in tuning stepsize, and can be incorporated with popular stochastic first-order optimization algorithms, including SGD, SGD with Nestrov momentum, and Adam. Surprisingly, it works well in practice. \n\nAlthough the theoretical analysis is weak that theorem 1 does not reveal the main reason for the benefits of such trick, considering their performance, I vote for acceptance.  But before that, there are several issues need to be addressed.  \n\n1, the derivation of the update of \\alpha relies on the expectation formulation.  I would like to see the investigation of the effect of the size of minibatch to reveal the variance of the gradient in the algorithm combined with such trick.  \n\n2, The derivation of the multiplicative rule of HD relies on a reference I cannot find. Please include this part for self-containing.  \n\n3, As the authors claimed, the Maclaurin et.al. 2015 is the most related work, however, they are not compared in the experiments.  Moreover, the empirical comparisons are only conducted on MNIST.  To be more convincing, it will be good to include such competitor and comparing on practical applications on CIFAR10/100 and ImageNet.  \n\nMinors: \n\nIn the experiments results figures, after adding the new trick, the SGD algorithms become more stable, i.e., the variance diminishes.  Could you please explain why such phenomenon happens?",1,1,1,1,1,1,1,1,1,-1
BkrSv0lA--R1,"In this paper, the authors propose a method of compressing network by means of weight ternarization.  The network weights ternatization is formulated in the form of loss-aware quantization, which originally proposed by Hou et al. (2017). \n\nTo this reviewer\u2019s understanding, the proposed method can be regarded as the extension of the previous work of LAB and TWN, which can be the main contribution of the work. \n\nWhile the proposed method achieved promising results compared to the competing methods, it is still necessary to compare their computational complexity, which is one of the main concerns in network compression. \n\nIt would be appreciated to have discussion on the results in Table 2, which tells that the performance of quantized networks is better than the full-precision network",1,1,1,1,1,1,1,1,1,-1
BkrSv0lA--R2,"This paper proposes a new method to train DNNs with quantized weights, by including the quantization as a constraint in a proximal quasi-Newton algorithm, which simultaneously learns a scaling for the quantized values (possibly different for positive and negative weights).  \n\nThe paper is very clearly written, and the proposal is very well placed in the context of previous methods for the same purpose.  The experiments are very clearly presented and solidly designed. \n\nIn fact, the paper is a somewhat simple extension of the method proposed by Hou, Yao, and Kwok (2017), which is where the novelty resides.  Consequently, there is not a great degree of novelty in terms of the proposed method, and the results are only slightly better than those of previous methods.\ n\nFinally, in terms of analysis of the algorithm, the authors simply invoke a theorem from Hou, Yao, and Kwok (2017), which claims convergence of the proposed algorithm.  However, what is shown in that paper is that the sequence of loss function values converges, which does not imply that the sequence of weight estimates also converges, because of the presence of a non-convex constraint ($b_j^t \\in Q^{n_l}$).  This may not be relevant for the practical results, but to be accurate, it can't be simply stated that the algorithm converges, without a more careful analysis.",1,1,1,1,1,1,1,1,1,-1
BkrSv0lA--R3,"This paper extends the loss-aware weight binarization scheme to ternarization and arbitrary m-bit quantization and demonstrate its promising performance in the experiments. \n\nReview:\n\nPros\nThis paper formulates the weight quantization of deep networks as an optimization problem in the perspective of loss and solves the problem with a proximal newton algorithm.   They extend the scheme to allow the use of different scaling parameters and to m-bit quantization.  Experiments demonstrate the proposed scheme outperforms the state-of-the-art methods.  \n\nThe experiments are complete and the writing is good. \n\nCons\nAlthough the work seems convincing, it is a little bit straight-forward derived from the original binarization scheme (Hou et al., 2017) to tenarization or m-bit since there are some analogous extension ideas (Lin et al., 2016b, Li & Liu, 2016b) . Algorithm 2 and section 3.2 and 3.3 can be seen as additive complementary. \n",1,1,1,1,-1,1,1,-1,1,-1
BkS3fnl0W-R1,"The idea of using GANs for outlier detection is interesting and the problem is relevant.  However, I have the following concerns about the quality and the significance:\n- The proposed formulation in Equation (2) is questionable.  The authors say that this is used to generate outliers, and since it will generate inliers when convergence, the authors propose the technique of early stopping in Section 4.1 to avoid convergence.  However, then what is learned though the proposed formulation?  Since this approach is not straightforward, more theoretical analysis of the proposed method is desirable. \n- In addition to the above point, I guess the expectation is needed as the original formulation of GAN.  Otherwise the proposed formulation does not make sense as it receives only specific data points and how to accumulate objective values across data points is not defined. \n- In experiments, although the authors say \""lots of datasets are used\"", only two datasets are used, which is not enough to examine the performance of outlier detection methods.  Moreover, outliers are artificially generated in these datasets, hence there is no evaluation on pure real-world datasets.  To achieve the better quality of the paper, I recommend to add more real-world datasets in experiments. \n- As discussed in Section 2, there are already many outlier detection methods, such as distance-based outlier detection methods, but they are not compared in experiments. \n  Although the authors argue that distance-based outlier detection methods do not work well for high-dimensional data, this is not always correct .\n  Please see the paper:\n  -- Zimek, A., Schubert, E., Kriegel, H.-P., A survey on unsupervised outlier detection in high-dimensional numerical data, Statistical Analysis and Data Mining (2012)\n  This paper shows that the performance gets even better for higher dimensional data if each feature is relevant. \n  I recommend to add some distance-based outlier detection methods as baselines in experiments.   \n- Since parameter tuning by cross validation cannot be used due to missing information of outliers, it is important to examine the sensitivity of the proposed method with respect to changes in its parameters (a_new, lambda, and others).  Otherwise in practice how to set these parameters to get better results is not obvious. \n\n* The clarity of this paper is not high as the proposed method is not well explained.  In particular, please mathematically formulate each proposed technique in Section 4. \n\n* Since the proposed formulation is not convincing due to the above reasons and experimental evaluation is not thorough, the originality is not high. \n\nMinor comments:\n- P.1, L.5 in the third paragraph: architexture -> architecture \n- What does \""Cor\"" of CorGAN mean? \n\nAFTER REVISION\nThank you to the authors for their response and revision.  Although the paper has been improved, I keep my rating due to the insufficient experimental evaluation",1,1,1,1,1,1,1,1,1,-1
BkS3fnl0W-R2,"The idea of the paper is to use a GAN-like training to learn a novelty detection approach.  In contrast to traditional GANs, this approach does not aim at convergence, where the generator has nicely learned to fool the discriminator with examples from the same data distribution.  The goal is to build up a series of generators that sample examples close the data distribution boundary but are regarded as outliers . To establish such a behavior, the authors propose early stopping as well as other heuristics.  \n\nI like the idea of the paper,;  however, this paper needs a revision in various aspects, which I simply list in the following:; \n* The authors do not compare with a lot of the state-of-the-art in outlier detection and the obvious baselines: SVDD/OneClassSVM without PCA, Gaussian Mixture Model, KNFST, Kernel Density Estimation, etc\n* The model selection using the AUC of \""inlier accepted fraction\"" is not well motivated in my opinion.  This model selection criterion basically leads too a probability distribution with rather steep borders and indirectly prevents the outlier to be too far away from the positive data.  The latter is important for the GAN-like training. \n* The experiments are not sufficient: Especially for multi-class classification tasks, it is easy to sample various experimental setups for outlier detection. This allows for robust performance comparison.  \n* With the imbalanced training as described in the paper, it is quite natural that the confidence threshold for the classification decision needs to be adapted (not equal to 0.5) \n* There are quite a few heuristic tricks in the paper and some of them are not well motivated and analyzed (such as the discriminator training from multiple generators) \n* A cross-entropy loss for the autoencoder does not make much sense in my opinion (?) \n\n\nMinor comments:\n* Citations should be fixed (use citep to enclose them in ()) \n* The term \""AI-related task\"" sounds a bit too broad \n* The authors could skip the paragraph in the beginning of page 5 on the AUC performance. AUC is a standard choice for evaluation in outlier detection. \n* Where is Table 1? \n* There are quite a lot of typos. \n\n*After revision statement*\nI thank the authors for their revision, but I keep my rating.  The clarity of the paper has improved;  but the experimental evaluation is lacking realistic datasets and further simple baselines (as also stated by the other reviewers)",1,1,1,1,1,1,1,1,1,-1
BkS3fnl0W-R3,"This paper addresses the problem of one class classification.  The authors suggest a few techniques to learn how to classify samples as negative (out of class) based on tweaking the GAN learning process to explore large areas of the input space which are out of the objective class. \n\nThe suggested techniques are nice and show promising results.  But I feel a lot can still be done to justify them, even just one of them.  For instance, the authors manipulate the objective of G using a new parameter alpha_new and divide heuristically the range of its values.  But, in the experimental section results are shown only for a  single value, alpha_new=0.9 The authors also suggest early stopping but again (as far as I understand) only a single value for the number of iterations was tested.  \n\nThe writing of the paper is also very unclear, with several repetitions and many typos e.g.:\n\n'we first introduce you a'\n'architexture'\n'future work remain to'\n'it self'\n\nI believe there is a lot of potential in the approach(es) presented in the paper.  In my view a much stronger experimental section together with a clearer presentation and discussion could overcome the lack of theoretical discussion",1,1,1,1,1,-1,1,1,1,-1
BkTQ8UckG-R1,"The paper describes learning joint embedding of sentences and images.  The main point is in using a triplet loss that is applied to hardest-negatives, instead of averaging over all triplets.  This led to improvement over SOTA in a task of caption ranking on MS-COCO, nd good performance on flickr30K.  \n\nThe main issue with this paper is novelty. Using hard negatives is routinely  used in many embedding tasks, and has been discussed in many publications.  For instance recently Wu et all in ICCV2017, bu also many other papers.  \nWhen used in practice wit real-world datasets, taking the max (hardest negative) tends to be very sensitive to label noise, since the hardest negative is sometime just a positive sample with incorrect label.  In these cases focusing on the hardest negative reduces performance.  \n\nWhile it is good to know that using hard negatives improves recall measures on coco, it is not clear that this paper provides enough novel insight to be interesting enough for the ICLR audience.  It may be a better fit in a conference that stresses empirical performance, like in machine vision conferences. \n",1,1,1,1,-1,1,1,1,1,-1
BkUDW_lCb-R1,"The paper claims to develop a novel method to map natural language queries to SQL.  They claim to have the following contributions: \n\n1. Using a grammar to guide decoding  \n2. Using a new loss function for pointer / copy mechanism.  For each output token, they aggregate scores for all positions that the output token can be copied from. \n\nI am confident that point 1 has been used in several previous works.  Although point 2 seems novel, I am not convinced that it is significant enough for ICLR.  I was also not sure why there is a need to copy items from the input question, since all SQL query nouns will be present in the SQL table in some form.   What will happen if we restrict the copy mechanism to only copy from SQL table. \n\nThe references need work. There are repeated entries for the same reference (one form arxiv and one from conference).  Please cite the conference version if one is available, many arxiv references have conference versions. \n\nRebuttal Response: I am still not confident about the significance of contribution 1, so keeping the score the same.",1,1,1,1,1,1,1,1,1,1
BkUDW_lCb-R2,"This paper proposes a model for solving the WikiSQL dataset that was released recently. \n\nThe main issues with the paper is that its contributions are not new. \n\n* The first claimed contribution is to use typing at decoding time (they don't say why but this helps search and learning).  Restricting the type of the decoded tokens based on the programming language has already been done by the Neural Symbolic Machines of Liang et al. 2017.  Then Krishnamurthy et al. expanded that in EMNLP 2017 and used typing in a grammar at decoding time.  I don't really see why the authors say their approach is simpler, it is only simpler because the sub-language of sql used in wikisql makes doing this in an encoder-decoder framework very simple, but in general sql is not regular.  Of course even for CFG this is possible using post-fix notation or fixed-arity pre-fix notation of the language as has been done by Guu et al. 2017 for the SCONE dataset, and more recently for CNLVR by Goldman et al., 2017. \n\nSo at least 4 papers have done that in the last year on 4 different datasets, and it is now close to being common practice so I don't really see this as a contribution. \n\n* The authors explain that they use a novel loss function that is better than an RL based function used by Zhong et al., 2017.  If I understand correctly they did not implement Zhong et al. only compared to their numbers which is a problem because it is hard to judge the role of optimization in the results. \n\nMoreover, it seems that the problem they are trying to address is standard - they would like to use cross-entropy loss when there are multiple tokens that could be gold.  the standard solution to this is to just have uniform distribution over all gold tokens and minimize the cross-entropy between the predicted distribution and the gold distribution which is uniform over all tokens.  The authors re-invent this and find it works better than randomly choosing a gold token or taking the max.  But again, this is something that has been done already in the context of pointer networks and other work like See  et al. 2017 for summarization and Jia et al., 2016 for semantic parsing. \n\n* As for the good results - the data is new, so it is probable that numbers are not very fine-tuned yet so it is hard to say what is important and what not for final performance.  In general I tend to agree that using RL for this task is probably unnecessary when you have the full program as supervision.",1,1,1,1,-1,1,1,1,1,-1
BkUDW_lCb-R3,"This paper presents a neural architecture for converting natural language queries to SQL statements.  The model utilizes a simple typed decoder that chooses to copy either from the question / table or generate a word from a predefined SQL vocabulary.  The authors try different methods of aggregating attention for the decoder copy mechanism and find that summing token probabilities works significantly better than alternatives; this result could be useful beyond just Seq2SQL models (e.g., for summarization).  Experiments on the WikiSQL dataset demonstrate state-of-the-art results, and detailed ablations measure the impact of each component of the model.  Overall, even though the architecture is not very novel,;  the paper is well-written and the results are strong;  as such, I'd recommend the paper for acceptance. \n\nSome questions:\n- How can the proposed approach scale to more complex queries (i.e., those not found in WikiSQL)?  Could the output grammar be extended to support joins, for instance?  As the grammar grows more complex, the typed decoder may start to lose its effectiveness.Some discussion of these issues would be helpful.  \n- How does the additional preprocessing done by the authors affect the performance of the original baseline system of Zhong et al.?  In general, some discussion of the differences in preprocessing between this work and Zhong et al. would be good (do they also use column annotation)?",1,1,1,1,1,-1,1,1,1,-1
BkUHlMZ0b-R1,"The work claims a measure of robustness of networks that is attack-agnostic.  Robustness measure is turned into the problem of finding a local Lipschitz constant which is given by the maximum of the norm of the gradient of the associated function.  That quantity is then estimated by sampling from the domain of maximization and observing the maximum value of the norm out of those samples.  Such a maximum process is then described by the reverse Weibull distribution which is used in the estimation. \n\nThe paper closely follows Hein and Andriushchenko (2017).  There is a slight modification that enlarges the class of functions for which the theory is applicable (Lemma 3.3).  As far as I know, the contribution of the work starts in Section 4 where the authors show how to practically estimate the maximum process through back-prop where mini-batching helps increase the number of samples.  This is a rather simple idea that is shown to be effective in Figure 3.  The following section (the part starting from 5.3) presents the key to the success of the proposed measure.  \n\nThis is an important problem and the paper attempts to tackle it in a computationally efficient way.  The fact that the norms of attacks are slightly above the proposed score is promising, however, there is always the risk of finding a lower bound that is too small (zeros and large gaps in Figure 3).  It would be nice to be able to show that one can find corresponding attacks that are not too far away from the proposed score. \n\nFinally, a minor point: Definition 3.1 has a confusing notation, f is a K-valued vector throughout the paper but it also denotes the number that represents the prediction in Definition 3.1. I believe this is just a typo. \n\nEdit: Thanks for the fixes and clarification of essential parts in the paper",1,1,1,1,1,1,1,1,1,-1
BkUHlMZ0b-R2,"Summary\n========\n\nThe authors present CLEVER, an algorithm which consists in evaluating the (local) Lipschitz constant of a trained network around a data point.  This is used to compute a lower-bound on the minimal perturbation of the data point needed to fool the network. \n\nThe method proposed in the paper already exists for classical function, they only transpose it to neural networks.  Moreover, the lower bound comes from basic results in the analysis of Lipschitz continuous functions. \n\n\nClarity\n=====\n\nThe paper is clear and well-written. \n\n\nOriginality\n=========\n\nThis idea is not new: if we search for \""Lipschitz constant estimation\"" in google scholar, we get for example\nWood, G. R., and B. P. Zhang. \""Estimation of the Lipschitz constant of a function.\"" (1996)\nwhich presents a similar algorithm (i.e., estimation of the maximum slope with reverse Weibull). \n\n\nTechnical quality\n==============\n\nThe main theoretical result in the paper is the analysis of the lower-bound on \\delta, the smallest perturbation to apply on\na data point to fool the network.  This result is obtained almost directly by writing the bound on Lipschitz-continuous function\n | f(y)-f(x) | < L || y-x ||\nwhere x = x_0 and y = x_0 + \\delta. \n\nComments:\n- Lemma 3.1: why citing Paulavicius and Zilinskas for the definition of Lipschitz continuity?  Moreover, a Lipschitz-continuous function does not need to be differentiable at all (e.g. |x| is Lipschitz with constant 1 but sharp at x=0).  Indeed, this constant can be easier obtained if the gradient exists, but this is not a requirement. \n\n- (Flaw?) Theorem 3.2 : This theorem works for fixed target-class since g = f_c - f_j for fixed g. However, once g = min_j f_c - f_j, this theorem is not clear with the constant Lq. Indeed, the function g should be \ng(x) = min_{k \\neq c} f_c(x) - f_k(x). \nThus its Lipschitz constant is different, potentially equal to\nL_q = max_{k} \\| L_q^k \\|, \nwhere L_q^k is the Lipschitz constant of f_c-f_k.  If the theorem remains unchanged after this modification, you should clarify the proof.  Otherwise, the theorem will work with the maximum over all Lipschitz constants but the theoretical result will be weakened. \n\n- Theorem 4.1: I do not see the purpose of this result in this paper. This should be better motivated. \n\n\nNumerical experiments\n====================\n\nGlobally, the numerical experiments are in favor of the presented method.  The authors should also add information about the time it takes to compute the bound, the evolution of the bound in function of the number of samples and the distribution of the relative gap between the lower-bound and the best adversarial example. \n\nMoreover, the numerical experiments look to be realized in the context of targeted attack.  To show the real effectiveness of the approach, the authors should also show the effectiveness of the lower-bound in the context of non-targeted attack. \n\n\n#######################################################\n\nPost-rebuttal review\n---------------------------\n\nGiven the details the authors provided to my review, I decided to adjust my score.  The method is simple and shows to be extremely effective/accurate in practice. \n\nDetailed answers:\n\n1) Indeed, I was not aware that the paper only focuses on one dimensional functions.  However, they still work with less assumption, i.e., with no differential functions.  I was pointing out the similarities between their approach and your: the two algorithms (CLEVER and Slope) are basically the same, and using a limit you can go from \""slope\"" to \""gradient norm\"". \nIn any case, I have read the revision and the additional numerical experiment to compare Clever with their method is a good point. \n\n2) \"" Overall, our analysis is simple and more intuitive, and we further facilitate numerical calculation of the bound by applying the extreme value theory in this work.  \""\nThis is right. I am just surprised is has not been done before, since it requires only few lines of derivation.  I searched a bit but it is not possible to find any kind of similar results.  Moreover, this leads to good performances, so there is no needs to have something more complex. \n\n3) \""The usual Lipschitz continuity is defined in terms of L2 norm and the extension to an arbitrary Lp norm is not straightforward\""\nIndeed, people usually use the Lipschitz continuity using the L2norm, but the original definition is wider. \nQuickly, if you have a differential, scalar function from a space E -> R, then the gradient is a function from space E to E*, the dual of the space E. \nLet || . || the norm of space E. Then, || . ||* is the dual norm of ||.||, and also the norm of E*.\nIn that case, Lipschitz continuity writes\nf(x)-f(y) <= L || x-y ||, with L >= max_{x in E*} || f'(x) ||*\nIn the case where || . || is an \\ell-p norm, then || . ||* is an \\ell-q norm; with 1/p+1/q = 1. \n\nIf you are interested, there is a clear and concise explanation in the introduction of this paper: Accelerating the cubic regularization of Newton\u2019s method on convex problems, by Yurii Nesterov. \n\nI have no additional remarks for 4) -> 9), since everything is fixed in the new version of the paper",1,1,1,1,1,1,1,1,1,-1
BkUHlMZ0b-R3,"In this work, the objective is to analyze the robustness of a neural network to any sort of attack. \n\nThis is measured by naturally linking the robustness of the network to the local Lipschitz properties of the network function.  This approach is quite standard in learning theory, I am not aware of how original this point of view is within the deep learning community. \n\nThis is estimated by obtaining values of the norm of the gradient (also naturally linked to the Lipschitz properties of the function) by backpropagation. This is again a natural idea",1,1,-1,1,-1,-1,1,-1,1,-1
BkUp6GZRW-R1,"This paper studies a new architecture DualAC.  The author give strong and convincing justifications based on the Lagrangian dual of the Bellman equation (although not new, introducing this as the justification for the architecture design is plausible). \n\nThere are several drawbacks of the current format of the paper:\n1. The algorithm is vague.  Alg 1 line 5: 'closed form': there is no closed form in Eq(14).  It is just an MC approximation. \nline 6: Decay O(1/t^\\beta). This is indeed vague albeit easy to understand.  The algorithm requires that every step is crystal clear. \n\n2. Also, there are several format error which may be due to compiling, e.g., line 2 of Abstract,'Dual-AC ' (an extra space).  There are many format errors like this throughout the paper.  The author is suggested to do a careful format check. \n\n3. The author is suggested to explain more about the necessity of introducing path regularization and SDA. The current justification is reasonable but too brief. \n\n4. The experimental part is ok to me, but not very impressive. \n\nOverall, this seems to be a nice paper to me",1,1,1,1,1,-1,1,1,1,-1
BkUp6GZRW-R2,"The paper is well written, and the authors do an admirable job of motivating their primary contributions throughout the early portions of the paper.  Each extension to the Dual Actor-Critic is well motivated and clear in context.  Perhaps the presentation of these extensions could be improved by providing a less formal explanation of what each does in practice; multi-step updates, regularized against MC returns, stochastic mirror descent.  \n\nThe practical implementation section losses some of this clear organization, and could certainly be clarified each part tied into Algorithm 1, and this was itself made less high-level. But these are minor gripes overall. \n\nTurning to the experimental section, I think the authors did a good job of evaluating their approach with the ablation study and comparisons with PPO and TRPO.  There were a few things that jumped out to me that I was surprised by.  The difference in performance for Dual-AC between Figure 1 and Figure 2b is significant, but the only difference seems to be a reduce batch size, is this right?  This suggests a fairly significant sensitivity to this hyperparameter if so. \n\nReproducibility in continuous control is particularly problematic.  Nonetheless, in recent work PPO and TRPO performance on the same set of tasks seem to be substantively different than what the authors get in their experiments.  I'm thinking in particular of:\n\nProximal Policy Optimization Algorithms (Schulman et. al., 2017) \nMulti-Batch Experience Replay for Fast Convergence of Continuous Action Control (Han and Sung, 2017) \n\nIn both these cases the results for PPO and TRPO vary pretty significantly from what we see here, and an important one to look at is the InvertedDoublePendulum-v1 task, which I would think PPO would get closer to 8000, and TRPO not get off the ground.  Part of this could be the notion of an \""iteration\"", which was not clear to me how this corresponded to actual time steps.  Most likely, to my mind, is that the parameterization used (discussed in the appendix) is improving TRPO and hurting PPO. \n\nWith these in mind I view the comparison results with a bit of uncertainty about the exact amount of gain being achieved,;  which may beg the question if the algorithmic contributions are buying much for their added complexity? \n\nPros:\nWell written, thorough treatment of the approaches. \nImprovements on top of Dual-AC with ablation study show improvement. \n\nCons:\nEmpirical gains might not be very large",1,1,1,1,1,1,1,1,1,1
BkUp6GZRW-R3,"This paper proposes a method, Dual-AC, for optimizing the actor(policy) and critic(value function) simultaneously which takes the form of a zero-sum game resulting in a principled method for using the critic to optimize the actor.  In order to achieve that, they take the linear programming approach of solving the bellman optimality equations, outline the deficiencies of this approach, and propose solutions to mitigate those problems.  The discussion on the deficiencies of the naive LP approach is mostly well done.  Their main contribution is extending the single step LP formulation to a multi-step dual form that reduces the bias and makes the connection between policy and value function optimization much clearer without loosing convexity by applying a regularization.  They perform an empirical study in the Inverted Double Pendulum domain to conclude that their extended algorithm outperforms the naive linear programming approach without the improvements.  Lastly, there are empirical experiments done to conclude the superior performance of Dual-AC in contrast to other actor-critic algorithms.  \n\nOverall, this paper could be a significant algorithmic contribution, with the caveat for some clarifications on the theory and experiments.  Given these clarifications in an author response, I would be willing to increase the score.  \n\nFor the theory, there are a few steps that need clarification and further clarification on novelty.  For novelty, it is unclear if Theorem 2 and Theorem 3 are both being stated as novel results.  It looks like Theorem 2 has already been shown in \""Randomized Linear Programming Solves the Discounted Markov Decision Problem in Nearly-Linear Running Time\u201d.  There is a statement that \u201cChen & Wang (2016); Wang (2017) apply stochastic first-order algorithms (Nemirovski et al., 2009) for the one-step Lagrangian of the LP problem in reinforcement learning setting.  However, as we discussed in Section 3, their algorithm is restricted to tabular parametrization\u201d.  Is you Theorem 2 somehow an extension? Is Theorem 3 completely new? \n\nThis is particularly called into question due to the lack of assumptions about the function class for value functions.  It seems like the value function is required to be able to represent the true value function, which can be almost as restrictive as requiring tabular parameterizations (which can represent the true value function).  This assumption seems to be used right at the bottom of Page 17, where U^{pi*} = V^*.  Further, eta_v must be chosen to ensure that it does not affect (constrain) the optimal solution, which implies it might need to be very small.  More about conditions on eta_v would be illuminating.  \n\nThere is also one step in the theorem that I cannot verify.  On Page 18, how is the squared removed for difference between U and Upi?  The transition from the second line of the proof to the third line is not clear.  It would also be good to more clearly state on page 14 how you get the first inequality, for || V^* ||_{2,mu}^2.  \n\n\nFor the experiments, the following should be addressed. \n\n1. It would have been better to also show the performance graphs with and without the improvements for multiple domains. \n\n2. The central contribution is extending the single step LP to a multi-step formulation.  It would be beneficial to empirically demonstrate how increasing k (the multi-step parameter) affects the performance gains. \n\n3. Increasing k also comes at a computational cost.  I would like to see some discussions on this and how long dual-AC takes to converge in comparison to the other algorithms tested (PPO and TRPO). \n\n4. The authors concluded the presence of local convexity based on hessian inspection due to the use of path regularization.  It was also mentioned that increasing the regularization parameter size increases the convergence rate.  Empirically, how does changing the regularization parameter affect the performance in terms of reward maximization?  In the experimental section of the appendix, it is mentioned that multiple regularization settings were tried but their performance is not mentioned.  Also, for the regularization parameters that were tried, based on hessian inspection, did they all result in local convexity?  A bit more discussion on these choices would be helpful.  \n\nMinor comments:\n1. Page 2: In equation 5, there should not be a 'ds' in the dual variable constraint",1,1,1,1,1,1,1,1,1,-1
BkVf1AeAZ-R1,"The paper proposes to add an embedding layer for labels that constrains normal classifiers in order to find label representations that are semantically consistent.  The approach is then experimented on various image and text tasks. \n\nThe description of the model is laborious and hard to follow. Figure 1 helps but is only referred to at the end of the description (at the end of section 2.1), which instead explains each step without the big picture and loses the reader with confusing notation.  For instance, it only became clear at the end of the section that E was learned. \n\nOne of the motivations behing the model is to force label representations to be in a semantic space (where two labels with similar meanings would be nearby).  The assumption given in the introduction is that softmax would not yield such a representation, but nowhere in the paper this assumption is verified.  I believe that using cross-entropy with softmax should also push semantically similar labels to be nearby in the weight space entering the softmax.  This should at least be verified and compared appropriately. \n\nAnother motivation of the paper is that targets are given as 1s or 0s while soft targets should work better . I believe this is true, but there is a lot of prior work on these, such as adding a temperature to the softmax, or using distillation, etc. None of these are discussed appropriately in the paper. \n\nSection 2.2 describes a way to compress the label embedding representation, but it is not clear if this is actually used in the experiments. h is never discussed after section 2.2. \n\nExperiments on known datasets are interesting, but none of the results are competitive with current state-of-the-art results (SOTA), despite what is said in Appending D.  For instance, one can find SOTA results for CIFAR100 around 16% and for CIFAR10 around 3%. Similarly, one can find SOTA results for IWSLT2015 around 28 BLEU . It can be fine to not be SOTA as long as it is acknowledged and discussed appropriately",1,1,1,1,1,1,1,1,1,-1
BkVf1AeAZ-R2,"This paper proposes a label embedding network method that learns label embeddings during the training process of deep networks.  \nPros: Good empirical results. \nCons:  There is not much technical contribution.  The proposed approach is neither well motivated, nor well presented/justified.  The presentation of the paper needs to be improved.  \n\n1. Part of the motivation on page 1 does not make sense. In particular, for paragraph 3, if the classification task is just to separate A from B, then (1,0) separation should be better than (0.8, 0.2).  \n\n2. Label embedding learning has been investigated in many previous works.  The authors however ignored all the existing works on this topic, but enforce label embedding vectors as similarities between labels in Section 2.1 without clear motivation and justification.  This assumption is not very natural \u2014 though label embeddings can capture semantic information and label correlations, it is unnecessary that label embedding matrix should be m xm and each entry should represent the similarity between a pair of labels.   The paper needs to provide a clear rationale/justification for the assumptions made, while clarifying the difference (and reason) from the literature works.  \n\n3. The proposed model is not well explained.   \n(1) By using the objective in eq.(14), how to learn the embeddings E?  \n(2) The authors state \u201cIn back propagation, the gradient from z2 is kept from propagating to h\u201d.  This makes the learning process quite arbitrary under the objective in eq.(14).  \n(3) The label embeddings are not directly used for the classification (H(y, z\u2019_1)), but rather as auxiliary part of the objective.  How to decide the test labels",1,1,1,1,1,1,1,1,-1,-1
BkVf1AeAZ-R3,"The paper proposes a method which jointly learns the label embedding (in the form of class similarity) and a classification model.  While the motivation of the paper makes sense, the model is not properly justified, and I learned very little after reading the paper. \n\nThere are 5 terms in the proposed objective function. There are also several other parameters associated with them: for example, the label temperature of z_2\u2019\u2019 and and parameter alpha in the second last term etc. \n\nFor all the experiments, the same set of parameters are used, and it is claimed that \u201cthe method is robust in our experiment and simply works without fine tuning\u201d.  While I agree that a robust and fine-tuning-free model is ideal 1) this has to be justified by experiment. 2) showing the experiment with different parameters will help us understand the role each component plays.  This is perhaps more important than improving the baseline method by a few point, especially given that the goal of this work is not to beat the state-of-the-art",1,1,1,1,1,-1,1,1,-1,-1
BkVsWbbAW-R1,"This paper propose a variant of generative replay buffer/memory to overcome catastrophic forgetting.  They use multiple copy of their model DGMN as short term memories and then consolidate their knowledge in a larger DGMN as a long term memory.  \n\nThe main novelty of this work are 1-balancing mechanism for the replay memory.  2-Using multiple models for short and long term memory.  The most interesting aspect of the paper is using a generate model as replay buffer which has been introduced before.  As explained in more detail below, it is not clear if the novelties  introduced in this paper are important for the task or if they are they are tackling the core problem of catastrophic forgetting.  \n\nThe paper claims using the task ID (either from Oracle or from a HMM) is an advantage of the model.  It is not clear to me as why is the case, if anything it should be the opposite.  Humans and animal are not given task ID and it's always clear distinction between task in real world. \n\nDeep Generative Replay section and description of DGDMN are written poorly and is very incomprehensible.  It would have been more comprehensive if it was explained in more shorter sentences accompanied with proper definition of terms and an algorithm or diagram for the replay mechanism.  \n\nUsing the STTM during testing means essentially (number of STTM) + 1 models are used which is not same as preventing one network from catastrophic forgetting. \n\nBaselines: why is Shin et al. (2017) not included as one of the baselines?  As it is the closet method to this paper it is essential to be compared against. \n\nI disagree with the argument in section 4.2.  A good robust model against catastrophic forgetting would be a model that still can achieve close to SOTA.   Overfitting to the latest task is the central problem in catastrophic forgetting which this paper avoids it by limiting the model capacity. \n\n12 pages is very long, 8 pages was the suggested page limit. It\u2019s understandable if the page limit is extend by one page, but 4 pages is over stretching",1,1,1,1,1,1,1,1,1,-1
BkVsWbbAW-R2,"This paper reports on a system for sequential learning of several supervised classification tasks in a challenging online regime.  Known task segmentation is assumed and task specific input generators are learned in parallel with label prediction.  The method is tested on standard sequential MNIST variants as long as a class incremental variant.  Superior performance to recent baselines (e.g. EWC) is reported in several cases.  Interesting parallels with human cortical and hippocampal learning and memory are discussed. \n\nUnfortunately, the paper does not go beyond the relatively simplistic setup of sequential MNIST, in contrast to some of the methods used as baselines.  The proposed architecture implicitly reduces the continual learning problem to a classical multitask learning (MTL) setting for the LTM, where (in the best case scenario) i.i.d. data from all encountered tasks is available during training. This setting is not ideal, though.  There are several example of successful multitask learning, but it does not follow that a random grouping of several tasks immediately leads to successful MTL.  Indeed, there is good reason to doubt this in both supervised and reinforcement learning domains.  In the latter case it is well known that MTL with arbitrary sets of task does not guarantee superior, or even comparable performance to plain single-task learning, due to \u2018negative interference\u2019 between tasks [1, 2].  I agree that problems can be constructed where these assumptions hold, but this core assumption is limiting.  The requirement of task labels also rules out important use cases such as following a non-stationary objective function, which is important in several realistic domains, including deep RL. \n\n\n[1] Parisotto, Emilio; Lei Ba, Jimmy; Salakhutdinov, Ruslan: \t\nActor-Mimic: Deep Multitask and Transfer Reinforcement Learning. ICLR 2016.\n[2] Andrei A. Rusu, Sergio Gomez Colmenarejo, \u00c7aglar G\u00fcl\u00e7ehre, Guillaume Desjardins, James Kirkpatrick, Razvan Pascanu, Volodymyr Mnih, Koray Kavukcuoglu, Raia Hadsell: Policy Distillation. ICLR 2016.",1,1,1,1,-1,1,1,1,-1,-1
BkVsWbbAW-R3,"This paper introduces a neural network architecture for continual learning.  The model is inspired by current knowledge about long term memory consolidation mechanisms in humans.  As a consequence, it uses:\n-\tOne temporary memory storage (inspired by hippocampus) and a long term memory\n-\tA notion of memory replay, implemented by generative models (VAE), in order to simultaneously train the network on different tasks and avoid catastrophic forgetting of previously learnt tasks. \nOverall, although the result are not very surprising, the approach is well justified and extensively tested.  It provides some insights on the challenges and benefits of replay based memory consolidation. \n\nComments:\n\t\n1-\tThe results are somewhat unsurprising: as we are able to learn generative models of each tasks, we can use them to train on all tasks at the same time, a beat algorithms that do not use this replay approach.  \n2-\tIt is unclear whether the approach provides a benefit for a particular application: as the task information has to be available, training separate task-specific architectures or using classical multitask learning approaches would not suffer from catastrophic forgetting and perform better (I assume).  \n3-\tSo the main benefit of the approach seems to point towards the direction of what possibly happens in real brains.  It is interesting to see how authors address practical issues of training based on replay and it show two differences with real brains: 1/ what we know about episodic memory consolidation (the system modeled in this paper) is closer to unsupervised learning, as a consequence information such as task ID and dictionary for balancing samples would not be available, 2/ the cortex (long term memory) already learns during wakefulness, while in the proposed algorithm this procedure is restricted to replay-based learning during sleep. \n4-\tDue to these differences, I my view, this work avoids addressing directly the most critical and difficult issues of catastrophic forgetting, which relates more to finding optimal plasticity rules for the network in an unsupervised setting. \n5-\tThe writing could have been more concise and the authors could make an effort to stay closer to the recommended number of pages",1,1,1,1,1,-1,1,1,1,-1
BkwHObbRZ-R1,"[ =========================== REVISION ===============================================================]\nI am satisfied with the answers to my questions.  The paper still needs some work on clarity, and authors defer the changes to the next version (but as I understood, they did no changes for this paper as of now), which is a bit frustrating.  However I am fine accepting it. \n[ ============================== END OF REVISION =====================================================]\n\nThis paper concerns with addressing the issue of SGD not converging to the optimal parameters on one hidden layer network for a particular type of data and label (gaussian features, label generated using a particular function that should be learnable with neural net).  Authors demonstrate empirically that this particular learning problem is hard for SGD with l2 loss (due to apparently bad local optima) and suggest two ways of addressing it, on top of the known way of dealing with this problem (which is overparameterization).  First is to use a new activation function, the second is by designing a new objective function that has only global optima and which can be efficiently learnt with SGD. \n\nOverall the paper is well written.  The authors first introduce their suggested loss function and then go into details about what inspired its creation.  I do find interesting the formulation of population risk in terms of tensor decomposition, this is insightful \n\nMy issues with the paper are as follows:\n- The loss function designed seems overly complicated.  On top of that authors notice that to learn with this loss efficiently, much larger batches had to be used.  I wonder how applicable this in practice - I frankly didn't see insights here that I can apply to other problems that don't fit into this particular narrowly defined framework. \n- I do find it somewhat strange that no insight to the actual problem is provided (e.g. it is known empirically but there is no explanation of what actually happens and there is a idea that it is due to local optima), but authors are concerned with developing new loss function that has provable properties about global optima.  Since it is all empirical, the first fix (activation function) seems sufficient to me and new loss is very far-fetched.  \n- It seems that changing activation function from relu to their proposed one fixes the problem without their new loss, so i wonder whether it is a problem with relu itself and may be other activations funcs, like sigmoids will not suffer from the same problem \n- No comparison with overparameterization in experiments results is given, which makes me wonder why their method is better. \n\nMinor: fix margins in formula 2.7",1,1,1,1,1,-1,1,1,1,-1
BkwHObbRZ-R2,"This paper studies the problem of learning one-hidden layer neural networks and is a theory paper.  A well-known problem is that without good initialization, it is not easy to learn the hidden parameters via gradient descent.  This paper establishes an interesting connection between least squares population loss and Hermite polynomials.  Following from this connection authors propose a new loss function.  Interestingly, they are able to show that the loss function globally converges to the hidden weight matrix, Simulations confirm the findings. \n\nOverall, pretty interesting result and solid contribution.  The paper also raises good questions for future works.  For instance, is designing alternative loss function useful in practice?  In summary, I recommend acceptance.  The paper seems rushed to me so authors should polish up the paper and fix typos. \n\nTwo questions:\n1) Authors do not require a^* to recover B^*. Is that because B^* is assumed to have unit length rows?  If so they should clarify this otherwise it confuses the reader a bit. \n2) What can be said about rate of convergence in terms of network parameters?  Currently a generic bound is employed which is not very insightful in my opinion",1,1,1,1,1,-1,1,1,1,-1
BkwHObbRZ-R3,"This paper proposes a tensor factorization-type method for learning one hidden-layer neural network.  The most interesting part is the Hermite polynomial expansion of the activation function.  Such a decomposition allows them to convert the population risk function as a fourth-order orthogonal tensor factorization problem.  They further redesign a new formulation for the tensor decomposition problem, and show that the new formulation enjoys the nice strict saddle properties as shown in Ge et al. 2015.  At last, they also establish the sample complexity for recovery. \n\nThe organization and presentation of the paper need some improvement.  For example, the authors defer many technical details.  To make the paper accessible to the readers, they could provide more intuitions in the first 9 pages. \n\nThere are also some typos: For example, the dimension of a is inconsistent.  In the abstract, a is an m-dimensional vector, and on Page 2, a is a d-dimensional vector.  On Page 8, P(B) should be a degree-4 polynomial of B. \n\nThe paper does not contains any experimental results on real data.",1,1,1,1,1,-1,1,1,1,-1
BkXmYfbAZ-R1,"Summary: This paper proposes a different approach to deep multi-task learning using \u201csoft ordering. \u201d  Multi-task learning encourages the sharing of learned representations across tasks, thus using less parameters and tasks help transfer useful knowledge across.  Thus enabling the reuse of universally learned representations and reuse them by assembling them in novel ways for new unseen tasks.  The idea of \u201csoft ordering\u201d enforces the idea that there shall not be a rigid structure for all the tasks, but a soft structure would make the models more generalizable and modular.  \n\nThe methods reviewed prior work which the authors refer to as \u201cparallel order\u201d, which assumed that subsequences of the feature hierarchy align across tasks and sharing between tasks occurs only at aligned depths whereas in this work the authors argue that this shouldn\u2019t be the case.  They authors then extend the approach to \u201cpermuted order\u201d and finally present their proposed \u201csoft ordering\u201d approach.  The authors argue that their proposed soft ordering approach increase the expressivity of the model while preserving the performance.  \n\nThe \u201csoft ordering\u201d approach simply enable task specific selection of layers, scaled with a learned scaling factor, to be combined in which order to result for the best performance for each task.  The authors evaluate their approach on MNIST, UCI, Omniglot and CelebA datasets and compare their approach to \u201cparallel ordering\u201d and \u201cpermuted ordering\u201d and show the performance gain. \n\nPositives: \n- The paper is clearly written and easy to follow. \n- The idea is novel and impactful if its evaluated properly and consistently.  \n- The authors did a great job summarizing prior work and motivating their approach. \n\nNegatives: \n- Multi-class classification problem is one incarnation of Multi-Task Learning, there are other problems where the tasks are different (classification and localization) or auxiliary (depth detection for navigation).  CelebA dataset could have been a good platform for testing different tasks, attribute classification and landmark detection.  \u2028\n(TODO) I would recommend that the authors test their approach on such setting. \n- Figure 6 is a bit confusing, the authors do not explain why the \u201cPermuted Order\u201d performs worse than \u201cParallel Order\u201d.  Their assumptions and results as of this section should be consistent that soft order>permuted order>parallel order>single task.  \n\u2028(TODO) I would suggest that the authors follow up on this result, which would be beneficial for the reader. \n- Figure 4(a) and 5(b), the results shown on validation loss, how about testing error similar to Figure 6(a)?  How about results for CelebA dataset, it could be useful to visualize them as was done for MNIST, Omniglot and UCL. \u2028\n(TODO) I would suggest that the authors make the results consistent across all datasets and use the same metric such that its easy to compare. \n\nNotation and Typos:\n- Figure 2 is a bit confusing, how come the accuracy decreases with increasing number of training samples? Please clarify. \n1- If I assume that the Y-Axis is incorrectly labeled and it is Training Error instead, then the permuted order is doing worse than the parallel order. \n\u20282- If I assume that the X-Axis is incorrectly labeled and the numbering is reversed (start from max and ending at 0), then I think it would make sense. \n- Figure 4 is very small and not easy to read the text.  Does single task mean average performance over the tasks?  \n- In eq.(3) Choosing \\sigma_i for a task-specific permutation of the network is a bit confusing, since it could be thought of as a sigmoid function, I suggest using a different symbol. \n\u2028Conclusion: I would suggest that the authors address the concerns mentioned above.  Their approach and idea is very interesting and relevant, and addressing these suggestions will make the paper strong for publication",1,1,1,1,1,1,1,1,1,-1
BkXmYfbAZ-R2,"This paper proposes a new approach for multi-task learning.  While previous approaches assumes the order of shared layers are the same between tasks, this paper assume the order can vary across tasks, and the (soft) order is learned during training.   They show improved performance on a number of multi-task learning problems.  \n\nMy primary concern about this paper is the lack of interpretation on permuting the layers.  For example, in standard vision systems, low level filters \""V1\"" learn edge detectors (gabor filters) and higher level filters learn angle detectors [1].  It is confusing why permuting these filters make sense.  They accept different inputs (raw pixels vs edges).  Moreover, if the network contains pooling layers, different locations of the pooling layer result in different shapes of the feature map, and the soft ordering strategy Eq. (7) does not work.  \n\nIt makes sense that the more flexible model proposed by this paper performs better than previous models.  The good aspect of this paper is that it has some performance improvements.  But I still wonder the effect of permuting the layers.  The paper also needs more clarifications in the writing.  For example, in Section 3.3, how each s_(i, j, k) is sampled from S?  The \""parallel ordering\"" terminology also seems to be arbitrary... \n\n[1] Lee, Honglak, Chaitanya Ekanadham, and Andrew Y. Ng. \""Sparse deep belief net model for visual area V2.\"" Advances in neural information processing systems. 2008",1,1,1,1,1,1,1,1,-1,-1
BkXmYfbAZ-R3,"- The paper proposes to learn a soft ordering over a set of layers for multitask learning (MTL) i.e.\n  at every step of the forward propagation, each task is free to choose its unique soft (`convex')\n  combination of the outputs from all available layers.  This idea is novel and interesting. \n- The learning of such soft combination is done jointly while learning the tasks and is not set\n  manually cf. setting permutations of a fixed number of layer per task. \n- The empirical evaluation is done on intuitively related, superficially unrelated, and a real world\n  task.  The first three results are on small datasets/tasks, O(10) feature dimensions, and number of\n  tasks and O(1000) images; (i) distinguish two MNIST digits, (ii) 10 UCI tasks with feature sizes\n  4--30 and number of classes 2--10, (iii) 50 different character recognition on Omniglot dataset. \n  The last task is real world -- 40 attribute classification on the CelebA face dataset of 200K\n  images.  While the first three tasks are smaller proof of concept, the last task could have been\n  more convincing if near state-of-the-art methods were used.  The authors use a Resnet-50 which is a\n  smaller and lesser performing model, they do mention that benefits are expected to be \n  complimentary to say larger model, but in general it becomes harder to improve strong models. \n  While this does not significantly dilute the message, it would have made it much more convincing\n  if results were given with stronger networks.                       \n- The results are otherwise convincing and clear improvements are shown with the proposed method. \n- The number of layers over which soft ordering was tested was fixed however.  It would be\n  interesting to see what would the method learn if the number of layers was explicitly set to be\n  large and an identity layer was put as one of the option.  In that case the soft ordering could\n  actually learn the optimal depth as well, repeating identity layer beyond the option number of\n  layers.                                                             \n                                                                     \nOverall, the paper presents a novel idea, which is well motivated and clearly presented.  The \nempirical validation, while being limited in some aspects, is largely convincing",1,1,1,1,1,-1,1,1,1,-1
Bk_fs6gA--R1,"Learning to solve combinatorial optimization problems using recurrent networks is a very interesting research topic.  However, I had a very hard time understanding the paper.  It certainly doesn\u2019t help that I\u2019m not familiar with the architectures the model is based on, nor with state-of-the-art integer programming solvers. \n\nThe architecture was described but not really motivated.  The authors chose to study only random instances which are known to be bad representatives of real-world problmes, instead of picking a standard benchmark problem.  Furthermore, the insights on how the network is actually solving the problems and how the proposed components contribute to the solution are minimal, if any.\n\n The experimental issues (especially regarding the baseline) raised by the anonymous comments below were rather troubling; it\u2019s a pity they were left unanswered.\n\n Hopefully other expert reviewers will be able to provide constructive feedback.",1,1,1,1,-1,-1,-1,-1,-1,-1
Bk_fs6gA--R2,"# Summary\nThis paper proposes a neural network framework for solving binary linear programs (Binary LP).  The idea is to present a sequence of input-output examples to the network and train the network to remember input-output examples to solve a new example (binary LP).  In order to store such information, the paper proposes an external memory with non-differentiable reading/writing operations.  This network is trained through supervised learning for the output and reinforcement learning for discrete operations.  The results show that the proposed network outperforms the baseline (handcrafted) solver and the seq-to-seq network baseline. \n\n[Pros]\n- The idea of approximating a binary linear program solver using neural network is new. \n\n[Cons]\n- The paper is not clearly written (e.g., problem statement, notations, architecture description).  So, it is hard to understand the core idea of this paper. \n- The proposed method and problem setting are not well-justified.  \n- The results are not very convincing. \n\n# Novelty and Significance\n- The problem considered in this paper is new,  but it is unclear why the problem should be formulated in such a way..  To my understanding, the network is given a set of input (problem) and output (solution) pairs and should predict the solution given a new problem.  I do not see why this should be formulated as a \""sequential\"" decision problem.  Instead, we can just give access to all input/output examples (in a non-sequential way) and allow the network to predict the solution given the new input like Q&A tasks.  This does not require any \""memory\"" because all necessary information is available to the network. \n- The proposed method seems to require a set of input/output examples even during evaluation (if my understanding is correct), which has limited practical applications.  \n\n# Quality\n- The proposed reward function for training the memory controller sounds a bit arbitrary.  The entire problem is a supervised learning problem, and the memory controller is just a non-differentiable decision within the neural network.  In this case, the reward function is usually defined as the sum of log-likelihood of the future predictions (see [Kelvin Xu et al.] for training hard-attention) because this matches the supervised learning objective.  It would be good to justify (empirically) the proposed reward function.  \n- The results are not fully-convincing.  If my understanding is correct, the LTMN is trained to predict the baseline solver's output.  But, the LTMN significantly outperforms the baseline solver even in the training set.  Can you explain why this is possible? \n\n# Clarity\n- The problem statement and model description are not described well.  \n1) Is the network given a sequence of program/solution input?  If yes, is it given during evaluation as well?  \n2) Many notations are not formally defined.  What is the output (o_t) of the network?   Is it the optimal solution (x_t)?   \n3) There is no mathematical definition of memory addressing mechanism used in this paper.  \n- The overall objective function is missing.  \n\n[Reference]\n- Kelvin Xu et al., Show, Attend and Tell: Neural Image Caption Generation with Visual Attention",1,1,1,1,1,-1,1,1,1,-1
Bk_fs6gA--R3,"This paper proposes using long term memory to solve combinatorial optimization problems with binary variables.  The authors do not exhibit much knowledge of combinatorial optimization literature (as has been pointed out by other readers) and ignore a lot of previous work by the combinatorial optimization community.  In particular, evaluating on random instances is not a good measure of performance,  as has already been pointed out.  The other issue is with the baseline solver, which also seems to be broken since their solution quality seems extremely poor.  In light of these issues, I recommend reject.",1,1,1,1,-1,1,1,1,-1,-1
By03VlJGG-R1,"The paper introduces a new approach to learn embeddings of relational data using multimodal information such as images and text.  For this purpose, the method learns joint embeddings of symbolic data, images and text to predict the links in a knowledge graph.  The multimodal embeddings are evaluated on newly created datasets, which extend the MovieLens-100k and YAGO-10 with multimodal information. \n\nThe paper is written well, good to understand, and technically sound.  I especially liked the general idea of using multiple modalities to improve embeddings of relational data.  This direction is not only interesting because of the improvements it brings for link prediction tasks, but also because it is a promising direction towards constructing commonsense knowledge knowledge graphs via grounded embeddings.  The technical novelty of the paper is somewhat limited, as the proposed method consists of a mostly straightforward combination of existing methods. \n\nWith regard to related work: Recently, [1] proposed similar multimodal embeddings and showed that they improve embedding quality for semantic similarity tasks and entity-type prediction tasks.  This reference should be included in the related work.  The authors mention also in the last sentence of Section 3 that previous approaches cannot handle missing data or uncertainty.  This claim needs to be discussed clearer as it is not clear to me why this would be the case. \n\nWith regard to the evaluation: Overall, I found the evaluation to be good, especially with regard to the different ablations.  However, it would be nice to see results for more sophisticated models than DistMult (which, due to its symmetry, shouldn't be used on directed graphs anyway) as the improvements that can be gained might be less for these models.  It would also be interesting to see how predictions using only the non-symbolic modalities would do (e.g. in Table 3).  Furthermore, Section 5.3 would clearly benefit from a better analysis and discussion, as it isn't very informative in its current form and the analysis is quite hand-wavy (e.g. \""two of the predicted titles for Die Hard have something to do with dying and being buried\""). \n\nFurther comments:\n- The proposed method to incorporate numerical data seems quite ad hoc.  What are the motivations for this particular approach? \n- Are the image features fixed or learned?  In the later case: how much do the results change with pretrained CNNs (e.g., on ImageNet). \n- p.3: We use an appropriate encoder is repeated twice. \n- Since the datasets are newly introduced, it would be good to provide a more detailed analysis of their characteristics. \n\n\n[1] Thoma et al: \""Towards Holistic Concept Representations: Embedding Relational Knowledge, Visual Attributes, and Distributional Word Semantics\"", 2017",1,1,1,1,1,1,1,1,1,-1
By03VlJGG-R2,"This paper proposes to perform link prediction in Knowledge Bases by supplementing the original entities with multimodal information such as text description or images.  A model, based on DistMult, able to encode all sort of information when scoring triples is presented with experiments on 2 new datasets based on Yago and MovieLens. \n\nThis paper reads well and the results appear sound.  Unfortunately, the contribution seems rather small to be accepted for ICLR.  This is a straight application and combination of existing pieces with not much originality and without being backed up by very strong experimental results. \n\n* Having only results on new datasets makes it hard to compare the objective quality of the DistMult baselines and hence of the improvements due to the multimodal info.  Isn't there any existing benchmark where this could have an impact? \n* The much better performance of ConvE is worrying there.  It is suggested that the proposed approach could be incorporated in ConvE to lead to similar improvements than on DistMult. The paper would be much stronger with those.  \n* Are we sure that the textual description do not explicitly contain the information of the triple to be predicted?  This would explain the massive gains in Yago. \n* For Table 8, the similarities are not striking.  What were the nearest neighboring posters in the original VGG space? They should not be that bad too. \n* The work on multimodal embeddings like \""Multimodal Distributional Semantics\"" by Bruni et al. or \""Multi-and Cross-Modal Semantics Beyond Vision: Grounding in Auditory Perception.\"" by Kiela et al. could be discussed/cited",1,1,1,1,1,1,1,1,-1,1
By03VlJGG-R3,"The paper is well-written but is lacking detailed information in some areas (see list of questions).  The approach of incorporating all the different facts around an entity is worthwhile but pretty straight-forward.  The evaluation part of this paper is hard to assess due to the unavailability of the 2 datasets and appropriate baselines.  Therefore, I am currently leaning towards rejecting this paper. \n\n? p.3: What parts are pre-trained?  Is e_o fixed for the non-structured knowledge?  Or is it joint learning and you learn all LSTMs and CNNs yourself? (Besides the reuse of VGG, I could not find this information explicitly stated within the paper.).  \n? p.4: The word embeddings for the CNN are pre-trained word2vec/Glove/xyz embeddings?  How do you deal with words (or even the whole string) for which you have no word embedding?  \n? p.6: Do you have one model for all the relations or does every relation has its own LSTM, CNN, feed-forward network?  I.e. 1 or 3 feed-forward networks for age, zip code, and release dates? \n? p.6: How does \u201cRatings Only\u201d work as DistMult gets no information of the specific entities?  Is it just choosing the most common class? \n? p.7: What does \u201cfind the mid-point of the bin\u201d mean and should it not be 1018 instead of 1000 bins? \n\n+ Insights on how different modalities affect the prediction results. \n+ The approach is capable of theoretically handling all linked information to an entity as additional information to the link structure .\n- As the evaluation data is not available, it is really hard to assess the quality of the models.  No simple baseline like the Unstructured [1] + simple concatenation of an image vector is provided. \n- Training of CNNs, LSTMs and so on is not clear.(See question regarding whether the models are pre-trained or whether the models are also directly learned from the data.). \n\nFurther comments:\n* In Figure 1, the feed-forward network looks like an encoder-decoder network and it does not show the projection from r to R^d which is mentioned in the text. \n* The found hyperparameter of the grid-search would also be interesting to know. \n\n[1] Bordes, A.; Glorot, X.; Weston, J.; and Bengio, Y. 2012. A semantic matching energy function for learning with multi- relational data. Machine Learning 1\u201327",1,1,1,1,1,-1,1,1,1,1
By0ANxbRW-R1,"1. Summary\n\nThis paper introduced a method to learn a compressed version of a neural network such that the loss of the compressed network doesn't dramatically change. \n\n\n2. High level paper\n\n- I believe the writing is a bit sloppy.  For instance equation 3 takes the minimum over all m in C but C is defined to be a set of c_1, ..., c_k, and other examples (see section 4 below).  This is unfortunate because I believe this method, which takes as input a large complex network and compresses it so the loss in accuracy is small, would be really appealing to companies who are resource constrained but want to use neural network models. \n\n\n3. High level technical\n\n- I'm confused at the first and second lines of equation (19).  In the first line, shouldn't the first term not contain \\Delta W ?  In the second line, shouldn't the first term be \\tilde{\\mathcal{L}}(W_0 + \\Delta W) ? \n- For CIFAR-10 and SVHN you're using Binarized Neural Networks and the two nice things about this method are (a) that the memory usage of the network is very small, and (b) network operations can be specialized to be fast on binary data.  My worry is if you're compressing these networks with your method are the weights not treated as binary anymore?  Now I know in Binarized Neural Networks they keep a copy of real-valued weights so if you're just compressing these then maybe all is alright.  But if you're compressing the weights _after_ binarization then this would be very inefficient because the weights won't likely be binary anymore and (a) and (b) above no longer apply. \n- Your compression ratio is much higher for MNIST but your accuracy loss is somewhat dramatic, especially for MNIST (an increase of 0.53 in error nearly doubles your error and makes the network worse than many other competing methods: http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#4d4e495354).  What is your compression ratio for 0 accuracy loss?  I think this is a key experiment that should be run as this result would be much easier to compare with the other methods. \n- Previous compression work uses a lot of tricks to compress convolutional weights. Does your method work for convolutional layers? \n- The first paper to propose weight sharing was not Han et al., 2015, it was actually:\nChen W., Wilson, J. T., Tyree, S., Weinberger K. Q., Chen, Y. \""Compressing Neural Networks with the Hashing Trick\"" ICML 2015\nAlthough they did not learn the weight sharing function, but use random hash functions. \n\n\n4. Low level technical\n\n- The end of Section 2 has an extra 'p' character \n- Section 3.1: \""Here, X and y define a set of samples and ideal output distributions we use for training\"" this sentence is a bit confusing.  Here y isn't a distribution, but also samples drawn from some distribution. Actually I don't think it makes sense to talk about distributions at all in Section 3. \n- Section 3.1: \""W is the learnt model...\\hat{W} is the final, trained model\"" This is unclear: W and \\hat{W} seem to describe the same thing.  I would just remove \""is the learnt model and\"" \n\n\n5. Review summary\n\nWhile the trust-region-like optimization of the method is nice and I believe this method could be useful for practitioners, I found the paper somewhat confusing to read.  This combined with some key experimental questions I have make me think this paper still needs work before being accepted to ICLR",1,1,1,1,1,1,1,1,1,1
By0ANxbRW-R2,"The paper addresses an interesting problem of DNN model compression.  The main idea is to combine the approaches in (Han et al., 2015) and (Ullrich et al., 2017) to get a loss value constrained k-means encoding method for network compression.  An iterative algorithm is developed for model optimization.  Experimental results on MNIST, CIFAR-10 and SVHN are reported to show the compression performance.  \n\nThe reviewer would expect papers submitted for review to be of publishable quality.  However, this manuscript is not polished enough for publication: it has too many language errors and imprecisions which make the paper hard to follow.  In particular, there is no clear definition of problem formulation, and the algorithms are poorly presented and elaborated in the context.  \n\nPros: \n\n- The network compression problem is of general interest to ICLR audience.  \n\nCons:\n\n- The proposed approach follows largely the existing work and thus its technical novelty is weak.  \n\n- Paper presentation quality is clearly below the standard.  \n\n- Empirical results do not clearly show the advantage of the proposed method over state-of-the-arts",1,1,1,1,-1,1,1,1,1,-1
By0ANxbRW-R3,"1. This paper proposes a deep neural network compression method by maintaining the accuracy of deep models using a hyper-parameter.  However, all compression methods such as pruning and quantization also have this concern.  For example, the basic assumption of pruning is to discard subtle parameters has little impact on feature maps thus the accuracy of the original network can be preserved.  Therefore, the novelty of the proposed method is somewhat weak. \n\n2. There are a lot of new algorithms on compressing deep neural networks such as [r1][r2][r3].  However, the paper only did a very simple investigation on related works. \n[r1] CNNpack: packing convolutional neural networks in the frequency domain. \n[r2] LCNN: Lookup-based Convolutional Neural Network. \n[r3] Xnor-net: Imagenet classification using binary convolutional neural networks. \n\n3. Experiments in the paper were only conducted on several small datasets such as MNIST and CIFAR-10.  It is necessary to employ the proposed method on benchmark datasets to verify its effectiveness, e.g., ImageNet",1,1,1,1,1,1,1,1,1,-1
By3v9k-RZ-R1,"The authors propose the N-Gram machine to answer questions over long documents.  The model first encodes the document via tuple extraction.  An autoencoder objective is used to produce meaningful tuples.  Then, the model generates a program, based on the extracted tuple collection and the question, to find an answer. \n\nI am very disappointed in the authors' choice of evaluation, namely bAbI - a toy, synthetic task long abandoned by the NLP community because of its lack of practicality.  If the authors would like to demonstrate question answering on long documents, they have the luxury of choosing amongst several large scale, realistic question answering datasets such as the Stanford Question answering dataset or TriviaQA. \nBeyond the problem of evaluation, the model the authors propose does not provide new ideas, and rather merges existing ones. This, in itself, is not a problem . However, the authors decline to cite many, many important prior work. For example, the tuple extraction described by the authors has significant prior work in the information retrieval community (e.g. knowledge base population, relation extraction).  The idea of generating programs to query over populated knowledge bases, again, has significant related work in semantic parsing and program synthesis.  Question answering over (much more complex) probabilistic knowledge graphs have been proposed before as well (in fact I believe Matt Gardner wrote his entire thesis on this topic).  Finally, textual question answering (on realistic datasets) has seen significant breakthroughs in the last few years.  Non of these areas, with the exception of semantic parsing, are addressed by the author.  With sufficient knowledge of related works from these areas, I find that the authors' proposed method lacks proper evaluation and sufficient novelty.",1,1,1,1,-1,1,1,1,1,1
By3v9k-RZ-R2,"This paper presents the n-gram machine, a model that encodes sentences into simple symbolic representations (\""n-grams\"") which can be queried efficiently.  The authors propose a variety of tricks (stabilized autoencoding, structured tweaking) to deal with the huge search space, and they evaluate NGMs on five of the 20 bAbI tasks.  I am overall a fan of the general idea of this paper; scaling up to huge inputs is definitely a necessary research direction for QA.  However, I have some concerns about the specific implementation and model discussed here.  How much of the proposed approach is specific to getting good results on bAbI (e.g., conditioning the knowledge encoder on only the previous sentence, time stamps in the knowledge tuple, super small RNNs, four simple functions in the n-gram machine, structure tweaking) versus having a general-purpose QA model for natural language?  Addressing some of these issues would likely prevent scaling to millions of (real) sentences, as the scalability is reliant on programs being efficiently executed (by simple string matching) against a knowledge storage.  The paper is missing a clear analysis of NGM's limitations...  the examples of knowledge storage from bAbI in the supplementary material are also underwhelming as the model essentially just has to learn to ignore stopwords since the sentences are so simple.  In its current form, I am borderline but leaning towards rejecting this paper. \n\nOther questions:\n- is \""n-gram\"" really the most appropriate term to use for the symbolic representation?  N-grams are by definition contiguous sequences... The authors may want to consider alternatives. \n- why focus only on extractive QA?  The evaluations are only conducted on 5 of the 20 bAbI tasks, so  it is hard to draw any conclusions from the results as to the validity of this approach.  Can the authors comment on how difficult it will be to add functions to the list in Table 2 to handle the other 15 tasks? Or is NGM strictly for extractive QA? \n- beam search is performed on each sentence in the input story to obtain knowledge tuples... while the answering time may not change (as shown in Figure 4) as the input story grows, the time to encode the story into knowledge tuples certainly grows, which likely necessitates the tiny RNN sizes used in the paper.  How long does the encoding time take with 10 million sentences? \n- Need more detail on the programmer architecture, is it identical to the one used in Liang et al., 2017?\n",1,1,1,1,1,-1,1,1,1,-1
By3v9k-RZ-R3,"The paper presents an interesting framework for bAbI QA.   Essentially, the argument is that when given a very long paragraph, the existing approaches for end-to-end learning becomes very inefficient (linear to the number of the sentences).   The proposed alternative is to encode the knowledge of each sentence symbolically as n-grams, which is thus easy to index.   While the argument makes sense, it is not clear to me why one cannot simply index the original text.  The additional encode/decode mechanism seems to introduce unnecessary noise.   The framework does include several components and techniques from latest recent work, which look pretty sophisticated.  However, as the dataset is generated by simulation, with a very small set of vocabulary, the value of the proposed framework in practice remains largely unproven. \n\nPros:\n  1. An interesting framework for bAbI QA by encoding sentence to n-grams \n\nCons:\n  1. The overall justification is somewhat unclear \n  2. The approach could be over-engineered for a special, lengthy version of bAbI and it lacks evaluation using real-world data\n",1,1,1,1,-1,1,1,1,1,-1
By3VrbbAb-R1,"This paper presents methods for query completion that includes prefix correction, and some engineering details to meet particular latency requirements on a CPU.   Regarding the latter methods: what is described in the paper sounds like competent engineering details that those performing such a task for launch in a real service would figure out how to accomplish, and the specific reported details may or may not represent the 'right' way to go about this versus other choices that might be made.   The final threshold for 'successful' speedups feels somewhat arbitrary -- why 16ms in particular?   In any case, these methods are useful to document, but derive their value mainly from the fact that they allow the use of the completion/correction methods that are the primary contribution of the paper.   \n\nWhile the idea of integrating the spelling error probability into the search for completions is a sound one, the specific details of the model being pursued feel very ad hoc, which diminishes the ultimate impact of these results.   Specifically, estimating the log probability to be proportional to the number of edits in the Levenshtein distance is really not the right thing to do at all.   Under such an approach, the unedited string receives probability one, which doesn't leave much additional probability mass for the other candidates -- not to mention that the number of possible misspellings would require some aggressive normalization.   Even under the assumption that a normalized edit probability is not particularly critical (an issue that was not raised at all in the paper, let alone assessed), the fact is that the assumptions of independent errors and a single substitution cost are grossly invalid in natural language.   For example, the probability p_1 of 'pkoe' versus p_2 of 'zoze' as likely versions of 'poke' (as, say, the prefix of pokemon, as in your example) should be such that p_1 >>> p_2, not equal as they are in your model.   Probabilistic models of string distance have been common since Ristad and Yianlios in the late 90s, and there are proper probabilistic models that would work with your same dynamic programming algorithm, as well as improved models with some modest state splitting.   And even with very simple assumptions some unsupervised training could be used to yield at least a properly normalized model.   It may very well end up that your very simple model does as well as a well estimated model, but that is something to establish in your paper, not assume.   That such shortcomings are not noted in the paper is troublesome, particularly for a conference like ICLR that is focused on learned models, which this is not.   As the primary contribution of the paper is this method for combining correction with completion, this shortcoming in the paper is pretty serious. \n\nSome other comments:\n\nYour presentation of completion cost versus edit cost separation in section 3.3 is not particularly clear, partly since the methods are discussed prior to this point as extension of (possibly corrected) prefixes.   In fact, it seems that your completion model also includes extension of words with end point prior to the end of the prefix -- which doesn't match your prior notation, or, frankly, the way in which the experimental results are described.   \n\nThe notation that you use is a bit sloppy and not everything is introduced in a clear way.   For example, the s_0:m notation is introduced before indicating that s_i would be the symbol in the i_th position (which you use in section 3.3).   Also, you claim that s_0 is the empty string, but isn't it more correct to model this symbol as the beginning of string symbol?   If not, what is the difference between s_0:m and s_1:m?   If s_0 is start of string, the s_0:m is of length m+1 not length m. \n\nYou spend too much time on common, well-known information, such as the LSTM equations.   (you don't need them, but also why number if you never refer to them later? )  Also the dynamic programming for Levenshtein is foundational, not required to present that algorithm in detail, unless there is something specific that you need to point out there (which your section 3.3 modification really doesn't require to make that point). \n\nIs there a specific use scenario for the prefix splitting, other than for the evaluation of unseen prefixes?   This doesn't strike me as the most effective way to try to assess the seen/unseen distinction, since, as I understand the procedure, you will end up with very common prefixes alongside less common prefixes in your validation set, which doesn't really correspond to true 'unseen' scenarios.   I think another way of teasing apart such results would be recommended. \n\nYou never explicitly mention what your training loss is in section 5.1. \n\nOverall, while this is an interesting and important problem, and the engineering details are interesting and reasonably well-motivated, the main contribution of the paper is based on a pretty flawed approach to modeling correction probability, which would limit the ultimate applicability of the methods",1,1,1,1,1,1,1,1,1,-1
By3VrbbAb-R2,"This paper focuses on solving query completion problem with error correction which is a very practical and important problem.  The idea is character based.  And in order to achieve three important targets which are auto completion, auto error correction and real time, the authors first adopt the character-level RNN-based modeling which can be easily combined with error correction, and then carefully optimize the inference part to make it real time. \n\nPros:\n(1) the paper is very well organized and easy to read .\n(2) the proposed method is nicely designed to solve the specific real problem. For example, the edit distance is modified to be more consistent with the task. \n(3) detailed information are provided about the experiments, such as data, model and inference. \n\nCons:\n(1) No direct comparisons with other methods are provided.  I am not familiar with the state-of-the-art methods in this field.  If the performance (hit rate or coverage) of this paper is near stoa methods, then such experimental results will make this paper much more soli",1,1,1,1,1,-1,1,1,1,-1
By3VrbbAb-R3,"The authors describe a method for performing query completion with error correction using a neural network that can achieve real-time performance.  The method described uses a character-level LSTM, and modifies the beam search procedure with a an edit distance-based probability to handle cases where the prefix may contain errors.  Details are also given on how the authors are able to achieve realtime completion .\n\nOverall, it\u2019s nice a nice study of the query completion application.  The paper is well explained, and it\u2019s also nice that the runtime is shown for each of the algorithm blocks.  Could imagine this work giving nice guidelines for others who also want to run query completion using neural networks.  The final dataset is also a good size (36M search queries). \n\nMy major concerns are perhaps the fit of the paper for ICLR as well as the thoroughness of the final experiments.  Much of the paper provides background on LSTMs and edit distance, which granted, are helpful for explaining the ideas.  But much of the realtime completion section is also standard practice, e.g. maintaining previous hidden states and grouping together the different gates.  So the paper feels directed to an audience with less background in neural net LMs. \n\nSecondly, the experiments could have more thorough/stronger baselines.  I don\u2019t really see why we would try stochastic search. And expected to see more analysis of how performance was impacted as the number of errors increased, even if errors were introduced artificially, and expected analysis of how different systems scale with varying amounts of data.  The fact that 256 hidden dimension worked best while 512 overfit was also surprising, as character language models on datasets such as Penn Treebank with only 1 million words use hidden states far larger than that for 2 layers.  More regularization required",1,1,1,1,1,-1,1,1,-1,-1
By4HsfWAZ-R1,"The paper \u2018Deep learning for Physical Process: incorporating prior physical knowledge\u2019 proposes\nto question the use of data-intensive strategies such as deep learning in solving physical \ninverse problems that are traditionally solved through assimilation strategies.  They notably show\nhow physical priors on a given phenomenon can be incorporated in the learning process and propose \nan application on the problem of estimating sea surface temperature directly from a given \ncollection of satellite images. \n\nAll in all the paper is very clear and interesting.  The results obtained on the considered problem \nare clearly of great interest, especially when compared to state-of-the-art assimilation strategies\nsuch as the one of B\u00e9r\u00e9ziat.  While the learning architecture is not original in itself, it is \nshown that a proper physical regularization greatly improves the performance.  For these reasons I \nbelieve the paper has sufficient merits to be published at ICLR.  That being said, I believe that \nsome discussions could strengthen the paper:\n - Most classical variational assimilation schemes are stochastic in nature, notably by incorporating\nuncertainties in the observation or physical evolution models.  It is still unclear how those uncertainties \ncan be integrated in the model ;\n - Assimilation methods are usually independent of the type of data at hand.  It is not clear how the model\nlearnt on one particular type of data transpose to other data sequences.  Notably, the question of transfer\nand generalization is of high relevance here.  Does the learnt model performs well on other dataset (for instance,\nacquired on a different region or at a distant time).  I believe this type of issue has to be examinated \nfor this type of approach to be widely use in inverse physical problems",1,1,1,1,1,-1,1,1,1,-1
By4HsfWAZ-R2,"In this paper, the authors show how a Deep Learning model for sea surface temperature prediction can be designed to incorporate the classical advection diffusion model.  The architecture includes a differentiable warping scheme which allows back propagation of the error and is inspired by the fundamental solution of the PDE model.  They evaluate the suggested model on synthetic data and outperform the current state of the art in terms of accuracy. \n\npros\n- the paper is written in a clear and concise manner \n- it suggests an interesting connection between a traditional model and Deep Learning techniques\ n- in the experiments they trained the network on 64 x 64 patches and achieved convincing results \n\ncons\n- please provide the value of the diffusion coefficient for the sake of reproducibility \n- medium resolution of the resulting prediction \n\n\nI enjoyed reading this paper and would like it to be accepted. \n\nminor comments:\n- on page five in the last paragraph there is a left parenthesis missing in the inline formula nabla dot w_t(x))^2. \n- on page nine in the last paragraph there is the word 'flow' missing: '.. estimating the optical [!] between 2 [!] images.' \n- in the introduction (page two) the authors refer to SST prediction as a 'relatively complex physical modeling problem', whereas in the conclusion (page ten) it is referred to as 'a problem of intermediate complexity'. This seems to be inconsistent.",1,1,1,1,1,-1,1,1,1,1
By4HsfWAZ-R3,"The authors use deep learning to learn a surrogate model for the motion vector in the advection-diffusion equation that they use to forecast sea surface temperature.  In particular, they use a CNN encoder-decoder to learn a motion field, and a warping function from the last component to provide forecasting.  \n\nI like the idea of using deep learning for physical equations.  I would like to see a description of the algorithm with the pseudo-code in order to understand the flow of the method.  I got confused at several points because it was not clear what was exactly being estimated with the CNN.  Having an algorithmic environment would make the description easier.  I know that authors are going to publish the code, but this is not enough at this point of the revision.  \n\nPhysical processes in Machine learning have been studied from the perspective of Gaussian processes. Just to mention a couple of references \u201cLinear latent force models using Gaussian processes\u201d and \""Numerical Gaussian Processes for Time-dependent and Non-linear Partial Differential Equations \""\n\nIn Theorem 2, do you need to care about boundary conditions for your equation?  I didn\u2019t see any mention to those in the definition for I(x,t). You only mention initial conditions.  How do you estimate the diffusion parameter D?  Are you assuming isotropic diffusion? Is that realistic?  Can you provide more details about how you run the data assimilation model in the experiments? Did you use your own code",1,1,1,1,1,1,1,1,1,1
By5SY2gA--R1,"This paper proposed to use affect lexica to improve word embeddings.  They extended the training objective functions of Word2vec and Glove with the affect information.  The resulting embeddings were evaluated not only on word similarity tasks but also on a bunch of downstream applications such as sentiment analysis.  Their experimental results showed that their proposed embeddings outperformed standard Word2vec and Glove.  In sum, it is an interesting paper with promising results and the proposed methods were carefully evaluated in many setups. \n\nSome detailed comments are:\n-\tAlthough the use of affect lexica is innovative, the idea of extending the training objective function with lexica information is not new.  Almost the same method was proposed in K.A. Nguyen, S. Schulte im Walde, N.T. Vu. Integrating Distributional Lexical Contrast into Word Embeddings for Antonym-Synonym Distinction. In Proceedings of ACL, 2016. \n-\tAlthough the lexicons for valence, arousal, and dominance provide different information, their combination did not perform best.  Do the authors have any intuition why? \n-\tIn Figure 2, the authors picked four words to show that valence is helpful to improve Glove word beddings. It is not convincing enough for me.   I would like to see to the top k nearest neighbors of each of those words.\n",1,1,1,1,1,1,1,1,1,-1
By5SY2gA--R2,"This paper proposes integrating information from a semantic resource that quantifies the affect of different words into a text-based word embedding algorithm.  \n\nThe affect lexical seems to be a very interesting resource (although I'm not sure what it means to call it 'state of the art'), and definitely support the endeavour to make language models more reflective of complex semantic and pragmatic phenomena such as affect and sentiment.  \n\nThe justification for why we might want to do this with word embeddings in the manner proposed seems a little unconvincing to me: \n\n- The statement that 'delighted' and 'disappointed' will have similar contexts is not evident to me at least (other then them both being participle / adjectives). \n\n- Affect in language seems to me to be a very contextual phenomenon.  Only a tiny subset of words have intrinsic and context-free affect.  Most affect seems to me to come from the use of words in (phrasal, and extra-linguistic) contexts, so a more context-dependent model, in which affect is computed over phrases or sentences, would seem to be more appropriate. Consider words like 'expensive', 'wicked', 'elimination'... \n\nThe model proposes several applications (sentiment prediction, predicting email tone, word similarity) where the affect-based embeddings yield small improvements.  However, in different cases, taking different flavours of affect information (V, A or D) produces the best score, so it is not clear what to conclude about what sort of information is most useful.  \n\nIt is not surprising to me that an algorithm that uses both WordNet and running text to compute word similarity scores improves over one that uses just running text.  It also not surprising that adding information about affect improves the ability to predict sentiment and the tone of emails.  \n\nTo understand the importance of the proposed algorithm (rather than just the addition of additional data), I would like to see comparison with various different post-processing techniques using WordNet and the affect lexicon (i.e. not just Bollelaga et al.) including some much simpler baselines.  For instance, what about averaging WordNet path-based distance metrics and distance in word embedding space (for word similarity), and other ways of applying the affect data to email tone prediction",1,1,1,1,1,-1,1,1,1,-1
By5SY2gA--R3,"This paper introduces modifications the word2vec and GloVe loss functions to incorporate affect lexica to facilitate the learning of affect-sensitive word embeddings.  The resulting word embeddings are evaluated on a number of standard tasks including word similarity, outlier prediction, sentiment detection, and also on a new task for formality, frustration, and politeness detection. \n\nA considerable amount of prior work has investigated reformulating unsupervised word embedding objectives to incorporate external resources for improving representation learning.  The methodologies of Kiela et al (2015) and Bollegala et al (2016) are very similar to those proposed in this work.  The main originality seems to be captured in Algorithm 1, which computes the strength between two words.  Unlike prior work, this is a real-valued instead of a binary quantity.  Because this modification is not particularly novel, I believe this paper should primarily be judged based upon the effectiveness of the method rather than the specifics of the underlying techniques.  In this light, the performance relative to the baselines is particularly important.  From the results reported in Tables 1, 2, and 3, I do not see compelling evidence that +V, +A, +D, or +VAD consistently lead to significant performance increases relative to the baseline methods.  I therefore cannot recommend this paper for publication.",1,1,1,1,-1,1,1,1,1,-1
By5ugjyCb-R1,"The authors have addressed my concerns, and clarified a misunderstanding of the baseline that I had, which I appreciate . I do think that it is a solid contribution with thorough experiments.  I still keep my original rating of the paper because the method presented is heavily based on previous works, which limits the novelty of the paper.  It uses previously proposed clipping activation function for quantization of neural networks, adding a learnable parameter to this function.  \n_______________\nORIGINAL REVIEW:\n\nThis paper proposes to use a clipping activation function as a replacement of ReLu to train a neural network with quantized weights and activations.  It shows empirically that even though the clipping activation function obtains a larger training error for full-precision model, it maintains the same error when applying quantization, whereas training with quantized ReLu activation function does not work in practice because it is unbounded. \n\nThe experiments are thorough, and report results on many datasets, showing that PACT can reduce down to 4 bits of quantization of weights and activation with a slight loss in accuracy compared to the full-precision model.  \nRelated to that, it seams a bit an over claim to state that the accuracy decrease of quantizing the DNN with PACT in comparison with previous quantization methods is much less because the decrease is smaller or equal than 1%, when competing methods accuracy decrease compared to the full-precision model is more than 1%.  Also, it is unfair to compare to the full-precision model using clipping, because ReLu activation function in full-precision is the standard and gives much better results, and this should be the reference accuracy . Also, previous methods take as reference the model with ReLu activation function, so it could be that in absolute value the accuracy performance of competing methods is actually higher than when using PACT for quantizing DNN. \n\nOTHER COMMENTS:\n\n- the list of contributions is a bit strange . It seams that the true contribution is number 1 on the list, which is to introduce the parameter \\alpha in the activation function that is learned with back-propagation, which reduces the quantization error with respect to using ReLu as activation function.  To provide an analysis of why it works and quantitative results, is part of the same contribution I would say.",1,1,1,1,1,1,1,1,1,-1
By5ugjyCb-R2,"The parameterized clipping activation (PACT) idea is very clear: extend clipping activation by learning the clipping parameter.  Then,  PACT is combined with quantizing the activations.  \n\nThe proposed technique sounds.  The performance improvement is expected and validated by experiments.  \n\nBut I am not sure if the novelty is strong enough for an ICLR paper. \n",1,1,-1,1,-1,-1,1,1,1,-1
By5ugjyCb-R3,"This paper presents a new idea to use PACT to quantize networks, and showed improved compression and comparable accuracy to the original network.  The idea is interesting and novel that PACT has not been applied to compressing networks in the past.  The results from this paper is also promising that it showed convincing compression results.  \n\nThe experiments in this paper is also solid and has done extensive experiments on state of the art datasets and networks.  Results look promising too. \n\nOverall the paper is a descent one, but with limited novelty.  I am a weak reject",1,1,-1,1,-1,-1,1,1,1,-1
By9iRkWA--R1,This paper introduces a fairly elaborate model for reading comprehension evaluated on the SQuAD dataset.    The model is shown to improve on the published results but not as-of-submission leaderboard numbers. \n\nThe main weakness of the paper in my opinion is that the innovations seem to be incremental and not based on any overarching insight or general principle.   A less significant issue is that the English is often disfluent. \n\nSpecific comments: I would remove the significance daggers from table 2 as the standard deviations are already reported and the null hypothesis for which significance is measured seems unclear.   I am also concerned to see test performance significantly better than development performance in table 3.   Other systems seem to have development and test performance closer together.   Have the authors been evaluating many times on the test data,1,1,1,1,1,-1,1,1,1,-1
By9iRkWA--R2,"Summary: The paper introduces \""Phase Conductor\"", which consists of two phases, context-question attention phase and context-context (self) attention phase.  Each phase has multiple layers of attention, for which the paper uses a novel way to fuse the layers, and context-question attention uses different question embedding for getting the attention weight and getting the attention vector.  The paper shows that the model achieves state of the art on SQuAD among published papers, and also quantitatively and visually demonstrates that having multiple layers of attention is helpful for context-context attention, while it is not so helpful for context-question attention. \n\n\nNote: While I will mostly try to ignore recently archived, non-published papers when evaluating this paper, I would like to mention that the paper's ensemble model currently stands 11th on SQuAD leaderboard. \n\n\nPros:\n- The model achieves SOTA on SQuAD among published papers. \n- The sequential fusing (GRU-like) of the multiple layers of attention is interesting and novel.  Visual analysis of the attention map is convincing. \n- The paper is overall well-written and clear. \n\nCons:\n- Using different embedding for computing attention weights and getting attended vector is not entirely novel but rather an expected practice for many memory-based models, and should cite relevant papers.  For instance, Memory Networks [1] uses different embedding for key (computing attention weight) and value (computing attended vector). \n- While ablations for number of attention layers (1 or 2) were visually convincing, numerically there is a very small difference even for selfAtt.  For instance, in Table 4, having two layers of selfAtt (with two layers of question-passage) only increases max F1 by 0.34, where the standard deviation is 0.31 for the one layer.  While this may be statistically significant, it is a very small gain nonetheless. \n- Given the above two cons, the main contribution of the paper is 1.1% improvement over previous state of the art.  I think this is a valuable engineering contribution,;  but I feel that it is not well-suited / sufficient for ICLR audience.  \n\n\nQuestions:\n- page 7 first para: why have you not tried GloVe 300D, if you think it is a critical factor? \n\n\nErrors:\n- page 2 last para: \""gives an concrete\"" -> \""gives a concrete\""\n- page 2 last para: \""matching\"" -> \""matched\""\nFigure 1: I think \""passage embedding h\"" and \""question embedding v\"" boxes should be switched.\n- page 7 3.3 first para: \""evidence fully\"" -> \""evidence to be fully\"". \n\n\n[1] Jason Weston, Sumit Chopra, and Antoine Bordes. Memory Networks. ICLR 2015.",1,1,1,1,1,1,1,1,1,1
By9iRkWA--R3,"This paper proposes a new machine comprehension model, which integrates several contributions like different embeddings for gate function and passage representation function, self-attention layers and highway network based fusion layers.  The proposed method was evaluated on the SQuAD dataset only, and marginal improvement was observed compared to the baselines. \n\n(1) One concern I have for this paper is about the evaluation. The paper only evaluates the proposed method on the SQuAD data with systems submitted in July 2017, and the improvement is not very large.  As a result, the results are not suggesting significance or generalizability of the proposed method. \n\n(2) The paper gives some ablation tests like reducing the number of layers and removing the gate-specific question embedding, which help a lot for understanding how the proposed methods contribute to the improvement.  However, the results show that the deeper self-attention layers are indeed useful (but still not improving a lot, about 0.7-0.8%).  The other proposed components contribute less significant.  As a result, I suggest the authors add more ablation tests regarding (1) replacing the outer-fusion with simple concatenation (it should work for two attention layers); (2) removing the inner-fusion layer and only use the final layer's output, and using residual connections (like many NLP papers did) instead of the more complicated GRU stuff. \n\n(3) Regarding the ablation in Table 2, my first concern is that the improvement seems small (~0.5%).  As a result, I am wondering whether this separated question embedding really brings new information, or the similar improvement can be achieved by increasing the size of LSTM layers.  For example, if we use the single shared question embeddings, but increase the size from 128 to some larger number like 192, can we observe similar improvement.  I suggest the authors try this experiment as well and I hope the answer is no, as separated input embeddings for gate functions was verified to be useful in some \""old\"" works with syntactic features as gate values, like \""Semantic frame identification with distributed word representations\"" and \""Learning composition models for phrase embeddings\"" etc. \n\n(4) Please specify which version of the SQuAD leaderboard is used in Table 3.  Is it a snapshot of the Jul 14 one?  Because this paper is not comparing to the state-of-the-art, no specification of the leaderboard version may confuse the other reviewers and readers.  By the way, it will be better to compare to the snapshot of Oct 2017 as well, indicating the position of this work during the submission deadline. \n\nMinor issues:\n\n(1) There are typos in Figure 1 regarding the notations of Question Features and Passage Features. \n\n(2) In Figure 1, I suggest adding an \""N \\times\"" symbol to the left of the Q-P Attention Layer and remove the current list of such layers, in order to be consistent to the other parts of the figure. \n\n(3) What is the relation between the \""PhaseCond, QPAtt+\""\b in Table 2 and the \""PhaseCond\"" in Table 3?  I was assuming that those are the same system but did not see the numbers match each other",1,1,1,1,1,-1,1,1,1,-1
Bya8fGWAZ-R1,"The original value-iteration network paper assumed that it was trained on near-expert trajectories and used that information to learn a convolutional transition model that could be used to solve new problem instances effectively without further training. \n\nThis paper extends that work by\n- training from reinforcement signals only, rather than near-expert trajectories\n- making the transition model more state-depdendent\n- scaling to larger problem domains by propagating reward values for navigational goals in a special way \n\nThe paper is fairly clear and these extensions are reasonable .  However, I just don't think the focus on 2D grid-based navigation has sufficient interest and impact .  It's true that the original VIN paper worked in a grid-navigation domain, but they also had a domain with a fairly different structure;  I believe they used the gridworld because it was a convenient initial test case, but not because of its inherent value.  So, making improvements to help solve grid-worlds better is not so motivating   The work on dynamic environments was an interesting step:  it would have been interesting to see how the \""models\"" learned for the dynamic environments differed from those for static environments",1,1,1,1,1,1,1,1,1,-1
Bya8fGWAZ-R2,"ORIGINALITY & SIGNIFICANCE\n\nThe authors build upon value iteration networks: the idea that the value function can be computed efficiently from rewards and transitions using a dedicated convolutional network.  The authors point out that the original \""value iteration network\u201d (Tamar 2016) did not handle non-stationary dynamics models or variable size problems well and propose a new formulation to extend the model to this case which they call a value propagation network.   It seems useful and practical to compute value iteration explicitly as this will propagate values for us without having to learn the propagated form through extensive gradient update steps.  Extending to the scenario of non-stationary dynamics is important to make the idea applicable to common problems.  The work is therefore original and significant. \n\nThe algorithm is evaluated on the original obstacle grids from Tamar 2016 and larger grids generated to test scalability.  The authors Prop and MVProp are able to solve the grids with much higher reliability at the end of training and converge much faster.   The M in MVProp in particular seems to be very useful in scaling up to the large grids.  The authors also show that the algorithm handles non-stationary dynamics in an avalanche task where obstacles can fall over time. \n\n\nQUALITY\n\nThe symbol d_{rew} is never defined \u2014 what does \u201cnew\u201d stand for?  It appears to be the number of latent convolutional filters or channels generated by the state embedding network.  \n\nSection 2.2 Sentence 2: The final layer representing the encoding is given as ( R^{d_rew  x d_x x d_y }. \nBased on the description  in the first paragraph of section 2, it sounds like d_rew might be the number of channels or filters in the last convolutional layer.  \n\nIn equation 1, it wasn\u2019t obvious to me that the expression max_a q_{ij}^{k-1} q^{k} corresponds to an actual operation? \nThe h( \\Phi( x ), v^{k-1} ) sort of makes sense \u2026  value is only calculated with respect to only the observation of the maze obstacles but the policy \\pi is calculated with respect to the joint  observation and agent state.  \n\nThe expression \n\n   h_{aid} ( \\phi(0), v )   =   <  Wa,   [ \\phi(o) ; v ]   >   +   b\n\nmakes sense and reminds me of the Value Iteration network work where we take the previous value function, combine it with the reward function and use convolution to compute the expectation (the weights Wa encode the effect of transitions).  I gather the tensor Wa = R^{|A| x (d_{rew} x d_x x d_y } both converts the feature embedding \\phi{o} to rewards and represents the transition / propagation of reward across states due to transitions and discounts at the same time?  \n\nI didn\u2019t understand the r^in, r&out representation in section 4.1. These are given by the domain? \n\nI did get the overall idea of efficiently creating a local value function in the neighborhood of the current state and passing this to the policy so that it can make a local decision. \n\nA bit more detail defining terms, explaining their intuitive role and how the output of one module feeds into the next would be helpful. \n\n\nPOST REVISION COMMENTS:\n\n- I didn't reread the whole thing -  just used the diff tool.   \n- It looks like the typos in the equations got fixed\n- The new phrase \""enables to learn to plan\"" seems pretty awkward",1,1,1,1,1,1,-1,1,1,-1
Bya8fGWAZ-R3,"The paper introduces two alternatives to value iteration network (VIN) proposed by Tamar et al. VIN was proposed to tackle the task of learning to plan using as inputs a position and an image of the map of the environment.  The authors propose two new updates value propagation (VProp) and max propagation (MVProp), which are roughly speaking additive and multiplicative versions of the update used in the Bellman-Ford algorithm for shortest path.  The approaches are evaluated in grid worlds with and without other agents. \n\nI had some difficulty to understand the paper because of its presentation and writing (see below). \n\nIn Tamar's work, a mapping from observation to reward is learned.   As a consequence, those two methods need to take as input a new reward function for every new map.  Is that correct? \nI think this could explain the better experimental results\n\nIn the experimental part, the results for VIN are worse than those reported in Tamar et al.'s paper.  Why did you use your own implementation of VIN and not Tamar et al.'s, which is publicly shared as far as I know? \n\nI think the writing needs to be improved on the following points:\n- The abstract doesn't fit well the content of the paper.  For instance, \""its variants\"" is confusing because there is only other variant to VProp.  \""Adversarial agents\"" is also misleading because those agents act like automata. \n\n- The authors should recall more thoroughly and precisely the work of Tamar et al., on which their work is based to make the paper more self-contained, e.g., (1) is hardly understandable. \n\n- The writing should be careful, e.g., \nvalue iteration is presented as a learning algorithm (which in my opinion is not) \n\\pi^* is defined as a distribution over state-action space and then \\pi is defined as a function;  ...\n\n- The mathematical writing should be more rigorous;  e.g., \np.2:\nT: s \\to a \\to s', \\pi : s \\to a\nA denotes a set and its cardinal\nIn (1), shouldn't it be \\Phi(o)? all the new terms should be explained\np. 3:\ndefinition of T and R \nshouldn't V_{ij}^k depend on Q_{aij}^k?\nT_{::aij} should be defined\nIn the definition of h_{aij}, should \\Phi and b be indexed by a? \n\n- The typos and other issues should be fixed:\np. 3:\nK iteration\nwith capable\np.4:\nclose 0\np.5:\nour our\ns^{t+1} should be defined like the other terms\n\""The state is represented by the coordinates of the agent and 2D environment observation\"" should appear much earlier in the paper. \n\""\\pi_\\theta described in the previous sections\"", notation \\pi_\\theta appears the first time here...\n3x3 -> 3 \\times 3\nofB\nV_{\\theta^t w^t} \np.6:\nthe the\nFig.2's caption:\nWhat does \""both cases\"" refer to? They are three models. \nReferences:\net al.\nYI WU\n",1,1,1,1,1,1,1,1,-1,-1
ByaQIGg0--R1,"This paper proposes to use neural network and gradient descent to automatically design for engineering tasks.  It uses two networks, parameterization network and prediction network to model the mapping from design parameters to fitness.  It uses back propagation (gradient descent) to improve the design.  The method is evaluated on heat sink design and airfoil design. \n\nThis paper targets at a potentially very useful application of neural networks that can have real world impacts.  However, I have three main concerns:\n1) Presentation. The organization of the paper could be improved.  It mixes the method, the heat sink example and the airfoil example throughout the entire paper.  Sometimes I am very confused about what is being described.  My suggestion would be to completely separate these three parts: present a general method first, then use heat sink as the first experiment and airfoil as the second experiment.  This organization would make the writing much clearer. \n\n2) In the paragraph above Section 4.1, the paper made two arguments. I might be wrong, but I do not agree with either of them in general.  First of all, \""neural networks are good at generalizing to examples outside their train set\"". This depends entirely on whether the sample distribution of training and testing are similar and whether you have enough training examples that cover important sample space.  This is especially critical if a deep neural network is used since overfitting is a real issue.  Second, \""it is easy to imagine a hybrid system where a network is trained on a simulation and fine tuned ...\"". Implementing such a hybrid system is nontrivial due to the reality gap.  There is an entire research field about closing the reality gap and transfer learning.  So I am not convinced by these two arguments made by this paper.  They might be true for a narrow field of application. But in general, I think they are not quite correct. \n\n3) The key of this paper is to approximate the dynamics using neural network (which is a continuous mapping) and take advantage of its gradient computation.  However, many of dynamic systems are inherently discontinuous (collision/contact dynamics) or chaotic (turbulent flow).  In those scenarios, the proposed method might not work well and we may have to resort to the gradient free methods.  It seems that the proposed method works well for heat sink problem and the steady flow around airfoil, both of which do not fall into the more complex physics regime.  It would be great that the paper could be more explicit about its limitations. \n\nIn summary, I like the idea, the application and the result of this paper.  The writing could be improved.  But more importantly, I think that the proposed method has its limitation about what kind of physical systems it can model.  These limitation should be discussed more explicitly and more thoroughly",1,1,1,1,1,-1,1,1,1,-1
ByaQIGg0--R2,"This paper introduces an appealing application of deep learning: use a deep network to approximate the behavior of a complex physical system, and then design optimal devices (eg airfoil shapes) by optimizing this network with respect to its inputs.  Overall, this research direction seems fruitful, both in terms of different applications and in terms of extra machine learning that could be done to improve performance, such as ensuring that the optimization doesn't leave the manifold of reasonable designs.  \n\n On one hand, I would suggest that this work would be better placed in an engineering venue focused on fluid dynamics.  On the other hand, I think the ICLR community would benefit from about the opportunities to work on problems of this nature. \n\n =Quality=\nThe authors seem to be experts in their field.  They could have done a better job explaining the quality of their final results, though.  It is unclear if they are comparing to strong baselines. \n\n=Clarity=\nThe overall setup and motivation is clear. \n\n=Originality=\nThis is an interesting problem that will be novel to most member of the ICLR community.  I think that this general approach deserves further attention from the community. \n\n\n=Major Comments=\n* It's hard for me to understand if the performance of your method is actually good.  You show that it outperforms simulated annealing. Is this the state of the art?  How would an experienced engineer perform if he or she just sat down and drew the shape of an airfoil, without relying on any computational simulation at all? \n\n* You can afford to spend lots of time interacting with the deep network in order to optimize it really well with respect to the inputs.  Why not do lots of random initializations for the optimization?  Isn't that a good way to help avoid local optima? \n\n* I'd like to see more analysis of the reliability of your deep-network-based approximation to the physics simulator.  For example, you could evaluate the deep-net-predicted drag ratio vs. the simulator-predicted drag ratio at the value of the parameters corresponding to the final optimized airfoil shape.  If there's a gap, it suggests that your NN approximation might have not been that accurate. \n\n=Minor Comments=\n* \""We also found that adding a small amount of noise too the parameters when computing gradients helped jump out of local optima\""\nGenerally, people add noise to the gradients, not the values of the parameters.  See, for example, uses of Langevin dynamics as a non-convex optimization method. \n\n* You have a complicated method for constraining the parameters to be in [-0.5,0.5].  Why not just enforce this constraint by doing projected gradient descent?  For the constraint structure you have, projection is trivial (just clip the values) . \n\n * \""The gradient decent approach required roughly 150 iterations to converge where as the simulated annealing approach needed at least 800.\""\nThis is of course confounded by the necessary cost to construct the training set, which is necessary for the gradient descent approach.  I'd point out that this construction can be done in parallel, so it's less of a computational burden. \n\n* I'd like to hear more about the effects of different parametrizations of the airfoil surface.  You optimize the coefficients of a polynomial. Did you try anything else? \n\n* Fig 6: What does 'clean gradients' mean?  Can you make this more precise? \n\n* The caption for Fig 5 should explain what each of the sub figures is",1,1,1,1,1,-1,1,1,1,-1
ByaQIGg0--R3,"1. This is a good application paper, can be quite interesting in a workshop related to Deep Learning applications to physical sciences and engineering. \n2. Lacks in sufficient machine learning related novelty required to be relevant in the main conference \n3. Design, solving inverse problem using Deep Learning are not quite novel, see\nStoecklein et al. Deep Learning for Flow Sculpting: Insights into Efficient Learning using Scientific Simulation Data. Scientific Reports 7, Article number: 46368 (2017). \n4. However, this paper introduces two different types of networks for \""parametrization\"" and \""physical behavior\"" mapping, which is interesting, can be very useful as surrogate models for CFD simulations.  \n5. It will be interesting to see the impacts of physics based knowledge on choice of network architecture, hyper-parameters and other training considerations. \n6. Just claiming the generalization capability of deep networks is not enough, need to show how much the model can interpolate or extrapolate?  what are the effects of regulariazations in this regard?",1,1,1,1,1,1,1,1,1,-1
ByCPHrgCW-R1,"This paper proposes a hybrid Homomorphic encryption system that is well suited for privacy-sensitive data inference applications with the deep learning paradigm.  \nThe paper presents a well laid research methodology that shows a good decomposition of the problem at hand and the approach foreseen to solve it.  It is well reflected in the paper and most importantly the rationale for the implementation decisions taken is always clear. \n\nThe results obtained (as compared to FHEW) seem to indicate well thought off decisions taken to optimize the different gates' operations as clearly explained in the paper.  For example, reducing bootstrapping operations by two-complementing both the plaintext and the ciphertext, whenever the number of 1s in the plain bit-string is greater than the number of 0s (3.4/Page 6). \n\nResult interpretation is coherent with the approach and data used and shows a good understanding of the implications of the implementation  decisions made in the system and the data sets used. \nOverall, fine work, well organized, decomposed, and its rationale clearly explained.  The good results obtained support the design decisions made. \nOur main concern is regarding thorough comparison to similar work and provision of comparative work assessment to support novelty claims. \n\nNota: \n     - In Figure 4/Page 4: AND Table A(1)/B(0), shouldn't  A And B be 0? \n     - Unlike Figure 3/Page 3, in Figure 2/page 2, shouldn't  operations' precedence prevail (No brackets), therefore 1+2*2=5",1,1,1,1,1,1,1,-1,1,-1
ByCPHrgCW-R2,"Summary:\nThis paper proposes a framework for private deep learning model inference using FHE schemes that support fast bootstrapping. \nThe main idea of this paper is that in the two-party computation setting, in which the client's input is encrypted while the server's deep learning model is plain. \nThis \""hybrid\"" argument enables to reduce the number of necessary bootstrapping, and thus can reduce the computation time. \nThis paper gives an implementation of adder and multiplier circuits and uses them to implement private model inference. \n\nComments:\n1. I recommend the authors to tone down their claims.  For example, the authors mentioned that \""there has been no complete implementation of established deep learning approaches\"" in the abstract, however, the authors did not define what is \""complete\"".  Actually, the SecureML paper in S&P'17 should be able to privately evaluate any neural networks, although at the cost of multi-round information exchanges between the client and server. \n\nAlso, the claim that \""we show efficient designs\"" is very thin to me since there are no experimental comparisons between the proposed method and existing works.  Actually, the level FHE can be very efficient with a proper use of message packing technique such as [A] and [C].  For a relatively shallow model (as this paper has used), level FHE might be faster than the binary FHE. \n\n2. I recommend the author to compare existing adder and multiplier circuits with your circuits to see in what perspective your design is better.  I think the hybrid argument (i.e., when one input wire is plain) is a very common trick that used in the circuit design field, such as garbled circuit [B], to reduce the depth of the circuit.  \n\n3. I appreciate that optimizations such as low-precision and point-wise convolution are discussed in this paper.  Such optimizations are very common in deep learning field while less known in the field of security. \n\n[A]: Dowlin et al. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. \n[B]: V. Kolesnikov et al. Improved garbled circuit: free xor gates and applications.  \n[C]: Liu et al. Oblivious Neural Network Predictions via MiniONN transformations",1,1,1,1,1,1,1,1,1,-1
ByCPHrgCW-R3,"The paper presents a means of evaluating a neural network securely using homomorphic encryption . A neural network is already trained, and its weights are public.  The network is to be evaluated over a private input, so that only the final outcome of the computation-and nothing but that-is finally learned. \n\nThe authors take a binary-circuit approach: they represent numbers via a fixed point binary representation, and construct circuits of secure adders and multipliers, based on homomorphic encryption as a building block for secure gates.  This allows them to perform the vector products needed per layer; two's complement representation also allows for an \""easy\"" implementation of the ReLU activation function, by \""checking\"" (multiplying by) the complement of the sign bit.  The fact that multiplication often involves public weights is used to speed up computations, wherever appropriate.  A rudimentary  experimental evaluation with small networks is provided. \n\nAll of this is somewhat straightforward; a penalty is paid by representing numbers via fixed point arithmetic, which is used to deal with ReLU mostly.  This is somewhat odd: it is not clear why, e.g., garbled circuits where not used for something like this, as it would have been considerably faster than FHE. \n\nThere is also a work in this area that the authors do not cite or contrast to, bringing the novelty into question; please see the following papers and references therein:; \n\nGILAD-BACHRACH, R., DOWLIN, N., LAINE, K., LAUTER, K., NAEHRIG, M., AND WERNSING, J. Cryptonets: Applying neural networks to encrypted data with high throughput and accuracy. In Proceedings of The 33rd International Conference on Machine Learning (2016), pp. 201\u2013210. \n\nSecureML: A System for Scalable Privacy-Preserving Machine Learning\nPayman Mohassel and Yupeng Zhang. \n\nSHOKRI, R., AND SHMATIKOV, V. Privacy-preserving deep learning.  In\nProceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security (2015), ACM, pp. 1310\u20131321. \n\nThe first paper is the most related, also using homomorphic encryption, and seems to cover a superset of the functionalities presented here (more activation functions, a more extensive analysis, and faster decryption times).  The second paper uses arithmetic circuits rather than HE, but actually implements training an entire neural network securely. \n\n Minor details:\n\nThe problem scenario states that the model/weights is private, but later on it ceases to be so (weights are not encrypted). \n\n\""Both deep learning and FHE are relatively recent paradigms\"". Deep learning is certainly not recent, while Gentry's paper is now 7 years old. \n\n\""In theory, this system alone could be used to compute anything securely.\"" This is informal and incorrect.  Can it solve the halting problem? \n\n\""However in practice the operations were incredibly slow, taking up to 30 minutes in some cases.\"" It is unclear what operations are referred to here",1,1,1,1,-1,1,1,1,1,-1
BydjJte0--R1,"The basic idea is to train a neural network to predict various hyperparameters of a classifier from input-output pairs for that classifier (kennen-o approach).  It is surprising that some of these hyperparameters can even be predicted with more than chance accuracy.  As a simple example, it's possible that there are values of batch size for which the classifiers may become indistinguishable,  yet Table 2 shows that batch size can be predicted with much higher accuracy than chance . It would be good to provide insights into under what conditions and why hyperparameters can be predicted accurately . That would make the results much more interesting, and may even turn out to be useful for other problems, such as hyperparameter optimization .\n\nThe selection of the queries for kennen-o is not explained. What is the procedure for selecting the queries? How sensitive is the performance of kennen-o to the choice of the queries?  One would expect that there is significant sensitivity, in which case it may even make sense to consider learning to select a sequence of queries to maximize accuracy. \n\nIn table 3, it would be useful to show the results for kennen-o as well, because Split-E seems to be the more realistic problem setting and kennen-o seems to be a more realistic attack than kennen-i or kennen-io. \n\nIn the ImageNet classifier family prediction, how different are the various families from each other?  Without going through all the references, it is difficult to get a sense of the difficulty of the prediction task for a non-computer-vision reader. \n\nOverall the results seem interesting,  but without more insights it's difficult to judge how generally useful they are",1,1,1,1,1,-1,1,1,1,-1
BydjJte0--R2,"\n-----UPDATE------\n\nHaving read the responses from the authors, and the other reviews, I am happy with my rating and maintain that this paper should be accepted. \n\n----------------------\n\n\n\nIn this paper, the authors trains a large number of MNIST classifier networks with differing attributes (batch-size, activation function, no. layers etc.) and then utilises the inputs and outputs of these networks to predict said attributes successfully.  They then show that they are able to use the methods developed to predict the family of Imagenet-trained networks and use this information to improve adversarial attack .\n\nI enjoyed reading this paper.  It is a very interesting set up, and a novel idea. \n\nA few comments:\n\nThe paper is easy to read, and largely written well . The article is missing from the nouns quite often though so this is something that should be amended. There are a few spelling slip ups (\""to a certain extend\"" --> \""to a certain extent\"", \""as will see\"" --> \""as we will see\"")] \n\nIt appears that the output for kennen-o is a discrete probability vector for each attribute, where each entry corresponds to a possibility (for example, for \""batch-size\"" it is a length 3 vector where the first entry corresponds to 64, the second 128, and the third 256) . What happens if you instead treat it as a regression task, would it then be able to hint at intermediates (a batch size of 96) or extremes (say, 512).\ n\nA flaw of this paper is that kennen-i and io appear to require gradients from the network being probed (you do mention this in passing), which realistically you would never have access to. (Please do correct me if I have misunderstood this) \n\nIt would be helpful if Section 4 had a paragraph as to your thoughts regarding why certain attributes are easier/harder to predict . Also, the caption for Table 2 could contain more information regarding the network outputs. \n\nYou have jumped from predicting 12 attributes on MNIST to 1 attribute on Imagenet . It could be beneficial to do an intermediate experiment (a handful of attributes on a middling task) .\n\nI think this paper should be accepted as it is interesting and novel .\n\nPros\n------\n- Interesting idea \n- Reads well \n- Fairly good experimental results \n\nCons\n------\n- kennen-i seems like it couldn't be realistically deployed\n- lack of an intermediate difficulty task\",1,1,1,1,1,-1,1,1,1,-1
BydjJte0--R3,"The paper attempts to study model meta parameter inference e.g. model architecture, optimization, etc using a supervised learning approach . They take three approaches one whereby the target models are evaluated on a fixed set of inputs, one where the access to the gradients is assumed and using that an input is crafted that can be used to infer the target quantities and one where both approaches are combined.  The authors also show that these inferred quantities can be used to generate more effective attacks against the targets. \n\nThe paper is generally well written and most details for reproducibility are seem enough . I also find the question interesting and the fact that it works on this relatively broad set of meta parameters and under a rigorous train/test split intriguing . It is of course not entirely surprising that the system can be trained but that there is some form of generalization happening.  \n\nAside that I think most system in practical use will be much more different than any a priori enumeration/brute force search for model parameters . I suspect in most cases practical systems will be adapted with many subsequent levels of preprocessing, ensembling, non-standard data and a number of optimization and architectural tricks that are developer dependent . It is really hard to say what a supervised learning meta-model approach such as the one presented in this work have to say about that case . \n\nI have found it hard to understand what table 3 in section 4.2 actually means . It seems to say for instance that a model is trained on 2 and 3 layers then queried with 4 and the accuracy only slightly drops . Accuracy of what ? Is it the other attributes ? Is it somehow that attribute ? if so how can that possibly ?  \n\nMy main main concern is extrapolation out of the training set which is particularly important here. I don't find enough evidence in 4.2 for that point.  One experiment that i would find compelling is to train for instance a meta model on S,V,B,R but not D on imagenet, predict all the attributes except architecture and see how that changes when D is added . If these are better than random and the perturbations are more successful it would be a much more compelling story.",1,1,1,1,1,-1,1,1,1,1
Byht0GbRZ-R1,"Summary:\nThis paper introduces a structured attention mechanisms to compute alignment scores among all possible spans in two given sentences.  The span representations are weighted by the spans marginal scores given by the inside-outside algorithm.  Experiments on TREC-QA and SNLI show modest improvement over the word-based structured attention baseline (Parikh et al., 2016). \n\nStrengths:\nThe idea of using latent syntactic structure, and computing cross-sentence alignment over spans is very interesting.  \n\nWeaknesses:\nThe paper is 8.5 pages long .\n\nThe method did not out-perform other very related structured attention methods (86.8, Kim et al., 2017, 86.9, Liu and Lapata, 2017) \n\nAside from the time complexity from the inside-outside algorithm (as mentioned by the authors in conclusion), the comparison among all pairs of spans is O(n^4), which is more expensive.  Am I missing something about the algorithm? \n\nIt would be nice to show, quantitatively, the agreement between the latent trees and gold/supervised syntax.  The paper claimed \u201cthe model is able to recover tree structures that very closely mimic syntax\u201d, but it\u2019s hard to draw this conclusion from the two examples in Figure",1,1,1,1,1,1,1,1,1,-1
Byht0GbRZ-R2,"This paper proposes a model of \""structured alignments\"" between sentences as a means of comparing two sentences by matching their latent structures.  Overall, this paper seems a straightforward application of the model first proposed by Kim et al. 2017 with latent tree attention .\n\nIn section 3.1, the formula for p(c|x) looks wrong: c_{ijk} are indicator variables.  but where are the scores for each span?  I think it should be c_{ijk} * \\delta_{ijk} under the summations instead. \n\nIn the same section, the expression for \\alpha_{ij} seems to assume that \\delta_{ijk} = \\dlta_{ij} regardless of k. I.e. there are no production rule scores (transitions).  This seems rather limiting, can you comment on that? \n\nIn the answer selection and NLI experiments, the proposed model does not beat the SOTA, and is only marginally better than unstructured decomposable attention. This is rather disappointing.  \n\nThe plots in Fig 2 with the marginals on CKY charts are not very enlightening.  How do this marginals help solving the NLI task? \n\nMinor comments:\n- Sec. 3: \""Language is inherently tree structured\"" -- this is debatable... \n- page 8: (laf, 2008): bad formatted reference",1,1,1,1,1,1,1,1,1,-1
Byht0GbRZ-R3,"This paper describes the use of latent context-free derivations, using\na CRF-style neural model, as a latent level of representation in neural\nattention models that consider pairs of sentences.  The model implicitly\nlearns a distribution over derivations, and uses marginals under this\ndistribution to bias attention distributions over spans in one sentence\ngiven a span in another sentence. \n\nThis is an intriguing idea . I had a couple of reservations however:\n\n* The empirical improvements from the method seem pretty marginal, to the\npoint that it's difficult to know what is really helping the model.  I would\nliked to have seen more explanation of what the model has learned, and\nmore comparisons to other baselines that make use of attention over spans. \nFor example, what happens if every span is considered as an independent random\nvariable, with no use of a tree structure or the CKY chart? \n\n* The use of the \\alpha^0 vs. \\alpha^1 variables is not entirely clear.  Once they\nhave been calculated in Algorithm 1, how are they used?  Do the \\rho values\nsomewhere treat these two quantities differently? \n\n* I'm skeptical of the type of qualitative analysis in section 4.3, unfortunately. \nI think something much more extensive would be interesting here. As one\nexample, the PP attachment example with \""at a large venue\"" is highly suspect;\nthere's a 50/50 chance that any attachment like this will be correct, there's\nabsolutely no way of knowing if the model is doing something interesting/correct\nor performing at a chance level, given a single example.",1,1,1,1,1,-1,1,1,1,-1
ByhthReRb-R1,"The paper proposes to generate embedding of named-entities on the fly during dialogue sessions.  If the text is from the user, a named entity recognizer is used.  If it is from the bot response, then it is known which words are named entities therefore embedding can be constructed directly.  The idea has some novelty and the results on several tasks attempting to prove its effectiveness against systems that handle named entities in a static way. \n\nOne thing I hope the author could provide more clarification is the use of NER.  For example, the experimental result on structured QA task (section 3.1), where it states that the performance different between models of With-NE-Table and W/O-NE-Table is positioned on the OOV NEs not present in the training subset.  To my understanding, because of the presence of the NER in the With-NE-Table model, you could directly do update to the NE embeddings and query from the DB using a combination of embedding and the NE words (as the paper does), whereas the W/O-NE-Table model cannot because of lack of the NER.  This seems to prove that an NER is useful for tasks where DB queries are needed, rather than that the dynamic NE-Table construction is useful.  You could use an NER for W/O-NE-Table and update the NE embeddings, and it should be as good as With-NE-Table model (and fairer to compare with too). \n\nThat said, overall the paper is a nice contribution to dialogue and QA system research by pointing out a simple way of handling named entities by dynamically updating their embeddings.  It would be better if the paper could point out the importance of NER for user utterances, and the fact that using the knowledge of which words are NEs in dialogue models could help in tasks where DB queries are necessary",1,1,1,1,1,-1,1,1,1,-1
ByhthReRb-R2,"The paper addresses the task of dealing with named entities in goal oriented dialog systems.  Named entities, and rare words in general, are indeed troublesome since adding them to the dictionary is expensive, replacing them with coarse labels (ne_loc, unk) looses information, and so on.  The proposed solution is to extend neural dialog models by introducing a named entity table, instantiated on the fly, where the keys are distributed representations of the dialog context and the values are the named entities themselves.  The approach is applied to settings involving interacting to a database and a mechanism for handling the interaction is proposed.  The resulting model is illustrated on a few goal-oriented dialog tasks. \n\n\nI found the paper difficult to read.  The concrete mappings used to create the NE keys and attention keys are missing.  Providing more structure to the text would also be useful vs. long, wordy paragraphs.  Here are some specific questions:\n\n1. How are the keys generated?  That are the functions used?  Does the \""knowledge of the current user utterance\"" include the word itself?  The authors should include the exact model specification, including for the HRED model. \n\n2. According to the description, referring to an existing named entity must be done by \""generating a key to match the keys in the NE table and then retrieve the corresponding value and use it\"".  Is there a guarantee that a same named entity, appearing later in the dialog, will be given the same key?   Or are the keys for already found entities retrieved directly, by value? \n\n3. In the decoding phase, how does the system decide whether to query the DB? \n\n4. How is the model trained? \n\nIn its current form, it's not clear how the proposed approach tackles the shortcomings mentioned in the introduction.  Furthermore, while the highlighted contribution is the named entity table, it is always used in conjunction to the database approach.  This raises the question whether the named entity table can only work in this context. \n\nFor the structured QA task, there are 400 training examples, and 100 named entities. This means that the number of training examples per named entity is very small.  Is that correct?  If yes, then it's not very surprising that adding the named entities to the vocabulary leads to overfitting.  Have you compared with using random embeddings for the named entities? \n\nTypos: page 2, second-to-last paragraph: firs -> first, page 7, second to last paragraph: and and -> and",1,1,1,1,1,-1,1,1,1,-1
ByhthReRb-R3,"Properly capturing named entities for goal oriented dialog is essential, for instance location, time and cuisine for restaurant reservation.  Mots successful approaches have argued for separate mechanism for NE captures, that rely on various hacks and tricks.  This paper attempt to propose a comprehensive approach offers intriguing new ideas, but is too preliminary, both in the descriptions and experiments.  \n\nThe proposed methods and experiments are not understandable in the current way the paper is written: there is not a single equation, pseudo-code algorithm or pointer to real code to enable the reader to get a detailed understanding of the process.  All we have a besides text is a small figure (figure 1).  Then we have to trust the authors that on their modified dataset, the accuracies of the proposed method is around 100% while not using this method yields 0% accuracies? \n\nThe initial description (section 2)  leaves way too many unanswered questions:\n- What embeddings are used for words detected as NE?  Is it the same as the generated representation? \n- What is the exact mechanism of generating a representation for NE EECS545?  (end of page 2)\n- Is it correct that the same representation stored in the NE table is used twice?  (a) To retrieve the key (a vector) given the value (a string)  as the encoder input.  (b) To find the value that best matches a key at the decoder stage? \n- Exact description of the column attention mechanism: some similarity between a key embedding and embeddings representing each column? Multiplicative? Additive? \n- How is the system supervised?  Do we need to give the name of the column the Attention-Column-Query attention should focus on?  Because of this unknown, I could not understand the experiment setup and data formatting! \n\nThe list goes on...\n\nFor such a complex architecture, the authors must try to analyze separate modules as much as possible.  As neither the QA and the Babi tasks use the RNN dialog manager, while not start with something that only works at the sentence level. \n\nThe Q&A task could be used to describe a simpler system with only a decoder accessing the DB table.  Complexity for solving the Babi tasks could be added later",1,1,1,1,1,-1,1,1,1,1
ByJDAIe0b-R1,"This paper considers a new way to incorporate episodic memory with shallow-neural-nets RL using reservoir sampling.  The authors propose a reservoir sampling algorithm for drawing samples from the memory.  Some theoretical guarantees for the efficiency of reservoir sampling are provided.  The whole algorithm is tested on a toy problem with 3 repeats.  The comparisons between this episodic approach and recurrent neural net with basic GRU memory show the advantage of proposed algorithm. \n\nThe paper is well written and easy to understand.  Typos didn't influence reading.  It is a novel setup to consider reservoir sampling for episodic memory.  The theory part focuses on effectiveness of drawing samples from the reservoir.  Physical meanings of Theorem 1 are not well represented.  What are the theoretical advantages of using reservoir sampling?  \n\nFour simple, shallow neural nets are built as query, write, value, and policy networks.  The proposed architecture is only compared with a recurrent baseline with 10-unit GRU network.  It is not clear the better performance comes from reservoir sampling or other differences.  Moreover, the hyperparameters are not optimized on different architectures. It is hard to justify the empirically better performance without hyperparameter tuning.  The authors mentioned that the experiments are done on a toy problem, only three repeats for each experiment.The technically soundness of this work is weakened by the experiments",1,1,1,1,1,-1,1,1,1,-1
ByJDAIe0b-R2,"The paper proposes a modified approach to RL, where an additional \""episodic memory\"" is kept by the agent.  What this means is that the agent has a reservoir of n \""states\"" in which states encountered in the past can be stored.  There are then of course two main questions to address (i) which states should be stored and how  (ii) how to make use of the episodic memory when deciding what action to take.  \n\nFor the latter question, the authors propose using a \""query network\"" that based on the current state, pulls out one state from the memory according to certain probability distribution.  This network has many tunable parameters, but the main point is that the policy then can condition on this state drawn from the memory.  Intuitively, one can see why this may be advantageous as one gets some information from the past.  (As an aside, the authors of course acknowledge that recurrent neural networks have been used for this purpose with varying degrees of success.) \n\nThe first question, had a quite an interesting and cute answer. There is a (non-negative) importance weight associated with each state and a collection of states has weight that is simply the product of the weights.  The authors claim (with some degree of mathematical backing) that sampling a memory of n states where the distribution over the subsets of past states of size n is proportional to the product of the weights is desired. And they give a cute online algorithm for this purpose.  However, the weights themselves are given by a network and so weights may change (even for states that have been observed in the past) . There is no easy way to fix this and for the purpose of sampling the paper simply treats the weights as immutable.  \n\nThere is also a toy example created to show that this approach works well compared to the RNN based approaches. \n\nPositives:\n\n- An interesting new idea that has potential to be useful in RL \n- An elegant algorithm to solve at least part of the problem properly (the rest of course relies on standard SGD methods to train the various networks) \n\nNegatives:\n- The math is fudged around quite a bit with approximations that are not always justified \n- While overall the writing is clear, in some places I feel it could be improved . I had a very hard time understanding the set-up of the problem in Figure 2.  [In general, I also recommend against using figure captions to describe the setup. ]\n- The experiments only demonstrate the superiority of this method on an example chosen artificially to work well with this approach",1,1,1,1,1,1,1,1,1,-1
ByJDAIe0b-R3,"This paper proposes one RL architecture using external memory for previous states, with the purpose of solving the non-markov tasks.  The essential problems here are how to identify which states should be stored and how to retrieve memory during action prediction.  The proposed architecture could identify the \u2018key\u2019 states through assigning higher weights for important states, and applied reservoir sampling to control write and read on memory.  The weight assigning (write) network is optimized for maximize the expected rewards.  This article focuses on the calculation of gradient for write network, and provides some mathematical clues for that. \n\nThis article compares their proposed architecture with RNN (GRU with 10 hidden unit) in few toy tasks.  They demonstrate that proposed model could work better and rational of write network could be observed.  However, it seems that hyper-parameters for RNN haven\u2019t been tuned enough.  It is because the toy task author demonstrates is actually quite similar to copy tasks, that previous state should be remembered.  To my knowledge, copy task could be solved easily for super long sequence through RNN model.  Therefore, empirically, it is really hard to justify whether this proposed method could work better.  Also, intuitively, this episodic memory method should work better on long-term dependencies task, while this article only shows the task with 10 timesteps.  \n\nAccording to that, the experiments they demonstrated in this article are not well designed so that the conclusion they made in this article is not robust enough.",1,1,1,1,-1,-1,1,1,1,-1
ByJHuTgA--R1,"The submitted manuscript describes an exercise in performance comparison for neural language models under standardization of the hyperparameter tuning and model selection strategies and costs.  This type of study is important to give perspective to non-standardized performance scores reported across separate publications,  and indeed the results here are interesting as they favour relatively simpler structures. \n\nI have a favourable impression of this paper  but would hope another reviewer is more familiar with the specific application domain than I am.",1,-1,-1,1,-1,-1,1,1,-1,-1
ByJHuTgA--R2,"The authors did extensive tuning of the parameters for several recurrent neural architectures . The results are interesting.  However the corpus the authors choose are quite small,  the variance of the estimate will be quite high, I suspect whether the same conclusions could be drawn .\n\nIt would be more convincing if there are experiments on the billion word corpus or other larger datasets, or at least on a corpus with 50 million tokens.  This will use significant resources and is much more difficult,  but it's also really valuable, because it's much more close to real world usage of language models.  And less tuning is needed for these larger datasets.  \n\nFinally it's better to do some experiments on machine translation or speech recognition and see how the improvement on BLEU or WER could get.",1,1,1,1,1,-1,1,1,-1,-1
ByJHuTgA--R3,"The authors perform a comprehensive validation of LSTM-based word and character language models, establishing that recent claims that other structures can consistently outperform the older stacked LSTM architecture result from failure to fully explore the hyperparameter space.  Instead, with more thorough hyperparameter search, LSTMs are found to achieve state-of-the-art results on many of these language modeling tasks. \nThis is a significant result in language modeling and a milestone in deep learning reproducibility research.  The paper is clearly motivated and authoritative in its conclusions  but it's somewhat lacking in detailed model or experiment descriptions. \n\nSome further points:\n\n- There are several hyperparameters set to the \""standard\"" or \""default\"" value, like Adam's beta parameter and the batch size/BPTT length.  Even if it would be prohibitive to include them in the overall hyperparameter search, the community is curious about their effect and it would be interesting to hear if the authors' experience suggests that these choices are indeed reasonably well-justified. \n\n- The description of the model is ambiguous on at least two points. First, it wasn't completely clear to me what the down-projection is (if it's simply projecting down from the LSTM hidden size to the embedding size, it wouldn't represent a hyperparameter the tuner can set, so I'm assuming it's separate and prior to the conventional output projection).  Second, the phrase \""additive skip connections combining outputs of all layers\"" has a couple possible interpretations (e.g., skip connections that jump from each layer to the last layer or (my assumption) skip connections between every pair of layers?) .\n\n- Fully evaluating the \""claims of Collins et al. (2016), that capacities of various cells are very similar and their apparent\ndifferences result from trainability and regularisation\"" would likely involve adding a fourth cell to the hyperparameter sweep, one whose design is more arbitrary and is neither the result of human nor machine optimization. \n\n- The reformulation of the problem of deciding embedding and hidden sizes into one of allocating a fixed parameter budget towards the embedding and recurrent layers represents a significant conceptual step forward in understanding the causes of variation in model performance. \n\n- The plot in Figure 2 is clear and persuasive, but for reproducibility purposes it would also be nice to see an example set of strong hyperparameters in a table.  The history of hyperparameter proposals and their perplexities would also make for a fantastic dataset for exploring the structure of RNN hyperparameter spaces.  For instance, it would be helpful for future work to know which hyperparameters' effects are most nearly independent of other hyperparameters. \n\n- The choice between tied and clipped (Sak et al., 2014) LSTM gates, and their comparison to standard untied LSTM gates, is discussed only minimally, although it represents a significant difference between this paper and the most \""standard\"" or \""conventional\"" LSTM implementation (e.g., as provided in optimized GPU libraries).  In addition to further discussion on this point, this result also suggests evaluating other recently proposed \""minor changes\"" to the LSTM architecture such as multiplicative LSTM (Krause et al., 2016) \n\n- It would also have been nice to see a comparison between the variational/recurrent dropout parameterization \""in which there is further sharing of masks between gates\"" and the one with \""independent noise for the gates,\"" as described in the footnote.  There has been some confusion in the literature as to which of these parameterizations is better or more standard; simply justifying the choice of parameterization a little more would also help.",1,1,1,1,1,-1,1,1,1,1
Byk4My-RZ-R1,"Summary:\n\nThe paper proposes to learn new priors for latent codes z  for GAN training.   for this the paper shows that there is a mismatch between the gaussian prior and an estimated of the latent codes of real data by reversal of the generator .  To fix this the paper proposes to learn a second GAN to learn the prior distributions of \""real latent code\"" of the first GAN.  The first GAN then uses the second GAN as prior to generate the z codes.  \n \nQuality/clarity:\n\nThe paper is well written and easy to follow. \n\nOriginality:\n\npros:\n-The paper while simple sheds some light on important problem with the prior distribution used in GAN. \n- the second GAN solution trained on reverse codes from real data is interesting  \n- In general the topic is interesting, the solution presented is simple but needs more study \n\ncons:\n\n- It related to adversarial learned inference and BiGAN, in term of learning the mapping  z ->x, x->z and seeking the agreement.  \n- The solution presented is not end to end (learning a prior generator on learned models have been done in many previous works on encoder/decoder) \n\nGeneral Review:\n\nMore experimentation with the latent codes will be interesting: \n\n- Have you looked at the decay of the singular values of the latent codes obtained from reversing the generator?  Is this data low rank?  how does this change depending on the dimensionality of the latent codes?  Maybe adding plots to the paper can help. \n\n- the prior agreement score is interesting but assuming gaussian prior also for the learned latent codes from real data is maybe not adequate.   Maybe computing the entropy of the codes using a nearest neighbor estimate of the entropy  can help understanding the entropy difference wrt to the isotropic gaussian prior? \n\n- Have you tried to multiply the isotropic normal noise with the learned singular values and generate images from  this new prior  and compute inceptions scores etc?  Maybe also rotating the codes with the singular vector matrix V or \\Sigma^{0.5} V? \n\n- What architecture did you use for the prior generator GAN? \n\n- Have you thought of an end to end way to learn the prior generator GAN?  \n\n****** I read the authors reply. Thank you for your answers and for the SVD plots this is  helpful",1,1,1,1,1,1,1,1,1,-1
Byk4My-RZ-R2,"The paper demonstrates the need and usage for flexible priors in the latent space alongside current priors used for the generator network.  These priors are indirectly induced from the data - the example discussed is via an empirical diagonal covariance assumption for a multivariate Gaussian.  The experimental results show the benefits of this approach.  \nThe paper provides for a good read.  \n\nComments:\n\n1. How do the PAG scores differ when using a full covariance structure?  Diagonal covariances are still very restrictive.  \n2. The results are depicted with a latent space of 20 dimensions.  It will be informative to see how the model holds in high-dimensional settings.  And when data can be sparse.  \n3. You could consider giving the Discriminator, real data etc in Fig 1 for completeness as a graphical summary",1,-1,1,1,1,-1,1,-1,-1,-1
Byk4My-RZ-R3,"The paper proposes, under the GAN setting, mapping real data points back to the latent space via the \""generator reversal\"" procedure on a sample-by-sample basis (hence without the need of a shared recognition network) and then using this induced empirical distribution as the \""ideal\"" prior targeting which yet another GAN network might be trained to produce a better prior for the original GAN. \n\nI find this idea potentially interesting but am more concerned with the poorly explained motivation as well as some technical issues in how this idea is implemented, as detailed below. \n\n1. Actually I find the entire notion of an \""ideal\"" prior under the GAN setting a bit strange.  To start with, GAN is already training the generator G to match the induced P_G(x) (from P(z)) with P_d(x), and hence by definition, under the generator G, there should be no better prior than P(z) itself (because any change of P(z) would then induce a different P_G(x) and hence only move away from the learning target). \n\nI get it that maybe under different P(z) the difficulty of learning a good generator G can be different, and therefore one may wish to iterate between updating G (under the current P(z)) and updating P(z) (under the current G), and hopefully this process might converge to a better solution.  But I feel this sounds like a new angle and not the one that is adopted by the authors in this paper. \n\n2. I think the discussions around Eq. (1) are not well grounded.  Just as you said right before presenting Eq. (1), typically the goal of learning a DGM is just to match Q_x with the true data distrubution P_x. It is **not** however to match Q(x,z) with P(x,z).  And btw, don't you need to put E_z[ ... ] around the 2nd term on the r.h.s. ? \n\n3. I find the paper mingles notions from GAN and VAE sometimes and misrepresents some of the key differences between the two. \n\nE.g. in the beginning of the 2nd paragraph in Introduction, the authors write \""Generative models like GANs, VAEs and others typically define a generative model via a deterministic generative mechanism or generator ...\"". While I think the use of a **deterministic** generator is probably one of the unique features of GAN, and that is certainly not the case with VAE, where typically people still need to specify an explicit probabilistic generative model. \n\nAnd for this same reason, I find the multiple references of \""a generative model P(x|z)\"" in this paper inaccurate and a bit misleading. \n\n4. I'm not sure whether it makes good sense to apply an SVD decomposition to the \\hat{z} vectors.  It seems to me the variances \\nu^2_i shall be directly estimated from \\hat{z} as is.   Otherwise, the reference \""ideal\"" distribution would be modeling a **rotated** version of the \\hat{z} samples, which imo only introduces unnecessary discrepancies. \n\n5. I don't quite agree with the asserted \""multi-modal structure\"" in Figure 2.  Let's assume a 2d latent space, where each quadrant represents one MNIST digit (e.g. 1,2,3,4). You may observe a similar structure in this latent space yet still learn a good generator under even a standard 2d Gaussian prior. I guess my point is, a seemingly well-partitioned latent space doesn't bear an obvious correlation with a multi-modal distribution in it. \n\n6. The generator reversal procedure needs to be carried out once for each data point separately, and also when the generator has been updated, which seems to be introducing a potentially significant bottleneck into the training process.",1,1,1,1,1,-1,1,1,1,-1
BykJlIAbM-R1,"This paper proposed a NMT system that expands each sentence pair to two groups of similar sentences.  The idea of using similar sentence pairs as cluster-to-cluster translation is interesting.  \nThe experimental results seem promising, but the presentation can be improved.  Some parts of the paper are hard to read. \n \n \nMajor\n1. What is the model/baseline in Tables 3, 4, 5? \n2. What is the intuition in adding target cluster entropy in Eq. 3? \n3. In the adaptive cluster, I am a bit confused on the target of the parametric models. Where are X, Y of P(X|X*), P(Y|Y*) from?  Is it from pretrained models? It wasn't clear until I read the algorithm. Also, why are p(X|X*) called target cluster and P(Y|Y*) called source cluster? \n4. In section 4.2, the name cluster is a bit confusing with the one in section 3.1. What's the relationship? The symbols C(Y*) and C(X*) are not used afterward. \n5. In the conclusion, it claims the system is efficient in helping current model. What do you mean by \""efficient\""? \n6. The improvements of WMT are relatively small. Does it mean the proposed methods are not beneficial when there are large amounts of sentence pairs? \n7. What's the reward used in the experiments ?\n8. In the Monte-Carlo sampling, how many pairs are sampled?  \n \nMinor \n1. In Table 1, where is sigma defined? \n2. The notation D for a dataset in Section 3.3 is confusing with D in system D. \n3. There is some redundancy between Systems A, B, C, D and in the algorithm 1. I wonder whether it can be simplified. \n4. In section 4.3, backward NMT (X|Y) -> backward NMT P(X|Y). \n5. It will be great to show detailed derivation, for example from Eq. 9 to Eq. 10. \n6. Some recent results on WMT DE-EN are missing, such ashttps://arxiv.org/abs/1706.03762.\n",1,1,1,1,1,-1,1,1,1,-1
BykJlIAbM-R2,"This paper describes how to use a set of sentences in the source language mapped to another set of sentences in the target sentences instead of using single sentence to sentence samples.  The paper claims superior results using the described method. \n\nOverall, there are a few problems with the paper.  1) The arguments for using clusters instead of single sentences are questionable.   The paper claims several times that MLE training for NMT faces over-training (or data sparsity) -- while that can be true depending on the corpus and model used, there are well-known remedies for that, for example regularization via dropout (almost everybody uses that).  It is not clear why that is not used or at least compared to the method presented.  2) The writing of the paper is often unclear (and sometimes grammatically wrong, typos etc. but that aside), there are some made up words/concepts (What is 'Golden Centroid Augmentation\"" or \""Model Centroid Augmentation\""?  The reason for attention is not to better memorize input information, it is to be able to attend to certain regions in the input.  The reason to use RL is to focus on optimizing directly for BLEU score or other metrics instead of likelihood but not for improving on the train/test loss discrepancy.  There are lots more examples of unclear statements in this paper -- it should be heavily improved.  3) Section 3 and 4 are very hard/impossible to understand, it is not clear how the formulas help the reader to better understand the concept in any way.  5) The results presented in this paper given the complexity of the method are just not great -- for example, WMT en-de is 21.3 BLEU reported by you while much older papers report for example 24.67 BLEU (Google's Neural Machine Translation System) -- why not first try to get to state-of-the-art with already published methods and then try to improve on top of that? .   6) Finally, what is missing most is simply why a much simpler method (just generate some data using a trained system and use that as additional training data, with details on how much etc.) -- is not directly compared to this very complicated looking method",1,1,1,1,1,-1,1,1,1,-1
BykJlIAbM-R3,"This work tries to generalize the framework of reward augmented maximum likelihood criterion by introducing the notion of cluster, which represents a set of similar data point, e.g., sentence, according to a metric.  By employing the cluster, this work propose a joint source/target modeling by varying how sampling is performed, e.g., draw independently or conditionally, and how the cluster are constructed, e.g., model-wise or non-model.  Experiments on German/English and Chinese/English show gains over other reinforcement learning methods. \n\nIf my understanding is correct, the motivation is investigate alternative combination of how a cluster is constructed, e.g., sampling and model-based scoring.  However one of the problems of this paper is clarity. \n\n- The notion of cluster is still unclear and it took me long to understand it probably because it might be easily confused with other terminology, e.g., clustering.  Also, cluster-to-cluster might not fit well.  \n\n- It is hard to map System-{ABCD} to the underlying proposed methods described in Table 2.  Also, I feel algorithm 1 is spurious given that it merely switch by systems.  Probably better to introduce branch for key methods, parallel sampling/ translation broadcasting and inadaptive or adaptive model",1,1,1,1,1,-1,1,-1,-1,-1
ByL48G-AW-R1,"SUMMARY\nThe paper deal with the problem of RL.   It proposes a non-parametric approach that maps trajectories to the optimal policy.   It avoids learning parameterized policies.   The fundamental idea is to store passed trajectories.  When a policy is to be executed, it does nearest neighbor search to find then closest trajectory and executes it. \n\nCOMMENTS\n\nWhat happens if the agent finds it self  in a state that while is close to a state in the similar trajectory the action required to could be completely different. \n\nNot certain about the claim that standard RL policy learning algorithms make it difficult to assess the difficulty of a problem.  \n\nHow do you execute a trajectory?  Actions in RL are by definition stochastic, and this would make it unlikely that a same trajectory can be reproduced exactly",1,-1,1,1,1,-1,1,1,-1,-1
ByL48G-AW-R2,"This work shows that a simple non-parametric approach of storing state embeddings with the associated Monte Carlo returns is sufficient to solve several benchmark continuous control problems with sparse rewards (reacher, half-cheetah, double pendulum, cart pole) (due to the need to threshold a return the algorithms work less well with dense rewards, but with the introduction of a hyper-parameter is capable of solving several tasks there).  The authors argue that the success of these simple approaches on these tasks suggest that more changing problems need to be used to assess new RL algorithms. \n\nThis paper is clearly written  and it is important to compare simple approaches on benchmark problems . There are a number of interesting and intriguing side-notes and pieces of future work mentioned. \n\nHowever, the originality and significance of this work is a significant drawback.  The use non-parametric approaches to the action-value function go back to at least [1] (and probably much further). So the algorithms themselves are not particularly novel, and are limited to nearly-deterministic domains with either single sparse rewards (success or failure rewards) or introducing extra hyper-parameters per task. \n\nThe significance of this work would still be quite strong if, as the author's suggest, these benchmarks were being widely used to assess more sophisticated algorithms and yet these tasks were mastered by such simple algorithms with no learnable parameters.  Yet, the results do not support the claim.  Even if we ignore that for most tasks only the sparse reward (which favors this algorithm) version was examined, these author's only demonstrate success on 4, relatively simple tasks. \n\nWhile these simple tasks are useful for diagnostics, it is well-known that these tasks are simple and, as the author's suggest \""more challenging tasks  .... are necessary to properly assess advances made by sophisticated, optimization-based policy algorithms.\""  Lillicrap et al. (2015) benchmarked against 27 tasks, Houtfout et al. (2016) compared in the paper also used Walker2D and Swimmer (not used in this paper) as did [2], OpenAI Gym contains many more control environments than the 4 solved here and significant research is pursing complex manipulation and grasping tasks (e.g. [3]). This suggests the author's claim has already been widely heeded and this work will be of limited interest. \n\n[1] Juan, C., Sutton, R. S., & Ram, A. Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces.\n\n[2] Henderson, P., Islam, R., Bachman, P., Pineau, J., Precup, D., & Meger, D. (2017). Deep reinforcement learning that matters. arXiv preprint arXiv:1709.06560.\n\n[3] Nair, A., McGrew, B., Andrychowicz, M., Zaremba, W., & Abbeel, P. (2017). Overcoming exploration in reinforcement learning with demonstrations. arXiv preprint arXiv:1709.10089.",1,1,1,1,-1,1,1,1,1,-1
ByL48G-AW-R3,"This paper presents a nearest-neighbor based continuous control policy.   Two algorithms are presented: NN-1 runs open-loop trajectories from the beginning state, and NN-2 runs a state-condition policy that retrieves nearest state-action tuples for each state.   \n\nThe overall algorithm is very simple to implement and can do reasonably well on some simple control tasks, but quickly gets overwhelmed by higher-dimensional and stochastic environments.   It is very similar to \""Learning to Steer on Winding Tracks Using Semi-Parametric Control Policies\"" and is effectively an indirect form of tile coding (each could be seen as a fixed voronoi cell).   I am sure this idea has been tried before in the 90s but I am not familiar enough with all the literature to find it (A quick google search brings this up: Reinforcement Learning of Active Recognition Behaviors, with a chapter on nearest-neighbor lookup for policies: https://people.eecs.berkeley.edu/~trevor/papers/1997-045/node3.html). \n\nAlthough I believe there is work to be done in the current round of RL research using nearest neighbor policies, I don't believe this paper delves very far into pushing new ideas (even a simple adaptive distance metric could have provided some interesting results, nevermind doing a learned metric in a latent space to allow for rapid retrainig of a policy on new domains....),  and for that reason I don't think it has a place as a conference paper at ICLR.   I would suggest its submission to a workshop where it might have more use triggering discussion of further work in this area.",1,1,1,1,1,1,1,1,1,-1
Bym0cU1CZ-R1,"The authors use a distant supervision technique to add dialogue act tags as a conditioning factor for generating responses in open-domain dialogues.  In their evaluations, this approach, and one that additionally uses policy gradient RL with discourse-level objectives to fine-tune the dialogue act predictions, outperform past models for human-scored response quality and conversation engagement. \nWhile this is a fairly straightforward idea with a long history, the authors claim to be the first to use dialogue act prediction for open-domain (rather than task-driven) dialogue.  If that claim to originality is not contested, and the authors provide additional assurances to confirm the correctness of the implementations used for baseline models, this article fills an important gap in open-domain dialogue research and suggests a fruitful future for structured prediction in deep learning-based dialogue systems. \n\nSome points:\n1. The introduction uses \""scalability\"" throughout to mean something closer to \""ability to generalize.\"" Consider revising the wording here. \n2. The dialogue act tag set used in the paper is not original to Ivanovic (2005) but derives, with modifications, from the tag set constructed for the DAMSL project (Jurafsky et al., 1997; Stolcke et al., 2000).  It's probably worth citing some of this early work that pioneered the use of dialogue acts in NLP, since they discuss motivations for building DA corpora. \n3. In Section 2.1, the authors don't explicitly mention existing DA-annotated corpora or discuss specifically why they are not sufficient (is there e.g. a dataset that would be ideal for the purposes of this paper except that it isn't large enough?) \n3. The authors appear to consider only one option (selecting the top predicted dialogue act, then conditioning the response generator on this DA) among many for inference-time search over the joint DA-response space.  A more comprehensive search strategy (e.g. selecting the top K dialogue acts, then evaluating several responses for each DA) might lead to higher response diversity. \n4. The description of the RL approach in Section 3.2 was fairly terse and included a number of ad-hoc choices.  If these choices (like the dialogue termination conditions) are motivated by previous work, they should be cited.  Examples (perhaps in the appendix) might also be helpful for the reader to understand that the chosen termination conditions or relevance metrics are reasonable. \n5. The comparison against previous work is missing some assurances I'd like to see.  While directly citing the codebases you used or built off of is fantastic, it's also important to give the reader confidence that the implementations you're comparing to are the same as those used in the original papers, such as by mentioning that you can replicate or confirm quantitative results from the papers you're comparing to.  Without that there could always be the chance that something is missing from the implementation of e.g. RL-S2S that you're using for comparison. \n6. Table 5 is not described in the main text, so it isn't clear what the different potential outputs of e.g. the RL-DAGM system result from (my guess: conditioning the response generation on the top 3 predicted dialogue acts?) \n7. A simple way to improve the paper's clarity for readers would be to break up some of the very long paragraphs, especially in later sections.  It's fine if that pushes the paper somewhat over the 8th page. \n8. A consistent focus on human evaluation, as found in this paper, is probably the right approach for contemporary dialogue research. \n9. The examples provided in the appendix are great.  It would be helpful to have confirmation that they were selected randomly (rather than cherry-picked).",1,1,1,1,1,1,1,1,1,1
Bym0cU1CZ-R2,"The topic discussed in this paper is interesting.  Dialogue acts (DAs; or some other semantic relations between utterances) are informative to increase the diversity of response generation.  It is interesting to see how DAs are used for conversational modeling,;  however this paper is difficult for me to follow.  For example:\n\n1) the caption of section 3.1 is about supervised learning, however the way of describing the model in this section sounds like reinforcement learning.  Not sure whether it is necessary to formulate the problem with a RL framework, since the data have everything that the model needs as for a supervised learning. \n2) the formulation in equation 4 seems to be problematic \n3) \""simplify pr(ri|si,ai) as pr(ri|ai,ui\u22121,ui\u22122) since decoding natural language responses from long conversation history is challenging \"" to my understanding, the only difference between the original and simplified model is the encoder part not the decoder part. Did I miss something? \n4) about section 3.2, again I didn't get whether the model needs RL for training. \n5) \""We train m(\u00b7, \u00b7) with the 30 million crawled data through negative sampling.\"" not sure I understand the connection between training $m(\\cdot, \\cdot)$ and the entire model. \n6) the experiments are not convincing.  At least, it should show the generation texts were affected about DAs in a systemic way.  Only a single example in table 5 is not enough",1,1,1,1,-1,-1,1,1,1,-1
Bym0cU1CZ-R3,"The paper describes a technique to incorporate dialog acts into neural conversational agents.   This is very interesting work.   Existing techniques for neural conversational agents essentially mimic the data in large corpora of message-response pairs and therefore do not use any notion of dialog act.   A very important type of dialog act is \""switching topic\"", often done to ensure that the conversation will continue.   The paper describes a classifier that predicts the dialog act of the next utterance.   The next utterance is then generated based on this dialog act.   The paper also describes how to increase the relevance of responses and the length of conversations by self reinforcement learning. This is also very interesting.   The empirical evaluation demonstrates the effectiveness of the approach.   The paper is also well written.   I do not have any suggestion for improvement.  This is good work that should be published",1,1,-1,1,-1,-1,1,1,1,-1
Byni8NLHf-R1,"The authors propose first applying dependency parsing to documents, then using pairs of words connected via dependency as features in a similarity metric. \n\nWhile intriguing, a lot more work would be required to publish this at ICLR.  Namely, the following questions need to be answered:\n\n1. Does using linked-word-pairs truly raise the state of the art?  Unlike what is stated in the abstract, the experimental results only compare RBMs with and without this feature.  RBMs are not state-of-the-art in topic modeling, therefore it\u2019s difficult to assess whether this is helpful. \n2. If linked words does improve topic modeling, why does it do so? \n3. Are words that are linked via a dependency better than commonly co-occuring words?  Experiments need to be done to show that a full dependency parse is actually required, rather than simply looking for co-occuring words. \n4. How is this work related to the extensive work in NLP in applying parsing to various tasks? \n5. Can the selection of word pairs be done automatically, from data, rather than pre-computed with a known dependency parser?  After all, this is submitted to the International Conference on Learning Representations --- feature engineering papers can easily be published at EMNLP, ICML, etc. An excellent ICLR paper would show some way to either (a) use dependency parsing only at training time (to provide a hint), or (b) not require dependency parsing at all. \n\nA few suggestions for experiments:\nA. I would recommend first doing comparisons between bag-of-words representation and the dependency-bigram representation, just using log(tf)-idf as a distance metric.  By stripping away more advanced modeling, that could reveal whether the dependency bi-gram has utility .\nB. The authors may wish to consider applying LSA to both bag of words and dependency-bigrams, using log(tf)-idf weighting for both . From what I\u2019ve seen, log(tf)-idf LSA seems to perform about as well as LDA . Plain LSA takes into account correlations between words  --- it would be interesting to see whether dependency-bigrams can improve on LSA at all .\nC. Reiterating point (3) above, to really show whether the power of the dependency parse is being used, I would strongly suggest doing a null experiment with co-occuring nearby words. \n\n\nReferences:\n[1] Boyd-Graber, J. L., & Blei, D. M. (2009). Syntactic topic models. In Advances in neural information processing systems(pp. 185-192).\n[2] Pad\u00f3, S. and Lapata, M., 2007. Dependency-based construction of semantic space models. Computational Linguistics, 33(2), pp.161-199",1,1,1,1,1,1,1,1,1,-1
Byni8NLHf-R2,"This paper applies the word pairs, instead to bag of words, to current RBM models . The word pairs are extracted using Stanford parser . The word pairs are further filtered and clustered to improve the representation . The experiments show improvement over baselines. \n\nHowever, I think this paper has limited contribution and novelty,  and the experiments also need to be improved . The detailed comments are as follows:\n\n- The main contribution of this paper is to apply word pairs instead of words to RBM models . However, the main techniques such as RBM, parser to extract word pairs, tf-idf for filtering, and k-means for clustering, are all existing standard techniques.  It is more like an application of these methods, and has limited contribution and novelty. \n\n- For experiments, they apply k-means clustering in the process so k is one parameter to tune. K needs to be tuned on validation set instead of testing set.   This paper simply presents the results of different parameter k on testing set directly. \n\n- The structure of Section 3 needs to be improved.  Instead of listing each step in each subsection, a general introduction picture should be introduced first. More intuition is also needed for each step. .\n\n- The format of reference should be fixed in this paper.",1,1,1,1,1,-1,1,1,1,-1
Byni8NLHf-R3,"This submission does not fit ICLR .  \n\n- The center topic does not fit ICLR . The main novelty is about using word pair embedding to improve the Topic model . The word-pair was generated by the Standford dependency parser . \n\n- Many citation errors exist \n\n- No clear novelty \n\n- The experimental setup is problematic . The authors filtered the number of words and word-pairs to very small . It is hard to justify any of the results after these strategies . \n\n- The baselines are not thorough and lack proper justifications . \n\n- The experimental results are not properly presented, with many overlapping figures . No insights can be derived from the presented results",1,1,1,1,-1,-1,1,1,1,1
ByOExmWAb-R1,"Generating high-quality sentences/paragraphs is an open research problem that is receiving a lot of attention.  This text generation task is traditionally done using recurrent neural networks.  This paper proposes to generate text using GANs.  GANs are notorious for drawing images of high quality but they have a hard time dealing with text due to its discrete nature.  This paper's approach is to use an actor-critic to train the generator of the GAN and use the usual maximum likelihood with SGD to train the discriminator.  The whole network is trained on the \""fill-in-the-blank\"" task using the sequence-to-sequence architecture for both the generator and the discriminator.  At training time, the generator's encoder computes a context representation using the masked sequence.  This context is conditioned upon to generate missing words.  The discriminator is similar and conditions on the generator's output and the masked sequence to output the probability of a word in the generator's output being fake or real.  With this approach, one can generate text at test time by setting all inputs to blanks.  \n\nPros and positive remarks: \n--I liked the idea behind this paper.  I find it nice how they benefited from context (left context and right context) by solving a \""fill-in-the-blank\"" task at training time and translating this into text generation at test time.  \n--The experiments were well carried through and very thorough. \n--I second the decision of passing the masked sequence to the generator's encoder instead of the unmasked sequence.  I first thought that performance would be better when the generator's encoder uses the unmasked sequence.  Passing the masked sequence is the right thing to do to avoid the mismatch between training time and test time. \n\nCons and negative remarks:\n--There is a lot of pre-training required for the proposed architecture.  There is too much pre-training. I find this less elegant.  \n--There were some unanswered questions:\n            (1) was pre-training done for the baseline as well? \n            (2) how was the masking done?  how did you decide on the words to mask? was this at random? \n            (3) it was not made very clear whether the discriminator also conditions on the unmasked sequence.  It needs to but \n                  that was not explicit in the paper. \n--Very minor: although it is similar to the generator, it would have been nice to see the architecture of the discriminator with example input and output as well. \n\n\nSuggestion: for the IMDB dataset, it would be interesting to see if you generate better sentences by conditioning on the sentiment as well",1,1,1,1,1,-1,1,1,1,-1
ByOExmWAb-R2,"Quality: The work focuses on a novel problem of generating text sample using GAN and a novel in-filling mechanism of words.  Using GAN to generate samples in adversarial setup in texts has been limited due to the mode collapse and training instability issues.  As a remedy to these problems an in-filling-task conditioning on the surrounding text has been proposed.  But, the use of the rewards at every time step (RL mechanism) to employ the actor-critic training procedure could be challenging computationally challenging. \n\nClarity: The mechanism of generating the text samples using the proposed methodology has been described clearly.  However the description of the reinforcement learning step could have been made a bit more clear. \n\nOriginality: The work indeed use a novel mechanism of in-filling via a conditioning approach to overcome the difficulties of GAN training in text settings.  There has been some work using GAN to generate adversarial examples in textual context too to check the robustness of classifiers.  How this current work compares with the existing such literature? \n\nSignificance: The research problem is indeed significant since the use of GAN in generating adversarial examples in image analysis has been more prevalent compared to text settings.  Also, the proposed actor-critic training procedure via RL methodology is indeed significant from its application in natural language processing. \n\npros:\n(a) Human evaluations applications to several datasets show the usefulness of MaskGen over the maximum likelihood trained model in generating more realistic text samples. \n(b) Using a novel in-filling procedure to overcome the complexities in GAN training. \n(c) generation of high quality samples even with higher perplexity on ground truth set. \n\ncons:\n(a) Use of rewards at every time step to the actor-critic training procure could be computationally expensive. \n(b) How to overcome the situation where in-filling might introduce implausible text sequences with respect to the surrounding words? \n(c) Depending on the Mask quality GAN can produce low quality samples.  Any practical way of choosing the mask",1,1,1,1,1,1,1,1,1,-1
ByOExmWAb-R3,"This paper proposes MaskGAN, a GAN-based generative model of text based on\nthe idea of recovery from masked text.  \nFor this purpose, authors employed a reinceforcement learning approach to\noptize a prediction from masked text.  Moreover, authors argue that the \nquality of generated texts is not appropriately measured by perplexities,\nthus using another criterion of a diversity of generated n-grams as well as\nqualitative evaluations by examples and by humans. \n\nWhile basically the approach seems plausible, the issue is that the result is\nnot compared to ordinary LSTM-based baselines.  While it is better than a \nconterpart of MLE (MaskedMLE), whether the result is qualitatively better than\nordinary LSTM is still in question. \n\nIn fact, this is already appearent both from the model architectures and the\ngenerated examples: because the model aims to fill-in blanks from the text\naround (up to that time), generated texts are generally locally valid but not\nalways valid globally. This issue is also pointed out by authors in Appendix\nA.2.  \nWhile the idea of using mask is interesting and important, I think if this\nidea could be implemented in another way, because it resembles Gibbs sampling\nwhere each token is sampled from its sorrounding context, while its objective\nis still global, sentence-wise.  As argued in Section 1, the ability of \nobtaining signals token-wise looks beneficial at first, but it will actually\nbreak a global validity of syntax and other sentence-wise phenoma. \n\nBased on the arguments above, I think this paper is valuable at least\nconceptually, but doubt if it is actually usable in place of ordinary LSTM\n(or RNN)-based generation. \nMore arguments are desirable for the advantage of this paper, i.e. quantitative\nevaluation of diversity of generated text as opposed to LSTM-based methods. \n\n*Based on the rebuttals and thorough experimental results, I modified the global rating",1,1,1,1,1,-1,1,1,1,-1
ByOnmlWC--R1,"This is a highly interesting paper that proposes a set of methods that combine ideas from imitation learning, evolutionary computation and reinforcement learning in a novel way.  It combines the following ingredients:\na) a population-based setup for RL\nb) a pair-selection and crossover operator\nc) a policy-gradient based \u201cmutation\u201d operator\nd) filtering data by high-reward trajectories\ne) two-stage policy distillation; \n\nIn its current shape it has a couple of major flaws (but those can be fixed during the revision/rebuttal period): \n\n(1) Related work. It is presented in a somewhat ahistoric fashion.  In fact, ideas for evolutionary methods applied to RL tasks have been widely studied, and there is an entire research field called \u201cneuroevolution\u201d that specifically looks into which mutation and crossover operators work well for neural networks.  I\u2019m listing a small selection of relevant papers below, but I\u2019d encourage the authors to read a bit more broadly, and relate their work to the myriad of related older methods.  Ideally, a more reasonable form of parameter-crossover (see references) could be compared to -- the naive one is too much of a straw man in my opinion.  To clarify: I think the proposed method is genuinely novel, but a bit of context would help the reader understand which aspects are and which aspects aren\u2019t. \n\n(2) Ablations. The proposed method has multiple ingredients, and some of these could be beneficial in isolation: for example a population of size 1 with an interleaved distillation phase where only the high-reward trajectories are preserved could be a good algorithm on its own.  Or conversely, GPO without high-reward filtering during crossover.  Or a simpler genetic algorithm that just preserves the kills off the worst members of the population, and replaces them by (mutated) clones of better ones, etc.  \n\n(3) Reproducibility. There are a lot of details missing; the setup is quite complex, but only partially described.  Examples of missing details are: how are the high-reward trajectories filtered?  What is the total computation time of the different variants and baselines ? The x-axis on plots, does it include the data required for crossover/Dagger ? What are do the shaded regions on plots indicate?  The loss on \\pi_S should be made explicit.  An open-source release would be ideal. \n\nMinor points:\n- naively, the selection algorithm might not scale well with the population size (exhaustively comparing all pairs), maybe discuss that? \n- the filtering of high-reward trajectories is what estimation of distribution algorithms [2] do as well, and they have a known failure mode of premature convergence because diversity/variance shrinks too fast.  Did you investigate this?\ n- for Figure 2a it would be clearer to normalize such that 1 is the best and 0 is the random policy, instead of 0 being score 0. \n- the language at the end of section 3 is very vague and noncommittal -- maybe just state what you did, and separately give future work suggestions? \n- there are multiple distinct metrics that could be used on the x-axis of plots, namely: wallclock time, sample complexity, number of updates.  I suspect that the results will look different when plotted in different ways, and would enjoy some extra plots in the appendix.  For example the ordering in Figure 6 would be inverted if plotting as a function of sample complexity? \n- the A2C results are much worse, presumably because batchsizes are different?  So I\u2019m not sure how to interpret them: should they have been run for longer?  Maybe they could be relegated to the appendix? \n\nReferences:\n[1] Gomez, F. J., & Miikkulainen, R. (1999). Solving non-Markovian control tasks with neuroevolution. \n[2] Larranaga, P. (2002). A review on estimation of distribution algorithms. \n[3] Stanley, K. O., & Miikkulainen, R. (2002). Evolving neural networks through augmenting topologies.  \n[4] Igel, C. (2003). Neuroevolution for reinforcement learning using evolution strategies. \n[5] Hausknecht, M., Lehman, J., Miikkulainen, R., & Stone, P. (2014). A neuroevolution approach to general atari game playing. \n[6] Gomez, F., Schmidhuber, J., & Miikkulainen, R. (2006). Efficient nonlinear control through neuroevolution. \n\n\nPros:\n- results\n- novelty of idea\n- crossover visualization, analysis\n- scalability; \n\nCons:\n- missing background\n- missing ablations\n- missing details; \n\n[after rebuttal: revised the score from 7 to 8]",1,1,1,1,1,1,1,1,1,1
ByOnmlWC--R2,"The authors present an algorithm for training ensembles of policy networks that regularly mixes different policies in the ensemble together by distilling a mixture of two policies into a single policy network, adding it to the ensemble and selecting the strongest networks to remain (under certain definitions of a \""strong\"" network).  The experiments compare favorably against PPO and A2C baselines on a variety of MuJoCo tasks, although I would appreciate a wall-time comparison as well, as training the \""crossover\"" network is presumably time-consuming. \n\nIt seems that for much of the paper, the authors could dispense with the genetic terminology altogether - and I mean that as a compliment.  There are few if any valuable ideas in the field of evolutionary computing and I am glad to see the authors use sensible gradient-based learning for GPO, even if it makes it depart from what many in the field would consider \""evolutionary\"" computing.  Another point on terminology that is important to emphasize - the method for training the crossover network by direct supervised learning from expert trajectories is technically not imitation learning but behavioral cloning.  I would perhaps even call this a distillation network rather than a crossover network.  In many robotics tasks behavioral cloning is known for overfitting to expert trajectories, but that may not be a problem in this setting as \""expert\"" trajectories can be generated in unlimited quantities",1,1,1,1,1,-1,1,1,1,-1
ByOnmlWC--R3,"This paper proposes a genetic algorithm inspired policy optimization method, which mimics the mutation and the crossover operators over policy networks. \n\nThe title and the motivation about the genetic algorithm are missing leading and improper.  The genetic algorithm is a black-box optimization method, however, the proposed method has nothing to do with black-box optimization.  \n\nThe mutation is a method to sample individual independence of the objective function, which is very different with the gradient step.  Mimicking the mutation by a gradient step is very unreasonable.  \n\nThe crossover operator is the policy mixing method employed in game context (e.g., Deep Reinforcement Learning from Self-Play in Imperfect-Information Games, https://arxiv.org/abs/1603.01121 ).  It is straightforward if two policies are to be mixed. Although the mixing method is more reasonable than the genetic crossover operator, it is strange to compare with that operator in a method far away from the genetic algorithm. \n\nIt is highly suggested that the method is called as population-based method as a set of networks is maintained, instead of as \""genetic\"" method. \n\nAnother drawback, perhaps resulted from the \""genetic algorithm\"" motivation is that the proposed method has not been well explained.  The only explanation is that this method mimics the genetic algorithm. However, this explanation reveals nothing about why the method could work well -- a random exploration could also waste a lot of samples with a very high probability. \n\nThe baseline methods result in rewards much lower than those in previous experimental papers.  It is problemistic that if the baselines have bad parameters. \n1. Benchmarking Deep Reinforcement Learning for Continuous Control\n2. Deep Reinforcement Learning that Matter",1,1,1,1,1,-1,1,1,-1,-1
BypdvewVM-R1,"I liked this paper mostly because it surprised me and because it might spur the development of novel variants of Difference Target-Propagation (DTP).  The paper does a good job of highlighting the relevant background and issues and introduces a slight variation to DTP which actually works as well while being more biologically plausible.  There was a concern or assumption in the original DTP paper about the target for the penultimate layer (before the output layer) which seems to have been excessive, i.e., the DTP propagation rule actually works on the last layer and there is no need to use the exact gradient propagation for it, at least according to these experiments.  In call cases, the variant using the DTP target update everywhere works about as well as using the true gradient for the output layer.   Another quirk that the proposed variant (SDTP) removes from the orignal DTP paper is the way noise is handled, and I agree that denoising makes a lot of sense (than noise preservation) while being more biologically plausible.   Finally, the authors did a good job of establishing a benchmark which could be used by others attempting to evaluate new biologically plausible alternatives to backprop.  The paper is very clear and I have just outlined the original contributions and significance (DTP may have been a bit forgotten and is worth another look, apparently). \n\nIn the negatives, the paper should mention in the discussion and intro that all the TP variants ignore the issue of dynamics.  We know that there are of course lateral connections and that feedback connections do not operate independently of the feedforward one (or there would be a need for a precise 'clockwork' mechanism to sweep layers forward and backward, which seems not very plausible). \n\nIn the experimental results section, it would be good to report the CNN results as well (with shared weights, same architecture) . Also, training errors should be shown, since I suspect that underfitting may be happening especially in the case of ImageNet.  If that was the case, future work should first explore higher capacity (which may require larger-memory GPUs...).  Finally, in the description of architectures, please define the structure notation, e.g. (3 x 3, 32, 2, SAME).",1,1,1,1,1,-1,1,1,1,-1
BypdvewVM-R2,"Summary of the paper: The paper analysis how well difference target probation (DTP) - an optimisation algorithm designed to be biologically more plausible than backpropagation - scales to bigger datasets like CIFAR-10 and ImageNet.   The DTP algorithm is slightly adapted to make it more biologically plausible, by replacing the gradient computation the original paper applied between the highest hidden layers by target propagation (leading to the variant SDTP), and by making the optimisation of both involved losses parallel.   Furthermore, only feedforward and locally connected networks (CNN) are considered since their architecture is considered more biologically plausible than convolutional neural networks.  While on MNIST and CIFAR, DTP and SDTP performed as well as backprop, they perform worse on ImageNet . Furthermore, it becomes clear, that without CNN structure no really good performance is achieved neither on CIFAR nor on ImageNet  \n\nPros:\n- The paper is nicely written and good to follow. \n- Suggested modifications from DTP to STDP increase its biological plausibility without making its performance worse.  \n- The worse performance compared to backprop and CNNs underlines the open question how to yield biologically plausible AND efficient algorithms and network architectures.  \n\nCons:\n- The title of the paper seems to general to me, since target propagation is the only algorithm compared against backpropagation.  \n- Since the adaptions to DTP are rather small, the work does not contain much novelty.  It can rather be seen as an interesting empirical study, with \""negative result\ "".\n\n\nMinor comments:\n- page 1: The reference list could also include  http://www.mitpressjournals.org/doi/abs/10.1162/NECO_a_00934 and  https://arxiv.org/abs/1510.02777\n-  page 5: \u201cthe the degree\u201d , \u201cspecified as (\u2026.) followed by\u201d -> , \u201cas (\u2026.) followed by\u201d ?,\n- This notation probably stems from the code, but SAME and VALID could be nicer described as \u201c0 padding\u201d and \u201cno padding\u201d  for example.\n- page 8:  \u201capplying BP to the brain\u201d sounds strange to me.\n",1,1,1,1,1,1,1,1,1,-1
BypdvewVM-R3,"This is a high-quality and clear paper looking at biologically-plausible learning algorithms for deep neural networks.  The contributions here are: 1) experiments testing the DTP algorithm on more difficult datasets,  2) proposing a minor modification of the DTP algorithm at the output layer,  and 3) testing the DTP algorithm on locally-connected architectures.  These are all novel contributions, but each one seems incremental in the context of previous work on this and similar algorithms (E.G. Nokland, Direct Feedback Alignment Provides Learning in Deep Neural Networks, 2016; Baldi et al, Learning in the Machine: The Symmetries of the Deep Learning Channel, 2017). \n\n",1,1,1,1,-1,1,1,-1,1,-1
ByqFhGZCW-R1,"The game-theoretic approach to attacks with / defense against adversarial examples is an important direction of the security of deep learning and I appreciate the authors to initiate this kind of study.  \n\nLemma 1 summarizes properties of the solutions that are expected to have after reaching equilibria.  Important properties of saddle points in the min-max/max-min analysis assume that the function is convex/concave w.r.t. to the target variable.   In case of deep learning, the convexity is not guaranteed and the resulting solutions do not have necessarily follow Lemma 1. \u3000Nonetheless, this type of analysis can be useful under appropriate solutions if non-trivial claims are derived; however, Lemma 1 simply explains basic properties of the min-max solutions and max-min solutions works and does not contain non-tibial claims. \n\nAs long as the analysis is experimental, the state of the art should be considered.  As long as the reviewer knows, the CW attack gives the most powerful attack and this should be considered for comparison. The results with MNIST and CIFAR-10 are different.  In some cases, MNIST is too easy to consider the complex structure of deep architectures.  I prefer to have discussions on experimental results with both datasets. \n\nThe main takeaway from the entire paper is not clear very much.  It contains a game-theoretic framework of adversarial examples/training, novel attack method, and many experimental results. \n\nMinor:\nDefinition of g in the beginning of Sec 3.1 seems to be a typo.  What is u? This is revealed in the latter sections but should be specified here. \n\nIn Section 3.1, \n>This is in stark contrast with the near-perfect misclassification of the undefended classifier in Table 1. \nThe results shown in the table seems to indicate the \u201cperfect\u201d misclassification. \n\nSentence after eq. 15 seems to contain a grammatical error. \n\nThe paragraph after eq. 17 is duplicated with a paragraph introduced before",1,1,1,1,1,1,1,1,1,-1
ByqFhGZCW-R2,"This paper presents a sensitivity-penalized loss (the loss of the classifier has an additional term in squart of the gradient of the classifier w.r.t. perturbations of the inputs), and a minimax (or maximin) driven algorithm to find attacks and defenses.  It has a lemma which claims that the \""minimax and the maximin solutions provide the best worst-case defense and attack models, respectively\"", without proof, although that statement is supported experimentally. \n\n+ Prior work seem adequately cited and compared to, but I am not really knowledgeable in the adversarial attacks subdomain. \n- The experiments are on small/limited datasets (MNIST and CIFAR-10). Because of this, confidence intervals (over different initializations, for instance) would be a nice addition to Table 5. \n- There is no exact (\""alternating optimization\"" could be considered one) evaluation of the impact of the sensitivy loss vs. the minimax/maximin algorithm. \n- The paper is hard to follow at times (and probably that dealing with the point above would help in this regard), e.g. Lemma 1 and experimental analysis. \n- It is unclear (from Figures 3 and 7) that \""alternative optimization\"" and \""minimax\"" converged fully, and/or that the sets of hyperparameters were optimal. \n+ This paper presents a game formulation of learning-based attacks and defense in the context of adversarial examples for neural networks, and empirical findings support its claims. \n\n\nNitpicks:\nthe gradient descent -> gradient descent or the gradient descent algorithm\nseeming -> seemingly\narbitrary flexible -> arbitrarily flexible\ncan name \""gradient descent that maximizes\"": gradient ascent.\nThe mini- max or the maximin solution is defined -> are defined\nis the follow -> is the follower\n",1,1,1,1,1,1,1,1,1,1
ByqFhGZCW-R3,"The authors describe a mechanism for defending against adversarial learning attacks on classifiers.  They first consider the dynamics generated by the following procedure.  They begin by training a classifier, generating attack samples using FGSM, then hardening the classifier by retraining with adversarial samples, generating new attack samples for the retrained classifier, and repeating.   \n\nThey next observe that since FGSM is given by a simple perturbation of the sample point by the gradient of the loss, that the fixed point of the above dynamics can be optimized for directly using gradient descent.  They call this approach Sens FGSM, and evaluate it empirically against the various iterates of the above approach.  \n\nThey then generalize this approach to an arbitrary attacker strategy given by some parameter vector (e.g. a neural net for generating adversarial samples).  In this case, the attacker and defender are playing a minimax game, and the authors propose finding the minimax (or maximin) parameters using an algorithm which alternates between maximization and minimization gradient steps.  They conclude with empirical observations about the performance of this algorithm. \n\nThe paper is well-written and easy to follow.  However, I found the empirical results to be a little underwhelming.  Sens-FGSM outperforms the adversarial training defenses tuned for the \u201cwrong\u201d iteration, but it does not appear to perform particularly well with error rates well above 20%.  How does it stack up against other defense approaches (e.g. https://arxiv.org/pdf/1705.09064.pdf)?  Furthermore, what is the significance of FGSM-curr (FGSM-81) for Sens-FGSM?  It is my understanding that Sens-FGSM is not trained to a particular iteration of the \u201ccat-and-mouse\u201d game.  Why, then, does Sens-FGSM provide a consistently better defense against FGSM-81?  With regards to the second part of the paper, using gradient methods to solve a minimax problem is not especially novel (i.e. Goodfellow et al.),;  thus I would liked to see more thorough experiments here as well.  For example, it\u2019s unlikely that the defender would ever know the attack network utilized by an attacker.  How robust is the defense against samples generated by a different attack network?  The authors seem to address this in section 5 by stating that the minimax solution is not meaningful for other network classes. However, this is a bit unsatisfying.  Any defense can be *evaluated* against samples generated by any attacker strategy.  Is it the case that the defenses fall flat against samples generated by different architectures?  \n\n\nMinor Comments:\nSection 3.1, First Line. \u201df(ul(g(x),y))\u201d appears to be a mistake",1,1,1,1,1,1,1,1,1,-1
ByQpn1ZA--R1,"The submission describes an empirical study regarding the training performance\nof GANs; more specifically, it aims to present empirical evidence that the\ntheory of divergence minimization is more a tool to understand the outcome of\ntraining (i.e. Nash equillibrium) than a necessary condition to be enforce\nduring training itself .\n\nThe work focuses on studying \""non-saturating\"" GANs, using the modified generator\nobjective function proposed by Goodfellow et al. in their seminal GAN paper, and\naims to show increased capabilities of this variant, compared to the \""standard\""\nminimax formulation.  Since most theory around divergence minimization is based\non the unmodified loss function for generator G, the experiments carried out in\nthe submission might yield somewhat surprising results compared the theory. \n\nIf I may summarize the key takeaways from Sections 5.4 and 6, they are:\n- GAN training remains difficult and good results are not guaranteed (2nd bullet\n  point) ;\n- Gradient penalties work in all settings, but why is not completely clear; \n- NS-GANs + GPs seems to be best sample-generating combination, and faster than\n  WGAN-GP .\n- Some of the used metrics can detect mode collapse. \n\nThe submission's (counter-)claims are served by example (cf. Figure 2, or Figure\n3 description, last sentence), and mostly relate to statements made in the WGAN\npaper (Arjovsky et al., 2017). \n\nAs a purely empirical study, it poses more new and open questions on GAN\noptimization than it is able to answer; providing theoretical answers is\ndeferred to future studies.  This is not necessarily a bad thing, since the\nextensive experiments (both \""toy\"" and \""real\"") are well-designed, convincing and\ncomprehensible . Novel combinations of GAN formulations (non-saturating with\ngradient penalties) are evaluated to disentangle the effects of formulation\nchanges. \n\nOverall, this work is providing useful experimental insights, clearly motivating\nfurther study.\n",1,1,1,1,1,1,1,1,1,-1
ByQpn1ZA--R2,"Quality: The authors study non-saturating GANs and the effect of two penalized gradient approaches.  The authors consider a number of thought experiments to demonstrate their observations and validate these on real data experiments . \n\nClarity: The paper is well-written and clear . The authors could be more concise when reporting results . I would suggest keeping the main results in the main body and move extended results to an appendix. \n\nOriginality: The authors demonstrate experimentally that there is a benefit of using non-saturating GANs . More specifically, the provide empirical evidence that they can fit problems where Jensen-Shannon divergence fails.  They also show experimentally that penalized gradients stabilize the learning process. \n\nSignificance: The problems the authors consider is worth exploring further . The authors describe their finding in the appropriate level of details and demonstrate their findings experimentally . However, publishing this  work is in my opinion premature for the following reasons:\n\n- The authors do not provide further evidence of why non-saturating GANs perform better or under which mathematical conditions (non-saturating) GANs will be able to handle cases where distribution manifolds do not overlap ;\n- The authors show empirically the positive effect of penalized gradients, but do not provide an explanation grounded in theory ;\n- The authors do not provide practical recommendations how to set-up GANs and not that these findings did not lead to a bullet-proof recipe to train them.\n\n",1,1,1,1,1,-1,1,1,1,-1
ByQpn1ZA--R3,"This paper answers recent critiques about ``standard GAN'' that were recently formulated to motivate variants based on other losses, in particular using ideas from optimal transport.   It makes main points\n1) ``standard GAN'' is an ill-defined term that may refer to two different learning criteria, with different properties \n2) though the non-saturating variant (see Eq. 3) of ``standard GAN'' may converge towards a minimum of the Jensen-Shannon divergence, it does not mean that the minimization process follows gradients of the Jensen-Shannon divergence (and conversely, following gradient paths of the Jensen-Shannon divergence may not converge towards a minimum, but this was rather the point of the previous critiques about ``standard GAN'').  \n3) the penalization strategies introduced for ``non-standard GAN'' with specific motivations, may also apply successfully to the ``standard GAN'', improving robustness, thereby helping to set hyperparameters. \nNote that item 2) is relevant in many other setups in the deep learning framework and is often overlooked. \n\nOverall, I believe that the paper provides enough material to substantiate these claims, even if the message could be better delivered.  In particular, the writing is sometimes ambiguous (e.g. in Section 2.3, the reader who did not follow the recent developments on the subject on arXiv will have difficulties to rebuild the cross-references between authors, acronyms and formulae).  The answers to the critiques referenced in the \n paper are convincing, though I must admit that I don't know how crucial it is to answer these critics, since it is difficult to assess wether they reached or will reach a large audience. \n\nDetails:\n- p. 4 please do not qualify KL as a distance metric  \n- Section 4.3: \""Every GAN variant was trained for 200000 iterations, and 5 discriminator updates were done for each generator update\"" is ambiguous: what is exactly meant by \""iteration\"" (and sometimes step elsewhere)?  \n- Section 4.3: the performance measure is not relevant regarding distributions. The l2 distance is somewhat OK for means, but it makes little sense for covariance matrices.",1,1,1,1,1,-1,1,1,1,1
ByquB-WC--R1,"The paper proposes to address the quadratic memory/time requirement of Relation Network (RN) by sequentially attending (via multiple layers) on objects and gating the object vectors with the attention weights of each layer.  The proposed model obtains state of the art in bAbI story-based QA and bAbI dialog task .\n\nPros:\n- The model achieves the state of the art in bAbI QA and dialog. I think this is a significant achievement given the simplicity of the model. \n- The paper is clearly written. \n\nCons:\n- I am not sure what is novel in the proposed model.  While the authors use notations used in Relation Network (e.g. 'g'), I don't see any relevance to Relation Network.  Rather, this exactly resembles End-to-end memory network (MemN2N) and GMemN2N.  Please tell me if I am missing something, but I am not sure of the contribution of the paper.  Of course, I notice that there are small architectural differences, but if these are responsible for the improvements, I believe the authors should have conducted ablation study or qualitative analysis that show that the small tweaks are meaningful. \n \nQuestion:\n- What is the exact contribution of the paper with respect to MemN2N and GMemN2N?",1,1,1,1,1,1,1,1,-1,-1
ByquB-WC--R2,"This paper introduces Related Memory Network (RMN), an improvement over Relationship Networks (RN).  RMN avoids growing the relationship time complexity as suffered by RN (Santoro et. Al 2017).  RMN reduces the complexity to linear time for the bAbi dataset.  RN constructs pair-wise interactions between objects in RN to solve complex tasks such as transitive reasoning.  RMN instead uses a multi-hop attention over objects followed by an MLP to learn relationships in linear time. \n\nComments for the author:\n\nThe paper addresses an important problem since understanding object interactions are crucial for reasoning.  However, how widespread is this problem across other models or are you simply addressing a point problem for RN?  For example, Entnet is able to reason as the input is fed in and the decoding costs are low.  Likewise, other graph-based networks (which although may require strong supervision) are able to decode quite cheaply.  \n\nThe relationship network considers all pair-wise interactions that are replaced by a two-hop attention mechanism (and an MLP).  It would not be fair to claim superiority over RN since you only evaluate on bABi while RN also demonstrated results on other tasks.  For more complex tasks (even over just text), it is necessary to show that you outperform RN w/o considering all objects in a pairwise fashion.  More specifically, RN uses an MLP over pair-wise interactions, does that allow it to model more complex interactions than just selecting two hops to generate attention weights.  Showing results with multiple hops (1,2,..) would be useful here. \n\nMore details are needed about Figure 3.  Is this on bAbi as well?  How did you generate these stories with so many sentences?  Another clarification is the bAbi performance over Entnet which claims to solve all tasks.  Your results show 4 failed tasks, is this your reproduction of Entnet? \n\nFinally, what are the savings from reducing this time complexity?  Some wall clock time results or FLOPs of train/test time should be provided since you use multiple hops. \n\nOverall, this paper feels like a small improvement over RN.  Without experiments over other datasets and wall clock time results, it is hard to appreciate the significance of this improvement.  One direction to strengthen this paper is to examine if RMN can do better than pair-wise interactions (and other baselines) for more complex reasoning tasks",1,1,1,1,1,1,1,1,1,-1
ByquB-WC--R3,"This paper proposes an alternative to the relation network architecture whose computational complexity is linear in the number of objects present in the input.  The model achieves good results on bAbI compared to memory networks and the relation network model.  From what I understood, it works by computing a weighted average of sentence representations in the input story where the attention weights are the output of an MLP whose input is just a sentence and question (not two sentences and a question).  This average is then fed to a softmax layer for answer prediction.  I found it difficult to understand how the model is related to relation networks, since it no longer scores every combination of objects (or, in the case of bAbI, sentences), which is the fundamental idea behind relation networks.  Why is the approach not evaluated on CLEVR, in which the interaction between two objects is perhaps more critical (and was the main result of the original relation networks paper)?  The fact that the model works well on bAbI despite its simplicity is interesting, but it feels like the paper is framed to suggest that object-object interactions are not necessary to explicitly model, which I can't agree with based solely on bAbI experiments.  I'd encourage the authors to do a more detailed experimental study with more tasks,;  but I can't recommend this paper's acceptance in its current form. \n\nother questions / comments:\n- \""we use MLP to produce the attention weight without any extrinsic computation between the input sentence and the question.\"" isn't this statement false because the attention computation takes as input the concatenation of the question and sentence representation? \n- writing could be cleaned up for spelling / grammar (e.g., \""last 70 stories\"" instead of \""last 70 sentences\""), currently the paper is very hard to read and it took me a while to understand the model",1,1,1,1,1,1,1,1,1,-1
ByQZjx-0--R1,"In the paper titled \""Faster Discovery of Neural Architectures by Searching for Paths in a Large Model\"", the authors proposed an efficient algorithm which can be used for efficient (less resources and time) and faster architecture design for neural networks.  The motivation of the new algorithm is by sharing parameters across child models in the searching of archtecture.  The new algorithm is empirically evaluated on two datasets (CIFAR-10 and Penn Treeback);  --- the new algorithm is 10 times faster and requires only 1/100 resources, and the performance gets worse only slightly. \n\nOverall, the paper is well-written.  Although the methodology within the paper appears to be incremental over previous NAS method, the efficiency got improved quite significantly",1,1,-1,1,-1,-1,1,-1,1,-1
ByQZjx-0--R2,"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources.  The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing.  In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks.  In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric.  Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice.  In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. \n\nThe authors present two ENAS models: one for CNNs, and another for RNNs.  Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.  However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space.  This is a limitation, as the model space is not as flexible as one would desire in a discovery task.  Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections.  Thus, they are essentially learning the skip connections while using a human-selected model.  \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections.  Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections.  Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. \n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process.  Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea.  It is also impressive how much faster their model performs on tasks without sacrificing much performance.  The main limitation is that the best architectures as currently described are less about discovery and more about human input;  -- finding a more efficient search path would be an important next step",1,1,1,1,1,-1,1,1,1,-1
ByQZjx-0--R3,"In this paper, the authors look to improve Neural Architecture Search (NAS), which has been successfully applied to discovering successful neural network architectures, albeit requiring many computational resources.  The authors propose a new approach they call Efficient Neural Architecture Search (ENAS), whose key insight is parameter sharing.  In NAS, the practitioners have to retrain for every new architecture in the search process, but in ENAS this problem is avoided by sharing parameters and using discrete masks.  In both approaches, reinforcement learning is used to  learn a policy that maximizes the expected reward of some validation set metric.  Since we can encode a neural network as a sequence, the policy can be parameterized as an RNN where every step of the sequence corresponds to an architectural choice.  In their experiments, ENAS achieves test set metrics that are almost as good as NAS, yet require significantly less computational resources and time. \n\nThe authors present two ENAS models: one for CNNs, and another for RNNs.  Initially it seems like the controller can choose any of B operations in a fixed number of layers along with choosing to turn on or off ay pair of skip connections.  However, in practice we see that the search space for modeling both skip connections and choosing convolutional sizes is too large, so the authors use only one restriction to reduce the size of the state space.  This is a limitation, as the model space is not as flexible as one would desire in a discovery task.  Moreover, their best results (and those they choose to report in the abstract) are due to fixing 4 parallel branches at every layer combined with a 1 x 1 convolution, and using ENAS to learn the skip connections.  Thus, they are essentially learning the skip connections while using a human-selected model.  \n\nENAS for RNNs is similar: while NAS searches for a new architecture, the authors use a recurrent highway network for each cell and use ENAS to find the skip connections.  Thus, it seems like the term Efficient Neural Architecture Search promises too much since in both tasks they are essentially only using the controller to find skip connections.  Although finding an appropriate architecture for skip connections is an important task, finding an efficient method to structure RNN cells seems like a significantly more important goal. \n\nOverall, the paper is well-written, and it brings up an important idea: that parameter sharing is important for discovery tasks so we can avoid re-training for every new architecture in the search process.  Moreover, using binary masks to control network path (essentially corresponding to training different models) is a neat idea.  It is also impressive how much faster their model performs on tasks without sacrificing much performance.  The main limitation is that the best architectures as currently described are less about discovery and more about human input;  -- finding a more efficient search path would be an important next step",1,1,1,1,1,-1,1,1,1,-1
ByRWCqvT--R1,"The authors propose a method for performing transfer learning and domain adaptation via a clustering approach.  The primary contribution is the introduction of a Learnable Clustering Objective (LCO) that is trained on an auxiliary set of labeled data to correctly identify whether pairs of data belong to the same class.  Once the LCO is trained, it is applied to the unlabeled target data and effectively serves to provide \""soft labels\"" for whether or not pairs of target data belong to the same class.  A separate model can then be trained to assign target data to clusters while satisfying these soft labels, thereby ensuring that clusters are made up of similar data points.  \n\nThe proposed LCO is novel and seems sound, serving as a way to transfer the general knowledge of what a cluster is without requiring advance knowledge of the specific clusters of interest.  The authors also demonstrate a variety of extensions, such as how to handle the case when the number of target categories is unknown, as well as how the model can make use of labeled source data in the setting where the source and target share the same task. \n\nThe way the method is presented is quite confusing, and required many more reads than normal to understand exactly what is going on.  To point out one such problem point, Section 4 introduces f, a network that classifies each data instance into one of k clusters.  However, f seems to be mentioned only in a few times by name, despite seeming like a crucial part of the method.  Explaining how f is used to construct the CCN could help in clarifying exactly what role f plays in the final model.  Likewise, the introduction of G during the explanation of the LCO is rather abrupt, and the intuition of what purpose G serves and why it must be learned from data is unclear.  Additionally, because G is introduced alongside the LCO, I was initially misled into understanding was that G was optimized to minimize the LCO.  Further text explaining intuitively what G accomplishes (soft labels transferred from the auxiliary dataset to the target dataset) and perhaps a general diagram of what portions of the model are trained on what datasets (G is trained on A, CCN is trained on T and optionally S') would serve the method section greatly and provide a better overview of how the model works. \n\nThe experimental evaluation is very thorough, spanning a variety of tasks and settings.  Strong results in multiple settings indicate that the proposed method is effective and generalizable.  Further details are provided in a very comprehensive appendix, which provides a mix of discussion and analysis of the provided results.  It would be nice to see some examples of the types of predictions and mistakes the model makes to further develop an intuition for how the model works.  I'm also curious how well the model works if, you do not make use of the labeled source data in the cross-domain setting, thereby mimicking the cross-task setup. \n\nAt times, the experimental details are a little unclear.  Consistent use of the A, T, and S' dataset abbreviations would help.  Also, the results section seems to switch off between calling the method CCN and LCO interchangeably.  Finally, a few of the experimental settings differ from their baselines in nontrivial ways.  For the Office experiment, the LCO appears to be trained on ImageNet data.  While this seems similar in nature to initializing from a network pre-trained on ImageNet, it's worth noting that this requires one to have the entire ImageNet dataset on hand when training such a model, as opposed to other baselines which merely initialize weights and then fine-tune exclusively on the Office data.  Similarly, the evaluation on SVHN-MNIST makes use of auxiliary Omniglot data, which makes the results hard to compare to the existing literature, since they generally do not use additional training data in this setting.  In addition to the existing comparison, perhaps the authors can also validate a variant in which the auxiliary data is also drawn from the source so as to serve as a more direct comparison to the existing literature. \n\nOverall, the paper seems to have both a novel contribution and strong technical merit.  However, the presentation of the method is lacking, and makes it unnecessarily difficult to understand how the model is composed of its parts and how it is trained.  I think a more careful presentation of the intuition behind the method and more consistent use of notation would greatly improve the quality of this submission. \n\n=========================\nUpdate after author rebuttal:\n=========================\nI have read the author's response and have looked at the changes to the manuscript.  I am satisfied with the improvements to the paper and have changed my review to 'accept'",1,1,1,1,1,-1,1,1,1,-1
ByRWCqvT--R2,"(Summary)\nThis paper tackles the cross-task and cross-domain transfer and adaptation problems.  The authors propose learning to output a probability distribution over k-clusters and designs a loss function which encourages the distributions from the similar pairs of data to be close (in KL divergence) and the distributions from dissimilar pairs of data to be farther apart (in KL divergence).  What's similar vs dissimilar is trained with a binary classifier. \n\n(Pros)\n1. The citations and related works cover fairly comprehensive and up-to-date literatures on domain adaptation and transfer learning. \n2. Learning to output the k class membership probability and the loss in eqn 5 seems novel. \n\n(Cons)\n1. The authors overclaim to be state of the art.  For example, table 2 doesn't compare against two recent methods which report results exactly on the same dataset.  I checked the numbers in table 2 and the numbers aren't on par with the recent methods.  1) Unsupervised Pixel-Level Domain Adaptation with Generative Adversarial Networks, Bousmalis et al. CVPR17, and 2) Learning Transferrable Representations for Unsupervised Domain Adaptation, Sener et al. NIPS16.  Authors selectively cite and compare Sener et al. only in SVHN-MNIST experiment in sec 5.2.3 but not in the Office-31 experiments in sec 5.2.2. \n2. There are some typos in the related works section and the inferece procedure isn't clearly explained.  Perhaps the authors can clear this up in the text after sec 4.3. \n\n(Assessment)\nBorderline. Refer to the Cons section above",1,1,1,1,1,1,1,1,1,-1
ByRWCqvT--R3,"pros:\nThis is a great paper - I enjoyed reading it.  The authors lay down a general method for addressing various transfer learning problems: transferring across domains and tasks and in a unsupervised fashion.  The paper is clearly written and easy to understand . Even though the method combines the previous general learning frameworks, the proposed algorithm for  LEARNABLE CLUSTERING OBJECTIVE (LCO) is novel, and fits very well in this framework.   Experimental evaluation is performed on several benchmark datasets - the proposed approach outperforms state-of-the-art for specific tasks in most cases.  \n\ncons/suggestions: \n- the authors should discuss in more detail the limitations of their approach: it is clear that when there is a high discrepancy between source and target domains, that the similarity prediction network can fail",1,1,1,1,1,-1,1,1,1,-1
ByrZyglCb-R1,"The paper is written well and clear.    The core contribution of the paper is the illustration that: under the assumption of flat, or curved decision boundaries with positive curvature small universal adversarial perturbations exist.   \n\nPros: the intuition and geometry is rather clearly presented.   \n\nCons: \nReferences to \""CaffeNet\""  and \""LeNet\"" (even though the latter is well-known) are missing.   In the experimental section used to validate the main hypothesis that the deep networks have positive curvature decision boundaries, there is no description of how these networks were trained.  \n\nIt is not clear why the authors have decided to use out-dated 5-layer \""LeNet\""  and NiN (Network in network) architectures instead of more recent and much better performing architectures (and less complex than NiN architectures).  It would be nice to see how the behavior and boundaries look in these cases.   \n\nThe conclusion is speculative:\n\""Our analysis hence shows that to construct classifiers that are robust to universal perturbations, it\nis key to suppress this subspace of shared positive directions, which can possibly be done through\nregularization of the objective function.  This will be the subject of future works. \"" \n\nIt is clear that regularization should play a significant role in shaping the decision boundaries.  Unfortunately, the paper does not provide details at the basic level, which algorithms,  architectures, hyper-parameters or regularization terms are used.  All these factors should play a very significant role in the experimental validation of their hypothesis. \n\nNotes: I did not check the proofs of the theorems in detail. \n",1,1,1,1,1,-1,1,1,1,1
ByrZyglCb-R2,"This paper discusses universal perturbations - perturbations that can mislead a trained classifier if added to most of input data points.  The main results are two-fold: if the decision boundary are flat (such as linear classifiers), then the classifiers tend to be vulnerable to universal perturbations when the decision boundaries are correlated.  If the decision boundary are curved, then vulnerability to universal perturbations is directly resulted from existence of shared direction along with the decision boundary positively curved.  The authors also conducted experiments to show that deep nets produces decision boundary that satisfies the curved model. \n\nThe main issue I am having is what are the applicable insight from the analysis: \n\n1. Why is universal perturbation an important topic (as opposed to adversarial perturbation). \n2. Does the result implies that we should make the decision boundary more flat, or curved but on different directions? And how to achieve that?  It might be my mis-understanding but from my reading a prescriptive procedure for universal perturbation seems not attained from the results presented.",1,1,1,1,1,-1,1,1,1,-1
ByrZyglCb-R3,"The paper develops models which attempt to explain the existence of universal perturbations which fool neural networks \u2014 i.e., the existence of a single perturbation which causes a network to misclassify most inputs.  The paper develops two models for the decision boundary:\n\n(a) A locally flat model in which the decision boundary is modeled with a hyperplane and the normals two the hyperplanes are assumed to lie near a low-dimensional linear subspace. \n\n(b) A locally positively curved model, in which there is a positively curved outer bound for the collection of points which are assigned a given label.  \n\nThe paper works out a probabilistic analysis arguing that when either of these conditions obtains, there exists a fooling perturbation which affects most of the data.  \n\nThe theoretical analysis in the paper is straightforward, in some sense following from the definition.  The contribution of the paper is to posit these two conditions which can predict the existence of universal fooling perturbations, argue experimentally that they occur in (some) neural networks of practical interest.  \n\nOne challenge in assessing the experimental claims is that practical neural networks are nonsmooth; the quadratic model developed from the hessian is only valid very locally.  This can be seen in some of the illustrative examples in Figure 5: there *is* a coarse-scale positive curvature, but this would not necessarily come through in a quadratic model fit using the hessian.  The best experimental evidence for the authors\u2019 perspective seems to be the fact that random perturbations from S_c misclassify more points than random perturbations constructed with the previous method.  \n\nI find the topic of universal perturbations interesting, because it potentially tells us something structural (class-independent) about the decision boundaries constructed by artificial neural networks.  To my knowledge, the explanation of universal perturbations in terms of positive curvature is novel.  The paper would be much stronger if it provided an explanation of *why* there exists this common subspace of universal fooling perturbations, or even what it means geometrically that positive curvature obtains at every data point.  \n\nVisually, these perturbations seem to have strong, oriented local high-frequency content \u2014 perhaps they cause very large responses in specific filters in the lower layers of a network, and conventional architectures are not robust to this?  \n\nIt would also be nice to see some visual representations of images perturbed with the new perturbations, to confirm that they remain visually similar to the original images",1,1,1,1,1,-1,1,1,1,-1
ByS1VpgRZ-R1,"\nI thank the authors for the thoughtful response and updated manuscript.  After reading through both, my review score remains unchanged. \n\n=================\n\nThe authors describe a new variant of a generative adversarial network (GAN) for generating images.  This model employs a 'projection discriminator' in order to incorporate image labels and demonstrate that the resulting model outperforms state-of-the-art GAN models. \n\nMajor comments:\n1) Spatial resolution. What spatial resolution is the model generating images at?  The AC-GAN work performed an analysis to assess how information is being introduced at each spatial resolution by assessing the gains in the Inception score versus naively resizing the image.  It is not clear how much the gains of this model is due to generating better lower resolution images and performing simple upscaling.  It would be great to see the authors address this issue in a serious manner. \n\n2) FID in real data. The numbers in Table 1 appear favorable to the projection model.  Please add error bars (based on Figure 4, I would imagine they are quite large).  Additionally, would it be possible to compute this statistic for *real* images?  I would be curious to know what the FID looks like as a 'gold standard'. \n\n3) Conditional batch normalization.  I am not clear how much of the gains arose from employing conditional batch normalization versus the proposed method for incorporating the projection based discriminator.  The former has been seen to be quite powerful in accomodating multi-modal tasks (e.g. https://arxiv.org/abs/1709.07871, https://arxiv.org/abs/1610.07629\n).  If the authors could provide some evidence highlighting the marginal gains of one technique, that would be extremely helpful. \n\nMinor comments:\n- I believe you have the incorrect reference for conditional batch normalization on Page 5. \nA Learned Representation For Artistic Style \nDumoulin, Shlens and Kudlur (2017)\nhttps://arxiv.org/abs/1610.07629 \n\n- Please enlarge images in Figure 5-8. Hard to see the detail of 128x128 images. \n\n- Please add citations for Figures 1a-1b.  Do these correspond with some known models? \n\nDepending on how the authors respond to the reviews, I would consider upgrading the score of my review.",1,1,1,1,1,1,1,1,1,-1
ByS1VpgRZ-R2,"The paper proposes a simple modification to conditional GANs, obtaining impressive results on both the quality and diversity of samples on ImageNet dataset.  Instead of concatenating the condition vector y to the input image x or hidden layers of the discriminator D as in the literature, the authors propose to project the condition y onto a penultimate feature space V of D (by simply taking an inner product between y and V) .  This implementation basically restricts the conditional distribution p(y|x) to be really simple and seems to be posing a good prior leading to great empirical results. \n\n+ Quality:\n- Simple method leading to great results on ImageNet! \n- While the paper admittedly leaves theoretical work for future work, the paper would be much stronger if the authors could perform an ablation study to provide readers with more intuition on why this work.  One experiment could be: sticking y to every hidden layer of D before the current projection layer, and removing these y's increasingly and seeing how performance changes. \n- Appropriate comparison with existing conditional models: AC-GANs and PPGNs. \n- Appropriate (extensive) metrics were used (Inception score/accuracy, MS-SSIM, FID) \n\n+ Clarity:\n- Should explicitly define p, q, r upfront before Equation 1 (or between Eq1 and Eq2).\n- PPG should be PPGNs.\ n\n+ Originality:\nThis work proposes a simple method that is original compared existing GANs. \n\n+ Significance:\nWhile the contribution is significant, more experiments providing more intuition into why this projection works so well would make the paper much stronger. \n\nOverall, I really enjoy reading this paper and recommend for acceptance",1,1,1,1,1,1,1,1,1,-1
ByS1VpgRZ-R3,"This manuscript makes the case for a particular parameterization of conditional GANs, specifically how to add conditioning information into the network.   It motivates the method by examining the form of the log density ratio in the continuous and discrete cases. \n\nThis paper's empirical work is quite strong, bringing to bare nearly all of the established tools we currently have for evaluating implicit image models (MS-SSIM, FID, Inception scores).  \n\nWhat bothers me is mostly that, while hyperparameters are stated (and thank you for that), they seem to be optimized for the candidate method rather than the baseline.  In particular, Beta1 = 0 for the Adam momentum coefficient seems like a bold choice based on my experience.  It would be an easier sell if hyperparameter search details were included and a separate hyperparameter search were conducted for the candidate and control, allowing the baseline to put its best foot forward. \n\nThe sentence containing \""assume that the network model can be shared\"" had me puzzled for a few minutes.  I think what is meant here is just that we can parameterize the log density ratio directly (including some terms that belong to the data distribution to which we do not have explicit access). This could be clearer.",1,1,1,1,1,-1,1,1,-1,1
Bys4ob-Rb-R1,"This paper develops a new differentiable upper bound on the performance of classifier when the adversarial input in l_infinity is assumed to be applied. \nWhile the attack model is quite general, the current bound is only valid for linear and NN with one hidden layer model, so the result is quite restrictive. \n\nHowever the new bound is an \""upper\"" bound of the worst-case performance which is very different from the conventional sampling based \""lower\"" bounds.  Therefore minimizing this upper bound together with a classification loss makes perfect sense and provides a theoretically sound approach to train a robust classifier. \nThis paper provides a gradient of this new upper bound with respect to model parameters so we can apply the usual first order optimization scheme to this joint optimization (loss + upper bound). \nIn conclusion, I recommend this paper to be accepted, since it presents a new and feasible direction of a principled approach to train a robust classifier,  and the paper is clearly written and easy to follow. \n \nThere are possible future directions to be developed. \n\n1. Apply the sum-of-squares (SOS) method.\nThe paper's SDP relaxation is the straightforward relaxation of Quadratic Program (QP), and in terms of SOS relaxation hierarchy, it is the first hierarchy.  One can increase the complexity going beyond the first hierarchy, and this should provides a computationally more challenging but tighter upper bound. \nThe paper already mentions about this direction and it would be interesting to see the experimental results. \n\n2. Develop a similar relaxation for deep neural networks.\nThe author already mentioned that they are pursuing this direction.  While developing the result to the general deep neural networks might be hard, residual networks maybe fine thanks to its structure.",1,1,1,1,1,-1,1,1,1,-1
Bys4ob-Rb-R2,"The authors propose a new defense against security attacks on neural networks.  The attack model involves a standard l_inf norm constraint.  Remarkably, the approach outputs a security certificate (security guarantee) on the algorithm, which makes it appealing for security use in practice.  Furthermore, the authors include an approximation of the certificate into their objective function, thus training networks that are more robust against attacks.  The approach is evaluated for several attacks on MNIST data. \n\nFirst of all, the paper is very well written and structured.  As standard in the security community, the attack model is precisely formalized (I find this missing in several other ML papers on the topic).  The certificate is derived with rigorous and sound math.  An innovative approximation based on insight into a relation to the MAXCUT algorithm is shown.  An innovative training criterion based on that certificate is proposed.  Both the performance of the new training objective and the tightness of the cerificate are analyzed empirically showing that good agreement with the theory and good results in terms of robustness against several attacks. \n\nIn summary, this is an innovative paper that treats the subject with rigorous mathematical formalism and is successful in the empirical evaluation.  For me, it is a clear accept.  The only drawback I see is the missing theoretical and empirical comparison to the recent NIPS 2017 paper by Hein et al.\n",1,1,1,1,-1,1,1,1,1,-1
Bys4ob-Rb-R3,"This paper derived an upper bound on adversarial perturbation for neural networks with one hidden layer.  The upper bound is derived via (1) theorem of middle value; (2) replace the middle value by the maximum (eq 4); (3) replace the maximum of the gradient value (locally) by the global maximal value (eq 5); (4) this leads to a non-convex quadratic program, and then the authors did a convex relaxation similar to maxcut to upper bound the function by a SDP, which then can be solved in polynomial time. \n\nThe main idea of  using upper bound (as opposed to lower bound) is reasonable.  However, I find there are some limitations/weakness of the proposed method:\n1. The method is likely not extendable to more complicated and more practical networks, beyond the ones discussed in the paper (ie with one hidden layer)\n2.  SDP while tractable, would still require very expensive computation to solve exactly. \n3. The relaxation seems a bit loose - in particular, in above step 2 and 3, the authors replace the gradient value by a global upper bound on that, which to me seems can be pretty loose.",1,1,1,1,-1,-1,1,1,-1,-1
BySRH6CpW-R1,"This paper proposes training binary and ternary weight distribution networks through the local reparametrization trick and continuous optimization.  The argument is that due to the central limit theorem (CLT) the distribution on the neuron pre-activations is approximately Gaussian, with a mean given by the inner product between the input and the mean of the weight distribution and a variance given by the inner product between the squared input and the variance of the weight distribution.  As a result, the parameters of the underlying discrete distribution can be optimized via backpropagation by sampling the neuron pre-activations with the reparametrization trick.  The authors further propose appropriate initialisation schemes and regularization techniques to either prevent the violation of the CLT or to prevent underfitting.  The method is evaluated on multiple experiments. \n\nThis paper proposed a relatively simple idea for training networks with discrete weights that seems to work in practice.  My main issue is that while the authors argue about novelty, the first application of CLT for sampling neuron pre-activations at neural networks with discrete r.v.s is performed at [1].  While [1] was only interested in faster convergence and not on optimization of the parameters of the underlying distribution, the extension was very straightforward.  I would thus suggest that the authors update the paper accordingly.  \n\nOther than that, I have some other comments :\n- The L2 regularization on the distribution parameters for the ternary weights is a bit ad-hoc; why not penalise according to the entropy of the distribution which is exactly what you are trying to achieve?  \n- For the binary setting you mentioned that you had to reduce the entropy thus added a \u201cbeta density regulariser\u201d. Did you add R(p) or log R(p) to the objective function?  Also, with alpha, beta = 2 the beta density is unimodal with a peak at p=0.5; essentially this will force the probabilities to be close to 0.5, i.e. exactly what you are trying to avoid.  To force the probability near the endpoints you have to use alpha, beta < 1 which results into a \u201cbowl\u201d shaped Beta distribution.  I thus wonder whether any gains you observed from this regulariser are just an artifact of optimization.   \n- I think that a baseline (at least for the binary case) where you learn the weights with a continuous relaxation, such as the concrete distribution, and not via CLT would be helpful.  Maybe for the network to properly converge the entropy for some of the weights needs to become small (hence break the CLT).  \n\n[1] Wang & Manning, Fast Dropout Training. \n\nEdit: After the authors rebuttal I have increased the rating of the paper:  \n- I still believe that the connection to [1] is stronger than what the authors allude to; eg. the first two paragraphs of sec. 3.2 could easily be attributed to [1]. \n- The argument for the entropy was to include a term (- lambda * H(p)) in the objective function with H(p) being the entropy of the distribution p.  The lambda term would then serve as an indicator to how much entropy is necessary. \n- There indeed was a misunderstanding with the usage of the R(p) regularizer at the objective function (which is now resolved). \n- The authors showed benefits compared to a continuous relaxation baseline",1,1,1,1,1,1,1,1,1,-1
BySRH6CpW-R2,"Summary of the paper:\nThe paper suggests to use stochastic parameters in combination with the local reparametrisation trick (previously introduced by Kingma et al. (2015)) to train neural networks with binary or ternary wights.  Results on MNIST, CIFAR-10 and ImageNet are very competitive . \n\nPros:\n- The proposed method leads to state of the art results . \n- The paper is easy to follow and clearly describes the implementation details needed to reach the results.  \n\nCons:\n- The local reprarametrisation trick it self is not new and applying it to a multinomial distribution (with one repetition) instead of a Gaussian is straight forward,;  but its application for learning discrete networks is to my best knowledge novel and interesting.  \n\nIt could be nice to include the results of Zuh et al (2017) in the results table and to indicate the variance for different samples of weights resulting from your methods in brackets.  \n\n\nMinor comments:\n- Some citations have a strange format: e.g. \u201cin Hubara et al. (2016); Restegari et al. (2016)\u201c would be better readable as   \u201cby Hubara et al. (2016) and Restegari et al. (2016)\u201c.  \n-  To improve notation, it could be directly written that W is the set of all w^l_{i,j} and \\mathcal{W} is the joint distribution resulting from independently sampling from  \\mathcal{W}^l_{i,j}.  \n- page 6: \u201con the last full precision network\u201d: should probably be \u201con the last full precision layer\u201d\n                     \u201c distributions has\u201d ->  \u201c distributions have\u201d",1,1,1,1,1,1,1,1,1,1
BySRH6CpW-R3,"This paper introduces the LR-Net, which uses the reparametrization trick inspired by a similar component in VAE.  Although the idea of reparametrization itself is not new, applying that for the purpose of training a binary or ternary network, and sample the pre-activations instead of weights is novel.   From the experiments, we can see that the proposed method is effective.  \n\nIt seems that there could be more things to show in the experiments part.  For example, since it is using a multinomial distribution for weights, it makes sense to see the entropy w.r.t. training epochs.  Also, since the reparametrization is based on the Lyapunov Central Limit Theorem, which assumes statistical independence, a visualization of at least the correlation between the pre-activation of each layer would be more informative than showing the histogram.  \n\nAlso, in the literature of low precision networks, people are concerning both training time and test time computation demand.  Since you are sampling the pre-activations instead of weights, I guess this approach is also able to reduce training time complexity by an order.  Thus a calculation of train/test time computation could highlight the advantage of this approach more boldly",1,1,1,1,1,-1,1,1,1,-1
Bys_NzbC--R1,"The authors studied the behavior that a strong regularization parameter may lead to poor performance in training of deep neural networks.  Experimental results on CIFAR-10 and CIFAR-100 were reported using AlexNet and VGG-16.  The results seem to show that a delayed application of the regularization parameter leads to improved classification performance. \n\nThe proposed scheme, which delays the application of regularization parameter, seems to be in contrast of the continuation approach used in sparse learning.  In the latter case, a stronger parameter is applied, followed by reduced regularization parameter.  One may argue that the continuation approach is applied in the convex optimization case, while the one proposed in this paper is for non-convex optimization.  It would be interesting to see whether deep networks can benefit from the continuation approach, and the strong regularization parameter may not be an issue because the regularization parameter decreases as the optimization progress goes on. \n\nOne limitation of the work, as pointed by the authors, is that experimental results on big data sets such as ImageNet is not reported",1,1,1,1,1,-1,1,1,1,-1
Bys_NzbC--R2,"The paper is well motivated and written.  However, there are several issues. \n1. As the regularization constant increases, the performance first increases and then falls down -- this specific aspect is well known for constrained optimization problems.  Further, the sudden drop in performance also follows from vanishing gradients problem in deep networks.  The description for ReLUs in section 2.2 follows from these two arguments directly, hence not novel . Several of the key aspects here not addressed are: \n1a. Is the time-delayed regularization equivalent to reducing the value (and there by bringing it back to the 'good' regime before the cliff in the example plots)?  \n1b. Why should we keep increasing the regularization constant beyond a limit?  Is this for compressing the networks (for which there are alternate procedures), or anything else.  In other words, for a non-convex problem (about whose landscape we know barely anything), if there are regimes of regularizers that work well (see point 2) -- why should we ask for more stronger regularizers?  Is there any optimization-related motivation here (beyond the single argument that networks are overparameterized)?  \n2. The proposed experiments are not very conclusive.  Firstly, the authors need to test with modern state-of-the-art architectures including inception and residual networks.  Secondly, more datasets including imagenet needs to be tested.  Unless these two are done, we cannot assertively say that the proposal seems to do interesting things.  Thirdly, it is not clear what Figure 5 means in terms of goodness of learning.  And lastly, although confidence intervals are reported for Figures 3,4 and Table 2, statistical tests needs to be performed to report p-values (so as to check if one model significantly beats the other)",1,1,1,1,1,-1,1,1,1,-1
Bys_NzbC--R3,"The work was prompted by  an interesting observation: a phase transition can be observed in deep learning with stochastic gradient descent and Tikhonov regularization.  When the regularization parameter exceeds a (data-dependent) threshold, the parameters of the model are driven to zero, thereby preventing any learning.  The authors then propose to moderate this problem by letting the regularization parameter to be zero for 5 to 10 epochs, and then applying the \""strong\"" penalty parameter.  In their experimental results, the phase transition is not observed anymore with their protocol.  This leads to better performances, by using penalty parameters that would have prevent learning with the usual protocol. \n\nThe problem targeted is important, in the sense that it reveals that some of the difficulties related to non-convexity and the use of SGD that are often overlooked.  The proposed protocol is reported to work well, but since it is really ad hoc, it fails to convince the reader that it provides the right solution to the problem.  I would have found much more satisfactory to either address the initialization issue by a proper warm-start strategy, or to explore standard optimization tools such as constrained optimization (i.e. Ivanov regularization) , that could be for example implemented by stochastic projected gradient or barrier functions.  I think that the problem would be better handled that way than with the proposed strategy,;  which seems to rely only on a rather limited amount of experiments, and which may prove to be inefficient when dealing with big databases. \n\nTo summarize, I believe that the paper addresses an important point,;  but that the tools advocated are really rudimentary compared with what has been already proposed elsewhere. \n\nDetails :\n- there is a typo in the definition of the proximal operator in Eq. (9)  \n- there are many unsubstantiated speculations in the comments of the experimental section that do not add value to the paper  \n- the figure showing the evolution of the magnitude of parameters arrives too late and could be completed by the evolution of the data-fitting term of the training criterion",1,1,1,1,1,-1,1,1,1,-1
ByUEelW0--R1,"The paper proposes to add a rotation operation in long short-term memory (LSTM) cells.  It performs experiments on bAbI tasks and showed that the results are better than the simple baselines with original LSTM cells.  There are a few problems with the paper.\n\nFirstly, the title and abstract discuss \""modifying memories\"", but the content is only about a rotation operation.  Perhaps the title should be \""Rotation Operation in Long Short-Term Memory\""? \n\nSecondly, the motivation of adding the rotation operation is not properly justified.  What does it do that a usual LSTM cell could not learn?  Does it reduce the excess representational power compared to the LSTM cell that could result in better models?   Or does it increase its representational capacity so that some pattern is modeled in the new cell structure that was not possible before?  This is not clear at all after reading the paper.  Besides, the idea of using a rotation operation in recurrent networks has been explored before [3]. \n\nFinally, the task (bAbI) and baseline models (LSTM from a Keras tutorial) are too weak.  There have been recent works that nearly solved the bAbI tasks to perfection (e.g., [1][2][4][5], and many others).  The paper presented a solution that is weak compared to these recent results. \n\nIn a summary, the main idea of adding rotation to LSTM cells is not properly justified in the paper, and the results presented are quite weak for publication in ICLR 2018. \n\n[1] Sainbayar Sukhbaatar, Jason Weston, Rob Fergus. End-to-end memory networks, NIPS 2015\n[2] Caiming Xiong, Stephen Merity, Richard Socher. Dynamic Memory Networks for Visual and Textual Question Answering, ICML 2016\n[3] Mikael Henaff, Arthur Szlam, Yann LeCun, Recurrent Orthogonal Networks and Long-Memory Tasks, ICML 2016 \n[4] Caglar Gulcehre, Sarath Chandar, Kyunghyun Cho, Yoshua Bengio, Dynamic Neural Turing Machine with Soft and Hard Addressing Schemes, ICLR 2017\n[5] Mikael Henaff, Jason Weston, Arthur Szlam, Antoine Bordes, Yann LeCun, Tracking the World State with Recurrent Entity Networks, ICLR 2017\n",1,1,1,1,1,1,1,1,1,-1
ByUEelW0--R2,"The paper proposes an additional transform in the recurrent neural network units.  The transform allows for explicit rotations and swaps of the hidden cell dimensions.  The idea is illustrated for LSTM units, where the transform is applied after the cell values are computed via the typical LSTM updates. \n\nMy first concern is the motivation.  I think the paper needs a more compelling example where swaps and rotations are needed and cannot otherwise be handled via gates.  In the proposed example, it's not clear to me why the gate is expected to be saturated at every time step such that it would require the memory swaps.  Alternatively, experimentally showing that the network makes use of swaps in an interpretable way (e.g. at certain sentence positions) could strengthen the motivation. \n\nSecondly, the experimental analysis is not very extensive.  The method is only evaluated on the bAbI QA dataset, which is a synthetic dataset.  I think a language modeling benchmark and/or a larger scale question answering dataset should be considered. \n\nRegarding the experimental setup, how are the hyper-parameters for the baseline tuned?  Have you considered training jointly (across the tasks) as well? \n\nAlso, is the setting the same as in Weston et al (2015)?  While for many tasks the numbers reported by Weston et al (2015) and the ones reported here for the LSTM baseline are aligned in the order of magnitude, suggesting that some tasks are easier or more difficult for LSTMs, there are large differences in other cases, for task #5 (here 33.6, Weston 70), for task #16 (here 48, Weston 23), and so on. \n\nFinally, do you have an intuition (w.r.t. to swaps and rotations) regarding the accuracy improvements on tasks #5 and #18? \n\nSome minor issues:\n- The references are somewhat inconsistent in style: some have urls, others do not; some have missing authors, ending with \""et al\"". \n- Section 1, second paragraph: senstence\n- Section 3.1, first paragraph: thorugh\n- Section 5: architetures",1,1,1,1,1,-1,1,1,1,-1
ByUEelW0--R3,"Summary: This paper introduces a model that combines the rotation matrices with the LSTMs.  They apply the rotations before the final tanh activation of the LSTM and before applying the output gate.  The rotation matrix is a block-diagonal one where each block is a 2x2 rotations and those rotations are parametrized by another neural network that predicts the angle of the rotations.  The paper only provides results on the bAbI task.  \n\nQuestions:\nHave you compared against to the other parametrizations of the LSTMs and rotation matrices? (ablation study) \nHave you tried on other tasks? \nWhy did you just apply the rotations only on d_{t}. \n\nPros:\nUses a simple parametrization of the rotation matrices. \n\nCons:\nNot clear justification and motivations \nThe experiments are really lacking:\nNo ablation study \nThe results are only limited to single toy task. \n\n\nGeneral Comments:\n\nThis paper proposes to use the rotation matrices with LSTMs.  However there is no clear justification why is this particular parametrization of rotation matrix is being used over others and why is it only applied before the output gate.  The experiments are seriously lacking, an ablation study should have been made and the results are not good enough.  The experiments are only limited to bAbI task which doesn\u2019t tell you much.  This paper is not ready for publication, and really feels like it is rushed. \n\nMinor Comment:\nThis paper needs more proper proof-reading.  There are some typos in it, e.g.:\n1st page, senstence --> sentence\n4th page, the the ..",1,1,1,1,1,-1,1,1,1,-1
ByuI-mW0W-R1,"`The papers aims to provide a quality measure/test for GANs.    The objective is ambitious an deserve attention.  As GANs are minimizing some f-divergence measure, the papers remarks that computing a  Wasserstein distance between two distributions made of a sum of Diracs is not a degenerate case and is tractable.  So they propose evaluate the current approximation of a distribution learnt by a GAN by using this distance as a baseline performance (in terms of W distance and computed on a hold out dataset).  \n\nA first remark is that the papers does not clearly develop the interest of puting things a trying to reach a treshold of performance in W distance rather than just trying to minimize the desired f-divergence.  More specifically as they assess the performance in terms of W distance I would would be tempted to just minimize the given criterion.  This would be very interesting to have arguments on why being better than the \""Dirac estimation\"" in terms of W2 distance would lead to better performance for others tasks (as other f-divergences or image generation). \n\nAccording to the authors the core claims are:\n\""1/ We suggest a formalisation of the goal of GAN training (/generative modelling more broadly) in terms of divergence minimisation.  This leads to a natural, testable notion of generalisation.  \""\nFormalization in terms of divergence minimization is not new (see O. Bousquet & all https://arxiv.org/pdf/1701.02386.pdf ). and I do not feel like this paper actually performs any \""test\"" (in a statistical sense).  In my opinion the contribution is more about exhibiting a baseline which has to be defeated for any algorithm interesting is learning the distribution in terms of W2 distance. \n\n\""2/ We use this test to evaluate the success of GAN algorithms empirically, with the Wasserstein distance as our divergence. \""\nHere the distance does not seems so good because the performance in generation does not seems to only be related to W2 distance.  Nevertheless, there is interesting observations in the paper about the sensitivity of this metric to the bluring of pictures.  I would enjoyed more digging in this direction.  The authors proposes to solve this issue by relying to an embedded space where the L2 distance makes more sense for pictures (DenseNet) . This is of course very reasonable but I would expect anyone working on distribution over picture to work with such embeddings.  Here I'm not sure if this papers opens a new way to improve the embedding making use on non labelled data.  One could think about allowing the weights of the embeddings to vary while f-divergence is minimized but this is not done in the submitted work. \n\n \""3/ We find that whether our proposed test matches our intuitive sense of GAN quality depends heavily on the ground metric used for the Wasserstein distance. \""\nThis claim is highly biased by who is giving the \""intuitive sense\"".  It would be much better evaluated thought a mechanical turk test. \n\n \""4/ We discuss how to use these insights to improve the design of WGANs more generally. \""\nAs our understanding of the GANs dynamics are very coarse, I feel this is not a good thing to claim that \""doing xxx should improve things\"" without actually trying it",1,1,1,1,1,1,1,1,1,-1
ByuI-mW0W-R2,"The quality of the paper is good, and clarity is mostly good.  The proposed metric is interesting,;  but it is hard to judge the significance without more thorough experiments demonstrating that it works in practice. \n\nPros:\n - clear definitions of terms\n - overall outline of paper is good\n - novel metric. \n\nCons\n - text is a bit over-wordy, and flow/meaning sometimes get lost.  A strict editor would be helpful, because the underlying content is good\n - odd that your definition of generalization in GANs appears immediately preceding the section titled \""Generalisation in GANs\""\n - the paragraph at the end of the \""Generalisation in GANs\"" section is confusing.  I think this section and the previous (\""The objective of unsupervised learning\"") could be combined, removing some repetition, adding some subtitles to improve clarity.  This would cut down the text a bit to make space for more experiments. \n - why is your definition of generalization that the test set distance is strictly less than training set ?  I would think this should be less-than-or-equal \n - there is a sentence that doesn't end at the top of p.3: \""... the original GAN paper showed that [ends here]. \""\n - should state in the abstract what your \""notion of generalization\"" for gans is, instead of being vague about it. \n - more experiments showing a comparison of the proposed metric to others (e.g. inception score, Mturk assessments of sample quality, etc.) would be necessary to find the metric convincing \n - what is a \""pushforward measure\""?  (p.2)\n - the related work section is well-written and interesting, but it's a bit odd to have it at the end.  Earlier in the work (e.g. before experiments and discussion) would allow the comparison with MMD to inform the context of the introduction. \n - there are some errors in figures that I think were all mentioned by previous commentators",1,1,1,1,1,1,1,1,1,-1
ByuI-mW0W-R3,"This paper proposed a procedure for assessing the performance of GANs by re-considering the key of observation.  And using the procedure to test and improve current version of GANs. It demonstrated some interesting stuff.  \n\nIt is not easy to follow the main idea of the paper.  The paper just told difference stories section by section.  Based on my understanding, the claims are 1) the new formalization of the goal of GAN training and 2) using this test to evaluate the success of GAN algorithms empirically?   I suggested that the author should reform the structure, ignore some unrelated content and make the clear claims about the contributions on the introduction part.   \n\nRegarding the experimental part, it can not make strong support for all the claims.  Figure 2 showed almost similar plots for all the varieties.  Meanwhile, the results are performed on some specific model configurations (like ResNet) and settings. It is difficult to justify whether it can generalize to other cases.  Some of the figures do not have the notations of curvey, making people hard to compare.  \n\nTherefore, I think the current version is not ready to be published. The author can make it stronger and consider next venue",1,1,1,1,1,-1,1,1,1,-1
ByuP8yZRb-R1,"The below review addresses the first revision of the paper . The revised version does address my concerns.  The fact that the paper does not come with substantial theoretical contributions/justification still stands out. \n\n---\n\nThe authors present a variant of the adversarial feature learning (AFL) approach by Edwards & Storkey.  AFL aims to find a data representation that allows to construct a predictive model for target variable Y, and at the same time prevents to build a predictor for sensitive variable S.  The key idea is to solve a minimax problem where the log-likelihood of a model predicting Y is maximized, and the log-likelihood of an adversarial model predicting S is minimized.  The authors suggest the use of multiple adversarial models, which can be interpreted as using an ensemble model instead of a single model. \n\nThe way the log-likelihoods of the multiple adversarial models are aggregated does not yield a probability distribution as stated in Eq. 2.  While there is no requirement to have a distribution here - a simple loss term is sufficient - the scale of this term differs compared to calibrated log-likelihoods coming from a single adversary.  Hence, lambda in Eq. 3 may need to be chosen differently depending on the adversarial model. Without tuning lambda for each method, the empirical experiments seem unfair.  This may also explain why, for example, the baseline method with one adversary effectively fails for Opp-L.  A better comparison would be to plot the performance of the predictor of S against the performance of Y for varying lambdas. The area under this curve allows much better to compare the various methods. \n\nThere are little theoretical contributions. Basically, instead of a single adversarial model - e.g., a single-layer NN or a multi-layer NN - the authors propose to train multiple adversarial models on different views of the data.  An alternative interpretation is to use an ensemble learner where each learner is trained on a different (overlapping) feature set.  Though, there is no theoretical justification why ensemble learning is expected to better trade-off model capacity and robustness against an adversary.  Tuning the architecture of the single multi-layer NN adversary might be as good? \n\nIn short, in the current experiments, the trade-off of the predictive performance and the effectiveness of obtaining anonymized representations effectively differs between the compared methods. This renders the comparison unfair.  Given that there is also no theoretical argument why an ensemble approach is expected to perform better,  I recommend to reject the paper.",1,1,1,1,-1,1,1,1,1,-1
ByuP8yZRb-R2,"- The authors propose the use of multiple adversaries over random subspaces of features in adversarial feature learning to produce censoring representations.  They show that their idea is effective in reducing private information leakage,  but this idea alone might not be signifcant enough as a contribution.  \n\n- The idea of training multiple adversaries over random subspaces is very similar to the idea of random forests which help with variance reduction.  Indeed judging from the large variance in the accuracy of predicting S in Table 1a-c for single adversaries, I suspect one of the main advantage of the current MARS method comes from variance reduction.  The author also mentioned using high capacity networks as adversaries does not work well in practice in the introduction, and this could also be due to the high model variance of such high capacity networks.   \n\n- The definition of S, the private information set, is not clear. There is no statement about it in the experiments section, and I assume S is the subject identity.  But this makes the train-test split described in 4.1 rather odd, since there is no overlap of subjects in the train-test split. We need clarifications on these experimental details.  \n\n- Judging from Figure 2 and Table 1, all the methods tested are not effective in hiding the private information S in the learned representation.  Even though the proposed method works better, the prediction accuracies of S are still high",1,1,1,1,1,-1,1,1,1,-1
ByuP8yZRb-R3,"MARS is suggested to combine multiple adversaries with different roles. \nExperiments show that it is suited to create censoring representations for increased anonymisation of data in the context of wearables. \n\nExperiments a are satisfying and show good performance when compared to other methods. \n\nIt could be made clearer how significance is tested given the frequent usage of the term. \n\nThe idea is slightly novel, and the framework otherwise state-of-the-art. \n\nThe paper is well written, but can use some proof-reading. \n\nReferencing is okay",1,1,-1,1,1,-1,1,1,1,-1
ByW5yxgA--R1,"The paper focuses on a very particular HMM structure which involves multiple, independent HMMs.  Each HMM emits an unobserved output with an explicit duration period . This explicit duration modelling captures multiple scales of temporal resolution.  The actual observations are a weighted linear combination of the emissions from each latent HMM.  The structure allows for fast inference using a spectral approach. \n\nI found the paper unclear and lacking in detail in several key aspects: \n\n1. It is unclear to me from Algorithm 2 how the weight vectors w are estimated. This is not adequately explained in the section on estimation. \n\n2. The authors make the assumption that each HMM injects noise into the unobserved output which then gets propagated into the overall observation.  What are reasons for his choice of model over a simpler model where the output of each HMM is uncorrupted? \n\n3. The simulation example does not really demonstrate the ability of the MSHMM to do anything other than recover structure from data simulated under an MSHMM.  It would be more interesting to apply to data simulated under non-Markovian or other setups that would enable richer frequency structures to be included and the ability of MSHMM to capture these. \n\n4. The real data experiments shows some improvements in predictive accuracy with fast inference.  However, the authors do not give a sufficiently broad exploration of the representations learnt by the model which allows us to understand the regimes in which the model would be advantageous. \n\nOverall, the paper presents an interesting approach  but the work lacks maturity.  Furthermore, simulation and real data examples to explore the properties and utility of the method are required.",1,1,1,1,1,-1,1,1,1,-1
ByW5yxgA--R2,"This paper proposes a variant of hierarchical hidden Markov Models (HMMs) where the chains operate at different time-scales with an associate d spectral estimation procedure that is computationally efficient. \n\nThe model is applied to artificially generated data and to high-frequency equity data showing promising results. \n\nThe proposed model and method are reasonably original and novel. \n\nThe paper is well written and the method reasonably well explained  (I would add an explanation of the spectral estimation in the Appendix, rather than just citing Rodu et al. 2013). \n\nAdditional experimental results would make it a stronger paper. \n\nIt would be great if the authors could include the code that implements the model",1,1,1,1,1,-1,1,1,1,1
ByW5yxgA--R3,"The paper presents an interesting spectral algorithm for multiscale hmm . The derivation and analysis seems correct.  However, it is well-known that spectral algorithm is not robust to model mis-specification.  It is not clear whether the proposed algorithm will be useful in practice.  How will the method compare to EM algorithms and neural network based approaches?",1,1,1,1,1,1,1,-1,1,-1
BywyFQlAW-R1,"Overview:\nThis paper proposes an approach to curriculum learning, where subsets of examples to train on are chosen during the training process.  The proposed method is based on a submodular set function over the examples, which is intended to capture diversity of the included examples and is added to the training objective (eq. 2).  The set is optimized to be as hard as possible (maximize loss), which results in a min-max problem.  This is in turn optimized (approximately) by alternating between gradient-based loss minimization and submodular maximization.  The theoretical analysis shows that if the loss is strongly convex, then the algorithm returns a solution which is close to the optimal solution.  Empirical results are presented for several benchmarks. \nThe paper is mostly clear and the idea seems nice.  On the downside, there are some limitations to the theoretical analysis and optimization scheme (see comments below). \n\nComments:\n- The theoretical result (thm. 1) studies the case of full optimization, which is different than the proposed algorithm (running a fixed number of weight updates).  It would be interesting to show results on sensitivity to the number of updates (p). \n- The algorithm requires tuning of quite a few hyperparameters (sec. 3). \n- Approximating a cluster with a single sample (sec. 2.3) seems rather crude.  There should be some theoretical and/or empirical study of its effect on quality of the solution. \n\nMinor/typos:\n- what is G(j|G\\j) in eq. (9)? \n- why cite Anonymous (2018) instead of Appendix...? \n- define V in Thm. 1.\n- in eq. (4) it may be clearer to denote g_k(w). Likewise in eq. (6) \\hat{g}_\\hat{A}(w), and in eq. (14) \\tilde{g}_{\\cal{A}}(w). \n- figures readability can be improved.",1,1,1,1,1,-1,1,1,1,-1
BywyFQlAW-R2,"This paper introduces MiniMax Curriculum learning, as an approach for adaptively train models by providing it different subsets of data.  The authors formulate the learning problem as a minimax problem which tries to choose diverse example and \""hard\"" examples, where the diversity is captured via a Submodular Loss function and the hardness is captured via the Loss function.  The authors formulate the problem as an iterative technique which involves solving a minimax objective at every iteration.  The authors argue the convergence results on the minimax objective subproblem, but do not seem to give results on the general problem.  The ideas for this paper are built on existing work in Curriculum learning, which attempts to provide the learner easy examples followed by harder examples later on.  The belief is that this learning style mimics human learners .\n\nPros:\n- The analysis of the minimax objective is novel and the proof technique introduces several interesting ideas. \n- This is a very interesting application of joint convex and submodular optimization, and uses properties of both to show the final convergence results. \n- Even through the submodular objective is only approximately solvable, it still translates into a convergence result. \n- The experimental results seem to be complete for the most part.  They argue how the submodular optimization does not really affect the performance and diversity seems to empirically bring improvement on the datasets tried. \n\nCons:\n- The main algorithm MCL is only a hueristic.  Though the MiniMax subproblem can converge, the authors use this in somewhat of a hueristic manner. \n- It seems somewhat hand wavy in the way the authors describe the hyper parameters of MCL, and it seems unclear when the algorithm converge and how to increase/decrease it over iterations. \n- The objective function also seems somewhat non-intuitive.  Though the experimental results seem to indicate that the idea works, I think the paper does not motivate the loss function and the algorithm well. \n- It seems to me the authors have experimented with smaller datasets (CIFAR, MNIST, 20NewsGroups).  This being mainly an empirical paper, I would have expected results on a few larger datasets (e.g. ImageNet, CelebFaces etc.), particularly to see if the idea also scales to these more real world larger datasets. \n\nOverall, I would like to see if the paper could have been stronger empirically.  Nevertheless, I do think there are some interesting ideas theoretically and algorithmically.  For this reason, I vote for a borderline accept",1,1,1,1,1,1,1,1,1,-1
BywyFQlAW-R3,"The main strength of this paper, I think, is the theoretical result in Theorem 1 . This result is quite nice.  I wish the authors actually concluded with the following minor improvement to the proof that actually strengthens the result further. \n\nThe authors ended the discussion on thm 1 on page 7 (just above Sec 2.3) by saying what is sufficiently close to w*.  If one goes back to (10), it is easy to see that what converges to w* when one of three things happen (assuming beta is fixed once loss L is selected). \n\n1) k goes to infinity\n2) alpha goes to 1\n3) g(w*) goes to 0 \n\nThe authors discussed how alpha is close to 1 by virtue of submodular optimization lower bounds there for what is close to w*.  In fact this proof shows the situation is much better than that.  \n\nIf we are really concerned about making what converge to w*, and if we are willing to tolerate the increasing computational complexity associated solving submodular problems with larger k, we can schedule k to increase over time which guarantees that both alpha goes to 1 and g(w*) goes to zero.  \n\nThere is also a remark that G(A) tends to be modular when lambda is small which is useful. \nFrom the algorithm, it seems clear that the authors recognized these two useful aspects of the objective and scheduled lambda to decrease exponentially and k to increase linearly. \n\nIt would be really nice to complete the analysis of Thm1 with a formal analysis of convergence speed for ||what-w*|| as lambda and k are scheduled in this fashion.  Such an analysis would help practitioners make better choices for the hyper parameters gamma and Delta.",1,1,1,1,1,-1,1,1,1,-1
ByxLBMZCb-R1,"Summary: The paper focuses on the characterization of the landscape of deep neural networks; i.e., when and why local minima are global, what are the conditions for saddle critical points, etc.  The paper covers a somewhat wide range of deep nets (from shallow with linear activation to deeper with non-linear activation); it focuces only on feed forward neural networks. \nAs the authors state, this paper provides a unifying perspective to the subject (it justifies the results of others through this unifying theory, but also provides new results; e.g., there are results that do not depend on assumptions on the target data matrix Y). \n\nOriginality: The paper provides similar results to previous work, while removing some of the assumptions made in previous work.  In that sense, the originality of the results is weak, but definitely there is some novelty in the methodology used to get to these results.  Thus, I would say original. \n\nImportance: The paper deals with the important problem of when and why training algorithms might get to global/local/saddle critical points.  While there are no direct connections with generalization properties, characterizing the landscape of neural networks is an important topic to make further steps into better understanding of deep learning. It will attract some attention at the conference.  \n\nClarity: The paper is well-written - some parts need improvement, but overall I'm satisfied with the current version. \n\nComments:\n1. If problem (4) is not considered at all in this paper (in its full generality that considers matrix completion and matrix sensing as special cases), then the authors could just start with the model in (5). \n\n2. Remark 1 has a nice example - could this example be shown with Y not being the all-zeros vector? \n\n3. In section 5, the authors make a connection with the work of Ge et al. 2016.  They state that the problems in (10)-(11) constitute generalizations of the symmetric matrix completion case, considered in Ge et al. 2016.  However, in that work, the main difficulty of proving global optimality comes from the randomness of the sampling mask operator (which introduces the notion of incoherence and requires results in expectation).  It is not clear, and maybe it is an overstatement, that the results in section 5 generalize that work.  If that is the case, could the authors describe this a bit further?",1,1,1,1,1,1,1,1,1,-1
ByxLBMZCb-R2,"Summary:\n\nThis paper studies the geometry of linear and neural networks and provides conditions under which the local minima of the loss are global minima for these non-convex problems.  The paper studies locally open maps, which preserve the local minima geometry.  Hence a local minima of l(F(W)) is a local minima of l(s) when s=F(W) is a locally open map.  Theorem 3 provides conditions under which the multiplication X*Y is a locally open map.  For a pyramidal feed forward net, if the weights in each layer have full rank,  input X is full rank, and the link function is invertible, then that local minima is a global minima.   \n\nComments:\n\nThe locally open maps (Behrends 2017) is an interesting concept.  However I am not convinced that the paper is able to show stronger results about the geometry of linear/neural networks.  Further the claims all over the paper, comparing with the existing works. are over the top and not justified.  I believe the paper needs a significant rewriting .\n\nThe results are not a strict improvement over existing works.  For neural networks, Nguyen and Hein (2017) assume the link function is differentiable.  This paper assumes the link function is invertible.  Both papers can handle sigmoid/tanh, but cannot handle ReLU. \n\nResults for linear networks are not an improvement over existing works.  Paper claims to remove assumption on Y, but they get much weaker results as they cannot differentiate between saddle points and global minima, for a critical point.   Results are also written in a confusing way as stating each critical point is a saddle or a global minima.  Instead the presentation can be simplified by just discussing the equivalency between local minima and global minima, as the proposed framework cannot handle critical points directly. \n\nProof of Lemma 7 seems to have typos/mistakes.  What is \\bar{W_i}? Why are the first two equations just showing d_i \\leq d_i ? How do you use this to conclude locally openness of \\mathcal{M}? \n\nAuthors claim their result extends the results for matrix completion from Ge et al. (2016) . This is false claim as (10) is not the matrix completion problem with missing entries, and the results in Ge et al. (2016) do not assume any non-degeneracy conditions on W.",1,1,1,1,1,1,1,1,1,-1
ByxLBMZCb-R3,"The paper studies the local optima of certain types of deep networks.  It uses the notion of a locally open map to draw equivalences between local optima and global optima.  The basic idea is that for fitting nonlinear models with a convex loss, if the mapping from the weights to the outputs is open, then every local optimum in weight space corresponds to a local optimum in output space; by convexity, in output space every local optimum is global.  \n\nThis is mostly a \u201ctheory building\u201d work.  With an appropriate fix, lemma 4 gives a cleaner set of assumptions than previous work in the same space (Nguyen + Hein \u201917), but yields essentially the same conclusions.  \n\nThe notion of local openness seems very well adapted to deriving these type of results in a clean manner.  The result in Section 3 on local openness of matrix multiplication on its range (which is substantially motivated by Behrends 2017) may be of independent interest.  I did not check the proof of this result in detail, but it appears to be correct.  For the linear, deep case, the paper corrects imprecisions in the previous work (Lu + Kawaguchi).  \n\nFor deep nonlinear networks, the results require the \u201cpyramidal\u201d assumption that the dimensionality is nonincreasing with respect to layer and (more restrictively) the feature dimension in the first layer is larger than the number of input points.  This seems to differ from typical practice, in the sense that it does not allow for wide intermediate layers.  This seems to be a limitation of the methodology: unless I'm missing something, this situation cannot be addressed using locally open maps.  \n\n\n\nThere are some imprecisions in the writing.  For example, Lemma 4 is not correct as written \u2014 an invertible mapping \\sigma is not necessarily locally open.  Take $\\sigma_k(t) = t for t rational and -t for t irrational$ as an example.  This is easy to fix, but not correct as written.  \n\nDespite mentioning matrix completion in the introduction and comparing to work of Ge et. al., the paper does not seem to have strong implications for matrix completion.  It extends results of Ge and collaborators for the fully observed symmetric case to non-symmetric problems.  But the main interest in matrix completion is in the undersampled case \u2014 in the full observed case, there is nothing to complete",1,1,1,1,1,1,1,1,1,-1
ByYPLJA6W-R1,"The paper considers distribution to distribution regression with MLPs.   The authors use an energy function based approach.   They test on a few problems, showing similar performance to other distribution to distribution alternatives, but requiring fewer parameters. \n\nThis seems to be a nice treatment of distribution to distribution regression with neural networks.  The approach is methodological similar to using expected likelihood kernels.   While similar performance is achieved with fewer parameters, it would be more enlightening to consider accuracy vs runtime instead of accuracy vs parameters.   That\u2019s what we really care about.  In a sense, because this problem has been considered several times in slightly different model classes, there really ought to be a pretty strong empirical investigation. In the discussion, it says.   \n\u201cFor future work, a possible study is to investigate what classes of problems DRN can solve. \u201d  It feels like in the present work there should have been an investigation about what classes of problems the DRN can solve.   Its practical utility is questionable.  It\u2019s not clear how much value there is adding yet another distribution to distribution regression approach, this time with neural networks, without some pretty strong motivation (which seems to be lacking), as well as experiments.   In the introduction, it would also improve the paper to outline clear points of methodological novelty",1,1,1,1,1,1,1,1,1,-1
ByYPLJA6W-R2,"Summary:\n\nThis paper presents a new network architecture for learning a regression of probability distributions. \n\nThe distribution output from a given node is defined in terms of a learned conditional probability function, and the output distributions of its input nodes.  The conditional probability function is an unnormalized distribution with the same form as the Boltzman distribution, and distributions are approximated from point estimates by discretizing the finite support into predefined equal-sized bins.  By letting the conditional distribution between nodes be unnormalized, and using an energy function that incorporates child nodes independently, the approach admits efficient computation that does not need to model the interaction between the distributions output by nodes at a given level. \n\nUnder these dynamics and discretization, the chain rule can be used to derive a matrix of gradients at each node that denotes the derivative of the discretized output distribution with respect to the current node's discretized distribution.  These gradients are in turn used to calculate updates for the network parameters with respect to the Jensen Shannon divergence between the predicted distribution and a target distribution. \n\nThe approach is evaluated on three tasks, two synthetic and one real world.  The baselines are the state of the art triple basis estimator (3BE) or a standard MLP that represents the output distribution using a softmax over quantiles.  On both of the synthetic tasks --- which involve predicting gaussians --- the proposed approach can fit the data reasonably using far fewer parameters than the baselines, although 3BE does achieve better overall performance.  On a real world task that involves predicting a distribution of future stock market prices from multiple input stock marked distributions, the proposed approach significantly outperforms both baselines.  However, this experiment uses 3BE outside of its intended use case --- which is for a single input distribution --- so it's not entirely clear how well the very simple proposed model is doing. \n\nNotes to authors:\n\nI'm not familiar with 3BE but the fact that it is used outside of its intended use case for the stock data is worrying.  How does 3BE perform at predicting the FTSE distribution at time t + k from the FTSE distribution at time t only?  Do the multiple input distributions actually help? \n\nYou use a kernel density estimate with a Gaussian kernel function to estimate the stock market pdf, but then you apply your network directly to this estimate. What would happen if you built more complex networks using the kernel values themselves as inputs? \n\nCould you also run experiments on the real-world datasets used by the 3BE paper? \n\nWhat is the structure of the DRN that uses > 10^3 parameters (from Fig. 4)?  The width of the network is bounded by the two input distributions, so is this network just incredibly deep?  Also, is it reasonable to assume that both the DRN and MLP are overfitting the toy task when they have access to an order of magnitude more parameters than datapoints. \n\nIt would be nice if section 2.4 was expanded to actually define the cost gradients for the network parameters, either in line or in an appendix",1,1,1,1,1,-1,1,1,1,-1
ByYPLJA6W-R3,"This is an intriguing paper on running regressions on probability distributions: i.e. a target distribution is expressed as a function of input distributions.  A well-written manuscript,;  though the introduction could have motivated the problem a little better (i.e. why would we want to do this).  The novelty in the paper is implementing such a regression in a layered network.  The paper shows how the densities at each nodes are computed (and normalised).  Optimisation by back propagation and discretization of the densities to carry out numerical integration are well explained and easy to follow.  The paper uses three problems to illustrate the idea -- a synthetic dataset, a mean reverting stochastic process and a prediction problem on stock indices.   \nMy only two reservations of this paper is the illustration on the stock index data -- it seems to me, returns on individual constituent stocks of an index are used as samples of the return on the index itself.   But this cannot be true when the index is a weighted sum of the constituent assets.   Secondly, it is not clear to me why one would force a kernel density estimate on the asset returns and then bin the density into 100 bins for numerical reasons;  -- does the smoothing that results from this give any advantage over a histogram of the returns in 100 bins",1,1,1,1,1,-1,1,1,1,-1
ByzvHagA--R1,"The authors propose a penalization term that enforces decorrelation between the dimensions of the representation  \nThey show that it can be included as additional term in cost functions to train generic models. \nThe idea is simple and it seems to work for the presented examples. \n\nHowever, they talk about gradient descent using this extra term, but I'd like to see the derivatives of the \nproposed term depending on the parameters of the model (and this depends on the model!).  On the other hand, \ngiven the expression of the proposed regulatization,\nit seems to lead to non-convex optimization problems which are hard to solve. Any comment on that? .\n\nMoreover, its results are not quantitatively compared to other Non-Linear generalizations of PCA/ICA designed for similar goals (e.g. those cited in the \""related work\"" section or others which have been proved to be consistent non-linear generalizations of PCA such as: Principal Polynomial Analysis, Dimensionality Reduction via Regression that follow the family introduced in the book of Jolliffe, Principal Component Analysis). \n\nMinor points: Fig.1 conveys not that much information",1,1,1,1,1,1,1,1,1,-1
ByzvHagA--R2,"\nI think the first intuition is interesting.  However I think the benefits are not clear enough.  Maybe finding better examples where the benefits of the proposed regularization are stressed could help.  \n\nThere is a huge amount of literature about ICA, unmixing, PCA, infomax... based on this principle that go beyond of the proposal.  I do not see a clear novelty in the proposal.   \n\nFor instance the proposed regularization can be achieved by just adding a linear combination at the layer which based on PCA.  As shown in [Szegedy et al 2014, \""Intriguing properties of neural networks\""] adding an extra linear transformation does not change the expressive power of the representation.     \n\n\n- \""Inspired by this, we consider a simpler objective: a representation disentangles the data well when its components do not correlate... \""\n\nThe first paragraph is confusing since jumps from total correlation to correlation without making clear the differences. \nAlthough correlation is a second oder approach to total correlation are not the same. This is extremely important since the whole proposal is based on that. \n\n- Sec 2.1. What prevents the regularization to enforce the weights in the linear layers to be very small and thus minimize the covariance.  I think the definition needs to enforce the out-diagonal terms in C to be small with respect to the terms in the diagonal.    \n\n- All the evaluation measures are based on linear relations, some of them should take into account non-linear relations (i.e. total correlation, mutual information...) in order to show that the method gets something interesting. \n\n- The first experiment (dim red) is not clear to me.  The original dimensionality of the data is 4, and only a linear relation is introduced. I do not understand the dimensionality reduction if the dimensionality of the transformed space is 10.  Also the data problem is extremely simple, and it is not clear the didactic benefit of using it.  I think a much more complicated data would be more interesting.  Besides L_1 is not well defined.  If it is L_1 norm on the output coefficients the comparison is misleading.  \n\n- Sec 3.3. As in general the model needs to be compared with other regularization techniques to stress its benefits .\n\n- Sec 3.4. Here the comparison makes clear that not a real benefit is obtained with the proposal.  The idea behind regularization is to help the model to avoid overfitting and thus improving the quality of the prediction in future samples.  However the MSE obtained when not using regularization is the same (or even smaller) than when using it",1,1,1,1,1,1,1,1,1,-1
ByzvHagA--R3,"This paper presents a regularization mechanism which penalizes covariance between all dimensions in the latent representation of a neural network.  This penalty is meant to disentangle the latent representation by removing shared covariance between each dimension.  \n\nWhile the proposed penalty is described as a novel contribution, there are multiple instances of previous work which use the same type of penalty (Cheung et. al. 2014, Cogswell et. al. 2016) . Like this work, Cheung et. al. 2014 propose the XCov penalty which penalizes cross-covariance to disentangle subsets of dimensions in the latent representation of autoencoder models.  Cogswell et. al. 2016 also proposes a similar penalty (DeCov) to this work for reducing overfitting in supervised learning. \n\nThe novel contribution of the regularizer proposed in this work is that it also penalizes the variance of individual dimensions along with the cross-covariance.  Intuitively, this should lead to dimensionality reduction as the model will discard variance in dimensions which are unnecessary for reconstruction.  But given the similarity to previous work, the authors need to quantitatively evaluate the value in additionally penalizing variance of each dimension as compared with earlier work.  Cogswell et. al. 2016 explicitly remove these terms from their regularizer to prevent the dynamic range of the activations from being unnecessarily rescaled.   It would be helpful to understand how this approach avoids this issues;  - i.e.,  if you penalize all the variance terms then you could just be arbitrarily rescaling the activities, so what prevents this trivial solution? \n\nThere doesn't appear to be a definition of the L1 penalty this paper compares against and it's unclear why this is a reasonable baseline.  The evaluation metrics this work uses (MAPC, CVR, TdV, UD) need to be justified more in the absence of their use in previous work.  While they evaluate their method on non-toy dataset such as CIFAR, they do not show what the actual utility of their proposed regularizer serves for such a dataset beyond having no-regularization at all.  Again, the utility of the evaluation metrics proposed in this work is unclear. \n\nThe toy examples are kind of interesting but it would be more compelling if the dimensionality reduction aspect extended to real datasets. \n\n> Our method has no penalty on the performance on tasks evaluated in the experiments, while it does disentangle the data \n\nThis needs to be expanded in the results as all the results presented appear to show Mean Squared Error increasing when increasing the weight of the regularization penalty",1,1,1,1,1,1,1,1,1,-1
H1-IBSgMz-R1,"The paper presents a proof of the self normalization of NCE as a result of being a low-rank matrix approximation of low-rank approximation of the normalized conditional probabilities matrix.   However, it seems that in equation 4, the authors assume that the noise distribution is a unigram model over words.  However, one is allowed to use any noise distribution in NCE, and convergence should be quicker with those distributions that are close to the true distribution.  Does the argument hold for general noise distributions ?  With this assumption, they can borrow easily from Goldberg and Levy, 2014 for the proof.  \nIn experiments, they find that while NCE does result in self-normalization, it is inversely correlated with perplexity which is a bit surprising.  The paper is interesting but lacks strong empirical results.  It could be stronger if they could exploit some of their findings to improve language modeling over a strong baseline.",1,1,1,1,1,-1,1,1,1,-1
H1-IBSgMz-R2,"This paper considers the problem of self-normalizing models. This kind\nof approaches, such as NCE (Noise Contrastive Estimation) is very\npromising and important to provide efficient and large vocabulary\nlanguage models. \n\nBy interpreting the NCE in terms of matrix factorization allows the\nauthors to better explain this learning criterion and more\nspecifically the self-normalizing mechanism. \n\nHowever, the first (theoritical) contribution is to make the link\nbetween matrix decomposition and sampling based objective.  This was\nalready shown for negative sampling in the paper of Melamud et al. in\nEMNLP 2017. Therefore, nothing new here, the difference is slight. \nMoreover, this paper is only cited in the experimental part, while the\ncontribution should be far more emphasized by the authors. \n\nThe second part makes the link with the self-normalization. This is\nnot really surprising. This was already explained in the same way in\npapers from Pihlaja,Gutmann and Hyvarinen published in 2010/12",1,1,1,1,-1,1,1,1,1,1
H1-IBSgMz-R3,It is hard to interpret this work as the authors do not mention the original work by Gutmann and his colleague on the NCE in the required details.  Their paper provides a proof that in the non-parametric case the optimum on NCE objective function is at the data distribution with normalisation constant either learned or held fixed (0 or any value you like).  What exactly is the purpose of this paper?  \n\nThere are a number of minor issues as well. In language modelling we do not compute normalisation term during NCE training or testing as explicitly stated by the authors you are referring to (Chen 2016) - that is the whole point of using NCE.  What is p(c) in equation 4 and where it comes from?,1,1,1,1,-1,1,1,-1,1,1
H1-oTz-Cb-R1,"The paper proposes an approach to learning a distribution over filters of a CNN.  The method is based on a adversarial training: the generator produces filters, and the discriminator aims to distinguish the activation maps produced by real filters from those produced by the generated ones.  \n\nPros:\n1) The general task of learning distributions over network weights is interesting \n2) To my knowledge, the proposed approach is new \n\nCons:\n1) Experimental evaluation is very substandard. The experiments on invariances seem to be the highlight of the paper, but they basically do not tell me anything.  \n - Figures 3 and 4 take 2 pages, but what should one see there? \n - There are no quantitative results.  Could there be a way to measure the invariances? \n - Can the results be applied to some practical task? Why are the results interesting and/or useful? \n2) The experiments are restricted to a single dataset - MNIST.  The authors mention that \u201cthe test accuracy obtained by following the above procedure is of 0.982, against a test accuracy of 0.971 for the real CNN\u201d - these are very poor accuracies for MNIST. So even the MNIST results do not seem convincing. \n3) Presentation is suboptimal, and many details are missing. For instance, architectures of networks are not provided. \n \nTo conclude, while the general direction is interesting and the proposed method might work, the experimental evaluation is very poor, and the paper absolutely cannot be accepted for publication.",1,1,1,1,-1,-1,1,1,1,-1
H1-oTz-Cb-R2,"Recent work on incorporating prior knowledge about invariances into neural networks suggests that the feature dimension in a stack of feature maps has some kind of group or manifold structure, similar to how the spatial axes form a plane.  This paper proposes a method to uncover this structure from the filters of a trained ConvNet . The method uses an InfoGAN to learn the distribution of filters. By varying the latent variables of the GAN, one can traverse the manifold of filters.  The effect of moving over the manifold can be visualized by optimizing an input image to produce the same activation profile when using a perturbed synthesized filter as when using an unperturbed synthesized filter. \n\nThe idea of empirically studying the manifold / topological / group structure in the space of filters is interesting.  A priori, using a GAN to model a relatively small number of filters seems problematic due to overfitting, but the authors show that their InfoGAN approach seems to work well. \n\nMy main concerns are:\n\nControls\nTo generate the visualizations, two coordinates in the latent space are varied, and for each variation, a figure is produced.  To figure out if the GAN is adding anything, it would be nice to see what would happen if you varied individual coordinates in the filter space (\""x-space\"" of the GAN), or varied the magnitude of filters or filter planes.  Since the visualizations are as much a function of the previous layers as they are a function of the filters in layer l which are modelled by the GAN, I would expect to see similar plots for these baselines. \n\nLack of new Insights\nThe visualizations produced in this paper are interesting to look at, but it is not clear what they tell us, other than \""something non-trivial is going on in these networks\"". In fact, it is not even clear that the transformations being visualized are indeed non-linear in pixel space (note that even a 2D diffeomorphism, which is a non-linear map on R^2, is a linear operator on the space of *functions* on R^2, i.e. on the space of images).  In any case, no attempt is made to analyze the results, or provide new insights into the computations performed by a trained ConvNet. \n\nInterpretation\nThis is a minor point, but I would not say (as the paper does) that the method captures the invariances learned by the model, but rather that it aims to show the variability captured by the model.  A ReLU net is only invariant to changes that are mapped to zero by the ReLU, or that end up in the kernel of one of the linear layers.  The presented method does not consider this and hence does not analyze invariances. \n\nMinor issues:\n- In the last equation on page 2, the right-hand side is missing a \""min max\"".",1,1,1,1,1,-1,1,1,1,-1
H1-oTz-Cb-R3,"This paper wants to probe the non-linear invariances learnt by CNNs. This is attempted by selecting a particular layer, and modelling the space of filters that result in activations that are indistinguishable from activations generated by the real filters (using a GAN).  For a GAN noise vector a plausible filter set is created, and for a data sample a set of plausible activations are computed.  If the noise vector is perturbed and a new plausible filter set is created, the input data can be optimised to find the input that produces the same set of activations.  The claim is that the found input represents the non-linear transformations that the layer is invariant to .\n\nThis is a really interesting perspective on probing invariances and should be explored more.  I am not convinced that this particular method is showing much information or highlighting anything particularly interesting, but could be refined in the future to do so. \n\nIt seems that the generated images are not actually plausible images at all and so not many conclusions can be drawn from this method.  Instead of performing the optimisation to find x' have you tried visualising the real data sample that gives the closest activations? \n\nI think you may want to consider minimising ||a(x'|z) - a(x|z_k)|| instead to show that moving from x -> x' is the same as is invariant under the transformation z -> z_k  (and thus the corresponding movement in filter space).  This (the space between x and x') I think is more interpretable as the invariance corresponding to the space between z and z_k. Have you tried that? \n\nThere is no notion of class invariance, so the GAN can find the space of filters that transform layer inputs into other classes, which may not be desirable. Have you tried conditioning the GAN on class? \n\nOverall I think this method is inventive and shows promise for probing invariances.  I'm not convinced the current incarnation is showing anything insightful or useful.  It also should be shown on more than a single dataset and for a single network, at the moment this is more of a workshop level paper in terms of breadth and depth of results.",1,1,1,1,1,-1,1,1,1,-1
H135uzZ0--R1,"This paper describes an implementation of reduced precision deep learning using a 16 bit integer representation.  This field has recently seen a lot of publications proposing various methods to reduce the precision of weights and activations.  These schemes have generally achieved close-to-SOTA accuracy for small networks on datasets such as MNIST and CIFAR-10.  However, for larger networks (ResNET, Vgg, etc) on large dataset such as ImageNET, a significant accuracy drop are reported.  In this work, the authors show that a careful implementation of mixed-precision dynamic fixed point computation can achieve SOTA on 4 large networks on the ImageNET-1K datasets.  Using a INT16 (as opposed to FP16) has the advantage of enabling the use of new SIMD mul-acc instructions such as QVNNI16.  \n\nThe reported accuracy numbers show convincingly that INT16 weights and activations can be used without loss of accuracy in large CNNs.  However, I was hoping to see a direct comparison between FP16 and INT16.   \n\nThe paper is written clearly and the English is fine.",1,1,1,1,-1,1,1,-1,1,-1
H135uzZ0--R2,"This paper is about low-precision training for ConvNets.  It proposed a \""dynamic fixed point\"" scheme that shares the exponent part for a tensor, and developed procedures to do NN computing with this format.  The proposed method is shown to achieve matching performance against their FP32 counter-parts with the same number of training iterations on several state-of-the-art ConvNets architectures on Imagenet-1K.  According to the paper, this is the first time such kind of performance are demonstrated for limited precision training. \n\nPotential improvements:\n\t\n  - Please define the terms like FPROP and WTGRAD at the first occurance. \n  - For reference, please include wallclock time and actual overall memory consumption comparisons of the proposed methods and other methods as well as the baseline (default FP32 training).",1,1,1,1,1,-1,1,1,1,-1
H135uzZ0--R3,"This work presents a CNN training setup that uses half precision implementation that can get 2X speedup for training.  The work is clearly presented and the evaluations seem convincing.  The presented implementations are competitive in terms of accuracy, when compared to the FP32 representation.   I'm not an expert in this area but the contribution seems relevant to me, and enough for being published.",1,1,-1,1,-1,-1,1,1,1,-1
H13WofbAb-R1,"This paper introduces a parameter server architecture to improve distributed training of CNNs in the presence of stragglers.  Specifically, the paper proposes partial pulling where a worker only waits for first b blocks rather than all the blocks of the parameters.  This technique is combined with existing methods such as partial pushing (Pan et. al. 2017) for a partial synchronous SGD method.  The method is evaluated with Resnet -50 using synthetic delays. \n\nComments for the author:\n\nThe paper is well-written and easy to follow.  The problem of synchronization costs being addressed is important but it is unclear how much of this is arising due to large blocks.\ n\n1) The partial pushing method (Pan et. al. 2017, section 3.1) shows a clear evidence for the problem using a real workload with a large number of workers.  Unfortunately, in your Figure 2, this is not as obvious and not real since it is using simulated delays.  More specifically, it is not clear how the workers behave in a real environment and whether you get a clear benefit from using a partial number of blocks as opposed to sending all of them. \n\n2) Did you modify your code to support block-wise sending of gradients (some description of how the framework was modified will be helpful)?  The idea is to send partial parameter blocks and when 'b' blocks are received, compute the gradients.  I feel that, with such a design, you may actually end up hurting the performance by sending a large number of small packets in the no failure case.  For real, large data centers, this may cause a packet storm and subsequent throughput collapse (e.g. the incast problem).  You need to show the evidence that you do not hurt the failure-free case for a large number of workers. \n\n3) The evaluation is on fairly small workloads (CIFAR-10).  Again, evaluating over Imagenet and demonstrating a clear speedup over existing sync methods will be helpful.  Furthermore, a clear description of your \u201cpull\u201d configuration (such as in Figure 1) i.e.  how many actual bytes or blocks are sent and what is the threshold will be helpful (beyond a vague 90%). \n\n4) Another concern with partial synchronization methods that I have is that how do you pick these configurations (pull 0.75 etc).  These appear to be dataset specific and finding the optimal configuration here requires significant experimentation that takes significantly more time than just running the baseline. \n\nOverall, I feel there is not enough evidence for the problem specifically generating large blocks of gradients and this needs to be clearly shown.  To propose a solution for stragglers, evaluation should be done in a datacenter environment with the presence of stragglers (and not small workloads with synthetic delays) . Furthermore, the proposed technique despite the simplicity appears as a rather incremental contribution.",1,1,1,1,1,1,1,1,1,-1
H13WofbAb-R2,"This paper considers distributed synchronous SGD, and proposes to use \""partial pulling\"" to alleviate the problem with slow servers. \n\nThe motivation is that the server may be a straggler.  The authors suggested one possibility, namely that the server and some workers are located on the same machine and the workers take most of the computational resource . However, if this is the case, a simple solution would be to move the server to a different node.  A more convincing argument for a slow server should be provided .\n\nThough the authors claimed that they used 3 techniques to accelerate synchronous SGD, only partial pulling is proposed by them (the other 2 are borrowed straightforwardly from existing papers).  The mechanism of partial pulling is very simple (just let SGD proceed after pulling a partial parameter block instead of the whole block).  As mentioned by the authors in section 1, any relaxation in synchrony brings more noise and higher variance to the updates, and also may cause slow convergence or convergence to a poor solution.  However, the authors provided no theoretical study on any of these aspects. \n\nExperimental results are not convincing.  Only one relatively small dataset (cifar10) is used Moreover, the slow server problem is only simulated by artificially adding delays to the server.",1,1,1,1,-1,1,1,1,1,-1
H13WofbAb-R3,"Paper proposes a weak synchronization approach to synchronous SGD with the goal of improving even with slow parameter servers.  This is an improvement on earlier proposals (e.g. Revisiting Synchronous SGD) that allow for slow workers.  Empirical results on ResNet50 on CIFAR show promising results for simulations with slow workers and servers, with the proposed approach. \n\nIssues with the paper:\n- Since the paper is focused on empirical results, having results only for ResNet50 on CIFAR is very limiting\n- Empirical results are based on simulations and not real workloads.  The choice of simulation constants (% delayed, and delay time) seems somewhat arbitrary as well. \n- For the simulated results, the comparisons seem unfair since the validation error is different . It will be useful to also provide time to a certain accuracy that all of them get to e.g. the validation error of 0.1609 (reached by the 3 important cases). \n\nOverall, the paper proposes an interesting improvement to this area of synchronous training, however it is unable to validate the impact of this proposal.",1,1,1,1,1,1,1,1,-1,-1
H15odZ-C--R1,"The paper concerns distributions used for the code space in implicit models, e.g. VAEs and GANs. The authors analyze the relation between the latent space dimension and the normal distribution which is commonly used for the latent distribution.  The well-known fact that probability mass concentrates in a shell of hyperspheres as the dimensionality grows is used to argue for the normal distribution being sub-optimal when interpolating between points in the latent space with straight lines. To correct this, the authors propose to use a Gamma-distribution for the norm of the latent space (and uniform angle distribution).  This results in more mass closer to the origin, and the authors show both that the midpoint distribution is natural in terms of the KL divergence to the data points, and experimentally that the method gives visually appealing interpolations.  \n\n While the contribution of using a standard family of distributions in a standard implicit model setup is limited, the paper does make interesting observations, analyses and an attempt to correct the interpolation issue.  The paper is clearly written and presents the theory and experimental results nicely.  I find that the paper can be accepted but the incremental nature of the contribution prevents a higher score.",1,1,1,1,-1,-1,1,1,1,-1
H15odZ-C--R2,"The authors discuss a direct Gamma sampling method for the interpolated samples in GANs, and show the improvements over usual normal sampling for CelebA, MNIST, CIFAR and SVHN datasets. \n\nThe method involves a nice, albeit minor, trick, where the chi-squared distribution of the sum of the z_{i}^{2} has its dependence on the dimensionality removed . However I am not convinced by the distribution of \\|z^\\prime\\|^{2} in the first place (eqn (2)): the samples from the gaussian will be approximately orthogonal in high dimensions, but the inner product will be at least O(1) . Thus although the \\|z_{0}\\|^{2} and \\|z_{1}\\|^{2} are chi-squared/gamma, I don't think \\|z^\\prime\\|^{2} is exactly gamma in general. \n\nThe experiments do show that the interpolated samples are qualitatively better, but a thorough empirical analysis for different dimensionalities would be welcome.  Figures 2 and 3 do not add anything to the story, since 2 is just a plot of gamma pdfs and 3 shows the difference between the constant KL and the normal case that is linear in d.  \n\nOverall I think the trick needs to be motivated better, and the experiments improved to really show the import of the d-independence of the KL.  Thus I think this paper is below the acceptance threshold.",1,1,1,1,1,-1,1,1,1,-1
H15odZ-C--R3,"The authors propose the use of a gamma prior as the distribution over \nthe latent representation space in GANs.  The motivation behind it is that \nin GANs interpolating between sampled points is common in the process of generating examples but the use of a normal prior results in samples that fall in low probability mass regions. The use of the proposed gamma distribution, as a simple alternative, overcomes this problem.  \n\nIn general, the proposed work is very interesting and the idea is neat.  \nThe paper is well presented and I want to underline the importance of this. \nThe authors did a very good job presenting the problem, motivation and solution in a coherent fashion and easy to follow.  \n\nThe work itself is interesting and can provide useful alternatives for the distribution over the latent space",1,1,-1,1,-1,-1,1,-1,1,-1
H18uzzWAZ-R1,"The paper discusses a method for adjusting image embeddings in order tease apart technical variation from biological signal.  A loss function based on the Wasserstein distance is used. \nThe paper is interesting  but could certainly do with more explanations.  \n\nComments:\n1. It is difficult for the reader to understand a) why Wasserstein is used "" and b) how exactly the nuisance variation is reduced."" \nA dedicated section on motivation is missing.` \n\n2. Does the Deep Metric network always return a '64-dim' vector?  \nHave you checked your model using different length vectors?  \n\n3. Label the y-axis in Fig 2. \n\n4. The fact that you have early-stopping as opposed to a principled regularizer also requires further substantiation.",1,1,1,1,1,-1,1,1,-1,-1
H18uzzWAZ-R2,"The authors present a method that aims to remove domain-specific information while preserving the relevant biological information between biological data measured in different experiments or \""batches\"".  A network is trained to learn the transformations that minimize the Wasserstein distance between distributions.  The wasserstein distance is also called the \""earth mover distance\"" and is traditionally formulated as the cost it takes for an optimal transport plan to move one distribution to another.  In this paper they have a neural network compute the wasserstein distance using a different formulation that was used in Arjovsky et al. 2017, finds a lipschitz function f, which shows the maximal difference when evaluated on samples from the two distributions.  Here these functions are formulated as affine transforms of the data with parameters theta that are computed by a neural network.  Results are examined mainly by looking at the first two PCA components of the data.   \n\n\nThe paper presents an interesting idea and is fairly well written.  However I have a few concerns:\n1. Most of the ideas presented in the paper rely on works by Arjovsky et al. (2017), Gulrajani et al. (2017), and Gulrajani et al. (2017).  Some selections, which are presented in the papers are not explained, for example, the gradient penalty, the choice of \\lambda and the choice of points for gradient computation. \n2. The experimental results are not fully convincing, they simply compare the first two PC components on this Broad Bioimage benchmark collection.  This section could be improved by demonstrating the approach on more datasets. \n3. There is a lack comparison to other methods such as Shaham et al. (2017).  Why is using earth mover distance better than MMD based distance?  They only compare it to a method named CORAL and to Typical\nVariation Normalization (TVN).   What about comparison to other batch normalization methods in biology such as SEURAT?   \n4. Why is the affine transform assumption valid in biology?   There can definitely be non-linear effects that are different between batches, such as ion detection efficiency differences.  \n5. Only early stopping seems to constrain their model to be near identity.  Doesn't this also prevent optimal results ?  How does this compare to the near-identity constraints in resnets in Shaham et al. ?\n\n",1,1,1,1,1,1,1,1,1,-1
H18uzzWAZ-R3,"This contribution deal with nuisance factors afflicting biological cell images with a domain adaptation approach: the embedding vectors generated from cell images show spurious correlation.  The authors define a Wasserstein Distance Network to find  a suitable affine transformation that reduces the nuisance factor.  The evaluation on a real dataset yields correct results, this approach is quite general and could be applied to different problems. \n\nThe contribution of this approach could be better highlighted.  The early stopping criteria tend to favor suboptimal solution, indeed relying on the Cramer distance is possible improvement. \n\nAs a side note, the k-NN MOA is central to for the evaluation of the proposed approach.  A possible improvement is to try other means for the embedding instead of the Euclidean one.\n\n",1,1,1,1,1,-1,1,1,1,-1
H196sainb-R1,"This paper presents a new method for obtaining a bilingual dictionary, without requiring any parallel data between the source and target languages.  The method consists of an adversarial approach for aligning two monolingual word embedding spaces, followed by a refinement step using frequent aligned words (according to the adversarial mapping).  The approach is evaluated on single word translation, cross-lingual word similarity, and sentence translation retrieval tasks.\ n\nThe paper presents an interesting approach which achieves good performance. The work is presented clearly, the approach is well-motivated and related to previous studies, and a thorough evaluation is performed.\ n\nMy one concern is that the supervised approach that the paper compares to is limited:  it is trained on a small fixed number of anchor points, while the unsupervised method uses significantly more words . I think the paper's comparisons are valid , but the abstract and introduction make very strong claims about outperforming \""state-of-the-art supervised approaches\"" . I think either a stronger supervised baseline should be included (trained on comparable data as the unsupervised approach), or the language/claims in the paper should be softened.  The same holds for statements like \""... our method is a first step ...\"", which is very hard to justify.  I also do not think it is necessary to over-sell, given the solid work in the paper. \n\nFurther comments, questions and suggestions:\n- It might be useful to add more details of your actual approach in the Abstract, not just what it achieves .\n- Given you use trained word embeddings, it is not a given that the monolingual word embedding spaces would be alignable in a linear way.  The actual word embedding method, therefore, has a big influence on performance (as you show).  Could you comment on how crucial it would be to train monolingual embedding spaces on similar domains /data with similar co-occurrence statistics, in order for your method to be appropriate? \n- Would it be possible to add weights to the terms in eq. (6), or is this done implicitly ?\n- How were the 5k source words for Procrustes supervised baseline selected ?\n- Have you considered non-linear mappings, or jointly training the monolingual word embeddings while attempting the linear mapping between embedding spaces ?\n- Do you think your approach would benefit from having a few parallel training points? \n\nSome minor grammatical mistakes/typos (nitpicking):\n- \""gives a good performance\"" ->  \""gives good performance\""\n- \""Recent works\"", \""several works\"", \""most works\"", etc.->  \""recent studies\"", \""several studies\"", etc.\n- \""i.e, the improvements\"" -> \""i.e., the improvements\""\n\ nThe paper is well-written, relevant and interesting . I therefore recommend that the paper be accepted.\n\n",1,1,1,1,1,1,1,1,1,-1
H196sainb-R2,"The paper proposes a method to learn bilingual dictionaries without parallel data using an adversarial technique.  The task is interesting and relevant, especially for in low-resource language pair settings.\ n\nThe paper, however, misses comparison against important work from the literature that is very relevant to their task \u2014 decipherment (Ravi, 2013; Nuhn et al., 2012; Ravi & Knight, 2011) and other approaches like CCA. \n\nThe former set of works, while focused on machine translation also learns a translation table in the process . Besides, the authors also claim that their approach is particularly suited for low-resource MT and list this as one of their contributions . Previous works have used non-parallel and comparable corpora to learn MT models and for bilingual lexicon induction.  The authors seem aware of corpora used in previous works (Tiedemann, 2012) yet provide no comparison against any of these methods.  While some of the bilingual lexicon extraction works are cited (Haghighi et al., 2008; Artetxe et al., 2017), they do not demonstrate how their approach performs against these baseline methods.  Such a comparison, even on language pairs which share some similarities (e.g., orthography), is warranted to determine the effectiveness of the proposed approach .\n\nThe proposed methodology is not novel, it rehashes existing adversarial techniques instead of other probabilistic models used in earlier works.  \n\nFor the translation task, it would be useful to see performance of a supervised MT baseline (many tools available in open-source) that was trained on similar amount of parallel training data (60k pairs) and see the gap in performance with the proposed approach. \n\nThe paper mentions that the approach is \u201cunsupervised\u201d.  However, it relies on bootstrapping from word embeddings learned on Wikipedia corpus, which is a comparable corpus even though individual sentences are not aligned across languages . How does the quality degrade if word embeddings had to be learned from scratch or initialized from a different source?",1,1,1,1,1,1,1,1,-1,1
H196sainb-R3,"An unsupervised approach is proposed to build bilingual dictionaries without parallel corpora, by aligning the monolingual word embeddings spaces, i.a. via adversarial learning. \n\nThe paper is very well-written and makes for a rather pleasant read, save for some need for down-toning the claims to novelty as voiced in the comment re: Ravi & Knight (2011) or simply in general:  it's a very nice paper, I enjoy reading it *in spite*, and not *because* of the text sales-pitching itself at times .\n\nThere are some gaps in the awareness of the related work in the sub-field of bilingual lexicon induction, e.g. the work by Vulic & Moens (2016).\ n\nThe evaluation is for the most part intrinsic, and it would be nice to see the approach applied downstream beyond the simplistic task of English-Esperanto translation: plenty of outlets out there for applying multilingual word embeddings.  Would be nice to see at least some instead of the plethora of intrinsic evaluations of limited general interest. \n\nIn my view, to conclude, this is still a very nice paper, so I vote clear accept, in hope to see these minor flaws filtered out in the revision.",1,1,1,1,1,1,1,1,1,-1
H1a37GWCZ-R1,"This paper presents simple but useful ideas for improving sentence embedding by drawing from more context.  The authors build on the skip thought model where a sentence is predicted conditioned on the previous sentence; they posit that one can obtain more information about a sentence from other \""governing\"" sentences in the document such as the title of the document, sentences based on HTML, sentences from table of contents, etc.  The way I understand it, previous sentence like in SkipThought provides more local and discourse context for a sentence whereas other governing sentences provide more semantic and global context. \n\nHere are the pros of this paper:\n1) Useful contribution in terms of using broader context for embedding a sentence. \n2) Novel and simple \""trick\"" for generating OOV words by mapping them to \""local\"" variables and generating those variables. \n3) Outperforms SkipThought in evals .\n\nCons:\n1) Coreference eval: No details are provided for how the data was annotated for the coreference task.  This is crucial to understanding the reliability of the evaluation as this is a new domain for coreference.  Also, the authors should make this dataset available for replicability.  Also, why have the authors not used this embedding for eval on standard coreference datasets like OntoNotes.  Please clarify.\n2) It is not clear to me how the model learns to generate specific OOV variables.  Can the authors clarify how does the decoder learns to generate these words. \n\nClarifications:\n1) In section 6.1, what is the performance of skip-thought with the same OOV trick as this paper? \n2) What is the exact heuristic in \""Text Styles\"" in section 3.1? Should be stated for replicability.",1,1,1,1,1,-1,1,1,1,1
H1a37GWCZ-R2,"1) This paper proposes a method for learning the sentence representations with sentences dependencies information.  It is more like a dependency-based version skip-thought on the sentence level.  The idea is interesting to me, but I think this paper still needs some improvements.  The introduction and related work part are clear with strong motivations to me.  But section 4 and 6 need a lot of details.  \n\n2) My comments are as follows:\ni) this paper claims that this is a general sentence embedding method, however, from what has been described in section 3, I think this dependency is only defined in HTML format document.  What if I only have pure text document without these HTML structure information?  So I suggest the authors do not claim that this method is a \""general-purpose\"" sentence embedding model. \n\nii) The authors do not have any descriptions for Figure 3.  Equation 1 is also very confusing. \n\niii) The experiments are insufficient in terms of details. How is the loss calculated? How is the detection accuracy calculated",1,1,1,1,1,1,1,1,-1,-1
H1a37GWCZ-R3,"This paper extends the idea of forming an unsupervised representation of sentences used in the SkipThought approach by using a broader set of evidence for forming the representation of a sentence.  Rather than simply encoding the preceding sentence and then generating the next sentence, the model suggests that a whole bunch of related \""sentences\"" could be encoded, including document title, section title, footnotes, hyperlinked sentences.This is a valid good idea and indeed improves results.  The other main new and potentially useful idea is a new idea for handling OOVs in this context where they are represented by positional placeholder variables. This also seems helpful.  The paper is able to show markedly better results on paraphrase detection that skipthought and some interesting and perhaps good results on domain-specific coreference resolution. \n\nOn the negative side, the model of the paper isn't very excitingly different. It's a fairly straightforward extension of the earlier SkipThought model to a situation where you have multiple generators of related text.  There isn't a clear evaluation that shows the utility of the added OOV Handler, since the results with and without that handling aren't comparable.  The OOV Handler is also related to positional encoding ideas that have been used in NMT but aren't reference.  And the coreference experiment isn't that clearly described nor necessarily that meaningful.  Finally, the finding of dependencies between sentences for the multiple generators is done in a rule-based fashion, which is okay and works, but not super neural and exciting. \n\nOther comments:\n - p.3. Another related sentence you could possibly use is first sentence of paragraph related to all other sentences? (Works if people write paragraphs with a \""topic sentence\"" at the beginning. \n - p.5. Notation seemed a bit non-standard. I thought most people use \\sigma for a sigmoid (makes sense, right?), whereas you use it for a softmax and use calligraphic S for a sigmoid. ...\n - p.5. Section 5 suggests the standard way to do OOVs is to average all word vectors. That's one well-know way, but hardly the only way. A trained UNK encoding and use of things like character-level encoders is also quite common. \n - p.6. The basic idea of the OOV encoder seems a good one. In domain specific contexts, you want to be able to refer to and re-use words that appear in related sentences, since they are likely to appear again and you want to be able to generate them.  A weakness of this section however is that it makes no reference to related work whatsoever. It seems like there's quite a bit of related work.  The idea of using a positional encoding so that you can generate rare words by position has previously been used in NMT, e.g. Luong et al. (Google brain) (ACL 2015).  More generally, a now quite common way to handle this problem is to use \""pointing\"" or \""copying\"", which appears in a number of papers. (e.g., Vinyals et al. 2015) and might also have been used here and might be expected to work too.  \n - p.7. Why such an old Wikipedia dump? Most people use a more recent one!\ n - p.7. The paraphrase results seem good and prove the idea works. It's a shame they don't let you see the usefulness of the OOV model. \n - p.8. For various reasons, the coreference results seem less useful than they could have been,  but they do show some value for the technique in the area of domain-specific coreference.\n\n",1,1,1,1,1,1,1,1,1,-1
H1A5ztj3b-R1,"The paper discusses a phenomenon where neural network training in very specific settings can profit much from a schedule including large learning rates.  Unfortunately, this paper feels to be hastily written and can only be read when accompanied with several references as key parts (CLR) are not described and thus the work can not be reproduced from the paper. \n\nThe main claim of the author hinges of the fact that in some learning problems the surface of the objective function can be very flat near the optimum.  In this setting, a typical schedule with a decreasing learning rate would be a bad choice as the change of curvature must be corrected as well.  However, this is not a general problem in neural network training and might not be generalizable to other datasets or architectures as the authors acknowledge. \n\nIn the end, the actual gain of this paper is only in the form of a hypothesis but there is only very little enlightenment, especially as the only slightly theoretical contribution in section 5 does not predict the observed behavior.  \n\nPersonally i would not use the term \""convergence\"" in this setting at all as the runs are very short and thus we might not be close to any region of convergence.  Most of the plots shown are actually not converged and convergence in test accuracy is not the same as convergence in training loss, which is not shown at all.  The results of smaller test error with larger learning rates on small training sets might therefore just be the inability of the optimizer to get closer to the optimum as steps are too long to decrease the expected loss, thus having a similar effect as early stopping. \n\nPros:\n- Many experiments which try to study the effect \nCons:\n-The described phenomenon seems to depend strongly on the problem surface and might never \nbe encountered on any problem aside of Cifar-10 \n- Only single runs are shown, considering the noise on those the results might not be reproducible. \n-Experiments are not described in detail\n-Experiment design feels \""ad-hoc\"" and unstructured\n-The role and value of the many LR-plots remains unclear to me. \n\nForm:\n- The paper does not maker clear how the exact schedules work.  The terms are introduced but the paper misses the most basic formulas\n- Figures are not properly described, e.g. axes in Figures 3 a) and b)\n- Explicit references to code are made which require familiarity with the used framework(if at all published)",1,1,1,1,-1,-1,1,1,1,1
H1A5ztj3b-R2,"In this paper, the authors analyze training of residual networks using large cyclic learning rates (CLR).  The authors demonstrate (a) fast convergence with cyclic learning rates and (b) evidence of large learning rates acting as regularization which improves performance on test sets \u2013 this is called \u201csuper-convergence\u201d.  However, both these effects are only shown on a specific dataset, architecture, learning algorithm and hyper parameter setting.  \n\n\nSome specific comments by sections:\n\n2. Related Work: This section loosely mentions other related works on SGD, topology of loss function and adaptive learning rates.  The authors mention Loshchilov & Hutter in next section but do not compare it to their work.  The authors do not discuss a somewhat contradictory claim from NIPS 2017 (as pointed out in the public comment): http://papers.nips.cc/paper/6770-train-longer-generalize-better-closing-the-generalization-gap-in-large-batch-training-of-neural-networks.pdf\n\n3.  Super-convergence: This is a well explained section where the authors describe the LR range test and how it can be used to understand potential for super-convergence for any architecture. The authors also provide sufficient intuition for super-convergence.  Since CLRs were already proposed by Smith (2015), the originality of this work would be specifically tied to their application to residual units.  It would be interesting to see a qualitative analysis on how the residual error is impacting super-convergence. \n\n4. Regularization: While Fig 4 demonstrates the regularization property, the reference to Fig 1a with better test error compared to typical training methods could simply be a result of slower convergence of typical training methods.  \n5. Optimal LRs: Fig.5b shows results for 1000 iterations whereas the text says 10000 (seems like a typo in scaling the plot).  Figs 1 and 5 illustrate only one cycle (one increase and one decrease) of CLR. It would be interesting to see cases where more than one cycle is required and to see what happens when the LR increases the second time. \n\n6. Experiments: This is a strong section where the authors show extensive reproducible experimentation to identify settings under which super-convergence works or does not work.  However, the fact that the results only applies to CIFAR-10 dataset and could not be observed for ImageNet or other architectures is disappointing and heavily takes away from the significance of this work.  \n\nOverall, the work is presented as a positive result in very specific conditions but it seems more like a negative result.  It would be more appealing if the paper is presented as a negative result and strengthened by additional experimentation and theoretical backing.",1,1,1,1,1,1,1,1,1,1
H1A5ztj3b-R3,"This paper discusses the phenomenon of a fast convergence rate for training resnet with cyclical learning rates under a few particular setting.  It tries to provide an explanation for the phenomenon and a procedure to test when it happens.  However, I don't find the paper of high significance or the proposed method solid for publication at ICLR. \n\nThe paper is based on the cyclical learning rates proposed by Smith (2015, 2017). I don't understand what is offered beyond the original papers.  The \""super-convergence\"" occurs under special settings of hyper-parameters for resnet only and therefore I am concerned if it is of general interest for deep learning models.  Also, the authors do not give a conclusive analysis under what condition it may happen. \n\nThe explanation of the cause of \""super-convergence\"" from the perspective of  transversing the loss function topology in section 3 is rather illustrative at the best without convincing support of arguments.  I feel most content of this paper (section 3, 4, 5) is observational results, and there is lack of solid analysis or discussion behind these observations.",1,1,1,1,-1,1,1,1,1,-1
H1aIuk-RW-R1,"After reading rebuttals from the authors: The authors have addressed all of my concerns.  THe additional experiments are a good addition. \n\n************************\nThe authors provide an algorithm-agnostic active learning algorithm for multi-class classification.  The core technique is to construct a coreset of points whose labels inform the labels of other points.   The coreset construction requires one to construct a set of  points which can cover the entire dataset.  While this is NP-hard problem in general, the greedy algorithm is 2-approximate . The authors use a variant of the greedy algorithm along with bisection search to solve a series of feasibility problems to obtain a good cover of the dataset each time.  This cover tells us which points are to be queried.  The reason why choosing the cover is a good idea is because under suitable Lipschitz continuity assumption the generalization error can be controlled via an appropriate value of the covering radius in the data space.   The authors use the coreset construction with a CNN to demonstrate an active learning algorithm for multi-class classification.  \nThe experimental results are convincing enough to show that it outperforms other active learning algorithms.  However, I have a few major and minor comments.\n\nMajor comments:\n\n1. The proof of Lemma 1 is incomplete.  We need the Lipschitz constant of the loss function. The loss function is a function of the CNN function and the true label.  The proof of lemma 1 only establishes the Lipschitz constant of the CNN function.  Some more extra work is needed to derive the lipschitz constant of the loss function from the CNN function.  \n\n2. The statement of Prop 1 seems a bit confusing to me. the hypothsis says that the loss on the coreset = 0. But the equation in proposition 1 also includes the loss on the coreset.  Why is this term included. Is this term not equal to 0?\ n\n3. Some important works are missing.  Especially works related to pool based active learning, and landmark results on labell complexity of agnostic active learning .\nUPAL: Unbiased Pool based active learning by Ganti & Gray. http://proceedings.mlr.press/v22/ganti12/ganti12.pdf\nEfficient active learning of half-spaces by Gonen et al. http://www.jmlr.org/papers/volume14/gonen13a/gonen13a.pdf\nA bound on the label complexity of agnostic active learning. http://www.machinelearning.org/proceedings/icml2007/papers/375.pdf\n\n4.   The authors use L_2 loss as their objective function. This is a bit of a weird choice given that they are dealing with multi-class classification and the output layer is a sigmoid layer, making it a natural fit to work with something like a cross-entropy loss function.  I guess the theoretical results do not extend to cross-entropy loss, but the authors do not mention these points anywhere in the paper.  For example, the ladder network, which is one of the networks used by the authors is a network that uses cross-entropy for training. \n\nMinor-comment: \n1. The feasibility program in (6) is an MILP. However, the way it is written it does not look like an MILP.  It would have been great had the authors mentioned that u_j \\in {0,1}.  \n\n2. The authors write on page 4, \""Moreover, zero training error can be enforced by converting average loss into maximal loss\"". It is not clear to me what the authors mean here.  For example, can I replace the average error in proposition 1, by maximal loss? Why can I do that?  Why would that result in zero training error? \n\nOn the whole this is interesting work and the results are very nice . But, the proof for Lemma 1 seems incomplete to me, and some choices (such as choice of loss function) are unjustified.  Also, important references in active learning literature are missing.",1,1,1,1,1,1,1,1,1,-1
H1aIuk-RW-R2,"Active learning for deep learning is an interesting topic and there is few useful tool available in the literature. It is happy to see such paper in the field.  This paper proposes a batch mode active learning algorithm for CNN as a core-set problem.  The authors provide an upper bound of the core-set loss, which is the gap between the training loss on the whole set and the core-set.  By minimizing this upper bound, the problem becomes a K-center problem which can be solved by using a greedy approximation method, 2-OPT.  The experiments are performed on image classification problem (CIFAR, CALTECH, SVHN datasets), under either supervised setting or weakly-supervised setting.  Results show that the proposed method outperforms the random sampling and uncertainty sampling by a large margin.  Moreover, the authors show that 2-OPT can save tractable amount of time in practice with a small accuracy drop. \n\nThe proposed algorithm is new and writing is clear.  However, the paper is not flawless.  The proposed active learning framework is under ERM and cover-set, which are currently not supported by deep learning.  To validate such theoretical result, a non-deep-learning model should be adopted.  The ERM for active learning has been investigated in the literature, such as \""Querying discriminative and representative samples for batch mode active learning\"" in KDD 2013, which also provided an upper bound loss of the batch mode active learning and seems applicable for the problem in this paper.  Another interesting question is most of the competing algorithm is myoptic active learning algorithms. The comparison is not fair enough.  The authors should provide more competing algorithms in batch mode active learning.",1,1,1,1,1,1,1,1,1,-1
H1aIuk-RW-R3,"This paper studies active learning for convolutional neural networks . Authors formulate the active learning problem as core-set selection and present a novel strategy. \n\nExperiments are performed on three datasets to validate the effectiveness of the proposed method comparing with some baselines. \n\nTheoretical analysis is presented to show the performance of any selected subset using the geometry of the data points. \n\nAuthors are suggested to perform experiments on more datasets to make the results more convincing. \n\nThe initialization of the CNN model is not clearly introduced, which however, may affect the performance significantly",1,1,1,1,1,-1,1,1,1,-1
H1BHbmWCZ-R1,"The paper is motivated with building robots that learn in an open-ended way, which is really interesting.  What it actually investigates is the performance of existing image classifiers and object detectors.  I could not find any technical contribution or something sufficiently mature and interesting for presenting in ICLR. \n\nSome issues:\n- submission is supposed to be double blind but authors reveal their identity at the start of section 2.1. \n- implementation details all over the place (section 3. is called \""Implementation\"", but at that point no concrete idea has been proposed, so it seems too early for talking about tensorflow and keras)",1,1,-1,1,-1,-1,1,1,1,-1
H1BHbmWCZ-R2,"This work explores some approaches in the object detection field of computer vision: (a) a soft attention map based on the activations on convolutional layers, (b) a classification regarding the location of an object in a 3x3 grid over the image, (c) an autoencoder that the authors claim to be aware of the multiple object instances in the image.  These three proposals are presented in a framework of a robot vision module, although neither the experiments nor the dataset correspond to this domain. \n\nFrom my perspective, the work is very immature and seems away from current state of the art on object detection, both in the vocabulary, performance or challenges.  The proposed techniques are assessed in a dataset which is not described and whose results are not compared with any other technique.  This important flaw in the evaluation prevents any fair comparison with the state of the art. \n\nThe text is also difficult to follow. The three contributions seem disconnected and could have been presented in separate works with a more deeper discussion.  In particular, I have serious problems understanding:\n\n1. What is exactly the contribution of the CNN pre-trained with IMageNet when learning the soft-attention maps ?  The reference to a GAN architecture seems very forced and out of the scope. \n\n2. What is the interest of the localization network ? The task it addresses seems very simple and in any case it requires a manual annotation of a dataset of objects in each of the predefined locations in the 3x3 grid. \n\n3. The authors talk about an autoencoder architecture, but also on a classification network where the labels correspond to the object count. I could not undertstand what is exactly assessed in this section. \n\nFinally, the authors violate the double-bind review policy by clearly referring to their previous work on Experiental Robot Learning. \n\nI would encourage the authors to focus in one of the research lines they point in the paper and go deeper into it, with a clear understanding of the state of the art and the specific challenges these state of the art techniques may encounter in the case of robotic vision.",1,1,1,1,1,1,1,1,1,-1
H1BHbmWCZ-R3,"The authors disclosed their identity and violated the terms of double blind reviews.\nPage 2 \""In our previous work (Aly & Dugan, 2017)\n\nAlso the page 1 is full of typos and hard to read.",1,-1,-1,1,1,-1,-1,1,-1,1
H1bhRHeA--R1,"The paper presents interesting algorithms for minimizing softmax with many classes.  The objective function is a multi-class classification problem (using softmax loss) and with linear model.  The main idea is to rewrite the obj as double-sum using the dual formulation and then apply SGD to solve it.  At each iteration, SGD samples a subset of training samples and labels.  The main contribution of this paper is: 1) proposing a U-max trick to improve the numerical stability and 2) proposing an implicit SGD approach.  It seems the implicit SGD approach is better in the experimental comparisons.  \n\nI found the paper quite interesting, but meanwhile I have the following comments and questions:  \n\n- As pointed out by the authors, the idea of this formulation and doubly SGD is not new.  (Raman et al, 2016) has used a similar trick to derive the double-sum formulation and solved it by doubly SGD.  The authors claim that  the algorithm in (Raman et al) has an O(NKD) cost for updating u at the end of each epoch.  However, since each epoch requires at least O(NKD) time anyway (sometimes larger, as in Proposition 2), is another O(NKD) a significant bottleneck?  Also, since the formulation is similar to (Raman et al., 2016), a comparison is needed.  \n\n- I'm confused by Proposition 1 and 2. In appendix E.1, the formulation of the update is derived, but why we need Newton to get log(1/epsilon) time complexity?  I think most first order methods instead of Newton will have linear converge (log(1/epsilon) time)?  Also, I guess we are assuming the obj is strongly convex? \n\n- The step size is selected in one dataset and used for all others. This might lead to divergence of other algorithms, since usually step size depends on data.  As we can see, OVE, NCE and IS diverges on Wiki-small, which may be fixed if the step size is chosen for each data (in practice we can choose using subsamples for each data).  \n\n- All the comparisons are based on \""epochs\"", but the competing algorithms are quite different and can have very different running time for each epoch.  For example, implicit SGD has another iterative solver for each update. Therefore, the timing comparison is needed in this paper to justify that implicit SGD is faster.  \n\n- The claim that \""implicit SGD never overshoots the optimum\"" needs more supports. Is it proved in some previous papers?  \n\n- The presentation can be improved.  I think it will be helpful to state the algorithms explicitly in the main paper.",1,1,1,1,1,1,1,1,1,-1
H1bhRHeA--R2,"The problem of numerical instability in applying SGD to soft-max minimization is the motivation.  It would have been helpful if the author(s) could have made a formal statement.  \nSince the main contributions are two algorithms for stable SGD it is not clear how one can formally say that they are stable. For this a formal problem statement is necessary.  The discussion around eq (7) is helpful but is intuitive and it is difficult to get a formal problem which we can use later to examine the proposed algorithms. \n\nThe proposed algorithms are variants of SGD but it is not clear why they should converge faster than existing strategies. \nSome parts of the text are badly written, see for example the following line(see paragraph before Sec 3)\n\n\""Since the converge of SGD is\ninversely proportional to the magnitude of its gradients (Lacoste-Julien et al., 2012), we expect the\nformulation to converge faster.\""\n \nwhich could have shed more light on the matter.  \n\nThe title is also misleading in using the word \""exact\"" . I have understand it correct the proposed SGD method solves the optimization problem to an additive error.\ n\nIn summary the algorithms are novel variants of SGD  but the associated claims of numerical stability and speed of convergence vis-a-vis existing methods are missing.  The choice of word exact is also not clear",1,1,1,1,1,-1,1,1,1,-1
H1bhRHeA--R3,"The paper develops an interesting approach for solving multi-class classification with softmax loss. \n\nThe key idea is to reformulate the problem as a convex minimization of a \""double-sum\"" structure via a simple conjugation trick.   SGD is applied to the reformulation: in each step samples a subset of the training samples and labels, which appear both in the double sum.   The main contributions of this paper are: \""U-max\"" idea (for numerical stability reasons) and an \""\""proposing an \""implicit SGD\"" idea.\n\nUnlike the first review, I see what the term \""exact\"" in the title is supposed to mean. I believe this was explained in the paper.  I agree with the second reviewer that the approach is interesting.  However, I also agree with the criticism (double sum formulations exist in the literature; comments about experiments); and will not repeat it here.  I will stress though that the statement about Newton in the paper is not justified. Newton method does not converge globally with linear rate. Cubic regularisation is needed for global convergence. Local rate is quadratic.   \n\nI believe the paper could warrant acceptance if all criticism raised by reviewer 2 is addressed. \n\nI apologise for short and late review: I got access to the paper only after the original review deadline.",1,1,1,1,1,-1,1,1,1,-1
H1BLjgZCb-R1,"Quality: Although the research problem is an interesting direction  the quality of the work is not of a high standard.  My main conservation is that the idea of perturbation in semantic latent space has not been described in an explicit way.  How different it will be compared to a perturbation in an input space?  \n\nClarity: The use of the term \""adversarial\"" is not quite clear in the context as in many of those example classification problems the perturbation completely changes the class label (e.g. from \""church\"" to \""tower\"" or vice-versa) \n\nOriginality: The generation of adversarial examples in black-box classifiers has been looked in GAN literature as well and gradient based perturbations are studied too . What is the main benefit of the proposed mechanism compared to the existing ones? \n\nSignificance: The research problem is indeed a significant one as it is very important to understand the robustness of the modern machine learning methods by exposing them to adversarial scenarios where they might fail. \n\npros:\n(a) An interesting problem to evaluate the robustness of black-box classifier systems \n(b) generating adversarial examples for image classification as well as text analysis. \n(c) exploiting the recent developments in GAN literature to build the framework forge generating adversarial examples. \n\ncons:\n(a) The proposed search algorithm in the semantic latent space could be computationally intensive.  any remedy for this problem? \n(b) Searching in the latent space z could be strongly dependent on the matching inverter $I_\\gamma(.)$.  any comment on this? \n(c) The application of the search algorithm in case of imbalanced classes could be something that require further investigation",1,1,1,1,1,1,1,1,1,-1
H1BLjgZCb-R2,"\nSummary:\n A method for creation of semantical adversary examples in suggested.  The \u2018semantic\u2019 property is measured by building a latent space with mapping from this space to the observable (generator) and back (inverter).  The generator is trained with a WGAN optimization.  Semantic adversarials examples are them searched for by inverting an example to its sematic encoding and running local search around it in that space.  The method is tested for generation of images on MNist and part of LSUM data and for creation of text examples which are adversarial in some sense to inference and translation sentences.  It is shown that the distance between adversarial example and the original example in the latent space is proportional to the accuracy of the classifier inspected. \nPage 3: It seems that the search algorithm has a additional parameter: r_0, the size of the area in which search is initiated.  This should be explicitly said and the parameter value should be stated.\nPage 4: \n-\tthe implementation details of the generator, critic and invertor networks are not given in enough details, and instead the reader is referred to other papers.  This makes this paper non-clear as a stand alone document, and is a problem for a paper which is mostly based on experiments and their results: the main networks used are not described. \n-\tthe visual examples are interesting, but it seems that they are able to find good natural adversary examples only for a weak classifier.  In the MNist case, the examples for thr random forest are nautral and surprising, but those for the LE-Net are often not: they often look as if they indeed belong to the other class (the one pointed by the classifier).  In the churce-vs. tower case, a  relatively weak MLP classifier was used.  It would be more instructive to see the results for a better, convolutional classifier. \nPage 5:\n-\tthe description of the various networks used for text generation is insufficient for understanding:\no\tThe AREA is described in two sentences.  It is not clear how this module is built, was loss was it used to optimize in the first place, and what elements of it are re0used for the current task\no\t \u2018inverter\u2019 here is used in a sense which is different than in previous sections of the paper: earlier it denoted the mapping from output (images) to the underlying latent space. Here it denote  a mapping between two latent spaces. \no\t It is not clear what the \u2018four-layers strided CNN\u2019 is: its structure, its role in the system. How is it optimized? \no\tIn general: a block diagram showing the relation between all the system\u2019s components may be useful, plus the details about the structure and optimization of the various modules.  It seems that the system here contains 5 modules instead of the three used before (critic, generator and inverter), but this is not clear enough.  Also which modules are pre-trained, which are optimized together,a nd which are optimized separately is not clear.\no\tSNLI data should be described: content, size, the task it is used for \n\n\nPro:\n-\tA novel idea of producing natural adversary examples with a GAN \n-\tThe generated examples are in some cases useful for interpretation and network understandin g \n-\tThe method enables creation of adversarial examples for block box classifiers \nCons\n-\tThe idea implementation is basic . Specifically search algorithm presented is quite simplistic, and no variations other than plain local search were developed and tested \n-\tThe generated adversarial examples created for successful complex classifiers are often not impressive and useful (they are either not semantical, or semantical but correctly classified by the classifier).  Hence It is not clear if the latent space used by the method enables finding of interesting adversarial examples for accurate classifiers.",1,1,1,1,1,-1,1,1,1,-1
H1BLjgZCb-R3,"The authors of the paper propose a framework to generate natural adversarial examples by searching adversaries in a latent space of dense and continuous data representation (instead of in the original input data space) . The details of their proposed method are covered in Algorithm 1 on Page 12, where an additional GAN (generative adversarial network) I_{\\gamma}, which can be regarded as the inverse function of the original GAN G_{\\theta}, is trained to learn a map from the original input data space to the latent z-space.  The authors empirically evaluate their method in both image and text domains and claim that the corresponding generated adversaries are natural (legible, grammatical, and semantically similar to the input). \n\nGenerally, I think that the paper is written well (except some issues listed at the end).  The intuition of the proposed approach is clearly explained and it seems very reasonable to me.   \nMy main concern, however, is in the current sampling-based search algorithm in the latent z-space, which the authors have already admitted in the paper. The efficiency of such a search method decreases very fast when the dimensions of the z-space increases.  Furthermore, such an approximation solution based on the sampling may be not close to the original optimal solution z* in Equation (3).  This makes me feel that there is large room to further advance the paper.  Another concern is that the authors have not provided sufficient number of examples to show the advantages of their proposed method over the other method (such as FGSM) in generating the adversaries.  The example in Table 1 is very good; but more examples (especially involving the quantitative comparison) are needed to demonstrate the claimed advantages.  For example, could the authors add such a comparison in Human Evaluation in Section 4 to support the claim that the adversaries generated by their method are more natural?  \n\nOther issues are listed as follows:\n(1). Could you explicitly specify the dimension of the latent z-space in each example in image and text domain in Section 3? \n(2). In Tables 7 and 8, the human beings agree with the LeNet in >= 58% of cases. Could you still say that your generated \u201cadversaries\u201d leading to the wrong decision from LeNet?  Are these really \u201cadversaries\u201d? \n(3). How do you choose the parameter \\lambda in Equation (2)?\n",1,1,1,1,1,-1,1,1,1,-1
H1bM1fZCW-R1,"The paper proposes a method to train deep multi-task networks using gradient normalization.  The key idea is to enforce the gradients from multi tasks balanced so that no tasks are ignored in the training.  The authors also demonstrated that the technique can improve test errors over single task learning and uncertainty weighting on a large real-world dataset. \n\nIt is an interesting paper with a novel approach to multi-task learning . To improve the paper, it would be helpful to evaluate the method under various settings.  My detailed comments are below.\n\n1. Multi-task learning can have various settings. For example, we may have multiple groups of tasks, where tasks are correlated within groups but tasks in different groups are not much correlated.  Also, tasks may have hierarchical correlation structures.  These patterns often appear in biological datasets.  I am wondering how a variety of multi-task settings can be handled by the proposed approach.  It would be helpful to discuss the conditions where we can benefit from the proposed method. \n\n2. One intuitive approach to task balancing would be to weight each task objective based on the variance of each task.   It would be helpful to add a few simple and intuitive baselines in the experiments.  \n\n3. In Section 4, it would be great to have more in-depth simulations (e.g., multi-task learning in various settings).  Also, in the bottom right panel in Figure 2, GrandNorm and equal weighting decrease test errors effectively even after 15000 steps but uncertainty weighting seems to reach a plateau. Discussions on this would be useful. \n\n4. It would be useful to discuss the implementation of the method as well.",1,1,1,1,1,-1,1,1,1,-1
H1bM1fZCW-R2,"Paper summary:\nExisting works on multi-task neural networks typically use hand-tuned weights for weighing losses across different tasks.  This work proposes a dynamic weight update scheme that updates weights for different task losses during training time by making use of the loss ratios of different tasks.  Experiments on two different network indicate that the proposed scheme is better than using hand-tuned weights for multi-task neural networks. \n\n\nPaper Strengths:\n- The proposed technique seems simple yet effective for multi-task learning. \n- Experiments on two different network architectures showcasing the generality of the proposed method. \n\n\nMajor Weaknesses:\n- The main weakness of this work is the unclear exposition of the proposed technique.  Entire technique is explained in a short section-3.1 with many important details missing.  There is no clear basis for the main equations 1 and 2.  How does equation-2 follow from equation-1? Where is the expectation coming from? What exactly does \u2018F\u2019 refer to? There is dependency of \u2018F\u2019 on only one of sides in equations 1 and 2? More importantly, how does the gradient normalization relate to loss weight update?  It is very difficult to decipher these details from the short descriptions given in the paper. \n- Also, several details are missing in toy experiments . What is the task here? What are input and output distributions and what is the relation between input and output? Are they just random noises? If so, is the network learning to overfit to the data as there is no relationship between input and output?  \n\n\nMinor Weaknesses:\n- There are no training time comparisons between the proposed technique and the standard fixed loss learning. \n- Authors claim that they operate directly on the gradients inside the network.  But, as far as I understood, the authors only update loss weights in this paper.  Did authors also experiment with gradient normalization in the intermediate CNN layers? \n- No comparison with state-of-the-art techniques on the experimented tasks and datasets. \n\n\nClarifications:\n- See the above mentioned issues with the exposition of the technique. \n- In the experiments, why are the input images downsampled to 320x320? \n- What does it mean by \u2018unofficial dataset\u2019 (page-4) . Any references here ?\n- Why is 'task normalized' test-time loss as good measure for comparison between models in the toy example (Section 4)?  The loss ratios depend on initial loss, which is not important for the final performance of the system. \n\n\nSuggestions:\n- I strongly suggest the authors to clearly explain the proposed technique to get this into a publishable state.  \n- The term \u2019GradNorm\u2019 seem to be not defined anywhere in the paper. \n\n\nReview Summary:\nDespite promising results, the proposed technique is quite unclear from the paper.  With its poor exposition of the technique, it is difficult to recommend this paper for publication.",1,1,1,1,1,-1,1,1,1,-1
H1bM1fZCW-R3,"The paper addresses an important problem in multitask learning.  But its current form has several serious issues.  \n\nAlthough I get the high-level goal of the paper, I find Sec. 3.1, which describes the technical approach, nearly incomprehensible.  There are many things unclear. For example:\n\n-  it starts with talking about multiple tasks, and then immediately talks about a \""filter F\"", without defining what the kind of network is being addressed.  \n\n- Also it is not clear what L_grad is. It looks like a loss, but Equation 2 seems to define it to be the difference between the gradient norm of a task and the average over all tasks.  It is not clear how it is used. In particular, it is not clear how it is used to \""update the task weights\ ""\n\n- Equation 2 seems sloppy. \u201cj\u201d appears as a free index on the right side, but it doesn\u2019t appear on the left side.  \n\nAs a result, I am unable to understand how the method works exactly, and unable to judge its quality and originality. \n\nThe toy experiment is not convincing.  \n\n- the evaluation metric is the sum of the relative losses, that is, the sum of the original losses weighted by the inverse of the initial loss of each task.  This is different from the sum of the original losses, which seems to be the one used to train the \u201cequal weight\u201d baseline.  A more fair baseline is to directly use the evaluation metric as the training loss. \n- the curves seem to have not converged. \n\nThe experiments on NYUv2 involves non-standard settings, without a good justification.  So it is not clear if the proposed method can make a real difference on state of the art systems.  \n\nAnd the reason that the proposed method outperforms the equal weight baseline seems to be that the method prevents overfitting on some tasks (e.g. depth) . However, the method works by normalizing the norms of the gradients, which does not necessarily prevent overfitting \u2014 it can in fact magnify gradients of certain tasks and cause over-training and over-fitting.  So the performance gain is likely dataset dependent, and what happens on NYU depth can be a fluke and does not necessarily generalize to other datasets.",1,1,1,1,-1,-1,1,1,-1,1
H1BO9M-0Z-R1,"This paper presents a lifelong learning method for learning word embeddings.   Given a new domain of interest, the method leverages previously seen domains in order to hopefully generate better embeddings compared to ones computed over just the new domain, or standard pre-trained embeddings. \n\nThe general problem space here -- how to leverage embeddings across several domains in order to improve performance in a given domain -- is important and relevant to ICLR.   However, this submission needs to be improved in terms of clarity and its experiments. \n\nIn terms of clarity, the paper has a large number of typos (I list a few at the end of this review) and more significantly, at several points in the paper is hard to tell what exactly was done and why.   When presenting algorithms, starting with an English description of the high-level goal and steps of the algorithm would be helpful.   What are the inputs and outputs of the meta-learner, and how will it be used to obtain embeddings for the new domain?   The paper states the purpose of the meta learning is \""to learn a general word context similarity from the first m domains\"", but I was never sure what this meant.   Further, some of the paper's pseudocode includes unexplained steps like \""invert by domain index\"" and \""scanco-occurrence\"".   \n\nIn terms of the experiments, the paper is missing some important baselines that would help us understand how well the approach works.   First, besides the GloVe common crawl embeddings used here, there are several other embedding sets (including the other GloVe embeddings released along with the ones used here, and the Google News word2vec embeddings) that should be considered.   Also, the paper never considers concatenations of large pre-trained embedding sets with each other and/or with the new domain corpus -- such concatenations often give a big boost to accuracy,;  see :\n\""Think Globally, Embed Locally\u2014Locally Linear Meta-embedding of Words\"", Bollegala et al., 2017\nhttps://arxiv.org/pdf/1709.06671.pdf\n\nThat paper is not peer reviewed to my knowledge so it is not necessary to compare against the new methods introduced there, but their baselines of concatenation of pre-trained embedding sets should be compared against in the submission. \n\nBeyond trying other embeddings, the paper should also compare against simpler combination approaches, including simpler variants of its own approach.   What if we just selected the one past domain that was most similar to the new domain, by some measure?   And how does the performance of the technique depend on the setting of m?   Investigating some of these questions would help us understand how well the approach works and in which settings. \n\nMinor:\n\nSecond paragraph, GloVec should be GloVe\n\n\""given many domains with uncertain noise for the new domain\"" -- not clear what \""uncertain noise\"" means, perhaps \""uncertain relevance\"" would be more clear. \n\nThe text refers to a Figure 3 which does not exist, probably means Figure 2.   I didn't understand the need for both figures, Figure 1 is almost contained within Figure 2 \n\nWhen m is introduced, it would help to say that m < n and justify why dividing the n domains into two chunks (of m and n-m domains) is necessary. \n\n\""from the first m domain corpus\"" -> \""from the first m domains\""? \n\n\""may not helpful\"" -> \""may not be helpful\""\n\n\""vocabularie\"" -> \""vocabulary\""\n\n\""system first retrieval\"" -> \""system first retrieves\"". \n\nCOMMENTS ON REVISIONS: I appreciate the authors including the new experiments against concatenation baselines.   The concatenation does fairly comparably to LL in Tables 3&4.   LL wins by a bit more in Table 2.   Given these somewhat close/inconsistent wins, it would help the paper to include an explanation of why and under what conditions the LL approach will outperform concatenation",1,1,1,1,1,1,1,1,1,-1
H1BO9M-0Z-R2,"Summary:\nThis paper proposes an approach to learn embeddings in new domains by leveraging the embeddings from other domains in an incremental fashion . The proposed approach will be useful when the new domain does not have enough data available.  The baselines chosen are 1). no embeddings 2). generic embeddings from english wiki, common crawl and combining data from previous and new domains.  Empirical performance is shown on 3 downstream tasks: Product-type classification, Sentiment Classification and Aspect Extraction.  The proposed embeddings just barely beat the baseline on product classification and sentiment classification, but significantly beat them on aspect extraction task. \n\n\nComments:\n\nThe paper puts itself nicely in context of the previous work and the addressed problem of learning word embeddings for new domain in the absence of enough data is an important one that needs to be addressed.  There is reasonable novelty in the proposed method compared to the existing literature.  But, I was a little disappointed by the paper as several details of the model were unclear to me and the paper's writing could definitely be improved to make things clearer.  \n\n1). In the \""Meta-learner\"" section 4.1, the authors talk about word features (u{_w_{i,j,k}},u{_w_{i,j',k}}).  It is unclear what these word features are. Are they one-hot encodings or embeddings or something else?  It would really help if the paper gave some expository examples. \n\n2). In Algorithm 1, how do you deal with vocabulary items in the new domain that do not exist in the previous domains i.e. when the intersection of V_i and V_{n+1} is the null set.  This is very important because the main appeal of this work is its applicability to new domains with scarce data which have far fewer words and hence the above scenario is more likely to happen. \n\n3). The results in Table 3 are a little confusing.  Why do the lifelong word embeddings relatively perform far worse on precision but significantly better on recall compared to the baselines?  What is driving those difference in results? \n\n4). Typos: In Section 3, \""...is depicted in Figure 1 and Figure 3\"". I think you mean \""Figure 1 and Figure 2\"" as there is no Figure 3",1,1,1,1,1,1,1,1,1,-1
H1BO9M-0Z-R3,"In this paper, the authors proposed to learn word embedding for the target domain in the lifelong learning manner.  The basic idea is to learn a so-call meter learner to measure similarities of the same words between the target domain and the source domains for help learning word embedding for the target domain with a small corpus.  \n\nOverall, the descriptions of the proposed model (Section 3 - Section 5) are hard to follow.  This is not because the proposed model is technically difficult to understand.  On the contrary, the model is heuristic, and simple, but the descriptions are unclear.  Section 3 is supposed to give an overview and high-level introduction of the whole model using the Figure 1, and Figure 2 (not Figure 3 mentioned in text).  However, after reading Section 3, I do not catch any useful information about the proposed model expect for knowing that a so-called meta learner is used.  Section 4 and  Section 5 are supposed to give details of different components of the proposed model and explain the motivations.  However, descriptions in these two sections are very confusing, e.g, many symbols in Algorithm 1 are presented with any descriptions.  Moreover, the motivations behind the proposed methods for different components are missing.   Also, a lot of types make the descriptions more difficult to follow, e.g., \""may not helpful or even harmful\"", '\""Figure 3\"", \""we show this Section 6\"", \""large size a vocabulary\"", etc. \n\nAnother major concern is that the technical contributions of the proposed model is quite limited.  The only technical contributions are (4) and the way to construct the co-occurrence information A.  However, such contributions are quite minor, and technically heuristic.  Moreover, regarding the aggregation layer in the pairwise network, it is similar to feature engineering.  In this case, why not just train a flat classifier, like logistic regression, with rich feature engineering, in stead of using a neural network. \n\nRegarding experiments, one straight-forward baseline is missing.  As n domains are supposed to be given in advance before the n+1 domain (target domain) comes, one can use multi-domain learning approaches with ensemble learning techniques to learn word embedding for the target domain.  For instance, one can learn n pairwise (1 out of n sources + the target) cross-domain word embedding, and combine them using the similarity between each source and the target as the weight",1,1,1,1,1,-1,1,1,1,-1
H1cKvl-Rb-R1,"This paper paper uses an ensemble of networks to represent the uncertainty in deep reinforcement learning. \nThe algorithm then chooses optimistically over the distribution induced by the ensemble. \nThis leads to improved learning / exploration, notably better than the similar approach bootstrapped DQN. \n\nThere are several things to like about this paper:\n- It is a clear paper, with a simple message and experiments that back up the claims. \n- The proposed algorithm is simple and could be practical in a lot of settings and even non-DQN variants. \n- It is interesting that Bootstrapped DQN gets such poor performance, this suggests that it is very important in the original paper https://arxiv.org/abs/1602.04621 that \""ensemble voting\"" is applied to the test evaluation... (why do you think this is by the way, do you think it has something to do with the data being *more* off-policy / diverse under a TS vs UCB scheme?) \n\nOn the other hand:\n- The novelty/scope of this work is somewhat limited... this is more likely (valuable) incremental work than a game-changer. \n- Something feels wrong/hacky/incomplete about just doing \""ensemble\"" for uncertainty without bootstrapping/randomization... if we had access to more powerful optimization techniques then this certainly wouldn't be sensible - I think that you should mention that you are heavily reliant on \""random initialization + SGD/Adam + specific network architecture\"" to maintain this idea of uncertainty.  For example, this wouldn't work for linear value functions!\n- I think the original bootstrapped DQN used \""ensemble voting\"" at test time, so maybe you should change the labels or the way this is introduced/discussed.  It's definitely very interesting that *essentially* the learning benefit is coming from ensembling (rather than \""raw\"" bootstrapped DQN) and UCB still looks like it does better. \n- I'm not convinced that page 4 and the \""Bayesian\"" derivation really add too much value to this paper... alternatively, maybe you could introduce the actual algorithm first (train K models in parallel) and then say \""this is similar to particle filter\"" and add the mathematical derivation after, rather than as if it was some complex formula derived.  If you want to reference some justification/theory for ensemble-based uncertainty approximation you might consider https://arxiv.org/pdf/1705.07347.pdf instead. \n- I think this paper might miss the point of the \""bigger\"" problem of efficient exploration in RL... or even how to get \""deep\"" exploration with deep RL.  Yes this algorithm sees improvements across Atari, but it's not clear why/if this is a step change versus simply increasing the amount of replay or tuning the learning rate.  (Actually I do believe this algorithm can demonstrate deep exploration... but it looks like we're not seeing the big improvements on the \""sub-human\"" games you might hope.) \n\nOverall I do think this is a pretty good short paper/evaluation of UCB-ensembles on Atari. \nThe scope/insight of the paper isn't groundbreaking, but I think it delivers a clear short message on the Atari benchmark. \nPerhaps this will encourage people to dig deeper into some of these issues... I vote accept.\n",1,1,1,1,1,1,1,1,1,-1
H1cKvl-Rb-R2,"The authors propose a new exploration algorithm for Deep RL . They maintain an ensemble of Q-values (based on different initialisations) to model uncertainty over Q. The ensemble is then used to derive a confidence interval at each step, which is used to select actions UCB-style. \n\nThere is some attempt at a Bayesian interpretation for the Bellman update. But to me it feels a bit like shoehorning the probabilistic interpretation into an already existing update - I\u2019m not sure this is justified and necessary here. Moreover, the UCB strategy is generally not considered a Bayesian strategy, so I wasn\u2019t convinced by the link to Bayesian RL in this paper.\ n\nI liked the actual proposed method otherwise, and the experimental results on Atari seem good (but see also latest SOTA Atari results, for example the Rainbow paper).  Some questions about the results:\n-How does it perform compared to epsilon-greedy added on top of Alg1, or is there evidence that this produces any meaningful exploration versus noise?  \n-How does the distribution of Q values look like during different phases of learning? \n-Was epsilon-greedy used in addition to UCB exploration? Question for both Alg 1 and Alg 2. \n-What\u2019s different between Alg 1 and bootstrapped DQN (other than the action selection)? \n\nMinor things:\n-Missing propto in Eq 7 ?\n-Maybe mention that the leftarrows are not hard updates. Maybe you already do somewhere\u2026\n-it looks more a Bellman residual update as written in (11).\n",1,1,1,1,1,1,1,1,1,-1
H1cKvl-Rb-R3,"This paper introduces a number of different techniques for improving exploration in deep Q learning.  The main technique is to use UCB (upper confidence bound) to speedup exploration.  The authors also introduces \""Ensemble voting\"" facilitate exploitation .\n\nThis paper shows improvement over baselines.  But does not seem to offer significant insight or dramatic improvement.  The techniques introduced are a small permutation of previous results.  The baselines are not particularly strong either. \n\nThe paper appeared to have be rushed. The presentation is not always clear. \n\nI also have the following questions I hope the authors could help me with:\n\n1. I failed to understand how Eqn (5). Could you please clarify. \n\n2. What is the significance of the math introduced in section 3? All that was proposed was: (1) Majority voting, (2) UCB exploration. \n\n3. Why comparing to A3C+ which is not necessarily better than A3C in final performance? \n\n4. Why not comparing to Bootstrapped DQN since the proposed method is based on it? \n\n5. Why is the proposed method better than Bootstrapped DQN, since UCB does not necessarily outperform Thompson sampling in the case of bandits? \n\n6. If there is a section on INFOGAIN exploration, why not mention it in the main text?",1,1,1,1,1,1,1,1,1,-1
H1cWzoxA--R1,"Pros: \nThe paper proposes a \u201cbi-directional block self-attention network (Bi-BloSAN)\u201d for sequence encoding, which inherits the advantages of multi-head (Vaswani et al., 2017) and DiSAN (Shen et al., 2017) network but is claimed to be more memory-efficient.  The paper is written clearly and is easy to follow.  The source code is released for duplicability. The main originality is using block (or hierarchical) structures; i.e., the proposed models split the an entire sequence into blocks, apply an intra-block SAN to each block for modeling local context, and then apply an inter-block SAN to the output for all blocks to capture long-range dependency.  The proposed model was tested on nine benchmarks  and achieve good efficiency-memory trade-off.  \n\nCons:\n- Methodology of the paper is very incremental compared with previous models.  \n- Many of the baselines listed in the paper are not competitive; e.g.,  for SNLI, state-of-the-art results are not included in the paper.  \n- The paper argues advantages of the proposed models over CNN by assuming the latter only captures local dependency, which, however, is not supported by discussion on or comparison with hierarchical CNN. \n- The block splitting (as detailed in appendix) is rather arbitrary in terms of that it potentially divides coherent language segments apart.  This is unnatural, e.g., compared  with alternatives such as using linguistic segments as blocks. \n- The main originality of paper is the block style. However, the paper doesn\u2019t analyze how and why the block brings improvement.  \n-If we remove intra-block self-attention (but only keep token-level self-attention), whether the performance will be significantly worse?\n",1,1,1,1,1,1,1,1,1,-1
H1cWzoxA--R2,"This high-quality paper tackles the quadratic dependency of memory on sequence length in attention-based models, and presents strong empirical results across multiple evaluation tasks.  The approach is basically to apply self-attention at two levels, such that each level only has a small, fixed number of items, thereby limiting the memory requirement while having negligible impact on speed.  It captures local information into so-called blocks using self-attention, and then applies a second level of self-attention over the blocks themselves. \n\nThe paper is well organized and clearly written, modulo minor language mistakes that should be easy to fix with further proof-reading.  The contextualization of the method relative to CNNs/RNNs/Transformers is good, and the beneficial trade-offs between memory, runtime and accuracy are thoroughly investigated, and they're compelling. \n\nI am curious how the story would look if one tried to push beyond two levels...?  For example, how effective might a further inter-sentence attention level be for obtaining representations for long documents?  \n\nMinor points:\n- Text between Eq 4 & 5: W^{(1)} appears twice; one instance should probably be W^{(2)} .\n- Multiple locations, e.g. S4.1: for NLI, the word is *premise*, not *promise*.\n- Missing word in first sentence of S4.1: ... reason __ the ...",1,1,1,1,1,1,1,1,1,-1
H1cWzoxA--R3,"This paper introduces bi-directional block self-attention model (Bi-BioSAN) as a general-purpose encoder for sequence modeling tasks in NLP.  The experiments include tasks like natural language inference, reading comprehension (SquAD), semantic relatedness and sentence classifications.  The new model shows decent performance when comparing with Bi-LSTM, CNN and other baselines while running at a reasonably fast speed. \n\nThe advantage of this model is that we can use little memory (as in RNNs) and enjoy the parallelizable computation as in (SANs), and achieve similar (or better) performance. \n\nWhile I do appreciate the solid experiment section, I don't think the model itself is sufficient contribution for a publication at ICLR.  First, there is not much innovation in the model architecture.  The idea of the Bi-BioSAN model simply to split the sentence into blocks and compute self-attention for each of them, and then using the same mechanisms as a pooling operation followed by a fusion level.  I think this more counts as careful engineering of the SAN model rather than a main innovation.  Second, the model introduces much more parameters. In the experiments, it can easily use 2 times parameters than the commonly used encoders.  What if we use the same amount of parameters for Bi-LSTM encoders? Will the gap between the new model and the commonly used ones be smaller? \n\n====\n\nI appreciate the answers the authors added and I change the score to 6.",1,1,1,1,1,-1,1,1,1,-1
H1DGha1CZ-R1,"The key argument authors present against ReLU+BN is the fact that using ReLU after BN skews the values resulting in non-normalized activations.  Although the BN paper suggests using BN before non-linearity many articles have been using BN after non-linearity which then gives normalized activations (https://github.com/ducha-aiki/caffenet-benchmark/blob/master/batchnorm.md) and also better overall performance.  The approach of using BN after non-linearity is termed \""standardization layer\"" (https://arxiv.org/pdf/1301.4083.pdf).  I encourage the authors to validate their claims against simple approach of using BN after non-linearity.",1,1,1,1,1,-1,1,1,-1,-1
H1DGha1CZ-R2,"This paper proposes an activation function, called displaced ReLU (DReLU), to improve the performance of CNNs that use batch normalization.  Compared to ReLU, DReLU cut the identity function at a negative value rather than the zero.  As a result, the activations outputted by DReLU can have a mean closer to 0 and a variance closer to 1 than the standard ReLU.  The DReLU is supposed to remedy the problem of covariate shift better.  \n\nThe presentation of the paper is clear.  The proposed method shows encouraging results in a controlled setting (i.e., all other units, like dropout, are removed).  Statistical tests are performed for many of the experimental results, which is solid. \n\nHowever, I have some concerns. \n1) As DReLU(x) = max{-\\delta, x}, what is the optimal strategy to determine \\delta?  If it is done by hyperparameter tuning with cross-validation, the training cost may be too high. \n2) I believe the control experiments are encouraging,  but I do not agree that other techniques like Dropouts are not useful.  Using DReLU to improve the state-of-art neural network in an uncontrolled setting is important.  The arguments for skipping this experiments are respectful,  but not convincing enough.   \n3) Batch normalization is popular, especially for the convolutional neural networks.  However, its application is not universal, which can limit the use of the proposed DReLU. It is a minor concern, anyway",1,1,1,1,1,-1,1,1,1,-1
H1DGha1CZ-R3,"This paper describes DReLU, a shift version of ReLU.DReLU shifts ReLU from (0, 0) to (-\\sigma, -\\sigma). The author runs a few CIFAR-10/100 experiments with DReLU. \n\nComments:\n\n1. Using expectation to explain why DReLU works well is not sufficient and convincing.  Although DReLU\u2019s expectation is smaller than expectation of ReLU, but it doesn\u2019t explain why DReLU is better than very leaky ReLU, ELU etc. \n2. CIFAR-10/100 is a saturated dataset and it is not convincing DReLU will perform will on complex task, such as ImageNet, object detection, etc. \n3. In all experiments, ELU/LReLU are worse than ReLU, which is suspicious.  I personally have tried ELU/LReLU/RReLU on Inception V3 with Batch Norm, and all are better than ReLU.  \n\nOverall, I don\u2019t think this paper meet ICLR\u2019s novelty standard, although the authors present some good numbers, but they are not convincing.",1,1,1,1,-1,-1,1,1,1,-1
H1dh6Ax0Z-R1,"# Update after the rebuttal\nThank you for the rebuttal. \nThe authors claim that the source of objective mismatch comes from n-step Q-learning, and their method is well-justified in 1-step Q-learning . However, there is still a mismatch even with 1-step Q-learning because the bootstrapped target is also computed from the TreeQN.  More specifically, there can be a mismatch between the optimal action sequences computed from TreeQN at time t and t+1 if the depth of TreeQN is equal or greater than 2.  Thus, the author's response is still not convincing to me. \nI like the overall idea of using a tree-structured neural network which internally performs planning as an abstraction of Q-function, which makes implementation simpler compared to VPN.  However, the particular method (TreeQN) proposed in this paper introduces a mismatch in the model learning as mentioned above.  One could argue that TreeQN is learning an \""abstract\"" planning rather than \""grounded\"" planning.  However, the fact that reward prediction loss is used to train TreeQN significantly weakens this claim, and there is no such an evidence in the paper.  \nIn conclusion, I think the research direction is worth pursuing, but the proposed modification from VPN is not well-justifie d.\n\n# Summary\nThis paper proposes TreeQN and ATreeC which perform look-ahead planning using neural networks.  TreeQN simulates the future by predicting rewards/values of the future states and performs tree backup to construct Q-values.  ATreeC is an actor-critic architecture that uses a softmax over TreeQN.  The architecture is trained through n-step Q-learning with reward prediction loss.  The proposed methods outperform DQN baseline on 2D Box Pushing domain and outperforms VPN on Atari games. \n\n[Pros]\n- The paper is easy to follow. \n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea was proposed by [O'Donoghue et al., Schulman et al.]. \n\n[Cons]\n- The proposed method has a technical issue. \n- The proposed idea (TreeQN) and underlying motivation are almost same as those of VPN [Oh et al.], but there is no in-depth discussion that shows why TreeQN is potentially better than VPN.  \n- Comparison to VPN on Atari is not much convincing.  \n\n# Novelty and Significance\n- The underlying motivation (planning without predicting observations), the architecture (transition/reward/value functions applied to the latent state space), and the algorithm (n-step Q-learning with reward prediction loss) are same as those of VPN.  But, the paper does not provide in-depth discussion on this.  The following is the differences that I found from this paper, so it would be important to discuss why such differences are important. \n\n1) The paper emphasizes the \""fully-differentiable tree planning\"" aspect in contrast to VPN that back-propagates only through \""non-branching\"" trajectories during training.  However, differentiating TreeQN also amounts to back-propagating through a \""single\"" trajectory in the tree that gives the maximum Q-value.  Thus, the only difference between TreeQN and VPN is that TreeQN follows the best (estimated) action sequence, while VPN follows the chosen action sequence in retrospect during back-propagation.  Can you justify why following the best estimated action sequence is better than following the chosen action sequence during back-propagation (see Technical Soundness section for discussion) ?\n\n2) TreeQN only sets targets for the final Q-value after tree backup, whereas VPN sets targets for all intermediate value predictions in the tree.  Why is TreeQN's approach better than VPN's approach?  \n\n- The application to actor-critic setting (ATreeC) is novel, though the underlying idea of combining Q-learning with policy gradient was proposed by [O'Donoghue et al.] and [Schulman et al.]. \n\n# Technical Soundness\n- The proposed idea of setting targets for the final Q-value after tree backup can potentially make the temporal credit assignment difficult, because the best estimated actions during tree planning does not necessarily match with the chosen actions.  Suppose that TreeQN estimated \""up-right-right\"" as the best future action sequence the during 3-step tree planning, while the agent actually ended up with choosing \""up-left-left\"" (this is possible because the agent re-plans at every step and follows epsilon-greedy policy).  Following n-step Q-learning procedure, we end up with setting target Q-value based on on-policy action sequence \""up-left-left\"", while back-propagating through \""up-right-right\"" action sequence in the TreeQN's plan.  This causes a wrong temporal credit assignment, because TreeQN can potentially increase/decrease value estimates in the wrong direction due to the mismatch between the planned actions and chosen actions.  So, it is unclear why the proposed algorithm is technically correct or better than VPN's approach (i.e., back-propagating through the chosen actions in the search tree). \n \n# Quality\n- Comparison to VPN on Atari is not convincing because TreeQN-1 is actually (almost) equivalent to VPN-1, but the results show that TreeQN-1 performs much better than VPN on many games.  Since the authors took the numbers from [Oh et al.] rather than replicating VPN, it is possible that the gap comes from implementation details (e.g., hyperparameter).  \n\n# Clarity\n- The paper is overall easy to follow and the description of the proposed method is clear",1,1,1,1,1,1,1,1,1,-1
H1dh6Ax0Z-R2,"The authors propose a new network architecture for RL that contains some relevant inductive biases about planning.  This fits into the recent line of work on implicit planning where forms of models are learned to be useful for a prediction/planning task.  The proposed architecture performs something analogous to a full-width tree search using an abstract model (learned end-to-end).  This is done by expanding all possible transitions to a fixed depth before performing a max backup on all expanded nodes.  The final backup value is the Q-value prediction for a given state, or can represent a policy through a softmax. \n\nI thought the paper was clear and well-motivated.  The architecture (and various associated tricks like state vector normalization) are well-described for reproducibility.  \n\nExperimental results seem promising but I wasn\u2019t fully convinced of its conclusions.  In both domains, TreeQN and AtreeC are compared to a DQN architecture, but it wasn\u2019t clear to me that this is the right baseline.  Indeed TreeQN and AtreeC share the same conv stack in the encoder (I think?), but also have the extra capacity of the tree on top.  Can the performance gain we see in the Push task as a function of tree depth be explained by the added network capacity?  Same comment in Atari, but there it\u2019s not really obvious that the proposed architecture is helping.  Baselines could include unsharing the weights in the tree, removing the max backup, having a regular MLP with similar capacity, etc. \n\nPage 5, the auxiliary loss on reward prediction seems appropriate, but it\u2019s not clear from the text and experiments whether it actually was necessary.  Is it that makes interpretability of the model easier (like we see in Fig 5c)? Or does it actually lead to better performance?   \n\nDespite some shortcomings in the result section, I believe this is good work and worth communicating as is.",1,1,1,1,1,-1,1,1,1,1
H1dh6Ax0Z-R3,"\n This was an interesting read.   \n\nI feel that there is a mismatch between intuition of what a model could do (based on the structure of the architecture) versus what a model does.  Just because the transition function is shared and the model could learn to construct a tree, when trained end-to-end the system is not sufficiently constrained to learn this specific behaviour.  More to a point. I think the search tree perspective is interesting,  but isn\u2019t this just a deeper model with shared weights? And a max operation?  It seems no loss is used to force the embeddings produced by the transition model to match the embeddings that you would get if you take a particular action in a particular state, right?  Is there any specific attempt to visualize or understand the embeddings inside the tree?  The same regarding the rewards. If there is no auxiliary loss attempting to force the intermediary prediction to be valid rewards, why would the model use those free latent variables to encode rewards?  I think this is a pitfall that many deep network papers fall, where by laying out a particular structure it is directly inferred that the model discovers or follows a particular solution (where the latent have prescribed semantics). I would argue that is rarely the case.  When the system is learned end-to-end, the structure does not impose the behaviour of the model, and is up to the authors of the paper to prove that the trained model does anything similar to expanding a tree. And this is not by showing final performance on a game.  If indeed the model does anything similar to search, than all intermediary representations should correspond to what semantically they should.  \nIgnoring my verbose comment, another view is that the baseline are disadvantaged to the treeQN, because they have less parameters (and are less deep which has a huge impact on the learnability and expressivity of the deep network)",1,1,1,1,-1,-1,1,-1,-1,-1
H1DkN7ZCZ-R1,"In this paper the author propose a CNN based solution for somatic mutation calling at ultra low allele frequencies. \nThe tackled problem is a hard task in computational biology, and the proposed solution Kittyhawk, although designed with very standard ingredients (several layers of CNN inspired to the VGG structure), seems to be very effective on both the shown datasets. \nThe paper is well written (up to a few misprints), the introduction and the biological background very accurate (although a bit technical for the broader audience) and the bibliography reasonably complete.  Maybe the manuscript part with the definition of the accuracy measures may be skipped.  Moreover, the authors themselves suggest how to proceed along this line of research with further improvements. \nI would only suggest to expand the experimental section with further (real) examples to strengthen the claim. \nOverall, I rate this manuscript in the top 50% of the accepted papers.",1,1,1,1,1,-1,1,1,1,-1
H1DkN7ZCZ-R2,"Summary:\n\nIn this paper the authors offer a new algorithm to detect cancer mutations from sequencing cell free DNA (cfDNA).  The idea is that in the sample being sequenced there would also be circulating tumor DNA (ctDNA) so such mutations could be captured in the sequencing reads.  The issue is that the ctDNA are expected to be found with low abundance in such samples, and therefore are likely to be hit by few or even single reads.  This makes the task of differentiating between sequencing errors and true variants due to ctDNA hard.  The authors suggest to overcome this problem by training an algorithm that will identify the sequence context that characterize sequencing errors from true mutations.  To this, they add channels based on low base quality, low mapping quality.  The algorithm for learning the context of sequencing reads compared to true mutations is based on a multi layered CNN, with 2/3bp long filters to capture di and trinucleotide frequencies, and a fully connected layer to a softmax function at the top.  The data is based on mutations in 4 patients with lung cancer for which they have a sample both directly from the tumor and from a healthy region.  One more sample is used for testing and an additional cancer control which is not lung cancer is also used to evaluate performance. \n\nPros:\n\nThe paper tackles what seems to be both an important and challenging problem.  We also liked the thoughtful construction of the network and way the reference, the read, the CIGAR and the base quality were all combined as multi channels to make the network learn the discriminative features of from the context.  Using matched samples of tumor and normal from the patients is also a nice idea to mimic cfDNA data. \n\nCons:\n\nWhile we liked both the challenge posed and the idea to solve it we found several major issues with the work.  \n\nFirst, the writing is far from clear. There are typos and errors all over at an unacceptable level.  Many terms are not defined or defined after being introduced (e.g. CIGAR, MF, BQMQ).  A more reasonable CS style of organization is to first introduce the methods/model and then the results, but somehow the authors flipped it and started with results first, lacking many definitions and experimental setup to make sense of those.  Yet Sec. 2 \u201cResults\u201d p. 3 is not really results but part of the methods.  The \u201cpipeline\u201d is never well defined, only implicitly in p.7 top, and then it is hard to relate the various figures/tables to bottom line results (having the labels wrong does not help that). \n\nThe filters by themselves seem trivial and as such do not offer much novelty.  Moreover, the authors filter the \u201cnormal\u201d samples using those (p.7 top), which makes the entire exercise a possible circular argument.  \n\nIf the entire point is to classify mutations versus errors it would make sense to combine their read based calls from multiple reads per mutations (if more than a single read for that mutation is available) - but the authors do not discuss/try that.  \n\nThe entire dataset is based on 4 patients.  It is not clear what is the source of the other cancer control case.  The authors claim the reduced performance show they are learning lung cancer-specific context. What evidence do they have for that? Can they show a context they learned and make sense of it?  How does this relate to the original papers they cite to motivate this direction (Alexandrov 2013)?  Since we know nothing about all these samples it may very well be that that are learning technical artifacts related to their specific batch of 4 patients.  As such, this may have very little relevance for the actual problem of cfDNA.  \n\nFinally, performance itself did not seem to improve significantly compared to previous methods/simple filters, and the novelty in terms of ML and insights about learning representations seemed limited. \n\nAlbeit the above caveats, we iterate the paper offers a nice construction for an important problem.  We believe the method and paper could potentially be improved and make a good fit for a future bioinformatics focused meeting such as ISMB/RECOMB.\n",1,1,1,1,1,1,1,1,1,-1
H1DkN7ZCZ-R3,"his paper proposes a deep learning framework to predict somatic mutations at extremely low frequencies which occurs in detecting tumor from cell-free DNA.  They key innovation is a convolutional architecture that represents the invariance around the target base.  The method is validated on simulations as well as in cfDNA and is s\nhown to provide increased precision over competing methods. \n\nWhile the method is of interest, there are more recent mutation callers that should be compared.  For example, Snooper which uses a RandomForest  (https://bmcgenomics.biomedcentral.com/articles/10.1186/s12864-016-3281-2) and hence would be of interest as another machine learning framework.  They also should compare to Strelka whic\nh interestingly they included only to make final calls of mutations but not in the comparison. \n\nFurther, I  would also have liked to see the use of standard benchmark datasets for mutation calling ( https://www.nature.com/articles/ncomms10001) .\n\nIt appears that the proposed method (Kittyhawk) has a steep decrease in PPV and enrichment for low tumor fraction which are presumably the parameter of greatest interest. The authors should explore this behavior in greater detail.",1,1,1,1,1,1,1,1,1,-1
H1Dy---0Z-R1,"This paper examines a distributed Deep RL system in which experiences, rather than gradients, are shared between the parallel workers and the centralized learner.  The experiences are accumulated into a central replay memory and prioritized replay is used to update the policy based on the diverse experience accumulated by all the of the workers.  Using this system, the authors are able to harness much more compute to learn very high quality policies in little time.  The results very convincingly show that Ape-X far outperforms competing algorithms such as recently published Rainbow.  \n\nIt\u2019s hard to take issue with a paper that has such overwhelmingly convincing experimental results.  However, there are a couple additional experiments that would be quite nice:\n\u2022\tIn order to understand the best way for training a distributed RL agent, it would be nice to see a side-by-side comparison of systems for distributed gradient sharing (e.g. Gorila) versus experience sharing (e.g. Ape-X).  \n\u2022\tIt would be interesting to get a sense of how Ape-X performs as a function of the number of frames it has seen, rather than just wall-clock time.  For example, in Table 1, is Ape-X at 200M frames doing better than Rainbow at 200M frames? \n\nPros:\n\u2022\tWell written and clear .\n\u2022\tVery impressive results .\n\u2022\tIt\u2019s remarkable that Ape-X preforms as well as it does given the simplicity of the algorithm. \n\nCons:\n\u2022\tHard to replicate experiments without the deep computational pockets of DeepMind.\n",1,1,1,1,1,1,1,1,1,-1
H1Dy---0Z-R2,"A parallel aproach to DQN training is proposed, based on the idea of having multiple actors collecting data in parallel, while a single learner trains the model from experiences sampled from a central replay memory.  Experiments on Atari game playing and two MuJoCo continuous control tasks show significant improvements in terms of training time and final performance compared to previous baselines. \n\nThe core idea is pretty straightforward but the paper does a very good job at demonstrating that it works very well, when implemented efficiently over a large cluster (which is not trivial).  I also appreciate the various experiments to analyze the impact of several settings (instead of just reporting a new SOTA).  Overall I believe this is definitely a solid contribution that will benefit both practitioners and researchers... as long as they got the computational resources to do so! \n\nThere are essentially two more things I would have really liked to see in this paper (maybe for future work?):\n- Using all Rainbow components\n- Using multiple learners (with actors cycling between them for instance) \nSharing your custom Tensorflow implementation of prioritized experience replay would also be a great bonus! \n\nMinor points:\n- Figure 1 does not seem to be referenced in the text  \n- \u00ab In principle, Q-learning variants are off-policy methods \u00bb => not with multi-step unless you do some kind of correction! I think it is important to mention it even if it works well in practice (just saying \u00ab furthermore we are using a multi-step return \u00bb is too vague) \n- When comparing the Gt targets for DQN vs DPG it strikes me that DPG uses the delayed weights phi- to select the action, while DQN uses current weights theta. I am curious to know if there is a good motivation for this and what impact this can have on the training dynamics. \n- In caption of Fig. 5 25K should be 250K \n- In appendix A why duplicate memory data instead of just using a smaller memory size? \n- In appendix D it looks like experiences removed from memory are chosen by sampling instead of just removing the older ones as in DQN. Why use a different scheme? \n- Why store rewards and gamma\u2019s at each time step in memory instead of just the total discounted reward? \n- It would have been better to re-use the same colors as in Fig. 2 for plots in the appendix \n- Would Fig. 10 be more interesting with the full plot and a log scale on the x axis?",1,1,1,1,1,-1,1,1,1,1
H1Dy---0Z-R3,"This paper proposes a distributed architecture for deep reinforcement learning at scale, specifically, focusing on adding parallelization in actor algorithm in Prioritized Experience Replay framework.  It has a very nice introduction and literature review of Prioritized experience replay  and also suggested to parallelize the actor algorithm by simply adding more actors to execute in parallel, so that the experience replay can obtain more data for the learner to sample and learn.  Not surprisingly, as this framework is able to learn from way more data (e.g. in Atari), it outperforms the baselines, and Figure 4 clearly shows the more actors we have the better performance we will have.  \n\nWhile the strength of this paper is clearly the good writing as well as rigorous experimentation,  the main concern I have with this paper is novelty.  It is in my opinion a somewhat trivial extension of the previous work of Prioritized experience replay in literature; hence the challenge of the work is not quite clear.  Hence, I feel adding some practical learnings of setting up such infrastructure might add more flavor to this paper, for example.",1,1,1,1,1,1,1,1,1,-1
H1eJxngCW-R1,"Summary:\nThe paper proposes a new dataset for reading comprehension, called DuoRC.  The questions and answers in the DuoRC dataset are created from different versions of a movie plot narrating the same underlying story.  The DuoRC dataset offers the following challenges compared to the existing reading comprehension (RC) datasets \u2013 1) low lexical overlap between questions and their corresponding passages, 2) requires use of common-sense knowledge to answer the question, 3) requires reasoning across multiples sentences to answer the question, 4) consists of those questions as well that cannot be answered from the given passage.  The paper experiments with two types of models \u2013 1) a model which only predicts the span in a document and 2) a model which generates the answer after predicting the span.  Both these models are built off of an existing model on SQuAD \u2013 the Bidirectional Attention Flow (BiDAF) model.  The experimental results show that the span based model performs better than the model which generates the answers.  But the accuracy of both the models is significantly lower than that of their base model (BiDAF) on SQuAD, demonstrating the difficulty of the DuoRC dataset.  \n\t\nStrengths:\n\n1.\tThe data collection process is interesting.  The challenges in the proposed dataset as outlined in the paper seem worth pushing for. \n2.\tThe paper is well written making it easy to follow. \n3.\tThe experiments and analysis presented in the paper are insightful. \n\nWeaknesses:\n\n1.\tIt would be good if the paper can throw some more light on the comparison between the existing MovieQA dataset and the proposed DuoRC dataset, other than the size. \n2.\tThe dataset is motivated as consisting of four challenges (described in the summary above) that do not exist in the existing RC datasets.  However, the paper lacks an analysis on what percentage of questions in the proposed dataset belong to each category of the four challenges.  Such an analysis would helpful to accurately get an estimate of the proportion of these challenges in the dataset. \n3.\tIt is not clear from the paper how should the questions which are unanswerable be evaluated.  As in, what should be the ground-truth answer against which the answers should such questions be evaluated.  Clearly, string matching would not work because a model could say \u201cdon\u2019t know\u201d whereas some other model could say \u201cunanswerable\u201d.  So, does the training data have a particular string as the ground truth answer for such questions, so that a model can just be trained to spit out that particular string when it thinks it can\u2019t answer the questions?   \n4.\tOne of the observations made in the paper is that \u201ctraining on one dataset and evaluating on the other results in a drop in the performance. \u201d However, in table 4, evaluating on Paraphrase RC is better when trained on Self RC as opposed to when trained on Paraphrase RC.  This seems to be in conflict with the observation drawn in the paper.  Could authors please clarify this?  Also, could authors please throw some light on why this might be happening? \n5.\tIn the third phase of data collection (Paraphrase RC), was waiting for 2-3 weeks the only step taken in order to ensure that the workers for this stage are different from those in stage 2, or was something more sophisticated implemented which did not allow a worker who has worked in stage 2 to be able to participate in stage 3? \n6.\tTypo: Dataset section, phrases --> phases. \n\nOverall: The challenges proposed in the DuoRC dataset are interesting.  The paper is well written and the experiments are interesting.  However, there are some questions (as mentioned in the Weaknesses section) which need to be clarified before I can recommend acceptance for the paper",1,1,1,1,1,-1,1,1,1,-1
H1eJxngCW-R2,"This paper presents a useful dataset for testing reading comprehension while avoiding significant lexical overlap between question and document.  The paper rightly mentions that existing reading comprehension datasets (e.g. SQuAD) where the current methods are already performing at the human level largely due to large lexical overlap between question and document.  The authors have devised a clever way to create a reading comprehension dataset without a lot of lexical overlap by using parallel plots of movies from Wikipedia and IMDB.  \n\nThis paper contributes a useful new dataset that fixes some of the shortcomings of existing reading comprehension datasets where the task is made easier by lexical overlap.  The authors also present an analysis of the data by applying one of the SOTA techniques on SQuAD to this data.  They also analyze the effect of various span-identification steps and preprocessing steps on the performance.  Overall, this paper contributes a useful new dataset that can be quite useful for reading comprehension",1,1,1,1,-1,1,1,-1,1,-1
H1eJxngCW-R3,"1) This paper proposes a new dataset for Reading Comprehension (RC).  Different from other existing RC datasets, the authors claim that this new dataset requires background and common-sense knowledge,  and across sentences reasoning in order to answer the questions correctly.  \n\nOverall, I think this dataset is very useful for RC.  The collection process is also carefully designed to reduce the lexical overlap between question and answer pairs. \n\n2) I have the questions as follows:\ni) in the abstract, authors mentioned the workers set one only takes care of creating questions from version one of the plots, and workers set two is in charge of generating answers from another version of plots.  However, in bullet 2 of section 3, it seems that the workers set one is also required to answer the questions in selfRC.  Is there any mistake in the description of the abstract? \n\nii) What is the standard for creating the questions?  I noticed that the time and location information was used to generate questions sometime, but sometimes these kinds of questions are ignored. \n\niii) Why the SelfRC is about QA pairs but for ParaphraseRC, you need to include documents?  \n\niv) What is the average length of the answers in both ParaphraseRC and SelfRC?  I found that the answers are usually very short, which is more like factoid QA.  It would be great if the authors could design some non-factoid QA pairs which require more reasoning and background knowledge.  \n\nv) During NLP pre-processing (section 4), how do you prune the irrelevant documents?\n",1,1,1,1,1,-1,1,1,1,-1
H1K6Tb-AZ-R1,"An approach to adjust inference speed, power consumption or latency by using incomplete dot products McDanel et al. (2017) is investigated. \n\nThe approach is based on `profile coefficients\u2019 which are learned for every channel in a convolution layer, or for every column in the fully connected layer.  Based on the magnitude of this profile coefficient, which determines the importance of this `filter,\u2019 individual components in a neural net are switched on or off. McDanel et al. (2017) propose to train such an approach in a stage-by-stage manner. \n\nDifferent from a recently proposed method by McDanel et al. (2017), the authors of this submission argue that the stage-by-stage training doesn\u2019t fully utilize the deep net performance.  To address this issue a `loss aggregation\u2019 is proposed which jointly optimizes a deep net when multiple fractions of incomplete products are used. \n\nThe method is evaluated on the MNIST and CIFAR-10 datasets and shown to outperform work on incomplete dot products by McDanel et al. (2017) by 32% in the low resource regime. \n\nSummary:\n\u2014\u2014\nIn summary, I think the paper proposes an interesting approach but more work is necessary to demonstrate the effectiveness of the discussed method.  The results are preliminary and should be extended to CIFAR-100 and ImageNet to be convincing.  In addition, the writing should be improved as it is often ambiguous.  See below for details.\n\nReview:\n\u2014\u2014\u2014\u2014\u2014\n1. Experiments are only provided on very small datasets. According to my opinion, this isn\u2019t sufficient to illustrate the effectiveness of the proposed approach.  As a reader I wouldn\u2019t want to see results on CIFAR-100 and ImageNet using multiple network architectures, e.g., AlexNet and VGG16.\n\n2.  Usage of the incomplete dot product for the fully connected layer and the convolutional layer seems inconsistent.  More specifically, while the profile coefficient is applied for every input element in Eq. (1), it\u2019s applied based on output channels in Eq. (2). This seems inconsistent and a comment like `These two approaches, however, are equivalent with negligible difference induced by the first hidden layer\u2019 is more confusing than clarifying.\n\n3.  The writing should be improved significantly and statements should be made more precise, e.g., `From now on, x% DP, where \\leq x \\geq 100, means the x% of terms used in dot products\u2019. While sentences like those can be deciphered, they aren\u2019t that appealing. \n\n4. The loss functions in Eq. (3) should be made more precise. It remains unclear whether the profile coefficients and the weights are trained jointly, separately, incrementally etc. \n\n5. Algorithm 1 and Algorithm 2 call functions that aren\u2019t described/defined. \n\n6. Baseline numbers for training on datasets without incomplete dot products should be provided.\n",1,1,1,1,1,1,1,1,1,-1
H1K6Tb-AZ-R2,"This paper presents a modification of a numeric solution: Incomplete Dot Product (IDP), that allows a trained network to be used under different hardware constraints.  The IDP method works by incorporating a 'coefficient' to each layer (fully connected or convolution), which can be learned as the weights of the model are being optimized.  These coefficients can be used to prune subsets of the nodes or filters, when hardware has limited computational capacity.  \n\nThe original IDP method (cited in the paper) is based on iteratively training for higher hardware capacities.  This paper improves upon the limitation of the original IDP by allowing the weights of the network be trained concurrently with these coefficients, and authors present a loss function that is linear combination of loss function under original or constrained network setting.  They also present results for a 'harmonic' combination which was not explained in the paper at all. \n\nOverall the paper has very good motivation and significance.  \nHowever the writing is not very clear and the paper is not self-contained at all.  I was not able to understand the significance of early stopping and how this connects with loss aggregation, and how the learning process differs from the original IDP paper, if they also have a scheduled learning setting.  \n\nAdditionally, there were several terms that were unexplained in this paper such as 'harmonic' method highlighted in Figure  3.  As is, while results are promising,  I can't fully assess that the paper has major contributions.",1,1,1,1,1,-1,1,1,1,-1
H1K6Tb-AZ-R3,"The authors propose a method for reducing the computational burden when performing inference in deep neural networks.  The method is based a previously-developed approach called incomplete dot products, which works by pruning some of the inputs in the dot products via the introduction of pre-specified coefficients.  The authors of this paper extend the method by introducing a task-wise learning procedure that sequentially optimizes a loss function for decreasing percentage of included features in the dot product.  \n\nUnfortunately, this paper was hard to follow for someone who does not actively work in this field, making it hard to judge if the contribution is significant or not.  While the description of the problem itself is adequate, when it comes to describing the TESLA procedure and the alternative training procedure, the relevant passages are, in my opinion, too vague to allow other researchers to implement this procedure. \n\nPositive points:\n- The application seems relevant, and the task-wise procedure seems like an improvement over the original IDP proposal.\n- Application to two well-known benchmarking datasets. \n\nNegative points:\n- The method is not described in sufficient detail to allow reproducibility, the algorithms are no more than sketches. \n- It is not clear to me what the advantage of this approach is, as opposed to alternative ways of compressing the network (e.g. via group lasso regularization), or training an emulator on the full model for each task. \n\nMinor point:\n- Figure 1 is unclear and requires a better caption.",1,1,1,1,-1,-1,1,1,1,1
H1l8sz-AW-R1,"\nGENERAL IMPRESSION:\n\nOverall, the revised version of the paper is greatly improved.  The new derivation of the method yields a much simpler interpretation, although the relation to the natural gradient remains weak (see below).  The experimental evaluation is now far more solid.  Multiple data sets and network architectures are tested, and equally important, the effect of parameter settings is investigated.  I enjoyed the investigation of the effect of L_2 regularization on qualitative optimization behavior. \n\n\nCRITICISM:\n\nMy central criticism is that the introduction of the L_2 norm as a replacement of KL divergence is completely ad-hoc; how it is related to KL divergence remains unclear.  It seems that other choices are equally well justified, including the L_2 norm in parameter space, which then defeats the central argument of the paper.  I do believe that L_2 distance is more natural in function space than in parameter space, but I am missing a strict argument for this in the paper. \n\nAlthough related work is discussed in detail in section 1, it remains unclear how exactly the proposed algorithm overlaps with existing approaches.  I am confident that it is easy to identify many precursors in the optimization literature, but I am not an expert on this.  It would be of particular interest to highlight connections to algorithm regularly applied to neural network training.  Adadelta, RMSprop, and ADAM are mentioned explicitly, but what exactly are differences and similarities? \n\nThe interpretation of figure 2 is off.  It is deduced that HCGD generalizes better, however, this is the case only at the very end of training, while SGD with momentum and ADAM work far better initially.  With the same plot one could sell SGD as the superior algorithm.  Overall, also in the light of figure 4, the interpretation that the new algorithm results in better generalization seems to stand on shaky ground, since differences are small. \n\nI like the experiment presented in figure 5 in particular.  It adds insights that are of value even if the method should turn out to have significant overlap with existing work (see above), and perform \""only\"" on par with these:;  it adds an interesting perspective to the discussion of how network optimization \""works\"", how it handles local optima and which role they play, and how the objective function landscape is \""perceived\"" by different optimizers.  This is where I learned something new. \n\n\nMINOR POINTS:\n\npage 5: \""the any\"" (typo)\n\npage 5: \""ture\"" -> \""true\"" (typo)",1,1,1,1,1,1,1,1,1,-1
H1l8sz-AW-R2,"I have read comments and rebuttal - i do not have the luxury of time to read in depth the revision. \nIt seems that the authors have made an effort to accommodate reviewers' comments.  I upgraded the rating. \n\n-----------------------------------------------------------------------------------------------------------------------\n\nSummary: The paper considers the use of natural gradients for learning.  The added twist is the substitution of the KL divergence with the Wasserstein distance, as proposed in GAN training.  The authors suggest that Wasserstein regularization improves generalization over SGD with a little extra cost. \n\nThe paper is structured as follows:\n1. KL divergence is used as a similarity measure between two distributions. \n2. Regularizing the objective with KL div. seems promising, but expensive. \n3. We usually approximate the KL div. with its 2nd order approximation - this introduces the Hessian of the KL divergence, known as Fisher information matrix. \n4. However, computing and inverting the Fisher information matrix is computationally expensive. \n5. One solution is to approximate the solution F^{-1} J using gradient descent.  However, still we need to calculate F.  There are options where F could be formed as the outer product of a collection gradients of individual examples ('empirical Fisher'). \n6. This paper does not move towards Fisher information, but towards Wasserstein distance: after a \""good\"" initialization via SGD is obtained, the inner loop continues updating that point using the Wasserstein regularized objective.  \n7. No large matrices need to be formed or inverted, however more passes needed per outer step. \n\nImportance:\nSomewhat lack of originality and poor experiments lead to low importance. \n\nClarity:\nThe paper needs major revision w.r.t. presenting and highlighting the new main points.  E.g., one needs to get to page 5 to understand that the paper is just based on the WGAN ideas in Arjovsky et al., but with a different application (not GANS). \n\nOriginality/Novelty:\nThe paper, based on WGAN motivation, proposes Wasserstein distance regularization over KL div. regularization for training of simple models, such as neural networks.  Beyond this, the paper does not provide any futher original idea.  So, slight to no novelty. \n\nMain comments:\n1. Would the approximation of C_0 by its second-order Taylor expansion (that also introduces a Hessian) help?  This would require the combination of two Hessian matrices. \n\n2. Experiments are really demotivating: it is not clear whether using plain SGD or the proposed method leads to better results.  \n\nOverall:\nRejection",1,1,1,1,-1,-1,1,1,1,-1
H1l8sz-AW-R3,"The paper presents an additive regularization scheme to encourage parameter updates that lead to small changes in prediction (i.e. adjusting updates based on their size in the output space instead of the input space).  This goal is to achieve a similar effect to that of natural gradient, but with lighter computation.  The authors claim that their regularization is related to Wasserstein metric (but the connection is not clear to me, read below).  Experiments on MNIST with show improved generalization (but the baseline is chosen poorly, read below). \n\nThe paper is easy to read and organized very well, and has adequate literature review.  However, the contribution of the paper itself needs to be strengthened in both the theory and empirical sides. \n\nOn the theory side, the authors claim that their regularization is based on Wasserstein metric (in the title of the paper as well as section 2. 2). However, this connection is not very clear to me [if there is a rigorous connection, please elaborate].  From what I understand, the authors argue that their proposed loss+regularization is equivalent to the Kantorovich-Rubinstein form.  However, in the latter, the optimization objective is the f itself (sup E[f_1]-E[f_2]) but in your scheme you propose adding the regularization term (which can be added to any objective function, and then the whole form loses its connection to Wasserstrin metric). \n\nOn the practical side, the chosen baseline is very poor.  The authors only experiment with MNIST dataset.  The baseline model lacks both \""batch normalization\"" and \""dropout\"", which I guess is because otherwise the proposed method would under-perform against the baseline.  It is hard to tell if the proposed regularization scheme is something significant under such poorly chosen baseline",1,1,1,1,1,-1,1,1,1,-1
H1LAqMbRW-R1,"The paper proposes to use a pretrained model-free RL agent to extract the developed state representation and further re-use it for learning forward model of the environment and planning. \nThe idea of re-using a pretrained agent has both pros and cons.  On one hand, it can be simpler than learning a model from scratch because that would also require a decent exploration policy to sample representative trajectories from the environment.  On the other hand, the usefulness of the learned representation for planning is unclear.  A model-free agent can (especially if trained with certain regularization) exclude a lot of information which is potentially useful for planning, but is it necessary for reactively taking actions. \nA reasonable experiment/baseline thus would be to train a model-free agent with a small reconstruction loss on top of the learned representation. \u2028In addition to that, one could fine-tune the representation during forward model training.  \nIt would be interesting to see if this can improve the results. \n\nI personally miss a more technical and detailed exposition of the ideas.  For example, it is not described anywhere what loss is used for learning the model.  MCTS is not described and a reader has to follow references and infer how exactly is it used in this particular application which makes the paper not self-contained.  \nAgain, due to lack of equations, I don\u2019t completely understand the last paragraph of 3.2, I suggest re-writing it (as well as some other parts) in a more explicit way. \nI also could find the details on how figure 1 was produced . As I understand, MCTS was not used in this experiment.  If so, how would one play with just a forward model? \n\nIt is a bit disappointing that authors seem to consider only deterministic models which clearly have very limited applicability.  Is mini-RTS a deterministic environment?  \nWould it be possible to include a non-deterministic baseline in the experimental comparison? \n\nExperimentally, the results are rather weak compared to pure model-free agents.  Somewhat unsatisfying, longer-term prediction results into weaker game play.  Doesn\u2019t this support the argument about need in stochastic prediction?  \n\nTo me, the paper in it\u2019s current form is not written well and does not contain strong enough empirical results, so that I can\u2019t recommend acceptance.  \n\nMinor comments:\n* MatchA and PredictPi models are not introduced under such names \n* Figure 1 that introduces them contains typos.  \n* Formatting of figure 8 needs to be fixed.  This figure does not seem to be referred to anywhere in the text and the broken caption makes it hard to understand what is happening there",1,1,1,1,1,-1,1,1,1,-1
H1LAqMbRW-R2,"Summary:\n\nThis paper studies learning forward models on latent representations of the environment, and use these for model-based planning (e.g. via MCTS) in partial-information real-time-strategy games.  The testbed used is MiniRTS, a simulation environemnt for 1v1 RTS. \n\nForecasting the future suffers from buildup / propagation of prediction errors, hence the paper uses multi-step errors to stabilize learning. \n\nThe paper:\n1. describes how to train strong agents that might have learned an informative latent representation of the observed state-space. \n2. Evaluates how informative the latent states are via state reconstruction. \n3. trains variatns of a forward model f on the hidden states of the various learned agents. \n4. evaluates different f within MCTS for MiniRTS. \n\nPro:\n- This is a neat idea and addresses the important question of how to learn accurate models of the environment from data, and how to integrate them with model-free methods. \n- The experimental setting is very non-trivial and novel. \n\nCon:\n- The manuscript is unclear in many parts -- this should be greatly improved. \n1. The different forward models are not explained well (what is MatchPi, MatchA, PredN?).  Which forward model is trained from which model-free agent? \n2. How is the forward model / value function used in MCTS?  I assume it's similar to what AlphaGo does, but right now it's not clear at all how everything is put together. \n\n- The paper devotes a lot of space (sect 4.1) on details of learning and behavior of the model-free agents X.  Yet it is unclear how this informs us about the quality of the learned forward models f.  It would be more informative to focus in the main text on the aspects that inform us about f, and put the training details in an appendix. \n\n- As there are many details on how the model-free agents are trained and the system has many moving parts, it is not clear what is important and what is not wrt to the eventual winrate comparisons of the MCTS models.  Right now, it is not clear to me why MatchA / PredN differ so much in Fig 8. \n\n- The conclusion seems quite negative: the model-based methods fare *much* worse than the model-free agent.  Is this because of the MCTS approach?  Because f is not good? Because the latent h is not informative enough? This requires a much more thorough evaluation. \n\nOverall:\nI think this is an interesting direction of research, but the current manuscript does provide a complete and clear analysis .\n\nDetailed:\n- What are the right prediction tasks that ensure the latent space captures enough of the forward model? \n- What is the error of the raw h-predictions?  Only the state-reconstruction error is shown now. \n- Figure 6 / sect 4.2: which model-free agent is used?  Also fig 6 misses captions. \n- Figure 8: scrambled caption. \n- Does scheduled sampling / Dagger (Ross et al.) improve the long-term stability in this case",1,1,1,1,1,-1,1,1,1,-1
H1LAqMbRW-R3,"Summary: This paper proposes to use the latent representations learned by a model-free RL agent to learn a transition model for use in model-based RL (specifically MCTS).  The paper introduces a strong model-free baseline (win rate ~80% in the MiniRTS environment) and shows that the latent space learned by this baseline does include relevant game information.  They use the latent state representation to learn a model for planning, which performs slightly better than a random baseline (win rate ~25%). \n\nPros:\n- Improvement of the model-free method from previous work by incorporating information about previously observed states, demonstrating the importance of memory. \n- Interesting evaluation of which input features are important for the model-free algorithm, such as base HP ratio and the amount of resources available. \n\nCons:\n- The model-based approach is disappointing compared to the model-free approach. \n\nQuality and Clarity:\n\nThe paper in general is well-written and easy to follow and seems technically correct,;  though I found some of the figures and definitions confusing, specifically:\n\n- The terms for different forward models are not defined (e.g. MatchPi, MatchA, etc.).  I can infer what they mean based on Figure 1 but it would be helpful to readers to define them explicitly. \n- In Figure 3b, it is not clear to me what the difference between the red and blue curves is. \n- In Figure 4, it would be helpful to label which color corresponds to the agent and which to the rule-based AI. \n- The caption in Figure 8 is malformatted. \n- In Figure 7, the baseline of \\hat{h_t}=h_{t-2} seems strange---I would find it more useful for Figure 7 to compare to the performance if the model were not used (i.e. if \\hat{h_t}=h_t) to see how much performance suffers as a result of model error. \n\nOriginality:\n\nI am unfamiliar with the MiniRTS environment, but given that it is only published in this year's NIPS (and that I couldn't find any other papers about it on Google Scholar) it seems that this is the first paper to compare model-free and model-based approaches in this domain.  However, the model-free approach does not seem particularly novel in that it is just an extension of that from Tian et al. (2017) plus some additional features.  The idea of learning a model based on the features from a model-free agent seems novel but lacks significance in that the results are not very compelling (see below). \n\nSignificance:\n\nI feel the paper overstates the results in saying that the learned forward model is usable in MCTS.  The implication in the abstract and introduction (at least as I interpreted it) is that the learned model would outperform a model-free method, but upon reading the rest of the paper I was disappointed to learn that in reality it drastically underperforms.  The baseline used in the paper is a random baseline, which seems a bit unfair---a good baseline is usually an algorithm that is an obvious first choice, such as the model-free approach",1,1,1,1,1,1,1,1,1,-1
H1mCp-ZRZ-R1,"In this work, the authors suggest the use of control variate schemes for estimating gradient values, within a reinforcement learning  framework.  The authors also introduce a specific control variate technique based on the so-called Stein\u2019s identity.  The paper is interesting and well-written. \n\nI have some question and some consideration that can be useful for improving the appealing of the paper. \n\n- I believe that different Monte Carlo (or Quasi-Monte Carlo) strategies can be applied in order to estimate the integral (expected value) in Eq. (1), as also suggested in this work.  Are there other alternatives in the literature?  Please, please discuss and cite some papers if required.   \n\n- I suggest to divide Section 3.1 in two subsections. The first one introducing Stein\u2019s identity and the related comments that you need, and a second one, starting after Theorem 3.1, with title \u201cStein Control Variate\u201d. \n\n-  Please also discuss the relationships, connections, and possible applications of your technique to other algorithms used in Bayesian optimization, active learning and/or sequential learning, for instance as\n\nM.  U. Gutmann and J. Corander, \u201cBayesian optimization for likelihood-free inference of simulator-based statistical mod- els,\u201d Journal of Machine Learning Research, vol. 16, pp. 4256\u2013 4302, 2015. \n\nG. da Silva Ferreira and D. Gamerman, \u201cOptimal design in geostatistics under preferential sampling,\u201d Bayesian Analysis, vol. 10, no. 3, pp. 711\u2013735, 2015. \n\nL. Martino, J. Vicent, G. Camps-Valls, \""Automatic Emulator and Optimized Look-up Table Generation for Radiative Transfer Models\"", IEEE International Geoscience and Remote Sensing Symposium (IGARSS), 2017. \n\n-  Please also discuss the dependence of your algorithm with respect to the starting baseline function \\phi_0.",1,-1,1,1,1,1,1,1,-1,-1
H1mCp-ZRZ-R2,"This paper proposed a class of control variate methods based on Stein's identity.  Stein's identity has been widely used in classical statistics and recently in statistical machine learning literature.  Nevertheless, applying Stein's identity to estimating policy gradient is a novel approach in reinforcement learning community.  To me, this approach is the right way of constructing control variates for estimating policy gradient.  The authors also did a good job in connecting with existing works and gave concrete examples for Gaussian policies.  The experimental results also look promising. \n\nIt would be nice to include some theoretical analyses like under what conditions, the proposed method can achieve smaller sample complexity than existing works.     \n\nOverall this is a strong paper and I recommend to accept.\n \n\n\n",1,1,1,1,1,1,1,1,1,-1
H1mCp-ZRZ-R3,"The paper proposes action-dependent baselines for reducing variance in policy gradient, through the derivation based on Stein\u2019s identity and control functionals.  The method relates closely to prior work on action-dependent baselines,  but explores in particular on-policy fitting and a few other design choices that empirically improve the performance.  \n\nA criticism of the paper is that it does not require Stein\u2019s identity/control functionals literature to derive Eq. 8, since it can be derived similarly to linear control variate and it has also previously been discussed in IPG [Gu et. al., 2017] as reparameterizable control variate.  The derivation through Stein\u2019s identity does not seem to provide additional insights/algorithm designs beyond direct derivation through reparameterization trick. \n\nThe empirical results appear promising, and in particular in comparison with Q-Prop, which fits Q-function using off-policy TD learning.  However, the discussion on the causes of the difference should be elaborated much more, as it appears there are substantial differences besides on-policy/off-policy fitting of the Q, such as:\n\n-FitLinear fits linear Q (through parameterization based on linearization of Q) using on-policy learning, rather than fitting nonlinear Q and then at application time linearize around the mean action.  A closer comparison would be to use same locally linear Q function for off-policy learning in Q-Prop.\n\n-The use of on-policy fitted value baseline within Q-function parameterization during on-policy fitting is nice.  Similar comparison should be done with off-policy fitting in Q-Prop. \n\nI wonder if on-policy fitting of Q can be elaborated more. Specifically, on-policy fitting of V seems to require a few design details to have best performance [GAE, Schulman et. al., 2016]: fitting on previous batch instead of current batch to avoid overfitting  (this is expected for your method as well, since by fitting to current batch the control variate then depends nontrivially on samples that are being applied), and possible use of trust-region regularization to prevent V from changing too much across iterations.  \n\nThe paper presents promising results with direct on-policy fitting of action-dependent baseline, which is promising since it does not require long training iterations as in off-policy fitting in Q-Prop.  As discussed above, it is encouraged to elaborate other potential causes that led to performance differences.  The experimental results are presented well for a range of Mujoco tasks.  \n\nPros:\n\n-Simple, effective method that appears readily available to be incorporated to any on-policy PG methods without significantly increase in computational time\n\n-Good empirical evaluation \n\nCons:\n\n-]The name Stein control variate seems misleading since the algorithm/method does not rely on derivation through Stein\u2019s identity etc.  and does not inherit novel insights due to this derivation.\n",1,1,1,1,1,1,1,1,1,-1
H1MczcgR--R1,"The paper discusses the problems of meta optimization with small look-ahead: do small runs bias the results of tuning?  The result is yes and the authors show how differently the tuning can be compared to tuning the full run.  The Greedy schedules are far inferior to hand-tuned schedules as they focus on optimizing the large eigenvalues while the small eigenvalues can not be \""seen\"" with a small lookahead.  The authors show that this effect is caused by the noise in the obective function. \n\npro:\n- Thorough discussion of the issue with theoretical understanding on small benchmark functions as well as theoretical work\n- Easy to read and follow \n\ncons:\n-Small issues in presentation: \n* Figure 2 \""optimal learning rate\"" -> \""optimal greedy learning rate\"", also reference to Theorem 2 for increased clarity. \n* The optimized learning rate in 2.3 is not described. This reduces reproducibility. \n* Figure 4 misses the red trajectories, also it would be easier to have colors on the same (log?)-scale.  \n  The text unfortunately does not explain why the loss function looks so vastly different\n  with different look-ahead.  I would assume from the description that the colors are based\n  on the final loss values obtaine dby choosing a fixed pair of decay exponent and effective LR.  \n\nTypos and notation:\npage 7 last paragraph: \""We train the all\"" -> We train all\nnotation page 5: i find \\nabla_{\\theta_i} confusing when \\theta_i is a scalar, i would propose \\frac{\\partial}{\\partial \\theta_i}\npage 2: \""But this would come at the expense of long-term optimization process\"": at this point of the paper it is not clear how or why this should happen.  Maybe add a sentence regarding the large/Small eigenvalues?",1,1,1,1,1,-1,1,1,1,1
H1MczcgR--R2,"This paper proposes a simple problem to demonstrate the short-horizon bias of the learning rate meta-optimization. \n\n- The idealized case of quadratic function the analytical solution offers a good way to understand how T-step look ahead can benefit the meta-algorithm. \n- The second part of the paper seems to be a bit disconnected to the quadratic function analysis.  It would be helpful to understand if there is gap between gradient based meta-optimization and the best effort(given by the analytical solution) \n- Unfortunately, no guideline or solution is offered in the paper. \n\nIn summary, the idealized model gives a good demonstration of the problem itself.  I think it might be of interest to some audiences in ICLR",1,1,1,1,-1,-1,1,1,1,-1
H1MczcgR--R3,"This paper studies the issue of truncated backpropagation for meta-optimization.  Backpropagation through an optimization process requires unrolling the optimization, which due to computational and memory constraints, is typically restricted or truncated to a smaller number of unrolled steps than we would like. \n\nThis paper highlights this problem as a fundamental issue limiting meta-optimization approaches.  The authors perform a number of experiments on a toy problem (stochastic quadratics) which is amenable to some theoretical analysis as well as a small fully connected network trained on MNIST.   \n\n(side note: I was assigned this paper quite late in the review process, and have not carefully gone through the derivations--specifically Theorems 1 and 2) .\n\nThe paper is generally clear and well written. \n\nMajor comments\n-------------------------\nI was a bit confused why 1000 SGD+mom steps pre-training steps were needed. As far as I can tell, pre-training is not typically done in the other meta-optimization literature?   The authors suggest this is needed because \""the dynamics of training are different at the very start compared to later stages\"", which is a bit vague. Perhaps the authors can expand upon  this point? \n\nThe conclusion suggests that the difference in greedy vs. fully optimized schedule is due to the curvature (poor scaling) of the objective --but Fig 2. and earlier discussion talked about the noise in the objective as introducing the bias (e.g. from earlier in the paper, \""The noise in the problem adds uncertainty to the objective, resulting in failures of greedy schedule\"").  Which is the real issue, noise or curvature?  Would running the problem on quadratics with different condition numbers be insightful? \n\nMinor comments\n-------------------------\nThe stochastic gradient equation in Sec 2.2.2 is missing a subscript: \""h_i\"" instead of \""h\""\ n\nIt would be nice to include the loss curve for a fixed learning rate and momentum for the noisy quadratic in Figure 2, just to get a sense of how that compares with the greedy and optimized curves.\ n\nIt looks like there was an upper bound constraint placed on the optimized learning rate in Figure 2--is that correct?  I couldn't find a mention of the constraint in the paper. (the optimized learning rate remains at 0.2 for the first ~60 steps)? \n\nFigure 2 (and elsewhere): I would change 'optimal' to 'optimized' to distinguish it from an optimal curve that might result from an analytic derivation. 'Optimized' makes it more clear that the curve was obtained using an optimization process. \n\nFigure 2: can you change the line style or thickness so that we can see both the red and blue curves for the deterministic case?  I assume the red curve is hiding beneath the blue one--but it would be good to see this explicitly. \n\nFigure 4 is fantastic--it succinctly and clearly demonstrates the problem of truncated unrolls . I would add a note in the caption to make it clear that the SMD trajectories are the red curves, e.g.: \""SMD trajectories (red) during meta-optimization of initial effective ...\"".  I would also change the caption to use \""meta-training losses\"" instead of \""training losses\"" (I believe those numbers are for the meta-loss, correct?) . Finally, I would add a colorbar to indicate numerical values for the different grayscale values. \n\nSome recent references that warrant a mention in the text:\n- both of these learn optimizers using longer numbers of unrolled steps:\nLearning gradient descent: better generalization and longer horizons, Lv et al, ICML 2017\nLearned optimizers that scale and generalize, Wichrowska et al, ICML 2017\n- another application of unrolled optimization:\nUnrolled generative adversarial networks, Metz et al, ICLR 2017 \n\nIn the text discussing Figure 4 (middle of pg. 8) , \""which is obtained by using...\"" should be \""which are obtained by using... \""\n\nIn the conclusion, \""optimal for deterministic objective\"" should be \""deterministic objectives\""",1,1,1,1,1,1,1,1,1,1
H1meywxRW-R1,Summary:\nThis paper proposed an extension of the dynamic coattention network (DCN) with deeper residual layers and self-attention.  It also introduced a mixed objective with self-critical policy learning to encourage predictions with high word overlap with the gold answer span.  The resulting DCN+ model achieved significant improvement over DCN. \n\nStrengths:\nThe model and the mixed objective is well-motivated and clearly explained.\nNear state-of-the-art performance on SQuAD dataset (according to the SQuAD leaderboard). \n\nOther questions and comments:\nThe ablation shows 0.7 improvement on EM with mixed objective.  It is interesting that the mixed objective (which targets F1) also brings improvement on EM. \n,1,1,-1,1,-1,-1,1,-1,1,-1
H1meywxRW-R2,"This paper proposed an improved version of dynamic coattention networks, which is used for question answering tasks.  Specifically, there are 2 aspects to improve DCN: one is to use a mixed objective that combines cross entropy with self-critical policy learning, the other one is to imporve DCN with deep residual coattention encoder.  The proposed model achieved STOA performance on Stanford Question Asnwering Dataset  and several ablation experiments show the effectiveness of these two improvements.  Although DCN+ is an improvement of DCN, I think the improvement is not incremental.  \n\nOne question is that since the model is compicated, will the authors release the source code to repeat all the experimental results?",1,-1,1,1,1,-1,1,1,1,1
H1meywxRW-R3,"The authors of this paper propose some extensions to the Dynamic Coattention Networks models presented last year at ICLR.  First they modify the architecture of the answer selection model by adding an extra coattention layer to improve the capture of dependencies between question and answer descriptions.  The other main modification is to train their DCN+ model using both cross entropy loss and F1 score (using RL supervision) in order to  reward the system for making partial matching predictions.  Empirical evaluations conducted on the SQuAD dataset indicates that this architecture achieves an improvement of at least 3%, both on F1 and exact match accuracy, over other comparable systems.  An ablation study clearly shows the contribution of the deep coattention mechanism and mixed objective training on the model performance.  \n\nThe paper is well written, ideas are presented clearly and the experiments section provide interesting insights such as the impact of RL on system training or the capability of the model to handle long questions and/or answers.  It seems to me that this paper is a significant contribution to the field of question answering systems",1,1,1,1,-1,-1,1,1,1,-1
H1Nyf7W0Z-R1,"The paper proposes another training objective for training neural sequence-to-sequence models.  The objective is based on alpha-divergence between the true input-output distribution q and the model distribution p.  The new objective generalizes  Reward-Augmented Maximum Likelihood (RAML) and entropy-regularized Reinforcement Learning (RL), to which it presumably degenerates when alpha goes to 1 or to 0 respectively. \n\nThe paper has significant writing issues.  In Paragraph \u201cMaximum Likelihood\u201d, page 2, the formalization of the studied problem is unclear.  Do X and Y denote the complete input/output spaces, or do they stand for the training set examples only?   In the former case, the statement \u201cx is uniformly sampled from X\u201d does not make sense because X is practically infinite.  Same applies to the dirac distribution q(y|x), the true conditional distribution of outputs given inputs is multimodal even for machine translation.  If X and Y were meant to refer to the training set, it would be worth mentioning the existence of the test set.  Furthermore, in the same Section 2 the paper fails to mention that reinforcement learning training also does not completely correspond to the evaluation approach, at which stage greedy search or beam search is used. \n\nThe proposed method is evaluated on just one dataset.  Crucially, there is no comparison to a trivial linear combination of ML and RL, which in one way or another was used in almost all prior work, including GNMT, Bahdanau et al, Ranzato et al.  The paper does not argue why alpha divergence is better that the aforementioned combination method and also does not include it in the comparison. \n\nTo sum up, I can not recommend the paper to acceptance,  because (a) an important baseline is missing  (b) there are serious writing issues",1,1,1,1,-1,1,1,1,1,-1
H1Nyf7W0Z-R2,"This paper considers a dichitomy between ML and RL based methods for sequence generation.  It is argued that the ML approach has some \""discrepancy\"" between the optimization objective and the learning objective, and the RL approach suffers from bad sample complexity.  An alpha-divergence formulation is considered to combine both methods. \n\nUnfortunately, I do not understand main points made in this paper and am thus not able to give an accurate evaluation of the technical content of this paper.  I therefore have no option but to vote for reject of this paper, based on my educated guess.  \n\nBelow are the points that I'm particularly confused about:\n\n1. For the ML formulation, the paper made several particularly confusing remarks. Some of them are blatantly wrong to me . For example, \n\n1.1 The q(.|.) distribution in Eq. (1) *cannot* really be the true distribution, because the true distribution is unknown and therefore cannot be used to construct estimators.  From the context, I guess the authors mean \""empirical training distribution\""? \n\n1.2 I understand that the ML objective is different from what the users really care about (e.g., blue score), but this does not seem a \""discrepancy\"" to me.  The ML estimator simply finds a parameter that is the most consistent to the observed sequences; and if it fails to perform well in some other evaluation criterion such as blue score, it simply means the model is inadequate to describe the data given, or the model class is so large that the give number of samples is insufficient, and as a result one should change his/her modeling to make it more apt to describe the data at hand.  In summary, I'm not convinced that the fact that ML optimizes a different objective than the blue score is a problem with the ML estimator. \n\nIn addition, I don't see at all why this discrepancy is a discrepancy between training and testing data.  As long as both of them are identically distributed, then no discrepancy exists. \n\n1.3 In point (ii) under the maximum likelihood section, I don't understand it at all and I think both sentences are wrong.  First, the model is *not* trained on the true distribution which is unknown.  The model is trained on an empirical distribution whose points are sampled from the true distribution.  I also don't understand why it is evaluated using p_theta; if I understand correctly, the model is evaluated on a held-out test data, which is also generated from the underlying true distribution. \n\n2. For the RL approach, I think it is very unclear as a formulation of an estimator.  For example, in Eq. (2), what is r and what is y*? It is mentioned that r is a \""reward\"" function, but I don't know what it means and the authors should perhaps explain further.  I just don't see how one obtains an estimated parameter theta from the formulation in Eq. (2), using training examples.",1,-1,1,1,-1,-1,-1,1,-1,-1
H1Nyf7W0Z-R3,"Summary of the paper: \n\nThis paper presents a method, called \\alpha-DM (the authors used this name because they are using \\alpha-Divergence to measure the distance between two distributions),   that addresses three important problems simultaneously: \n(a) Objective score discrepancy: i.e., in ML we minimize a cost function but we measure performance using something else, e.g., minimizing cross entropy and then measuring performance using BLEU score in Machine Translation (MT).  \n(b) Sampling distribution discrepancy: The model is trained using samples from true distribution but evaluated using samples from the learned distribution\n (c) Sample inefficiency: The RL model might rarely draw samples with high rewards which makes it difficult to compute gradients accurately for objective function\u2019s optimization  \n\nThen the authors present the results for machine translation task and also analysis of their proposed method. \n\nMy comments / feedback: \n\nThe paper is well written and the problem addressed by the paper is an important one.  My main concerns about this work are have two aspects: \n(a)\tNovelty\n1.\tThe idea is a good one and is great incremental research building on the top of previous ideas.  I do not agree with statements like \u201cWe demonstrate that the proposed objective function generalizes ML and RL objective functions \u2026\u201d that authors have made in the abstract. There is not enough evidence in the paper to validate this statement. \n(b)\tExperimental Results\n2.\tThe performance of the proposed method is not significantly better than other models in MT task.  I am also wondering why authors have not tried their method on at least one more task?  E.g., in CNN+LSTM based image captioning, the perplexity is minimized as cost function but the performance is measured by BLEU etc.   \n\nSome minor comments: \n\n1.\tIn page 2, 6th line after eq (1), \u201c\u2026 these two problems\u201d --> \u201c\u2026 these three problems\u201d \n2. \tIn page 2, the line before the last line, \u201c\u2026 resolbing problem\u201d --> \u201c\u2026 resolving problem\u201d\n",1,1,1,1,1,-1,1,1,1,-1
H1O0KGC6b-R1,"Summary: \nBased on ideas within the context of kernel theory, the authors consider post-training of NNs as an extra training step, which only optimizes the last layer of the network. \nThis additional step makes sure that the embedding, or representation, of the data is used in the best possible way for the considered task (which is also reflected in the experiments). \n\nAccording to the authors, the contributions are the following:\n1. Post-training step: keeping the rest of the NN frozen (after training), the method trains the last layer in order to \""make sure\"" that the representation learned is used in the most efficient way. \n2. Highlighting connections with kernel techniques and RKHS optimization (like kernel ridge regression). \n3. Experimental results. \n\nClarity:\nThe paper is well-written, the main ideas well-clarified.  \n\nImportance:\nWhile the majority of papers nowadays focuses on the representation part (i.e., how we get to \\Phi_{L-1}(x)), this paper assumes this is given and proposes how to optimize the weights in the final step of the algorithm.  This by itself is not enough to boost the performance universally (e.g., if \\Phi_{L-1} is not well-trained, the problem is deeper than training the last layer); however, it proposes an additional step that can be used in most NN architectures.  From that front (i.e., proposing to do something different than simply training a NN), I find the paper interesting, that might attract some attention at the conference. \n\nOn the other hand, to my humble opinion, the experimental results do not show a significant gain in the performances of all networks (esp. Figure 3 and Table 1 are within the range of statistical error).  In order to state something like this universally, either one needs to perform experiments with more than just MNIST/CIFAR datasets, or even more preferably, prove that the algorithm performs better. \n\nOriginality:\nIt would be great to have some more theory (if any) for the post-training step, or investigate more cases, rather than optimizing only the last layer. \n\nComments:\n1. I assume the authors focused in the last layer of the NN for simplicity, but is there a reason why one might want to focus only on the last layer?  One reason is convexity in W of the problem (2). Any other?  \n\n2. Have the authors considered (even in practice only) to include training of the last 2 layers of the NN?  The authors state this question in the future direction, but it would make the paper more complete to consider it here.\n",1,1,1,1,1,-1,1,1,1,-1
H1O0KGC6b-R2,"This paper proposes to fine-tune the last layer while keeping the others fixed, after initial end-to-end training, viewing the last layer learning under the light of kernel theory (well actually it's just a linear model). \n\nSummary of evaluation\n\nThere is not much novelty in this idea (of optimizing carefully only the last layer as a post-training stage or treating the last layer as kernel machine in a post-processing step), which dates back at least a decade,  so the only real contribution would be in the experiments.  However the experimental setup is questionable as it does not look like the same care has been given to control overfitting with the 'regular training' method. \n\nMore details\n\nPrevious work on the same idea: at least a decade old, e.g., Huang and LeCun 2006. See a review of such work in 'Deep Learning using Linear Support Vector Machines' more recently. \n\nExperiments\n\nYou should also have a weight norm penalty in the end-to-end ('regular training') case and make sure it is appropriately and separately tuned (not necessarily the same value as for the post-training).  Otherwise, the 'improvements' may simply be due to better regularization in one case vs the other, and the experimental curves suggest that interpretation is correct.\n",1,1,1,1,1,1,1,1,-1,-1
H1O0KGC6b-R3,"This paper demonstrate that by freezing all the penultimate layers at the end of regular training improves generalization.  However, the results do not convince this reviewer to switch to using 'post-training'. \n\nLearning features and then use a classifier such as a softmax or SVM is not new and were actually widely used 10 years ago.  However, freezing the layers and continue to train the last layer is of a minor novelty.  The results of the paper show a generalization gain in terms of better test time performance, however, it seems like the gain could be due to the \\lambda term which is added for post-training but not added for the baseline.  c.f. Eq 3 and Eq 4.\nTherefore, it's unclear whether the gain in generalization is due to an additional \\lambda term or from the post-training training itself. \n\nA way to improve the paper and be more convincing would be to obtain the state-of-the-art results with post-training that's not possible otherwise. \n\nOther notes, \n\nRemark 1: While it is true that dropout would change the feature function, to say that dropout 'should not be' applied, it would be good to support that statement with some experiments. \n\nFor table 1, please use decimal points instead of commas.\n",1,1,1,1,1,-1,1,1,1,-1
H1OQukZ0--R1,"Summary of the paper\n---------------------------\nThe paper addresses the issue of online optimization of hyper-parameters customary involved in deep architectures learning.   The covered framework is limited to regularization parameters.  These hyper-parameters, noted $\\lambda$, are updated along the training of model parameters $\\theta$ by relying on the generalization performance (validation error).  The paper proposes a dynamical system including the dynamical update of $\\theta$ and the update of the gradient $y$, derivative of $\\theta$ w.r.t. to the hyper-parameters.  The main contribution of the paper is to propose a way to re-initialize $y$ at each update of $\\lambda$ and a clipping procedure of $y$ in order to maintain the stability of the dynamical system.  Experimental evaluations on synthetic or real datasets are conducted to show the effectiveness of the approach. \n\nComments\n-------------\n- The materials of the paper sometimes may be quite not easy to follow.  Nevertheless the paper is quite well written. \n- The main contributions of the paper can be seen as an incremental version of (Franceschi et al, 2017) based on the proposal in (Luketina et al., 2016) . As such the impact of the contributions appears rather limited even though the experimental results show a better stability of the method compared to competitors. \n- One motivation of the approach is to fix the slow convergence of the method in (Franceschi et al, 2017).  The paper will gain in quality if a theoretical analysis of the speed-up brought by the proposed approach is discussed. \n- The goal of the paper is to address automatically the learning of regularization parameters.  Unfortunately, Algorithm 1 involves several other hyper-parameters (namely clipping factor $r$, constant $c$ or $\\eta$) which choices are not clearly discussed.  It turns that the paper trades a set of hyper-parameters for another one which tuning may be tedious.  This fact weakens the scope of the online hyper-parameter optimization approach. \n- It may be helpful to indicate the standard deviations of the experimental results",1,1,1,1,1,1,1,1,1,-1
H1OQukZ0--R2,"\n# Summary of paper\nThe paper proposes an algorithm for hyperparameter optimization that can be seen as an extension of Franceschi 2017 were some estimates are warm restarted to increase the stability of the method.  \n\n# Summary of review\nI find the contribution to be incremental, and the validation weak.  Furthermore, the paper discusses the algorithm using hand-waiving arguments and lacks the rigor that I would consider necessary on an optimization-based contribution.  None of my comments are fatal, but together with the incremental contribution I'm inclined as of this revision towards marginal reject.  \n\n# Detailed comments\n\n1. The distinction between parameters and hyperparameters (section 3) should be revised.  First, the definition of parameters should not include the word parameters.  Second, it is not clear what \""parameters of the regularization\"" means.  Typically, the regularization depends on both hyperparameters and parameters.  The real distinction between parameters and parameters is how they are estimated: hyperparameters cannot be estimated from the same dataset as the parameters as this would lead to overfitting and so need to be estimated using a different criterion, but both are \""begin learnt\"", just from different datasets. \n\n2. In Section 3.1, credit for the approach of computing the hypergradient by backpropagating through the training procedure is attributed to Maclaurin 2015. This is not correct.  This approach was first proposed in Domke 2012 and refined by Maclaurin 2015 (as correctly mentioned in Maclaurin 2015). \n\n3. Some quantities are not correctly specified. I should not need to guess from the context or related literature what the quantities refer to.  theta_K for example is undefined (although I could understand its meaning from the context) and sometimes used with arguments, sometimes without (i.e., both theta_K(lambda, theta_0) and theta_K are used). \n\n4. The hypothesis are not correctly specified.  Many of the results used require smoothness of the second derivative (e.g., the implicit function theorem) but these are nowhere stated. \n\n5. The algorithm introduces too many hyper-hyperparameters, although the authors do acknowledge this.  While I do believe that projecting into a compact domain is necessary (see Pedregosa 2016 assumption A3), the other parameters should ideally be relaxed or estimated from the evolution of the algorithm. \n\n# Minor\n\nmissing . after \""hypergradient exactly\"". \n\n\""we could optimization the hyperparam-\"" (typo). \n\nReferences:\n Justin  Domke.    Generic  methods  for  optimization-based modeling.  In\nInternational Conference on Artificial Intelligence and Statistics, 2012",1,1,1,1,1,1,1,1,1,1
H1OQukZ0--R3,"Summary of paper:\n\nThis work proposes an extension to an existing method (Franceschi 2017) to optimize regularization hyperparameters.  Their method claims increased stability in contrast to the existing one. \n\nSummary of review:\n\nThis is an incremental change of an existing method.  This is acceptable as long as the incremental change significantly improves results or the paper presents some convincing theoretical arguments.  I did not find either to be the case.  The theoretical arguments are interesting but lacking in rigor.  The proposed method introduces hyper-hyperparameters which may be hard to tune.  The experiments are small scale and it is unclear how much the method improves random grid search.  For these reasons, I cannot recommend this paper for acceptance. \n\nComments:\n1. Paper should cite Domke 2012 in related work section. \n2. Should state and verify conditions for application of implicit function theorem on page 2. \n3. Fix notation on page 3.  Dot is used on the right hand side to indicate an argument but not left hand side for equation after \""with respect to \\lambda\"". \n4. I would like to see more explanation for the figure in Appendix A.  What specific optimization is being depicted?  This figure could be moved into the paper's main body with some additional clarification. \n5. I did not understand the paragraph beginning with \""This poor estimation\"".  Is this just a restatement of the previous paragraph, which concluded convergence will be slow if \\eta is too small? \n6. I do understand the notation used in equation (8) on page 4. Are <, > meant to denote less than/greater than or something else? \n7. Discussion of weight decay on page 5 seems tangential to main point of the paper. Could be reduced to a sentence or two. \n8. I would like to see some experimental verification that the proposed method significantly reduces the dropout gradient variance (page 6), if the authors claim that tuning dropout probabilities is an area they succeed where others don't. \n9. Experiments are unconvincing. First, only one hyperparameter is being optimized and random search/grid search are sufficient for this.  Second, it is unclear how close the proposed method is to finding the optimal regularization parameter \\lambda.  All one can conclude is that it performs slightly better than grid search with a small number of runs.  I would have preferred to see an extensive grid search done to find the best possible \\lambda, then seen how well the proposed method does compared to this. \n10. I would have liked to see a plot of how the value of lambda changes throughout optimization.  If one can initialize lambda arbitrarily and have this method find the optimal lambda, that is more impressive than a method that works simply because of a fortunate initialization. \n\n\nTypos:\n1. Optimization -> optimize (bottom of page 2) \n2. Should be a period after sentence starting \""Several algorithms\"" on page 2. \n3. In algorithm box on page 5, enable_projection is never used.  Seems like warmup_time should also be an input to the algorithm",1,1,1,1,1,1,1,1,-1,1
H1pri9vTZ-R1,"This paper extends the framework of neural networks for finite-dimension to the case of infinite-dimension setting, called deep function machines.  This theory seems to be interesting and might have further potential in applications.",1,-1,-1,1,-1,-1,1,-1,1,-1
H1pri9vTZ-R2,"The main idea of this paper is to replace the feedforward summation\ny = f(W*x + b)\nwhere x,y,b are vectors, W is a matrix\nby an integral\n\\y = f(\\int W \\x + \\b)\nwhere \\x,\\y,\\b are functions, and W is a kernel.  A deep neural network with this integral feedforward is called a deep function machine.  \n\nThe motivation is along the lines of functional PCA: if the vector x was obtained by discretization of some function \\x, then one encounters the curse of dimensionality as one obtains finer and finer discretization.  The idea of functional PCA is to view \\x as a function is some appropriate Hilbert space, and expands it in some appropriate basis.  This way, finer discretization does not increase the dimension of \\x (nor its approximation), but rather improves the resolution.  \n\nThis paper takes this idea and applies it to deep neural networks.  Unfortunately, beyond rather obvious approximation results, the paper does not get major mileage out of this idea.  This approach amounts to a change of basis - and therefore the resolution invariance is not surprising.  In the experiments, results of this method should be compared not against NNs trained on the data directly, but against NNs trained on dimension reduced version of the data (eg: first fixed number of PCA components).  Unfortunately, this was not done. I suspect that in this case, the results would be very similar. \n\n",1,1,1,1,1,-1,1,1,1,-1
H1pri9vTZ-R3,"This paper deals with the problem of learning nonlinear operators using deep learning.  Specifically, the authors propose to extend deep neural networks to the case where hidden layers can be infinite-dimensional.  They give results on the quality of the approximation using these operator networks, and show how to build neural network layers that are able to take into account topological information from data.  Experiments on MNIST using the proposed deep function machines (DFM) are provided.  \n\nThe paper attempts to make progress in the region between deep learning and functional data analysis (FDA). This is interesting.  Unfortunately, the paper requires significant improvements, both in terms of substance and in terms of presentation.  My main concerns are the following:\n\n1) One motivation of DFM is that in many applications data is a discretization of a continuous process and then can be represented by a function.  FDA is the research field that formulated the ideas about the statistical data analysis of data samples consisting of continuous functions, where each function is viewed as one sample element.  This paper fails to consider properly the work in its FDA context.  Operator learning has been already studied in FDA. See for e.g. the problem of functional regression with functional responses.  Indeed the functional model considered in the linear case is very similar to Eq. 2.5 or Eq. 3.2. Moreover, extension to nonparametric/nonlinear situations were also studied.  The authors should add more information about previous work on this topic so that their results can be understood with respect to previous studies.\ n\n2) The computational aspects of DFM are not clear in the paper.  From a practical computational perspective, the algorithm will be implemented on a machine which processes on finite representations of data.  The paper does not clearly provide information about how the functional nature and the infinite dimensional can be handled in practice. In FDA, generally this is achieved via basis function approximations. \n\n3) Some parts of the paper are hard to read. Sections 3 and 4 are not easy to understand.  Maybe adding a section about the notation and developing more the intuition will improve the reading of the manuscript.  \n\n4) The experimental section can be significantly improved.  It will be interesting to compare more DFM with its discrete counterpart.  Also, other FDA approaches for operator learning should be discussed and compared to the proposed approach.\n",1,1,1,1,1,1,1,1,1,-1
H1q-TM-AW-R1,"This paper presents two complementary models for unsupervised domain adaptation (classification task): 1) the Virtual Adversarial Domain Adaptation (VADA) and 2) the Decision-boundary Iterative Refinement Training with a Teacher (DIRT-T).  The authors make use of the so-called cluster assumption, i.e., decision boundaries should not cross high-density data regions.  VADA extends the standard Domain-Adversarial training by introducing an additional objective L_t that measures the target-side cluster assumption violation, namely, the conditional entropy w.r.t. the target distribution.  Since the empirical estimate of the conditional entropy breaks down for non-locally-Lipschitz classifiers, the authors also propose to incorporate virtual adversarial training in order to make the classifier well-behaved.  The paper also argues that the performance on the target domain can be further improved by a post-hoc minimization of L_t using natural gradient descent (DIRT-T) which ensures that the decision boundary changes incrementally and slowly.     \n\nPros:\n+ The paper is written clearly and easy to read \n+ The idea to keep the decision boundary in the low-density region of the target domain makes sense \n+ The both proposed methods seem to be quite easy to implement and incorporate into existing DATNN-based frameworks \n+ The combination of VADA and DIRT-T performs better than existing DA algorithms on a range of visual DA benchmarks \n\nCons:\n- Table 1 can be a bit misleading as the performance improvements may be partially attributed to the fact that different methods employ different base NN architectures and different optimizers \n- The paper deals exclusively with visual domains; applying the proposed methods to other modalities would make this submission stronger \n\nOverall, I think it is a good paper and deserves to be accepted to the conference.  I\u2019m especially appealed by the fact that the ideas presented in this work, despite being simple, demonstrate excellent performance. \n\nPost-rebuttal revision:\nAfter reading the authors' response to my review, I decided to leave the score as is.",1,1,1,1,1,-1,1,1,1,-1
H1q-TM-AW-R2,"As there are many kinds of domain adaptation problems, the need to mix several learning strategies to improve the existing approaches is obvious. However, this task is not necessarily easy to succeed.  The authors proposed a sound approach to learn a proper representation (in an adversarial way) and comply the cluster assumption. \n\nThe experiments show that this Virtual Adversarial Domain Adaptation network (VADA) achieves great results when compared to existing learning algorithms.  Moreover, we also see the learned model is consistently improved using the proposed \""Decision-boundary Iterative Refinement Training with a Teacher\"" (DIRT-T) approach. \n\nThe proposed methodology relies on multiple choices that could sometimes be better studied and/or explained.  Namely, I would like to empirically see which role of the locally-Lipschitz regularization term (Equation 7).  Also, I wonder why this term is tuned by an hyperparameter (lamda_s) for the source, while a single hyperparamer (lambda_t) is used for the sum of the two target quantity. \n \nOn the theoretical side, the discussion could be improved. Namely, Section 3 about \""limitation of domain adversarial training\"" correctly explained that \""domain adversarial training may not be sufficient for domain adaptation if the feature extraction function has high-capacity\"".  It would be interesting to explain whether this observation is consistent with Theorem 1 of the paper (due to Ben-David et al., 2010), on which several domain adversarial approaches are based.  The need to consider supplementary assumptions (such as ) to achieve good adaptation can also be studied through the lens of more recent Ben-David's work, e.g. Ben-David and Urner (2014).  In the latter, the notion of \""Probabilistic Lipschitzness\"", which is a relaxation of the \""cluster assumption\"" seems very related to the actual work.\n\nReference:\nBen-David and Urner.  Domain adaptation-can quantity compensate for quality?, Ann. Math. Artif. Intell.,  2014\n\nPros:\n- Propose a sound approach to mix two complementary strategies for domain adaptation.\n- Great empirical results. \n\nCons:\n- Some choices leading to the optimization problem are not sufficiently explained.\n- The theoretical discussion could be improved. \n\nTypos:\n- Equation 14: In the first term (target loss), theta should have an index t (I think).\n- Bottom of page 6: \""... and that as our validation set\"" (missing word).\n",1,1,1,1,1,1,1,1,1,-1
H1q-TM-AW-R3,"The paper was a good contribution to domain adaptation. It provided a new way of looking at the problem by using the cluster assumption.  The experimental evaluation was very thorough and shows that VADA and DIRT-T performs really well.  \n\nI found the math to be a bit problematic. For example, L_d in (4) involves a max operator. Although I understand what the authors mean, I don't think this is the correct way to write this. (5) should discuss the min-max objective.  This will probably involve an explanation of the gradient reversal etc. Speaking of GRL, it's mentioned on p.6 that they replaced GRL with the traditional GAN objective.  This is actually pretty important to discuss in detail: did that change the symmetric nature of domain-adversarial training to the asymmetric nature of traditional GAN training? Why was that important to the authors? \n\nThe literature review could also include Shrivastava et al. and Bousmalis et al. from CVPR 2017. The latter also had MNIST/MNIST-M experiments.",1,1,1,1,1,1,1,-1,1,-1
H1srNebAZ-R1,"--------------------\nReview updates:\nRating 6 -> 7\nConfidence 2 -> 4\n\nThe rebuttal and update addressed a number of my concerns, cleared up confusing sections, and moved the paper materially closer to being publication-worthy, thus I\u2019ve increased my score.\n----- ---------------\n\nI want to love this paper.  The results seem like they may be very important.  However, a few parts were poorly explained, which led to this reviewer being unable to follow some of the jumps from experimental results to their conclusions.  I would like to be able to give this paper the higher score it may deserve, but some parts first need to be further explained. \n\nUnfortunately, the largest single confusion I had is on the first, most basic set of gradient results of section 4.1.  Without understanding this first result, it\u2019s difficult to decide to what extent the rest of the paper\u2019s results are to be believed. \n\nFig 1 shows \u201cthe histograms of the average sign of partial derivatives of the loss with respect to activations, as collected over training for a random neuron in five different layers. \u201d Let\u2019s consider the top-left subplot of Fig 1, showing a heavily bimodal distribution (modes near -1 and +1.). Is this plot made using data from a single neuron or from  multiple neurons?  For now let\u2019s assume it is for a single neuron, as the caption and text in 4.1 seem to suggest. If it is for a single neuron, then that neuron will have, for a single input example, a single scalar activation value and a single scalar gradient value.  The sign of the gradient will either be +1 or -1. If we compute the sign for each input example and then AGGREGATE over all training examples seen by this neuron over the course of training (or a subset for computational reasons), this will give us a list of signs.  Let\u2019s collect these signs into a long list: [+1, +1, +1, -1, +1, +1, \u2026].  Now what do we do with this list?  As far as I can tell, we can either average it (giving, say, .85 if the list has far more +1 values than -1 values) OR we can show a histogram of the list, which would just be two bars at -1 and +1.  But we can\u2019t do both, indicating that some assumption above was incorrect. Which assumption in reading the text was incorrect? \n\nFurther in this direction, Section 4.1 claims \u201cZero partial derivatives are ignored to make the signal more clear .\u201d Are these zero partial derivatives of the post-relu or pre-relu?  The text (Sec 3) points to activations as being post-relu, but in this case zero-gradients should be a very small set (only occuring if all neurons on the next layer had either zero pre-relu gradients, which is common for individual neurons but, I would think, not for all at once).  Or does this mean the pre-relu gradient is zero, e.g. the common case where the gradient is zeroed because the pre-activation was negative and the relu at that point has zero slope?  In this case we would be excluding a large set (about half!) of the gradient values, and it didn\u2019t seem from the context in the paper that this would be desirable. \n\nIt would be great if the above could be addressed.  Below are some less important comments.\n\nSec 5.1: great results! \n\nFig 3: This figure studies \u201cthe first and last layers of each network\u201d. Is the last layer really the last linear layer, the one followed by a softmax?  In this case there is no relu and the 0 pre-activation is not meaningful (softmax is shift invariant).  Or is the layer shown (e.g. \u201cstage3layer2\u201d) the penultimate layer?  Minor: in this figure, it would be great if the plots could be labeled with which networks/datasets they are from. \n\nSec 5.2 states \u201cneuron partitions the inputs in two distinct but overlapping categories of quasi equal size .\u201d This experiment only shows that this is true in aggregate, not for specific neurons?  I.e. the partition percent for each neuron could be sampled from U(45, 55) or from U(10, 90) and this experiment would not tell us which, correct?  Perhaps this statement could be qualified. \n\nTable 1: \u201c52th percentile vs actual 53 percentile shown\u201d. \n\n> Table 1: The more fuzzy, the higher the percentile rank of the threshold \n\nThis is true for the CIFAR net but the opposite is true for ResNet, right",1,1,1,1,1,-1,1,1,-1,-1
H1srNebAZ-R2,"The paper proposes to study the behavior of activations during training and testing to shed more light onto the inner workings of neural networks.  This is an important area and findings in this paper are interesting! \n\nHowever, I believe the results are preliminary and the paper lacks an adequate explanation/hypothesis for the observed phenomenon either via a theoretical work or empirical experiments. \n- Could we look at the two distributions of inputs that each neuron tries to separate?  \n- Could we perform more extensive empirical study to substantiate the phenomenon here? Under which conditions do neurons behave like binary classifiers? (How are network width/depth, activation functions affect the results). \n\nAlso, a binarization experiment (and finding) similar to the one in this paper has been done here:\n[1] Argawal et al. Analyzing the Performance of Multilayer Neural Networks for Object Recognition.  2014\n\n+ Clarity: The paper is easy to read.  A few minor presentation issues:\n- ReLu --> ReLU\n\n+  Originality: \nThe paper is incremental work upon previous research (Tishby et al. 2017; Argawal et al 2014). \n\n+ Significance:\nWhile the results are interesting,  the contribution is not significant as the paper misses an important explanation for the phenomenon.  I'm not sure what key insights can be taken away from this",1,1,1,1,1,1,1,1,1,-1
H1srNebAZ-R3,"This paper presents an experimental study on the behavior of the units of neural networks.  In particular, authors aim to show that units behave as binary classifiers during training and testing.  \n\nI found the paper unnecessarily longer than the suggested 8 pages.  The focus of the paper is confusing: while the introduction discusses about works on CNN model interpretability, the rest of the paper is focused on showing that each unit behaves consistently as a binary classifier, without analyzing anything in relation to interpretability.   I think some formal formulation and specific examples on the relevance of the partial derivative of the loss with respect to the activation of a unit will help to understand better the main idea of the paper.  Also, quantitative figures would be useful to get the big picture.  For example in Figures 1 and 2 the authors show the behavior of some specific units as examples, but it would be nice to see a graph showing quantitatively the behavior of all the units at each layer.  It would be also useful to see a comparison of different CNNs and see how the observation holds more or less depending on the performance of the network",1,1,1,1,1,-1,1,1,1,-1
H1sUHgb0Z-R1,"This paper proposes a method for learning from noisy labels, particularly focusing on the case when data isn't redundantly labeled (i.e. the same sample isn't labeled by multiple non-expert annotators).  The authors provide both theoretical and experimental validation of their idea.  \n\nPros:\n+ The paper is generally very clearly written.  The motivation, notation, and method are clear. \n+ Plentiful experiments against relevant baselines are included, validating both the no-redundancy and plentiful redundancy cases.  \n+ The approach is a novel twist on an existing method for learning from noisy data.  \n\nCons: \n- All experiments use simulated workers; this is probably common but still not very convincing. \n- The authors missed an important related work which studies the same problem and comes up with a similar conclusion: Lin, Mausam, and Weld. \""To re (label), or not to re (label).\"" HCOMP 2014. \n- The authors should have compared their approach to the \""base\"" approach of Natarajan et al.  \n- It seems too simplistic too assume all workers are either hammers or spammers; the interesting cases are when annotators are neither of these. \n- The ResNet used for each experiment is different, and there is no explanation of the choice of architecture. \n\nQuestions: \n- How would the model need to change to account for example difficulty?  \n- Why are Joulin 2016, Krause 2016 not relevant? \n- Best to clarify what the weights in the weighted sum of Natarajan are.  \n- \""large training error on wrongly labeled examples\"" -- how do we know they are wrongly labeled, i.e. do we have a ground truth available apart from the crowdsourced labels?  Where does this ground truth come from? \n- Not clear what \""Ensure\"" means in the algorithm description. \n- In Sec. 4.4, why is it important that the samples are fresh",1,1,1,1,1,1,1,1,1,-1
H1sUHgb0Z-R2,"This paper focuses on the learning-from-crowds problem when there is only one (or very few) noisy label per item.  The main framework is based on the Dawid-Skene model.  By jointly update the classifier weights and the confusion matrices of workers, the predictions of the classifier can help on the estimation problem with rare crowdsourced labels.  The paper discusses the influence of the label redundancy both theoretically and empirically.  Results show that with a fixed budget, it\u2019s better to label many examples once rather than fewer examples multiple times. \n\nThe model and algorithm in this paper are simple and straightforward.  However, I like the motivation of this paper and the discussion about the relationship between training efficiency and label redundancy.  The problem of label aggregation with low redundancy is common in practice but hardly be formally analyzed and discussed.  The conclusion that labeling more examples once is better can inspire other researchers to find more efficient ways to improve crowdsourcing. \n\nAbout the technique details, this paper is clearly written,  but some experimental comparisons and claims are not very convincing.  Here I list some of my questions:\n+About the MBEM algorithm, it\u2019s better to make clear the difference between MBEM and a standard EM. Will it always converge? What\u2019s its objective? \n+The setting of Theorem 4.1 seems too simple. Can the results be extended to more general settings, such as when workers are not identical? \n+When n = O(m log m), the result that \\epslon_1 is constant is counterintuitive, people usually think large redundancy r can bring benefits on estimation, can you explain more on this? \n+During CIFAR-10 experiments when r=1, each example only have one label. For the baselines weighted-MV and weighted-EM, they can only be directly trained using the same noisy labels. So can you explain why their performance is slightly different in most settings? Is it due to the randomly chosen procedure of the noisy labels? \n+For ImageNet and MS-COCO experiments with a fixed budget, you reduced the training set when increasing the redundancy, which is unfair.  The reduction of performance could mainly cause by seeing fewer raw images, but not the labels.  It\u2019s better to train some semi-supervised model to make the settings more comparable",1,1,1,1,1,-1,1,1,1,-1
H1sUHgb0Z-R3,"The authors proposed a supervised learning algorithm for modeling label and worker quality.  Further utilize it to study one of the important problems in crowdsourcing - How much redundancy is required in crowdsourcing and whether low redundancy with abundant noise examples lead to better labels. \n\nOverall the paper was well written.  The motivation of the work is clearly explained and supported with relevant related work.  The main contribution of the paper is in the bootstrapping algorithm which models the worker quality and labels in an iterative fashion.  Though limited to binary classification, the paper proposed a theoretical framework extending the existing work on VC dimension to compute the upper bound on the risk.  The authors also showed theoretically and empirically on synthetic data sets that the low redundancy and larger set of labels in crowdsourcing gives better results.  \n\nMore detailed comments\n1. Instead of considering multi-class classification as one-vs-all binary classification, can you extend the theoretical guarantee on the risk to multi-class set up like Softmax which is widely used in research nowadays. \n2. Can you introduce the Risk -R in the paper before using it in Theorem 4.1\n3 . Is there any limit on how many examples each worker has to label?  Can you comment more on how to pick that value in real-world settings? Just saying sufficiently many (Section 4.2) is not sufficient. \n4. Under the experiments, different variations of Majority Vote, EM and Oracle correction were used as baselines.  Can you cite the references and also add some existing state-of-the-art techniques mentioned in the related work section. \n5. For the experiments on synthetic datasets, workers are randomly sampled with replacements. Were the scores reported based on average of multiple runs.  If yes, can you please report the error bars. \n6. For the MS-COCO, examples can you provide more detailed results as shown for synthetic datasets? Majority vote is a very weak baseline.  \n\nFor the novel approach and the theoretical backing, I consider the paper to be a good one.  The paper has scope for improvement",1,1,1,1,1,1,1,1,1,-1
H1T2hmZAb-R1,"This paper defines building blocks for complex-valued convolutional neural networks: complex convolutions, complex batch normalisation, several variants of the ReLU nonlinearity for complex inputs, and an initialisation strategy.  The writing is clear, concise and easy to follow. \n\nAn important argument in favour of using complex-valued networks is said to be the propagation of phase information.  However, I feel that the observation that CReLU works best out of the 3 proposed alternatives contradicts this somewhat.  CReLU simply applies ReLU component-wise to the real and imaginary parts, which has an effect on the phase information that is hard to conceptualise . It definitely does not preserve phase, like modReLU would. \n\nThis makes me wonder whether the \""complex numbers\"" paradigm is applied meaningfully here, or whether this is just an arbitrary way of doing some parameter sharing in convnets that happens to work reasonably well (note that even completely random parameter tying can work well, as shown in \""Compressing neural networks with the hashing trick\"" by Chen et al.).  Some more insight into how phase information is used, what it represents and how it is propagated through the network would help to make sense of this. \n\nThe image recognition results are mostly inconclusive, which makes it hard to assess the benefit of this approach.  The improved performance on the audio tasks seems significant, but how the complex nature of the networks helps achieve this is not really demonstrated.  It is unclear how the phase information in the input waveform is transformed into the phase of the complex activations in the network (because I think it is implied that this is what happens). This connection is a bit vague.  Once again, a more in-depth analysis of this phase behavior would be very welcome. \n\nI'm on the fence about this work: I like the ideas and they are explained well,  but I'm missing some insight into why and how all of this is actually helping to improve performance (especially w.r.t. how phase information is used). \n\n\nComments:\n\n- The related work section is comprehensive but a bit unstructured, with each new paragraph seemingly describing a completely different type of work.  Maybe some subsection titles would help make it feel a bit more cohesive. \n\n- page 3: \""(cite a couple of them)\"" should be replaced by some actual references :) \n\n- Although care is taken to ensure that the complex and real-valued networks that are compared in the experiments have roughly the same number of parameters, doesn't the complex version always require more computation on account of there being more filters in each layer?  It would be nice to discuss computational cost as well. \n\n\nREVISION: I have decided to raise my rating from 5 to 7 as I feel that the authors have adequately addressed many of my comments.  In particular, I really appreciated the additional appendix sections to clarify what actually happens as the phase information is propagated through the network. \n\nRegarding the CIFAR results, I may have read over it, but I think it would be good to state even more clearly that these experiments constitute a sanity check, as both reviewer 1 and myself were seemingly unaware of this.  With this in mind, it is of course completely fine that the results are not better than for real-valued networks.\n",1,1,1,1,1,1,1,1,1,-1
H1T2hmZAb-R2,"The paper presents an extensive framework for complex-valued neural networks.  Related literature suggests a variety of motivations for complex valued neural networks: biological evidence, richer representation capacity, easier optimization, faster learning, noise-robust memory retrieval mechanisms and more.  \n\nThe contribution of the current work does not lie in presenting significantly superior results, compared to the traditional real-valued neural networks, but rather in developing an extensive framework for applying and conducting research with complex-valued neural networks.  Indeed, the most standard work nowadays with real-valued neural networks depends on a variety of already well-established techniques for weight initialization, regularization, activation function, convolutions, etc.  In this work, the complex equivalent of many of these basics tools are developed, such as a number of complex activation functions, complex batch normalization, complex convolution, discussion of complex differentiability, strategies for complex weight initialization, complex equivalent of a residual neural network.  \n\nEmpirical results show that the new complex-flavored neural networks achieve generally comparable performance to their real-valued counterparts, on a variety of different tasks.  Then again, the major contribution of this work is not advancing the state-of-the-art on many benchmark tasks,  but constructing a solid framework that will enable stable and solid application and research of these well-motivated models",1,1,1,1,1,1,1,1,1,-1
H1T2hmZAb-R3,"Authors present complex valued analogues of real-valued convolution, ReLU and batch normalization functions.  Their \""related work section\"" brings up uses of complex valued computation such as discrete Fourier transforms and Holographic Reduced Representations.  However their application don't seem to connect to any of those uses and simply reimplement existing real-valued networks as complex valued. \n\nTheir contributions are:\n\n1. Formulate complex valued convolution \n2. Formulate two complex-valued alternatives to ReLU and compare them \n3. Formulate complex batch normalization as a \""whitening\"" operation on complex domain \n4. Formulate complex analogue of Glorot weight normalization scheme \n\nSince any complex valued computation can be done with a real-valued arithmetic, switching to complex arithmetic needs a compelling use-case.  For instance, some existing algorithm may be formulated in terms of complex values, and reformulating it in terms of real-valued computation may be awkward.  However, cases the authors address, which are training batch-norm ReLU networks on standard datasets, are already formulated in terms of real valued arithmetic.  Switching these networks to complex values doesn't seem to bring any benefit, either in simplicity, or in classification performance.",1,1,1,1,-1,1,1,1,1,-1
H1tSsb-AW-R1,"This paper presents methods to reduce the variance of policy gradient using an action dependent baseline.  Such action dependent baseline can be used in settings where the action can be decomposed into factors that are conditionally dependent given the state.  The paper:\n(1) shows that using separate baselines for actions, each of which can depend on the state and other actions is bias-free \n(2) derive the optimal action-dependent baseline, showing that it does not degenerate into state-only dependent baseline, i.e. there is potentially room for improvement over state-only baselines. \n(3) suggests using marginalized action-value (Q) function as a practical baseline, generalizing the use of value function in state-only baseline case. \n(4) suggests using MC marginalization and also using the \""average\"" action to improve computational feasibility \n(5) combines the method with GAE techniques to further improve convergence by trading off bias and variance \n\nThe suggested methods are empirically evaluated on a number of settings.  Overall action-dependent baseline outperform state-only versions.  Using a single average action marginalization is on par with MC sampling, which the authors attribute to the low quality of the Q estimate.  Combining GAE shows that a hint of bias can be traded off with further variance reduction to further improve the performance. \n\nI find the paper interesting and practical to the application of policy gradient in high dimensional action spaces with some level of conditional independence present in the action space.  In light of such results, one might change the policy space to enforce such structure. \n\nNotes:\n- Elaborate further on the assumption made in Eqn 9.  Does it mean that the actions factors cannot share (too many) parameters in the policy construction, or that shared parameters can only be applied to the state? \n- Eqn 11 should use \\simeq. \n- How can the notion of average be extended to handle multi-modal distributions, or categorical or structural actions?  Consider expanding on that in section 4.5. \n- The discussion on the DAG graphical model is lacking experimental analysis (where separate baselines models are needed).  How would you train such baselines? \n- Figure 4 is impossible to read in print.  The fonts are too small for the numbers and the legends",1,1,1,1,1,-1,1,1,1,-1
H1tSsb-AW-R2,"In this paper, the authors investigate variance reduction techniques for agents with multi-dimensional policy outputs, in particular when they are conditionally independent ('factored').  With the increasing focus on applying RL methods to continuous control problems and RTS type games, this is an important problem and this technique seems like an important addition to the RL toolbox.  The paper is well written, the method is easy to implement, and the algorithm seems to have clear positive impact on the presented experiments. \n\n- The derivations in pages 4-6 are somewhat disconnected from the rest of the paper: the optimal baseline derivation is very standard (even if adapted to the slightly different situation situated here), and for reasons highlighted by the authors in this paper, they are not often used; the 'marginalized' baseline is more common, and indeed, the authors adopt this one as well.  In light of this (and of the paper being quite a bit over the page limit)- is this material (4.2->4.4) mostly not better suited for the appendix?  Same for section 4.6 (which I believe is not used in the experiments). \n\n- The experimental section is very strong; regarding the partial observability experiments, assuming actions are here factored as well, I could see four baselines \n(two choices for whether the baseline has access to the goal location or not, and two choices for whether the baseline has access to the vector $a_{-i}$).  It's not clear which two baselines are depicted in 5b - is it possible to disentangle the effect of providing $a_{-i}$ and the location of the hole to the baseline? \n\n(side note: it is an interesting idea to include information not available to the agent as input to the baseline though it does feel a bit 'iffy' ; the agent requires information to train, but is not provided the information to act.   Out of curiosity, is it intended as an experiment to verify the need for better baselines? Or as a 'fair' training procedure? )\n\n- Minor: in equation 2- is the correct exponent not t'?   Also since $\\rho_\\pi$ is define with a scaling $(1-\\gamma)$ (to make it an actual distribution), I believe the definition of $\\eta$ should also be multiplied by $(1-\\gamma)$ (as well as equation 2)",1,1,1,1,1,-1,1,1,1,-1
H1tSsb-AW-R3,"The paper proposes a variance reduction technique for policy gradient methods.  The proposed approach justifies the utilization of action-dependent baselines, and quantifies the gains achieved by it over more general state-dependent or static baselines. \n\n\nThe writing and organization of the paper is very well done.  It is easy to follow, and succinct while being comprehensive.  The baseline definition is well-motivated, and the benefits offered by it are quantified intuitively.  There is only one mostly minor issues with the algorithm development and the experiments need to be more polished.  \n\nFor the algorithm development, there is an relatively strong assumption that z_i^T z_j = 0.  This assumption is not completely unrealistic (for example, it is satisfied if completely separate parts of a feature vector are used for actions).  However, it should be highlighted as an assumption, and it should be explicitly stated as z_i^T z_j = 0 rather than z_i^T z_j approx 0.  Further, because it is relatively strong of an assumption, it should be discussed more thoroughly, with some explicit examples of when it is satisfied. \n\nOtherwise, the idea is simple and yet effective, which is exactly what we would like for our algorithms.  The paper would be a much stronger contribution, if the experiments could be improved.  \n- More details regarding the experiments are desirable - how many runs were done, the initialization of the policy network and action-value function, the deep architecture used etc. \n- The experiment in Figure 3 seems to reinforce the influence of \\lambda as concluded by the Schulman et. al. paper.  While that is interesting, it seems unnecessary/non-relevant here, unless performance with action-dependent baselines with each value of \\lambda is contrasted to the state-dependent baseline.  What was the goal here? \n- In general, the graphs are difficult to read; fonts should be improved and the graphs polished.  \n- The multi-agent task needs to be explained better - specifically how is the information from the other agent incorporated in an agent's baseline? \n- It'd be great if Plot (a) and (b) in Figure 5 are swapped. \n\nOverall I think the idea proposed in the paper is beneficial.  Better discussing the strong theoretical assumption should be incorporated.  Adding the listed suggestions to the experiments section would really help highlight the advantage of the proposed baseline in a more clear manner.  Particularly with some clarity on the experiments, I would be willing to increase the score.  \n\nMinor comments:\n1. In Equation (28) how is the optimal-state dependent baseline obtained?  This should be explicitly shown, at least in the appendix.  \n2. The listed site for videos and additional results is not active. \n3. Some typos\n- Section 2 - 1st para - last line: \""These methods are therefore usually more sample efficient, but can be less stable than critic-based methods.\"". \n- Section 4.1 - Equation (7) - missing subscript i for b(s_t,a_t^{-i}).  \n- Section 4.2 - \\hat{Q} is just Q in many places",1,1,1,1,1,-1,1,1,1,1
H1u8fMW0b-R1,"This paper introduces a machine learning adaptation of the active inference framework proposed by Friston (2010), and applies it to the task of image classification on MNIST through a foveated inspection of images.  It describes a cognitive architecture for the same, and provide analyses in terms of processing compression and \""confirmation biases\"" in the model. \n\u2013 Active perception, and more specifically recognition through saccades (or viewpoint selection) is an interesting biologically-inspired approach and seems like an intuitive and promising way to improve efficiency.  The problem and its potential applications are well motivated. \n\u2013 The perception-driven control formulation is well-detailed and simple to follow. \n\u2013 The achieved compression rates are significant and impressive, though additional demonstration of performance on more challenging datasets would have been more compelling \n\nQuestions and comments:\n\u2013 While an 85% compression rate is significant, 88% accuracy on MNIST seems poor.  A plot demonstrating the tradeoff of \naccuracy for compression (by varying Href or other parameters) would provide a more complete picture of performance.  Knowing baseline performance (without active inference) would help put numbers in perspective by providing a performance bound due to modeling choices. \n\u2013\u00a0What does the distribution of number of saccades required per recognition (for a given threshold) look like over the entire dataset, i.e. how many are dead-easy vs difficult? \n\u2013 Steady state assumption: How can this be relaxed to further generalize to non-static scenes? \n\u2013 Figure 3 is low resolution and difficult to read. \n\nPost-rebuttal comments:\n\nI have revised my score after considering comments from other reviewers and the revised paper.  While the revised version contains more experimental details,  the paper in its present form lacks comparisons to other gaze selection and saliency models which are required to put results in context.  The paper also contains grammatical errors and is somewhat difficult to understand.  Finally, while it proposes an interesting formulation of a well-studied problem , more comparisons and analysis are required to validate the approach",1,1,1,1,1,-1,1,1,1,-1
H1u8fMW0b-R2,"It is rather difficult to evaluate the manuscript. A large part of the manuscript reviews various papers from the active vision domain and subsequently proposes that this can directly be modeled using Friston\u2019s free energy principle, essentially, by \u201canalogy\u201d, as the authors state.  This extends up to page 4. I would argue, that this is quite a stretch, as the free energy principle is essentially blind to the idea of rewards and preferable states such that all tasks are essentially evaluated in terms surprise reduction.  The authors furthermore introduce a simplification of the setting, i.e. that nothing changes in a scene during saccadic exploration, which is rather unusual for active vision problems.  \nThe authors provide some detail about the actual implementation of their model, section 4, but the in depth details required at ICLR are missing.  No comparisons to other gaze selection models or saliency models are given.  \nFurthermore, the manuscript seems to suggest, that the simulation results are somehow related to human vision as it is stated:\n\u201cThe model provides apparently realistic saccades, for they cover the full range of the image and tend to point over regions that contain class-characteristic pixels.\u201d\nbut no actual comparisons or evaluations are provided.",1,1,1,1,-1,1,1,1,-1,-1
H1u8fMW0b-R3,"In this paper, the authors present a computational framework for the active vision problem.  Motivating the study biologically, the authors explain how the control policy can be learned to reduce the entropy of the posterior belief, and present an application (MNIST digit classification) to substantiate their proposal. \n\nI am not convinced about the novelty and contribution of the work.  The active vision/sensing problem has been well studied and both the information theory and Bayes risk formulations have already been considered in previous works (see Najemnik and Geisler, 2005; Butko and Movellan, 2010; Ahmad and Yu, 2013). \n\nThe paper is also rife with spelling mistakes and grammatical errors and needs a thorough revision. Examples: foveate inspection the data (abstract), may allow to (motivation), tu put it clear (motivation), on contrary to animals retina (footnote 1), minimize at most the current uncertainty (perception-driven control), center an keep (fovea-based implementation), degrade te recognition (outlook and perspective).  The citations are in non-standard format (section 1.2: Kalman (1960)). \n\nOverall, I think the paper considers an important problem but the contribution to the state of the art is minimal, and editing highly lacking.  \n\n1. J Najemnik and W S Geisler. Optimal eye movement strategies in visual search. Nature, 434(7031):387\u201391, 2005.\n2. N J Butko and J R Movellan. Infomax control of eye movements. IEEE Transactions on Autonomous Mental Development, 2(2):91\u2013107, 2010.\n3. S Ahmad and A J Yu. Active sensing as Bayes-optimal sequential decision-making. Uncertainty in Artificial Intelligence, 2013.",1,1,1,1,-1,1,1,1,1,1
H1uP7ebAW-R1,"The paper proposes to combine the recently proposed DenseNet architecture with LSTMs to tackle the problem of predicting different pathologic patterns from chest x-rays.  In particular, the use of LSTMs helps take into account interdependencies between pattern labels.  \n\nStrengths:\n- The paper is very well written.  Contextualization with respect to previous work is adequate.  Explanations are clear.  Novelties are clearly identified by the authors. \n- Quantitative improvement with respect to the state the art.  \n\nWeaknesses:\n- The paper does not introduce strong technical novelties -- mostly, it seems to apply previous techniques to the medical domain.  It could have been interesting to know if there are more insights / lessons learned in this process.  This could be of interest for a broader audience.  For instance, what are the implications of using higher-resolution images as input to DenseNet / decreasing the number of layers?  How do the features learned at different layers compare to the ones of the original network trained for image classification?  How do features of networks pre-trained on ImageNet, and then fine-tuned for the medical domain, compare to features learned from medical images from scratch?  \n- The impact of the proposed approach on medical diagnostics is unclear.  The authors could better discuss how the approach could be adopted in practice.  Also, it could be interesting also to discuss how the results in Table 2 and 3 compare to human classification capabilities, and if that performance would be already enough for building a computer-aided diagnosis system. \n\nFinally -- is it expected that the ordering of the factorization in Eq. 3 does not count much (results in Table 3)?  As a non-expert in the field, I'd expect that ordering between pathologic patterns matters more.",1,1,1,1,1,1,1,1,1,-1
H1uP7ebAW-R2,"This paper presents an impressive set of results on predicting lung pathologies from chest x-ray images.  \nAuthors present two architectures: one based on denseNet, and one based on denseNet + LSTM on output dimensions (i.e. similar to NADE model), and compare it to state of the art on the chest x-ray classification.  Experiments are clearly described and results are significantly better compared to state of the art. \n\nThe only issue with this paper is, that their proposed method, in practice is not tractable for inference on estimating probability of a single output, a task which would be critical in medical domain.  Considering that their paper is titled as a work to use \""dependencies\"" among labels, not being able to evaluate their network's, and lack of interpretable evaluation results on this model in the experiment section is a major limitation.  \n\nOn the other hand, there are many alternative models where one could simply use multi-task learning and shared parameter, to predict multiple outcomes extremely efficiently.  To be able to claim that this paper improved the prediction by better modeling of 'dependencies' among labels, I would need to see how the (much simpler) multi-task setting works as well.  \n\nThat said, the paper has several positive aspects in all areas:\n\nOriginality - the paper presents first combination of DenseNets with LSTM-based output factorization, \nWriting clarity - the paper is very well written and clear. \nQuality - (apart from the missing multi-task baseline), the results are significantly better than state of the art, and experiments are well done, \nSignificance - Apart from the issue of intractable inference which is arguably a large limitation of this work",1,1,1,1,1,1,1,1,1,-1
H1uP7ebAW-R3,"Well written and appropriately structured.  Well within the remit of the conference. \nNot much technical novelty to be found,  but the original contributions are adequately identified and they are interesting on their own. \n\nMy main concern (and complaint) is not technical, but application-based.  This study is (unfortunately) typical in that it focuses on and provides detail of the technical modeling issues, but ignores the medical applicability of the model and results.  This is exemplified by the fact that the data set is hardly described at all and the 14 abnormalities/pathologies, the rationale behind their choice and the possible interrelations and dependencies are never described from a medical viewpoint.  If I were a medical expert, I would not have a clue about how these results and models could be applied in practice, or about what medical insight I could achieve. \n\nThe bottom line seems to be: \""my model and approach works better than the other guys' model and approach\"", but one is left with the impression that these experiments could have been made with other data, other problems, other fields of application and they would not have not changed much",1,1,1,1,1,-1,1,1,1,-1
H1uR4GZRZ-R1,"This paper investigates a new approach to prevent a given classifier from adversarial examples.  The most important contribution is that the proposed algorithm can be applied post-hoc to already trained networks.  Hence, the proposed algorithm (Stochastic Activation Pruning) can be combined with algorithms which prevent from adversarial examples during the training. \n\nThe proposed algorithm is clearly described.  However there are issues in the presentation. \n\nIn section 2-3, the problem setting is not suitably introduced. \nIn particular one sentence that can be misleading:\n\u201cGiven a classifier, one common way to generate an adversarial example is to perturb the input in direction of the gradient\u2026\u201d\nYou should explain that given a classifier with stochastic output, the optimal way to generate an adversarial example is to perturb the input proportionally to the gradient.  The practical way in which the adversarial examples are generated is not known to the player.  An adversary could choose any policy.  The only thing the player knows is the best adversarial policy. \n\nIn section 4, I do not understand why the adversary uses only the sign and not also the value of the estimated gradient.  Does it come from a high variance?  If it is the case, you should explain that the optimal policy of the adversary is approximated by \u201cfast gradient sign method\u201d.  \n\nIn comparison to dropout algorithm, SAP shows improvements of accuracy against adversarial examples.  SAP does not perform as well as adversarial training, but SAP could be used with a trained network.  \n\nOverall, this paper presents a practical method to prevent a classifier from adversarial examples, which can be applied in addition to adversarial training.  The presentation could be improved",1,1,1,1,1,-1,1,1,1,-1
H1uR4GZRZ-R2,"This paper propose a simple method for guarding trained models against adversarial attacks.  The method is to prune the network\u2019s activations at each layer and renormalize the outputs.  It\u2019s a simple method that can be applied post-training and seems to be effective.\ n\nThe paper is well written and easily to follow.  Method description is clear.  The analyses are interesting and done well.  I am not familiar with the recent work in this area so can not judge if they compare against SOTA methods but they do compare against various other methods. \n\nCould you elaborate more on the findings from Fig 1.c Seems that  the DENSE model perform best against randomly perturbed images.  Would be good to know if the authors have any intuition why is that the case. \n\nThere are some interesting analysis in the appendix against some other methods, it would be good to briefly refer to them in the main text. \n\nI would be interested to know more about the intuition behind the proposed method.  It will make the paper stronger if there were more content arguing analyzing the intuition and insight that lead to the proposed method. \n\nAlso would like to see some notes about computation complexity of sampling multiple times from a larger multinomial. \n\nAgain I am not familiar about different kind of existing adversarial attacks, the paper seem to be mainly focus on those from Goodfellow et al 2014.  Would be good to see the performance against other forms of adversarial attacks as well if they exist",1,1,1,1,1,1,1,1,1,-1
H1uR4GZRZ-R3,"The authors propose to improve the robustness of trained neural networks against adversarial examples by randomly zeroing out weights/activations.  Empirically the authors demonstrate, on two different task domains, that one can trade off some accuracy for a little robustness -- qualitatively speaking. \n\nOn one hand, the approach is simple to implement and has minimal impact computationally on pre-trained networks.  On the other hand, I find it lacking in terms of theoretical support, other than the fact that the added stochasticity induces a certain amount of robustness.  For example, how does this compare to random perturbation (say, zero-mean) of the weights?  This adds stochasticity as well so why and why not this work?  The authors do not give any insight in this regard. \n\nOverall, I still recommend acceptance (weakly) since the empirical results may be valuable to a general practitioner.  The paper could be strengthened by addressing the issues above as well as including more empirical results (if nothing else)",1,1,1,1,1,-1,1,1,1,-1
H1U_af-0--R1,"The paper proposes to improve the kernel approximation of random features by using quadratures, in particular, stochastic spherical-radial rules.   The quadrature rules have smaller variance given the same number of random features, and experiments show its reconstruction error and classification accuracies are better than existing algorithms. \n\nIt is an interesting paper, but it seems the authors are not aware of some existing works [1, 2] on quadrature for random features.  Given these previous works, the contribution and novelty of the paper is limited. \n\n[1] Francis Bach. On the Equivalence between Kernel Quadrature Rules and Random Feature Expansions. JMLR, 2017.\n[2] Tri Dao, Christopher De Sa, Christopher R\u00e9. Gaussian Quadrature for Kernel Features. NIPS 2017",1,1,1,1,-1,1,1,1,1,-1
H1U_af-0--R2,"The authors offer a novel version of the random feature map approach to approximately solving large-scale kernel problems: each feature map evaluates the \""fourier feature\"" corresponding to the kernel at a set of randomly sampled quadrature points.  This gives an unbiased kernel estimator; they prove a bound its variance and provide experiment evidence that for Gaussian and arc-cos kernels, their suggested qaudrature rule outperforms previous random feature maps in terms of kernel approximation error and in terms of downstream classification and regression tasks.  The idea is straightforward,  the analysis seems correct,  and the experiments suggest the method has superior accuracy compared to prior RFMs for shift-invariant kernels.  The work is original, but I would say incremental, and the relevant literature is cited. \n\nThe method seems to give significantly lower kernel approximation errors,  but the significance of the performance difference in downstream ML tasks is unclear  --- the confidence intervals of the different methods overlap sufficiently to make it questionable whether the relative complexity of this method is worth the effort.  Since good performance on downstream tasks is the crucial feature that we want RFMs to have, it is not clear that this method represents a true improvement over the state-of-the-art.  The exposition of the quadrature method is difficult to follow, and the connection between the quadrature rules and the random feature map is never explicitly stated: e.g. equation 6 says how the kernel function is approximated as an integral, but does not give the feature map that an ML practitioner should use to get that approximate integral. \n\nIt would have been a good idea to include figures showing the time-accuracy tradeoff of the various methods, which is more important in large-scale ML applications than the kernel approximation error.  It is not clear that the method is *not* more expensive in practice than previous methods (Table 1 gives superior asymptotic runtimes, but I would like to see actual run times, as evaluating the feature maps sound relatively complicated compared to other RFMs).  On a related note, I would also like to have seen this method applied to kernels where the probability density in the Bochner integral was not the Gaussian density (e.g., the Laplacian kernel): the authors suggested that their method works there as well when one uses a Gaussian approximation of the density (which is not clear to me) ,  --- and it may be the case that sampling from their quadrature distribution is faster than sampling from the original non-Gaussian density.",1,1,1,1,1,1,1,1,1,-1
H1U_af-0--R3,"This paper shows that techniques due to Genz & Monahan (1998) can be used to achieve low kernel approximation error under the framework of random fourier feature. \n\nPros\n\n1. It is new to apply quadrature rules to improve kernel approximation.  The only other work I found is\nGaussian Quadrature for Kernel Features NIPS 2017. \nThe work is pretty recent so the author might not know it when submitting the paper.  But in either case, it will be good to discuss the connections. \n\n2. The proposed method is shown to outperform a few baselines empirically. \n\nCons\n\n1. I don\u2019t find the theoretical analysis to be very useful.  In particular, the theorem shows that the kernel approximation error is O(1/D), which is the same as the original RFF paper.  Unless the paper can provide a better characterization of the constants (like the ORF paper), it does not provide much insight in the proposed method.  Unlike deep neural networks, since RFF is such a simple model, I think providing precise theoretical understanding is crucial.  \n\n2. Approximating an integral is a well-studied topic. I do not find a good discussion on all the possible methods.  Why is Genz & Monahan 1998 better than other alternatives such as Monte-Carlo, QMC etc?  One argument seems to be \u201cfor kernels with specific specific integrand one can improve on its properties\u201d. But this trick can be used for Monte-Carlo as well. And I do not see benefit of this trick in the curves. \n\n3. When choosing the orthogonal matrix, I think one obvious choice is to sample a matrix from the Stiefel manifold (the Q matrix of a random Gaussian). This baseline should be added in additional to H and B. \n\n4. A wall-time experiment is needed to justify the speedup. \n\nMinor comments:\n\u201cFor kennels with q(w) other than Gaussian\u2026 obtain very accurate results with little effort by using Gaussian approximation of q(w)\u201d.  What is the citation of this in the kernel approximation context?",1,1,1,1,1,1,1,1,1,1
H1vCXOe0b-R1,"The paper intends to interpret a well-trained multi-class classification deep neural network by discovering the core units of one or multiple hidden layers for prediction making.  However, these discovered core units are specific to a particular class, which are retained to maintain the deep neural network\u2019s ability to separate that particular class from the other ones.  Thus, these non-core units for a particular class could be core units for separating another class from the remaining ones.  Consequently, the aggregation of all class-specific core units could include all hidden units of a layer.  Therefore, it is hard for me to understand what\u2019s the motivation to identify the core units in a one-vs-remaining manner.  At this moment, these identified class-specific core units are useful for neither reducing the size of the network, nor accelerating computation.",1,-1,1,1,1,-1,1,1,-1,-1
H1vCXOe0b-R2,"Pros\n- The paper proposes a novel formulation of the problem of finding hidden units\n  that are crucial in making a neural network come up with a certain output. \n- The method seems to be work well in terms of isolating a few hidden units that\n  need to be kept while preserving classification accuracy.\ n\nCons\n- Sections 3.1 and 3.2 are hard to understand.  There seem to be inconsistencies\n  in the notation.  For example,\n(1) It would help to clarify whether y^b_n is the prediction score or its\ntransformation into [0, 1].  The usage is inconsistent. \n(2) It is not clear how \""y^b_n can be expressed as \\sum_{k=1}^K z_{nk}f_k(x_n)\""\nin general.  This is only true for the penultimate layer, and when y^b_n denotes\nthe input to the output non-linearity.  However, this analysis seems to be\napplied for any hidden layer and y^b_n is the output of the non-linearity unit \n(\""The new prediction scores are transformed into a scalar ranging from 0 to 1,\ndenoted as y^b_n.\ "")\n(3) Section 3.1 denotes the DNN classifier as F(.), but section 3.2 denotes the\nsame classifier as f(.).\n(4) Why is r_n called the \""center\"" ?  I could not understand in what sense is\nthis the center, and of what ? It seems that the max value has been subtracted\nfrom all the logits into a softmax (which is a fairly standard operation). \n\n- The analysis seems to be about finding neurons that contribute evidence for\n  a particular class.  This does not address the issue of understanding why the\nnetwork makes a certain prediction for a particular input.  Therefore this\napproach will be of limited use.\ n\n- The paper should include more analysis of how this method helps interpret the\n  actions of the neural net, once the core units have been identified. \nCurrently, the focus seems to be on demonstrating that the classifier\nperformance is maintained as a significant fraction of hidden units are masked. \nHowever, there is not enough analysis on showing whether and how the identified\nhidden units help \""interpret\"" the model.\ n\nQuality\nThe idea explored in the paper is interesting and the experiments are described\nin enough detail.  However, the writing still needs to be polished. \n\nClarity\nThe problem formulation and objective function (Section 3.1) was hard to follow.\ n\nOriginality\nThis approach to finding important hidden units is novel .\n\nSignificance\nThe paper addresses an important problem of trying to have more interpretable\nneural networks.  However, it only identifies hidden units that are important for\na class, not what are important for any particular input.   Moreover, the main\nthesis of the paper is to describe a method that helps interpret neural network\nclassifiers.  However, the experiments only focus on identifying important hidden\nunits and fall short of actually providing an interpretation using these hidden\nunits.",1,1,1,1,1,-1,1,1,1,-1
H1vCXOe0b-R3,"The paper develops a technique to understand what nodes in a neural network are important\nfor prediction.  The approach they develop consists of using an Indian Buffet Process \nto model a binary activation matrix with number of rows equal to the number of examples.  \nThe binary variables are estimated by taking a relaxed version of the \nasymptotic MAP objective for this problem.  One question from the use of the \nIndian Buffet Process: how do the asymptotics of the feature allocation determine \nthe number of hidden units selected?  \n\nOverall, the results didn't warrant the complexity of the method.  The results are neat, but \nI couldn't tell why this approach was better than others. \n\nLastly, can you intuitively explain the additivity assumption in the distribution for p(y')",1,1,1,1,1,-1,1,1,1,-1
H1vEXaxA--R3,,1,1,1,1,1,1,1,1,1,-1
H1VGkIxRZ-R1,"\n-----UPDATE------\n\nThe authors addressed my concerns satisfactorily.  Given this and the other reviews I have bumped up my score from a 5 to a 6. \n\n----------------------\n\n\nThis paper introduces two modifications that allow neural networks to be better at distinguishing between in- and out- of distribution examples: (i) adding a high temperature to the softmax, and (ii) adding adversarial perturbations to the inputs.  This is a novel use of existing methods. \n\nSome roughly chronological comments follow:\n\nIn the abstract you don't mention that the result given is when CIFAR-10 is mixed with TinyImageNet. \n\nThe paper is quite well written aside from some grammatical issues.  In particular, articles are frequently missing from nouns.  Some sentences need rewriting (e.g. in 4.1 \""which is as well used by Hendrycks...\"", in 5.2 \""performance becomes unchanged\""). \n\n It is perhaps slightly unnecessary to give a name to your approach (ODIN) but in a world where there are hundreds of different kinds of GANs you could be forgiven. \n\nI'm not convinced that the performance of the network for in-distribution images is unchanged, as this would require you to be able to isolate 100% of the in-distribution images.  I'm curious as to what would happen to the overall accuracy if you ignored the results for in-distribution images that appear to be out-of-distribution (e.g. by simply counting them as incorrect classifications).  Would there be a correlation between difficult-to-classify images, and those that don't appear to be in distribution? \n\nWhen you describe the method it relies on a threshold delta which does not appear to be explicitly mentioned again. \n\nIn terms of experimentation it would be interesting to see the reciprocal of the results between two datasets.  For instance, how would a network trained on TinyImageNet cope with out-of-distribution images from CIFAR 10?\n\nSection 4.5 felt out of place, as to me, the discussion section flowed more naturally from the experimental results.  This may just be a matter of taste.\ n\nI did like the observations in 5.1 about class deviation, although then, what would happen if the out-of-distribution dataset had a similar class distribution to the in-distribution one?  (This is in part, addressed in the CIFAR80 20 experiments in the appendices). \n\nThis appears to be a borderline paper, as I am concerned that the method isn't sufficiently novel (although it is a novel use of existing methods). \n\nPros:\n- Baseline performance is exceeded by a large margin\n- Novel use of adversarial perturbation and temperature\n- Interesting analysis \n\nCons:\n- Doesn't introduce and novel methods of its own\n- Could do with additional experiments (as mentioned above)\n- Minor grammatical erro",1,1,1,1,1,-1,1,1,1,-1
H1VGkIxRZ-R2,"The paper proposes a new method for detecting out of distribution samples.  The core idea is two fold: when passing a new image through the (already trained) classifier, first preprocess the image by adding a small perturbation to the image pushing it closer to the highest softmax output and second, add a temperature to the softmax.  Then, a simple decision is made based on the output of the softmax of the perturbed image - if it is able some threshold then the image is considered in-distribution otherwise out-distribution. \n\nThis paper is well written, easy to understand and presents a simple and apparently effective method of detecting out of distribution samples.  The authors evaluate on cifar-10/100 and several out of distribution datasets and this method outperforms the baseline by significant margins.  They also examine the effects of the temperature and step size of the perturbation.  \n \nMy only concern is that the parameter delta (threshold used to determine in/out distribution) is not discussed much.  They seem to optimize over this parameter, but this requires access to the out of distribution set prior to the final evaluation.  Could the authors comment on how sensitive the method is to this parameter?  How much of the out of distribution dataset is used to determine this value, and what are the effects of this size during tuning?  What happens if you set the threshold using one out of distribution dataset and then evaluate on a different one?  This seems to be the central part missing to this paper  and if the authors are able to address it satisfactorily I will increase my score.",1,1,1,1,1,-1,1,1,1,-1
H1VGkIxRZ-R3,"Detecting out of distribution examples is important since it lets you know when neural network predictions might be garbage.  The paper addresses this problem with a method inspired by adversarial training, and shows significant improvement over best known method, previously published in ICLR 2017. \n\nPrevious method used at the distribution of softmax scores as the measure.  Highly peaked -> confidence, spread out -> out of distribution.  The authors notice that in-distribution examples are also examples where it's easy to drive the confidence up with a small step.  The small step is in the direction of gradient when top class activation is taken as the objective.  This is also the gradient used to determine influence of predictors, and it's the gradient term used for adversarial training \""fast gradient sign\"" method. \n\nTheir experiments show improvement across the board using DenseNet on collection of small size dataset (tiny imagenet, cifar, lsun).  For instance at 95% threshold (detect 95% of out of distribution examples), their error rate goes down from 34.7% for the best known method, to 4.3% which is significant enough to prefer their method to the previous work",1,1,1,1,-1,1,1,-1,1,-1
H1VjBebR--R2,,1,1,1,1,1,1,1,1,1,-1
H1VjBebR--R3,,1,1,1,1,1,1,1,1,1,-1
H1WgVz-AZ-R1,,1,1,1,1,1,-1,1,1,1,-1
H1WgVz-AZ-R2,,1,1,1,1,1,1,1,1,-1,-1
H1WgVz-AZ-R3,"This paper proposes an improvement in the speed of training/inference with structured prediction energy networks (SPENs) by replacing the inner optimization loop with a network trained to predict its outputs. \n\nSPENs are an energy-based structured prediction method, where the final prediction is obtained by optimizing min_y E_theta(f_phi(x), y), i.e., finding the label set y with the least energy, as computed by the energy function E(), using a set of computed features f_phi(x) which comes from a neural network.  The key innovation in SPENs was representing the energy function E() as an arbitrary neural network which takes the features f(x) and candidate labels y and outputs a value for the energy.  At inference time y can be optimized by gradient descent steps.  SPENs are trained using maximum-margin loss functions, so the final optimization problem is max -loss(y, y') where y' = argmin_y E(f(x), y). \n\nThe key idea of this paper is to replace the minimization of the energy function min_y E(f(x), y) with a neural network which is trained to predict the resulting output of this minimization.  The resulting formulation is a min-max problem at training time with a striking similarity to the GAN min-max problem, where the y-predicting network learns to predict labels with low energy (according to the E-computing network) and high loss while the energy network learns to assign a high energy to predicted labels which have a higher loss than true labels (i.e. the y-predicting network acts as a generator and the E-predicting network acts as a discriminator).\n\nThe paper explores multiple loss functions and techniques to train these models.  They seem rather finnicky, and the experimental results aren't particularly strong when it comes to improving the quality over SPENs but they have essentially the same test-time complexity as simple feedforward models while having accuracy comparable to full inference-requiring energy-based models.  The improved understanding of SPENs and potential for further work justify accepting this paper",1,1,1,1,-1,-1,1,1,1,-1
H1wt9x-RW-R1,"This is a well written paper on a compelling topic: how to train \""an automated teacher\"" to use intuitive strategies  that would also apply to humans.  \n\nThe introduction is fairly strong, but this reviewer wishes that the authors would have come up with an intuitive example that illustrates why the strategy \""1) train S on random exs; 2) train T to pick exs for S\"" makes sense.  Such an example would dramatically improve the paper's readability. \n\nThe paper appears to be original, and the related work section is quite extensive. \n\nA second significant improvement would be to add an in-depth  running example in section 3, so that the authors could illustrate why the BR strategy makes sense (Algorithm 2)",1,1,1,1,1,1,1,1,1,-1
H1wt9x-RW-R2,"The authors define a novel method for creating a pair of models, a student and a teacher model, that are co-trained in a manner such that the teacher provides useful examples to the student to communicate a concept that is interpretable to people.  They do this by adapting a technique from computational cognitive science called rational pedagogy.  Rather than jointly optimize the student and teacher (as done previously), they have form a coupled relation between the student and teacher where each is providing a best response to the other.  The authors demonstrate that their method provides interpretable samples for teaching in commonly used psychological domains and conduct human experiments to argue it can be used to teach people in a better manner than random teaching.  \n\nUnderstanding how to make complex models interpretable is an extremely important problem in ML for a number of reasons (e.g., AI ethics, explainable AI).  The approach proposed by the authors is an excellent first step in this direction, and they provide a convincing argument for why a previous approach (joint optimization) did not work.  It is an interesting approach that builds on computational cognitive science research and the authors provide strong evidence their method creates interpretable examples.  They second part of their article, where they test the examples created by their models using behavioral experiments was less convincing.  This is because they used the wrong statistical tests for analyzing the studies and it is unclear whether their results would stand with proper tests (I hope they will! \u2013 it seems clear that random samples will be harder to learn from eventually, but I also hoped there was a stronger baseline.). \n\nFor analysis, the authors use t-tests directly on KL-divergence and accuracy scores; however, this is inappropriate (see Jaeger, 2008; Categorical data analysis: Away from ANOVAs (transformation or not) and towards logit mixed models.  Journal of Memory and Language, 59(4), 434-446.).  This is especially applicable to the accuracy score results and the authors should reanalyze their data following the paper referenced above.  With respect to KL-divergence, a G-test can be used (see https://en.wikipedia.org/wiki/G-test#Relation_to_Kullback.E2.80.93Leibler_divergence).  I suspect the results will still be meaningful, but the appropriate analysis is essential to be able to interpret the human results.  \n\nAlso, a related article: One article testing rational pedagogy in more ML contexts and using it to train ML models that is\nHo, M. K., Littman, M., MacGlashan, J., Cushman, F., & Austerweil, J. L. (NIPS 2016).  Showing versus Doing. Teaching by Demonstration. \n\nFor future work, it would be nice to show that the technique works for finding interpretable examples in more complex deep learning networks, which motivated the current push for explainable AI in the first place",1,1,1,1,1,1,1,1,1,1
H1wt9x-RW-R3,"This paper looks at a specific aspect of the learning-to-teach problem, where the learner is assumed to have a teacher that selects training examples for the student according to a strategy.  The teacher's strategy should also be  learned from data.   In this case the authors look at finding interpretable teaching strategies.   The authors define the \""good\"" strategies as similar to intuitive strategies (based on human intuition about the structure of the domain) or strategies that are effective for teaching humans.   \nThe suggested method follow an iterative process in which the student and teacher are interchangeably used.  At each iteration the teacher generates  examples based on the students current concept.  \n\nI found it very difficult to follow the claims in the paper.  Why is it assumed that human intuition is necessarily good?   The experiments do not answer these questions, but are designed to show that the suggested approach follows human intuition.  There are not enough details to get a good grasp of the suggested method and the different choices for it,  and similarly the experiments are not described in a very convincing way.  Specifically - the domains picked seem very contrived,  there actual results are not reported, the size of the data seems minimal so it's not clear what is actually learned. \nHow would you analyze the teaching strategy in realistic cases, where there is no simple intuitive strategy? This would be more convincing.",1,1,1,1,-1,-1,1,1,-1,-1
H1Y8hhg0b-R1,"This paper presents a continuous surrogate for the ell_0 norm and focuses on its applications in regularized empirical regularized minimization.  The proposed continuous relaxation scheme allows for gradient based-stochastic optimization for binary discrete variables under the reparameterization trick, and extends the original binary concrete distribution by allowing the parameter taking values of exact zeros and ones, with additional stretching and thresholding operations.  Under a compound construction of sparsity, the proposed approach can easily incorporate group sparsity by sharing supports among the grouped variables, or be combined with other types of regularizations on the magnitude of non-zero components.  The efficacy of the proposed method in sparsification and speedup is demonstrated in two experiments with comparisons against a few baseline methods.  \n\nPros: \n\n- The paper is clearly written, self-contained and a pleasure to read.  \n- Based on the evidence provided, the procedure seems to be a useful continuous relaxation scheme to consider in handling optimization with spike and slab regularization \n\nCons: \n\n- It would be interesting to see how the induced penalty behaves in terms shrinkage comparing against ell_0 and other ell_p choices  \n- It is unclear what properties does the proposed hard-concrete distribution have, e.g., closed-form density, convexity, etc.    \n- If the authors can offer a rigorous analysis on the influence of base concrete distribution and provide more guidance on how to choose the stretching parameters in practice, this paper would be more significant\n",1,1,1,1,1,-1,1,1,1,-1
H1Y8hhg0b-R2,"Learning sparse neural networks through L0 regularisation\ n\nSummary: \n\nThe authors introduce a gradient-based approach to minimise an objective function with an L0 sparse penalty.  The problem is relaxed onto a continuous optimisation by changing an expectation over discrete variables (representing whether a variable is present or not) to an expectation over continuous variables, inspired by earlier work from Maddison et al (ICLR 2017) where a similar transformation was used to learn over discrete variable prediction tasks with neural networks . Here the application is to learn sparse feedforward networks in standard classification tasks, although the framework described is quite general and could be used to impose L0 sparsity to any objective function in principal.  The method provides equivalent accuracy and sparsity to published state-of-the-art results on these datasets  but it is argue that learning sparsity during the training process will lead to significant speed-ups - this is demonstrated by comparing to a theoretical benchmark (standard training with dropout) rather than through empirical testing against other implementations . \n\nPros:\n\nThe paper is well written and the derivation of the method is easy to follow with a good explanation of the underlying theory.  \n\nOptimisation under L0 regularisation is a difficult and generally important topic and certainly has advantages over other sparse inference objective functions that impose shrinkage on non-sparse parameters.  \n\nThe work is put in context and related to some previous relaxation approaches to sparsity.  \n\nThe method allows for sparsity to be learned during training rather than after training (as in standard dropout approaches) and this allows the algorithm to obtain significant per-iteration speed-ups, which improves through training.  \n\nCons:\n\nThe method is applied to standard neural network architectures and performance in terms of accuracy and final achieved sparsity is comparable to the state-of-the-art methods . Therefore the main advance is in terms of learning speed to obtain this similar performance.  However, the learning speed-up is presented against a theoretical FLOPs estimate per iteration for a similar network with dropout . It would be useful to know whether the number of iterations to achieve a particular performance is equivalent for all the different architectures considered,e.g. does the proposed sparse learning method converge at the same rate as the others?  I felt a more thorough experimental section would have greatly improved the work, focussing on this learning speed aspect.  \n\nIt was unclear how much tuning of the lambda hyper-parameter, which tunes the sparsity, would be required in a practical application since tuning this parameter would increase computation time.  It might be useful to provide a full Bayesian treatment so that the optimal sparsity can be chosen through hyper-parameter learning.  \n\nMinor point: it wasn\u2019t completely clear to me why the fact (3) is a variational approximation to a spike-and-slab is important (Appendix).  I don\u2019t see why the spike-and-slab is any more fundamental than the L0 norm prior in (2),  it is just more convenient in Bayesian inference because it is an iid prior and potentially allows an informative prior over each parameter.  In the context here this didn\u2019t seem a particularly relevant addition to the paper.",1,1,1,1,1,1,1,1,1,-1
H1Y8hhg0b-R3,"The paper introduces a technique for optimizing an L0 penalty on the weights of a neural network.  The basic problem is empirical risk minimization with a incremental penalty for each non zero weight . To tackle this problem, this paper proposes an expected surrogate loss that is then relaxed using a method related to recently introduced relaxations of discrete random variables.  The authors note that this loss can also be seen as a specific variational bound of a Bayesian model over the weights.  The key advantage of this method is that it gives a training time technique for sparsifying neural network computation, leading to potential wins in computation time during training.  \n\nThe results presented in the paper are convincing . They achieve results competitive with previous methods, with the additional advantage that their sparse models are available during training time.  They show order of magnitude reductions in computation time for small models, and more modest constant improvements for large models.  The hard concrete distribution is a small but nice contribution on its own. \n\nMy only concern is the lack of discussion on the relationship between this method and Concrete Dropout (https://arxiv.org/abs/1705.07832).  Although the focus is apparently different, these methods are clearly closely related . A discussion of this relationship seems really important.\ n\nSpecific comments/questions:\n- The reduction of computation time is the key advantage, and it would have been nice to see a more thorough investigation of this.  For example, it would have been interesting to see whether this method would work with structured L0 penalties that removed entire units (as opposed to single weights) or other subsets of the computation.  This would give a stronger sense of the kind of wins that are possible in this framework .\n- Hard concrete is a nice contribution, but there are clearly many possibilities for these relaxations.  Extra evaluations of different relaxations would be appreciated.  At the very least a comparison to concrete would be nice. \n- In equation 2, the equality of the L0 norm with the sum of z assumes that tilde{theta} is not",1,1,1,1,1,1,1,1,1,-1
H1Yp-j1Cb-R1,"It is well known that the original GAN (Goodfellow et al.) suffers from instability and mode collapsing . Indeed, existing work has pointed out that the standard GAN training process may not converge if we insist on obtaining pure strategies (for the minmax game).  The present paper proposes to obtain mixed strategy through an online learning approach.  Online learning (no regret) algorithms have been used in finding an equilibrium for zero sum game. However, most theoretical convergence results are known for convex-concave loss.  One interesting theoretical contribution of the paper is to show that convergence result can be proved if one player is a shallow network (and concave in M) .In particular, the concave player plays the FTRL algorithm with standard L2 regularization term.  The regret of concave player can be bounded using existing result for FTRL. The regret for the other player is more interesting: it uses the fact the adversary's strategy doesn't change too drastically.  Then a lemma by Kalai and Vempala can be used.  The theory part of the paper is reasonable and quite well written.  \n\nBased on the theory developed, the paper presents a practical algorithm.  Compared to the standard GAN training, the new algorithm returns mixed strategy and examine several previous models (instead of the latest) in each iteration.  The paper claims that this may help to prevent model collapsing. \n\nHowever, the experimental part is less satisfying.  From figure 2, I don't see much advantage of Checkhov GAN.  In other experiments, I don't see much improvement neither (CIFAR10 and CELEBA). The paper didn't really compare other popular GAN models, especially WGAN and its improved version , which is already quite popular by now and should be compared with. \n\nOverall, I think it is a borderline paper. \n\n-------------------------\nI read the response and the new experimental results regarding WGAN. \nThe experimental results make more sense now. \nIt would be interesting to see whether the idea can be applied to more recent GAN models and still perform better .\nI raised my score to 7",1,1,1,1,1,1,1,1,1,-1
H1Yp-j1Cb-R2,"This is an interesting paper, exploring GAN dynamics using ideas from online learning, in particular the pioneering \""sparring\"" follow-the-regularized leader analysis of Freund and Schapire (using what is listed here as Lemma 4).  By restricting the discriminator to be a single layer, the maximum player plays over a concave (parameter) space which stabilizes the full sequence of losses so that Lemma 3 can be proved, allowing proof of the dynamics' convergence to a Nash equilibrium.  The analysis suggests a practical (heuristic) algorithm incorporating two features which emerge from the theory: L2 regularization and keeping a history of past models.  A very simple queue for the latter is shown to do quite competitively in practice. \n\nThis paper merits acceptance on theoretical merits alone, because the FTRL analysis for convex-concave games is a very robust tool from theory (see also the more recent sequel [Syrgkanis et al. 2016 \""Fast convergence of regularized learning in games\""]) that is natural to employ to gain insight on the much more brittle GAN case.  The practical aspects are also interesting, because the incorporation of added randomness into the mixed generation strategy is an area where theoretical justifications do motivate practical performance gains; these ideas could clearly be developed in future work",1,1,1,1,1,1,1,1,1,-1
H1Yp-j1Cb-R3,"The paper applies tools from online learning to GANs.  In the case of a shallow discriminator, the authors proved some results on the convergence of their proposed algorithm (an adaptation of FTRL) in GAN games, by leveraging the fact that when D update is small, the problem setup meets the ideal conditions for no-regret algorithms.  The paper then takes the intuition from the semi-shallow case and propose a heuristic training procedure for deep GAN game.  \n\nOverall the paper is very well written.  The theory is significant to the GAN literature, probably less so to the online learning community.  In practice, with deep D, trained by single gradient update steps for G and D, instead of the \""argmin\"" in Algo 1., the assumptions of the theory break.  This is OK as long as sufficient experiment results verify that the intuitions suggested by the theory still qualitatively hold true.  However, this is where I have issues with the work:\n\n1) In all quantitative results, Chekhov GAN do not significantly beat unrolled GAN.  Unrolled GAN looks at historical D's through unrolled optimization, but not the history of G.  So this lack of significant difference in results raise the question of whether any improvement of Chekhov GAN is coming from the online learning perspective for D and G, or simply due to the fact that it considers historical D models (which could be motivated by sth other than the online learning theory). \n\n2) The mixture GAN approach suggested in Arora et al. (2017) is very related to this work, as acknowledged in Sec. 2.1, but no in-depth analysis is carried out.  I suggest the authors to either discuss why Chekhov GAN is obviously superior and hence no experiments are needed, or compare them experimentally.  \n\n3) In the current state, it is hard to place the quantitative results in context with other common methods in the recent literature such as WGAN with gradient penalty.  I suggest the authors to either report some results in terms of inception scores on cifar10 with similar architectures used in other methods for comparison.  Alternatively please show WGAN-GP and/or other method results in at least one or two experiments using the evaluation methods in the paper.  \n\nIn summary, almost all the experiments in the paper are trying to establish improvement over basic GAN, which would be OK if the gap between theory and practice is small.  But in this case, it is not.  So it is not entirely convincing that the practical Algo 2 works better for the reason suggested by the theory, nor it drastically improves practical results that it could become the standard technique in the literature",1,1,1,1,1,1,1,1,1,-1
H1zRea1Mf-R1,"This paper proposed an end-to-end network that generates computer tokens from a single GUI screenshot as input.  \n\nEven though the introduced dataset in this paper is interesting, there are some issues:\n- On the model side, this paper used the same architecture as the Karpathy & Fei-Fei (2015).  As a result, in my view, the paper has limited novelty and originality. \n- Experiments: The experiments are not enough at all, More specifically, the paper didn't establish any baseline to show the difficulty of this problem and the dataset.  Without a reasonable baseline, it is hard to see how difficult is this dataset and this problem and can't say anything about the significance of this problem in this paper. \n- The related works: Some recent papers in program synthesis are missing and should have been included in this paper such as:\nRobustFill: Neural Program Learning under Noisy I/O, ICML 2017 \n\nSome other comments:\n- In the last paragraph of section 3 and first paragraph section 3.1, the authors mentioned that \""...CNN to perform unsupervised feature learning...\"" . In my view, it is not a correct statement to call the feature extraction from a CNN unsupervised feature learning as the referred CNNs in this paper are trained in a supervised manner and the network in this paper is trained using supervised learning as well.      \n \n\nUnfortunately, At this point, I do not see a sufficient contribution to warrant publication in ICLR.",1,1,1,1,-1,1,1,1,1,-1
H1zRea1Mf-R2,"The paper studies the problem of inputting a screenshot of a user interface and outputting code that can be used to generate the interface . Similar to image captioning systems, the image is processed with a CNN and an LSTM is used to output tokens one at a time . Experiments are performed on three new synthetic datasets of user interfaces for iOS, Android, and HTML/CSS, which will be publicly released. \n\nPros:\n- Generating programs with neural networks is an exciting direction \n- Novel task of generating UI code from UI screenshots \n- Three new datasets of UI images and corresponding code\ n- Paper is clearly written\ n\nCons:\n- Limited technical novelt y\n- Limited experiments \n\nI agree that the general direction of automatically generating programs with neural networks is a very exciting direction of research.  Generating code for user interfaces from images of user interfaces is a novel and potentially useful task within this general area of interest.  The main novelty of the paper is the task itself, and the three synthetic datasets created to study the task .\n\nMy main concern with this paper is a lack of technical novelty.  The model combines a CNN with an LSTM, and as such looks nearly identical to baseline models for image captioning that have been in widespread use for a few years now.  Ideally I would have liked to see CNN+LSTM as a baseline, together with some technical innovations that specialize this general model to the particular task at hand. \n\nThe experiments in this paper are also lacking.  Given that the main contribution of the paper is the pix2code task and datasets, I would have liked to see more thorough experiments . The only model tested is CNN+LSTM with various beam sizes, and performance is only demonstrated through overall accuracy and qualitative examples.  I would have liked to see comparisons with other methods, such as nearest neighbor or other retrieval-based methods.  I would have also liked to see more innovation in evaluation.  Are there metrics other than overall accuracy that could be used to measure performance?  Compared to other tasks like image captioning, can you design metrics that capture the particular challenges involved in the pix2code task?  In general, in what types of circumstances does your model succeed or fail, and can you capture this quantitatively through carefully designed metrics?  Since the data is synthetic, could you generate different datasets of increasing complexity and measure performance as complexity increases?  How does performance change with different amounts of training data ? Would it be possible to somehow transfer knowledge of UI across datasets, where you pretrain on one dataset and somehow finetune on another ? I don\u2019t expect the authors to answer any of these questions in particular;  I list them to emphasize that there are a lot of interesting experiments that could have been done with this task and dataset. \n\nOn the whole I appreciate the novelty of the task and dataset,  but the paper suffers from a lack of technical novelty in the model and limited experimental validation.",1,1,1,1,1,-1,1,1,1,-1
H1zRea1Mf-R3,"This paper proposes to use a hybrid of convolutional and recurrent networks to predict the DSL specification of a GUI given a screenshot of the GUI.\ n\n\nPros:\n\n\nThe paper is clear and the proposed problem is novel and well-defined. \n\nThe training data is synthetic, allowing for arbitrarily large training sets to be generated.   The authors have made their synthetic dataset publicly available.\ n\nThe method seems to work well based on the samples and ROC curves presented. \n\n\nCons:\n\nThis is mostly an application of an existing method to a new domain - - as stated in the related work section, effectively the same convnet+RNN architecture has been in common use for image captioning and other vision applications. \n\nThe UIs that are represented in the dataset seem quite simple; it\u2019s not clear that this will transfer to arbitrarily complex and multi-page UIs. \n\nThe main motivation for the proposed system seems to be for non-technical designers to be able to implement UIs just by drawing a mockup screenshot .  However, the paper hasn\u2019t shown that this is necessarily possible assuming the hand-designed mockups aren\u2019t pixel-for-pixel matches with a screenshot that could be generated by the \u201cDSL code -> screenshot\u201d mapping that this system learns to invert. \n\nThere exist a number of \u201cdrag and drop\u201d style UI design products (at least for HTML) that would seem to accomplish the same basic goal as the proposed system in a more reliable way.  (Though the proposed system does have the advantage of only requiring a screenshot created using any software, rather than being restricted to a particular piece of software.) \n\n\nOverall, the paper is well-written but the novelty and applicability seems a bit limited",1,1,1,1,-1,1,1,1,1,-1
H1zriGeCZ-R1,"This paper looks at the problem of optimizing hyperparameters under the assumption that the unknown function can be approximated by a sparse and low degree polynomial in the Fourier basis.  The main result is that the approximate minimization can be performed over the boolean hypercube where the number of evaluations is linear in the sparsity parameter.  \n\nIn the presented experiments, the new spectral method outperforms the tool based on the Bayesian optimization, technique based on MAB and random search . Their result also has an application in learning decision trees where it significantly improves the sample complexity bound.\ n\nThe main theoretical result, i.e., the improvement in the sample complexity when learning decision trees, looks very strong.  However, I find this result to be out of the context with the main theme of the paper.  \n\nI find it highly unlikely that a person interested in using Harmonica to find the right hyperparamters for her deep network would also be interested in provable learning of decision trees in quasi-polynomial time along with a polynomial sample complexity.  Also the theoretical results are developed for Harmonica-1 while Harmonica-q is the main method used in the experiments .\n\nWhen it comes to the experiments only one real-world experiment is present . It is hard to conclude which method is better based on a single real-world experiment . Moreover, the plots are not very intuitive, i.e., one would expect that Random Search takes the smallest amount of time.  I guess the authors are plotting the running time that also includes the time needed to evaluate different configurations.  If this is the case, some configurations could easily require more time to evaluate than the others.  It would be useful to plot the total number of function evaluations for each of the methods next to the presented plots. \n\nIt is not clear what is the stopping criterion for each of the methods used in the experiments . One weakness of Harmonica is that it has 6 hyperparameters itself to be tuned.  It would be great to see how Harmonica compares with some of the High-dimensional Bayesian optimization methods.  \n\nFew more questions:\n\nWhich problem does Harmonica-q solves that is present in Harmonica-1, and what is the intuition behind the fact that it achieves better empirical results?\n \nHow do you find best t minimizers of g_i in line 4 of Algorithm 3?\",1,1,1,1,1,-1,1,1,1,-1
H1zriGeCZ-R2,- algorithm 1 has a lot of problem specific hyperparametes that may be difficult to get right . Not clear how important they are\n- they analyze the simpler (analytically and likely computationally) Boolean hyperparameter case (each hyperparameter is binary).  Not a realistic setting.  In their experiments they use these binary parameter spaces so I'm not sure how much I buy that it is straightforward to use continuous valued polynomials.  \n- interesting idea but I think it's more theoretical than practical.  Feels like a hammer in need of a nail,1,1,1,1,-1,-1,1,1,1,-1
H1zriGeCZ-R3,"The paper is about hyperparameter optimization, which is an important problem in deep learning due to the large number of hyperparameters in contemporary model architectures and optimization algorithms .\n\nAt a high-level, hyperparameter optimization (for the challenging case of discrete variables) can be seen as a black-box optimization problem where we have only access to a function evaluation oracle (but no gradients etc.).  In the entirely unstructured case, there are strong lower bounds with an exponential dependence on the number of hyperparameters . In order to sidestep these impossibility results, the current paper assumes structure in the unknown function mapping hyperparameters to classification accuracy.  In particular, the authors assume that the function admits a representation as a sparse and low-degree polynomial . While the authors do not empirically validate whether this is a good model of the unknown function, it appears to be a reasonable assumption (the authors *do* empirically validate their overall approach). \n\nBased on the sparse and low-degree assumption, the paper introduces a new algorithm (called Harmonica) for hyperparameter optimization.  The main idea is to leverage results from compressed sensing in order to recover the sparse and low-degree function from a small number of measurements (i.e., function evaluations).  The authors derive relevant sample complexity results for their approach.  Moreover, the method also yields new algorithms for learning decision trees. \n\nIn addition to the theoretical results , the authors conduct a detailed study of their algorithm on CIFAR10 . They compare to relevant recent work in hyperparameter optimization (Bayesian optimization, random search, bandit algorithms) and find that their method significantly improves over prior work.  The best parameters found by Harmonica improve over the hand-tuned results for their \""base architecture\"" (ResNets).\ n\nOverall, I find the main idea of the paper very interesting and well executed, both on the theoretical and empirical side.  Hence I strongly recommend accepting this paper. \n\n\nSmall comments and questions :\n\n1. It would be interesting to see how close the hyperparameter function is to a low-degree and sparse polynomial (e.g., MSE of the best fit) .\n\n2. A comparison without dummy parameters would be interesting to investigate the performance differences between the algorithms in a lower-dimensional problem. \n\n3. The current paper does not mention the related work on hyperparameter optimization using reinforcement learning techniques (e.g., Zoph & Le, ICLR 2017).  While it might be hard to compare to this approach directly in experiments, it would still be good to mention this work and discuss how it relates to the current paper. \n\n4. Did the authors tune the hyperparameters directly using the CIFAR10 test accuracy?  Would it make sense to use a slightly smaller training set and to hold out say 5k images for hyperparameter evaluation before making the final accuracy evaluation on the test set?  The current approach could be prone to overfitting .\n\n5. While random search does not explicitly exploit any structure in the unknown function, it can still implicitly utilize smoothness or other benign properties of the hyperparameter space.  It might be worth adding this in the discussion of the related work.\ n\n6. Algorithm 1: Why is the argmin for g_i  (what does the index i refer to)?\n\n7 . Why does PSR truncate the indices in alpha?  At least in \""standard\"" compressed sensing, the Lasso also has recovery guarantees without truncation (and empirically works sometimes better without).\ n\n9. Definition 3: Should C be a class of functions mapping {-1, 1}^n to R?  (Note the superscript.)\n\n10.  On Page 3 we assume that K = 1, but Theorem 6 still maintains a dependence on K . It might be cleaner to either treat the general K case throughout, or state the theorem for K = 1.\n\n11.  On CIFAR10, the best hyperparameters do not improve over the state of the art with other models (e.g., a wide ResNet) . It could be interesting to run Harmonica in the regime where it might improve over the best known models for CIFAR10.\n\n12 . Similarly, it would be interesting to see whether the hyperparameters identified by Harmonica carry over to give better performance on ImageNet.  The authors claim in C.3 that the hyperparameters identified by Harmonica generalize from small networks to large networks.  Testing whether the hyperparameters also generalize from a smaller to a larger dataset would be relevant as well.",1,1,1,1,1,1,1,1,1,-1
HJ1HFlZAb-R1,"The main idea is to use the accuracy of a classifier trained on synthetic training examples produced by a generative model to define an evaluation metric for the generative model.  Specifically, compare the accuracy of a classifier trained on a noise-perturbed version of the real dataset to that of a classifier trained on a mix of real data and synthetic data generated by the model being evaluated.  Results are shown on MNIST and Fashion MNIST. \n\nThe paper should discuss the assumptions needed for classifier accuracy to be a good proxy for the quality of a generative model that generated the classifier's training data.  It may be the case that even a \""bad\"" generative model (according to some other metric) can still result in a classifier that produces reasonable test accuracy.  Since a classifier can be a highly nonlinear function, it can potentially ignore many aspects of its input distribution such that even poor approximations (as measured by, say, KL) lead to similar test accuracy as good approximations. \n\nThe sensitivity of the evaluation metric defined in equation 2 to the choice of hyperparameters of the classifier and the metric itself (e.g., alpha) is not evaluated.  Is it possible that a different choice of hyperparameters can change the model ranking ? Should the hyperparameters be tuned separately for each generative model being evaluated? \n\nThe intuition behind comparing against a classifier trained on a noise-perturbed version of the data is not explained clearly.  Why not compare a classifier trained on only (unperturbed) real data to a classifier trained on both real and synthetic data? \n\nEvaluation on two datasets is not sufficient to provide insight into whether the proposed metric is useful.  Other datasets such as ImageNet, Cifar10/100, Celeb A, etc., should also be included",1,1,1,1,1,-1,1,1,1,-1
HJ1HFlZAb-R2,"The authors propose to evaluate how well generative models fit the training set by analysing their data augmentation capacity, namely the benefit brought by training classifiers on mixtures of real/generated data, compared to training on real data only.  Despite the the idea of exploiting generative models to perform data augmentation is interesting, using it as an evaluation metric does not constitute an innovative enough contribution.  \n\nIn addition, there is a fundamental matter which the paper does not address: when evaluating a generative model, one should always ask himself what purpose the data is generated for . If the aim is to have realistic samples, a visual turing test is probably the best metric.  If instead the purpose is to exploit the generated data for classification, well, in this case an evaluation of the impact of artificial data over training is a good option. \n\nPROS:\nThe idea is interesting.  \n\nCONS:\n1. The authors did not relate the proposed evaluation metric to other metrics cited (e.g., the inception score, or a visual turing test, as discussed in the introduction).  It would be interesting to understand how the different metrics relate . Moreover, the new metric is introduced with the following motivation \u201c[visual Turing test and Inception Score] do not indicate if the generator collapses to a particular mode of the data distribution\u201d.  The mode collapse issue is never discussed elsewhere in the paper. \ n\n2. Only two datasets were considered, both extremely simple: generating MNIST digits is nearly a toy task nowadays.  Different works on GANs make use of CIFAR-10 and SVHN, since they entail more variability: those two could be a good start.  \n\n3. The authors should clarify if the method is specifically designed for GANs and VAEs . If not, section 2.1 should contain several other works (as in Table 1). \n\n4 . One of the main statements of the paper \u201cOur approach imposes a high entropy on P(Y) and gives unbiased indicator about entropy of both P(Y|X) and P(X|Y)\u201d is never proved, nor discussed.\ n\n5. Equation 2 (the proposed metric) is not convincing: taking the maximum over tau implies training many models with different fractions of generated data, which is expensive.  Further, how many tau\u2019s one should evaluate?  In order to evaluate a generative model one should test on the generated data only (tau=1) I believe.  In the worst case, the generator experiences mode collapse and performs badly.  Differently, it can memorize the training data and performs as good as the baseline model.  If it does actual data augmentation, it should perform better .\n\n6. The protocol of section 3 looks inconsistent with the aim of the work, which is to evaluate data augmentation capability of generative models.  In fact, the limit of training with a fixed dataset is that the model \u2018sees\u2019 the data multiple times across epochs with the risk of memorizing . In the proposed protocol, the model \u2018sees\u2019 the generated data D_gen (which is fixed before training) multiple time across epochs.  This clearly does not allow to fully evaluate the capability of the generative model to generate newer and newer samples with significant variability. \n\n\nMinor: \nSection 2.2 might be more readable it divided in two (exploitation and evaluation)",1,1,1,1,1,1,1,1,1,-1
HJ3d2Ax0--R1,"This paper investigates an effect of time dependencies in a specific type of RNN. \n\nThe idea is important and this paper seems sound . However, I am not sure that the main result (Theorem 1) explains an effect of depth sufficiently. \n\n--Main comment\nAbout the deep network case in Theorem 1, how $L$ affects the bound on ranks?  In the current statement, the result seems independent to $L$ when $L \\geq 2$.  I think that this paper should quantify the effect of an increase of $L$. \n\n--Sub comment\nNumerical experiments for calculating the separation rank is necessary to provide evidence of the main result.  Only a simple example will make this paper more convincing",1,1,1,1,1,-1,1,1,1,-1
HJ3d2Ax0--R2,"After reading the authors's rebuttal I increased my score from a 7 to a 6.   I do think the paper would benefit from experimental results , but agree with the authors that the theoretical results are non-trivial and interesting on their own merit. \n\n------------------------\nThe paper presents a theoretical analysis of depth in RNNs (technically a variant called RACs) i.e. stacking RNNs on top of one another, so that h_t^l (i.e. hidden state at time t and layer l is a function of h_t^{l-1} and h_{t-1}^{l} )\n\nThe work is inspired by previous results for feed forward nets and CNNs . However, what is unique to RNNs is their ability to model long term dependencies across time.  \n\nTo analyze this specific property, the authors propose a concept called \""start-end rank\"" that essentially models the richness of the dependency between two disjoint subsets of inputs . Specifically, let S = {1, . . . , T/2} and E === {T/2 + 1, . . . , T}. sep_{S,E}(y) models the dependence between these two sets of time points.  Specifically sep_{S,E}(y) = K means there exists g_s^k and g_e^k for k=1...K such that y(x) = \\sum_{k} g_s^k(x_S) g_e^k(x_E) .\n\nTherefore sep_{S,E}(y) is the rank of a particular matricization of y (with respect to the partition S,E).  If sep_{S,E}=1 then it is rank 1 (and would correspond to independence if y(x) was a probability distribution) . A higher rank would correspond to more dependence across time . \n\n(Comment: I believe if I understood the above correctly, it would be easier to explain tensors/matricization first and then introduce separation rank,  since I think it much makes it clearer to explain . Right now the authors explain separation rank first and then discuss tensors / matricization). \n\nUsing this concept, the authors prove that deep recurrent networks can express functions that have exponentially higher start/end ranks than shallow RNNs. \n\nI overall like the paper's theoretical results,  but I have the following complaints:\n\n(1)  I have the same question as the other reviewer.  Why is Theorem 1 not a function of L?   Do the papers that prove similar theorems about ConvNets able to handle general L ? What makes this more challenging?  I feel if comparing L=2 vs L=3 is hard, the authors should be more up front about that in the introduction/abstract .\n\n(2) I think it would have been stronger if the authors would have provided some empirical results validating their claims. \n\n",1,1,1,1,1,-1,1,1,1,-1
HJ3d2Ax0--R3,"The paper proposes to use the start-end rank to measure the long-term dependency in RNNs.  It shows that deep RNN is signficantly better than shallow one in this metric.  \n\nThe theory part seems to be technical enough and interesting,  though I haven't checked all the details . The main concern with the paper is that I am not sure whether the RAC studied by the paper is realistic enough for practice.  Certain gating in RNN is very useful but I don't know whether one can train any reasonable RNN with all multiplicative gates . The paper will be much stronger if it has some experiments along this line.",1,-1,1,1,1,-1,1,1,-1,-1
HJ4IhxZAb-R1,"The approach solves an important problem as getting labelled data is hard.  The focus is on the key aspect, which is generalisation across heteregeneous data.  The novel idea is the dataset embedding so that their RL policy can be trained to work across diverse datasets.\n\nPros: \n1.  The approach performs well against all the baselines, and also achieves good cross-task generalisation in the tasks they evaluated on.  \n2. In particular, they alsoevaluated on test datasets with fairly different statistics from the training datasets,  which isnt very common in most meta-learning papers today, so it\u2019s encouraging that the method works in that regime.\n \nCons: \n1. The embedding strategy, especially the representative and discriminative histograms, is complicated.  It is unclear if the strategy is general enough to work on harder problems / larger datasets, or with higher dimensional data like images . More evidence in the paper for why it would work on harder problems would be great.  \n2. The policy network would have to output a probability for each datapoint in the dataset U,  which could be fairly large, thus the method is computationally much more expensive than random sampling.  A section devoted to showing what practical problems could be potentially solved by this method would be useful. \n3. It is unclear to me if the results in table 3 and 4 are achieved by retraining from scratch with an RBF SVM,  or by freezing the policy network trained on a linear SVM and directly evaluating it with a RBF SVM base learner. \n\nSignificance/Conclusion: The idea of meta-learning or learning to learn is fairly common now . While they do show good performance,  it\u2019s unclear if the specific embedding strategy suggested in this paper will generalise to harder tasks . \n\nComments: There\u2019s lots of typos, please proof read to improve the paper. \n\nRevision: I thank the authors for the updates and addressing some of my concerns.  I agree the computational budget makes sense for cross data transfer,  however the embedding strategy and lack of larger experiments makes it unclear if it'll generalise to harder tasks . I update my review to",1,1,1,1,1,-1,1,1,1,-1
HJ4IhxZAb-R2,"This reviewer has found the proposed approach quite compelling, but the empirical validation requires significant improvements: \n1) you should include in your comparison Query-by- Bagging & Boosting, which are two of the best out-of-the-box active learning strategies\ n2) in your empirical validation you have (arbitrarily) split the 14 datasets in 7 training and testing ones but many questions are still unanswered: \n -  would any 7-7 split work just as well (ie, cross-validate over the 14 domains) \n - do you what happens if you train on 1, 2, 3, 8, 10, or 13 domains?  are the results significantly different?  \n\nOTHER COMMENTS:\n- p3: both images in Figure 1 are labeled Figure 1.a\n- p3: typo \""theis\"" --> \""this\"" \n\nAbe & Mamitsuksa (ICML-1998) . Query Learning Strategies Using Boosting and Bagging.",1,1,1,1,1,1,1,1,-1,-1
HJ4IhxZAb-R3,"Overview\n\nThe authors propose a reinforcement learning approach to learn a general active query policy from multiple heterogeneous datasets.  The reinforcement learning part is based on a policy network, which selects the data instance to be labeled next. They use meta-learning on feature histograms to embed heterogeneous datasets into a fixed dimensional representation.  The authors argue that policy-based reinforcement learning allows learning the criteria of active learning non-myopically.  The experiments show the proposed approach is effective on 14 UCI datasets .\n\nstrength\n\n* The paper is mostly clear and easy to follow .\n* The overall idea is interesting and has many potentials. \n* The experimental results are promising on multiple datasets. \n* There are thorough discussion with related works .\n\nweakness\n\n* The graph in p.3 don't show the architecture of the network clearly. \n* The motivation of using feature histograms as embedding is not clear .\n* The description of the 2-D histogram on p.4 is not clear.  The term \""posterior value\"" sounds ambiguous .\n* The experiment sets a fixed budget of only 20 instances, which seems to be rather few in some active learning scenarios, especially for non-linear learners.  Also, the experiments takes a fixed 20K iterations for training, and the convergence status (e.g. whether the accumulated gradient has stabilized the policy) is not clear. \n* Are there particular reasons in using policy learning instead of other reinforcement learning approaches ?\n* The term A(Z) in the objective function can be more clearly described .\n* While many loosely-related works were surveyed, it is not clear why literally none of them were compared.  There is thus no evidence on whether a myopic bandit learner (say, Chu and Lin's work) is really worse than the RL policy.  There is also no evidence on whether adaptive learning on the fly is needed or not. \n* In Equation 2, should there be a balancing parameter for the reconstruction loss? \n* Some typos\n    - page 4: some duplicate words in discriminative embedding session\n    - page 4: auxliary -> auxiliary\n    - page 7: tescting -> testing\n\",1,1,1,1,1,1,1,1,1,-1
HJ5AUm-CZ-R3,"The paper presents some conceptually incremental improvements over the models in \u201cNeural Statistician\u201d and \u201cGenerative matching networks\u201d.  Nevertheless, it is well written and I think it is solid work with reasonable convincing experiments and good results.  Although, the authors use powerful PixelCNN priors and decoders and they do not really disentangle to what degree their good results rely on the capabilities of these autoregressive components.",1,1,1,1,-1,1,1,1,1,-1
HJ8W1Q-0Z-R1,"The authors present an evolution of the idea of fast weights: training a double recurrent neural network, one \""slow\"" trained as usual and one \""fast\"" that gets updated in every time-step based on the slow network.  The authors generalize this idea in a nice  way and present results on 1 experiment.  On the positive side, the paper is clearly written and while the fast-weights are not new, the details of the presented method are original.  On the negative side, the experimental results are presented on only 1 experiment with a data-set and task made up by the authors.  The results are good but the improvements are not too large, and they are measured over weak baselines implemented by the authors.  For a convincing result, one would require an evaluation on a number of tasks, including long-studied ones like language modeling, and comparison to stronger related models, such as the Neural Turing Machine or the Transformer (from \""Attention is All You Need\"").  Without comparison to stronger baselines and with results only on 1 task constructed by the authors, we have to recommend rejection.",1,1,1,1,-1,1,1,1,1,-1
HJ8W1Q-0Z-R2,"Summary\nThe paper proposes a neural network architecture for associative retrieval based on fast weights with context-dependent gated updates.  The architecture consists of a \u2018slow\u2019 network which provides weight updates for the \u2018fast\u2019 network which outputs the predictions of the system.  The experiments show that the architecture outperforms a couple of related models on an associative retrieval problem. \n\nQuality\nThe authors evaluate their architecture on an associative retrieval task which is similar to the variable assignment task used in Danihelka et al. (2016).  The difference with the original task seems to be that the network is also trained to predict a \u2018blank\u2019 symbol which indicates that no prediction has been made.  While this task is artificial, it does make sense in the context of what the authors want to show.  The fact that the authors compare their results with three sensible baselines and perform some form of hyper-parameter search for all of the models, adds to the quality of the experiment.  It is somewhat unfortunate that the paper doesn\u2019t give more detail about the precise hyper-parameters involved and that there is no comparison with the associative LSTM from Danihelka et al.  Did these hyper-parameters also include the sizes of the models?  Otherwise it\u2019s not very clear to me why the numbers of parameters are so much higher for the baseline models.  While I think that this experiment is well done, it is unfortunate that it is the only experiment the authors carried out and the paper would be more impactful if there would have been results for a wider variety of tasks.  It is commendable that the authors also discuss the memory requirements and increased wall clock time of the model. \n\nClarity\nI found the paper hard to read at times and it is often not very clear what the most important differences are between the proposed methods and earlier ones in the literature.  I\u2019m not saying those differences aren\u2019t there, but the paper simply didn\u2019t emphasize them very well and I had to reread the paper from Ba et al. (2016) to get the full picture.   \n\nOriginality/Significance\nWhile the architecture is new, it is based on a combination of previous ideas about fast weights, hypernetworks and activation gating and I\u2019d say that the novelty of the approach is average.  The architecture does seem to work well on the associative retrieval task, but it is not clear yet if this will also be true for other types of tasks.  Until that has been shown, the impact of this paper seems somewhat limited to me. \n\nPros\nExperiments seem well done. \nGood baselines. \nGood results. \n\nCons\nHard to extract the most important changes from the text. \nOnly a single synthetic task is reported.\n\",1,1,1,1,1,1,1,1,1,-1
HJ8W1Q-0Z-R3,"The paper proposed an extension to the fast weights from Ba et al. to include additional gating units for changing the fast weights learning rate adaptively.  The authors empirically demonstrated the gated fast weights outperforms other baseline methods on the associative retrieval task. \n\nComment:\n\n- I found the paper very hard to follow.  The authors could improve the clarity of the paper greatly by listing their contribution clearly for readers to digest.  The authors should emphasize the first half of the method section are from existing works and should go into a separate background section. \n\n- Overall, the only contribution of the paper seems to be the modification to Ba et al. is the Eq. (8).  The authors have only evaluated the method on a synthetic associative retrieval task.  Without additional experiments on other datasets, it is hard for the reader to draw any meaningful conclusion about the proposed method in general",1,1,1,1,1,1,1,1,1,-1
HJ94fqApW-R1,"In this paper, the authors propose a data-dependent channel pruning approach to simplify CNNs with batch-normalizations.  The authors view CNNs as a network flow of information and applies sparsity regularization on the batch-normalization scaling parameter \\gamma which is seen as a \u201cgate\u201d to the information flow . Specifically, the approach uses iterative soft-thresholding algorithm step to induce sparsity in \\gamma during the overall training phase of the CNN (with additional rescaling to improve efficiency . In the experiments section, the authors apply their pruning approach on a few representative problems and networks.  \n\nThe concept of applying sparsity on \\gamma to prune channels is an interesting one, compared to the usual approaches of sparsity on weights . However, the ISTA, which is equivalent to L1 penalty on \\gamma is in spirit same as \u201csmaller-norm-less-informative\u201d assumption.  Hence, the title seems a bit misleading.  \n\nThe quality and clarity of the paper can be improved in some sections . Some specific comments by section:\n\n3. Rethinking Assumptions:\n-\tWhile both issues outlined here are true in general, the specific examples are either artificial or can be resolved fairly easily . For example: L-1 norm penalties only applied on alternate layers is artificial and applying the penalties on all Ws would fix the issue in this case.  Also, the scaling issue of W can be resolved by setting the norm of W to 1, as shown in He et. al., 2017 . Can the authors provide better examples here? \n-\tCan the authors add specific citations of the existing works which claim to use Lasso, group Lasso, thresholding to enforce parameter sparsity? \n\n4. Channel Pruning\n-\tThe notation can be improved by defining or replacing \u201csum_reduced\u201d\n-\tISTA \u2013 is only an algorithm, the basic assumption is still L1 -> sparsity or smaller-norm-less-informative.  Can the authors address the earlier comment about \u201ca theoretical gap questioning existing sparsity inducing formulation and actual computational algorithms\u201d ?\n-\tCan the authors address the earlier comment on \u201chow to set thresholds for weights across different layers\u201d, by providing motivation for choice of penalty for each layer?  \n-\tCan the authors address the earlier comment on how their approach provides \u201cguarantees for preserving neural net functionality approximately\u201d?\n\n 5. Experiments\n-\tCIFAR-10: Since there is loss of accuracy with channel pruning, it would be useful to compare accuracy of a pruned model with other simpler models with similar param.size?  (like pruned-resnet-101 vs. resnet-50 in ISLVRC subsection)\n-\tISLVRC: The comparisons between similar param-size models is exteremely useful in highlighting the contribution of this.  However, resnet-34/50/101 top-1 error rates from Table 3/4 in (He et.al. 2016) seem to be lower than reported in table 3 here. Can the authors clarify?\ n-\tFore/Background: Can the authors add citations for datasets, metrics for this problem? \n\n\nOverall, the channel pruning with sparse \\gammas is an interesting concept and the numerical results seem promising . The authors have started with right motivation and the initial section asks the right questions, however,  some of those questions are left unanswered in the subsequent work as detailed above",1,1,1,1,1,1,1,1,1,-1
HJ94fqApW-R2,"This paper is well written and it was easy to follow . The authors propose prunning model technique by enforcing sparsity on the scaling parameter of batch normalization layers . This is achieved by forcing the output of some channels being constant during training.  This is achieved an adaptation of ISTA algorithm to update the batch-norm parameter . \n\nThe authors evaluate the performance of the proposed approach on different classification and segmentation tasks . The method seems to be relatively straightforward to train and achieve good performance (in terms of performance/parameter reduction) compared to other methods on Imagenet. \n\nSome of the hyperparameters used (alpha and specially rho) seem to be used very ad-hoc.  Could the authors explain their choices?  How sensible is the algorithm to these hyperparameters?\ nIt would be nice to see empirically how much of computation the proposed approach takes during training.  How much longer does it takes to train the model with the ISTA based constraint ?\n\nOverall this is a good paper and I believe it should be accepted, given the authors are more clear on the details pointed above",1,1,1,1,1,-1,1,1,1,-1
HJ94fqApW-R3,"This paper proposes an interesting  approach to prune a deep model from a computational point of view.  The idea is quite simple as pruning using the connection in the batch norm layer . It is interesting to add the memory cost per channel into the optimization process.  \n\nThe paper suggests normal pruning does not necessarily preserve the network function.  I wonder if this is also applicable to the proposed method and how can this be evidenced.  \n\nAs strong points, the paper is easy to follow and does a good review of existing methods.  Then, the proposal is simple and easy to reproduce and leads to interesting results . It is clearly written (there are some typos / grammar errors).  \n\nAs weak points:\n1) The paper claims the selection of \\alpha is critical but then, this is fixed empirically without proper sensitivity analysis.  I would like to see proper discussion here.  Why is \\alpha set to 1.0 in the first experiment while set to a different number elsewhere.  \n\n2) how is the pruning (as post processing) performed for the base model (the so called model A). \n\nIn section 4, in the algorithmic steps . How does the 4th step compare to the statement in the initial part of the related work suggesting zeroed-out parameters can affect the functionality of the network? \n\n3) Results for CIFAR are nice although not really impressive as the main benefit comes from the fully connected layer as expected.",1,1,1,1,1,1,1,1,1,1
HJBhEMbRb-R1,"Deep neural networks have found great success in various applications.  This paper presents a theoretical analysis for 2-layer neural networks (NNs) through a spectral approach.  Specifically, the authors develop a Fourier-based generalization bound.  Based on this, the authors show that the bandwidth, Fourier l_1 norm and the gradient for local minima of the population risk can be controlled for 2-layer NNs with SINE activation functions.  Numerical experimental results are also presented to verify the theory. \n\n(1) The scope is a bit limited.  The paper only considers 2-layer NNs. Is there an essential difficulty in extending the result here to NNs with more layers?  Also, the analysis for gradient-based method in section 6  is only for squared-error loss, SINE activation and a deterministic target variable.  What would happen if Y is random or the activation is ReLU? \n(2) The generalization bound in Corollary 3 is only for the gradient w.r.t. \\alpha_j. Perhaps, an object of more interest is the gradient w.r.t. W.  It would be intersting to present some analysis regarding the gradient w.r.t. W. \n(3) It is claimed that the bound is tighter than that obtained using only the Lipschitz property of the activation function.  However, no comparison is clearly made.  It would be better if the authors could explain this more? \n\nIn summary, the application domain of the theoretical results seems a bit restricted. \n\nMinor comments:\nEq. (1): d\\xi should be dx\nLemma 2: one \\hat{g} should be \\hat{f}",1,1,1,1,1,-1,1,1,-1,-1
HJBhEMbRb-R2,"\nThis work proposes to study the generalization of learning neural networks via the Fourier-based method.  It first gives a Fourier-based generalization bound, showing that Rademacher complexity of functions with small bandwidth and Fourier l_1 norm will be small.  This leads to generalization for 2-layer networks with appropriate bounded size.  For 2-layer networks with sine activation functions, assuming that the data distribution has nice spectral property (ie bounded bandwidth), it shows that the local minimum of the population risk (if with isolated component condition) will have small size, and also shows that the gradient of the empirical risk is close to that of the population risk.  Empirical results show that the size of the networks learned on random labels are larger than those learned on true labels, and shows that a regularizer implied by their Fourier-based generalization bound can effectively reduce the generalization gap on random labels.  \n\nThe idea of applying the Fourier-based method to generalization is interesting.  However, the theoretical results are not very satisfactory.  \n-- How do the bounds here compared to those obtained by directly applying Rademacher complexity to the neural network functions?  \n-- How to interpret the isolated components condition in Theorem 4?  Basically, it means that B(P_X) should be a small constant.  What type of distributions of X will be a good example?  \n-- It is not easy to put together the conclusions in Section 6.1 and 6.2.  Suppose SGD leads to a local minimum of the empirical loss.  One can claim that this is an approximate local minimum (ie, small gradient) by Corollary 3.  But to apply Theorem 4, one will need a version of Theorem 4 for approximate local minima.  Also, one needs to argue that the local minimum obtained by SGD will satisfy the isolated component condition.  The argument in Section 8.6 is not convincing, ie, there is potentially a large approximation error in (41) and one cannot claim that Lemma 1 and Theorem 4 are still valid without the isolated component condition.",1,1,1,1,1,-1,1,1,1,-1
HJBhEMbRb-R3,"This paper studies the generalization properties of 2-layer neural networks based on Fourier analysis.  Studying the generalization property of neural network is an important problem and Fourier-based analysis is a promising direction, as shown in (Lee et al., 2017).  However, I am not satisfied with the results in the current version. \n\n1) The main theoretical results are on the sin activation functions instead of commonly used ReLU functions.  \n\n2) Even if for sin activation functions, the analysis is NOT complete.  The authors claimed in the abstract that gradient-based methods will converge to generalizable local minima.  However, Corollary 3 is only a concentration bound on the gradient.  There is a gap that how this corollary implies generalization.  The paragraph below this corollary is only a high level intuition.",1,1,1,1,-1,1,1,1,-1,-1
HJcSzz-CZ-R1,"This paper is an extension of the \u201cprototypical network\u201d which will be published in NIPS 2017.  The classical few-shot learning has been limited to using the unlabeled data, while this paper considers employing the unlabeled examples available to help train each episode.  The paper solves a new semi-supervised situation, which is more close to the setting of the real world, with an extension of the prototype network.   Sufficient implementation detail and analysis on results. \n\nHowever, this is definitely not the first work on semi-supervised formed few-shot learning.  There are plenty of works on this topic [R1, R2, R3].  The authors are advised to do a thorough survey of the relevant works in Multimedia and computer vision community.  \n \nAnother concern is that the novelty.  This work is highly incremental since it is an extension of existing prototypical networks by adding the way of leveraging the unlabeled data.  \n\nThe experiments are also not enough. Not only some other works such as [R1, R2, R3]; but also the other na\u00efve baselines should also be compared, such as directly nearest neighbor classifier, logistic regression, and neural network in traditional supervised learning.  Additionally, in the 5-shot non-distractor setting on tiered ImageNet, only the soft kmeans method gets a little bit advantage against the semi-supervised baseline, does it mean that these methods are not always powerful under different dataset? \n\n[R1] \u201cVideostory: A new multimedia embedding for few-example recognition and translation of events,\u201d in ACM MM, 2014\n\n[R2] \u201cTransductive Multi-View Zero-Shot Learning\u201d, IEEE TPAMI 2015\n\n[R3] \u201cVideo2vec embeddings recognize events when examples are scarce,\u201d IEEE TPAMI 2014\n",1,1,1,1,1,1,1,1,1,1
HJcSzz-CZ-R2,"In this paper, the authors studied the problem of semi-supervised few-shot classification, by extending the prototypical networks into the setting of semi-supervised learning with examples from distractor classes.   The studied problem is interesting, and the paper is well-written.  Extensive experiments are performed to demonstrate the effectiveness of the proposed methods.   While the proposed method is a natural extension of the existing works (i.e., soft k-means and meta-learning). On top of that, It seems the authors have over-claimed their model capability at the first place as the proposed model cannot properly classify the distractor examples but just only consider them as a single class of outliers.  Overall, I would like to vote for a weakly acceptance regarding this paper",1,1,1,1,-1,1,1,1,1,-1
HJcSzz-CZ-R3,"This paper proposes to extend the Prototypical Network (NIPS17) to the semi-supervised setting with three possible \nstrategies.  One consists in self-labeling the unlabeled data and then updating the prototypes on the basis of the \nassigned pseudo-labels.  Another is able to deal with the case of distractors i.e.  unlabeled samples not beloning to\nany of the known categories.  In practice this second solution is analogous to the first, but a general 'distractor' class\nis added.  Finally the third technique learns to weight the samples according to their distance to the original prototypes. \n\nThese strategies are evaluated in a particular semi-supervised transfer learning setting:  the models are first trained \non some source categories with few labeled data and large unlabeled samples (this setting is derived by subselecting\nmultiple times a large dataset), then they are used on a final target task with again few labeled data and large \nunlabeled samples but beloning to a different set of categories. \n\n+ the paper is well written, well organized and overall easy to read \n+/-  this work builds largely on previous work.  It introduces only some small technical novelty inspired by soft-k-means\nclustering that anyway seems to be effective. \n+ different aspect of the problem are analyzed by varying the number of disctractors and varying the level of\nsemantic relatedness between the source and the target sets\n\nFew notes and questions \n1) why for the omniglot experiment the table reports the error results?  It would be better to present accuracy as for the other tables/experiments \n2) I would suggest to use source and target instead of train and test -- these two last terms are confusing because\nactually there is a training phase also at test time. \n3) although the paper indicate that there are different other few-shot methods that could be applicable here , \nno other approach is considered besides the prothotipical network and its variants.  An further external reference \ncould be used to give an idea of what would be the experimental result at least in the supervised case",1,1,1,1,1,1,1,1,1,-1
HJDV5YxCW-R1,"This paper suggests a method for varying the degree of quantization in a neural network during the forward propagation phase. \n\nThough this is an important direction to investigate, there are several issues:\n\n1. Comparison with previous results is misleading:\na.\t1-bit weights and floating point activations: Rastegari et al. got 56.8% accuracy on Alexnet, which is better than this paper 1.4bit result of 55.2%.\nb.\tHubara et al. got 51% results on 1-bit weights and 2-bit activations included also quantization first and last layer, in contrast to this paper.  Therefore, it is not clear if there is a significant benefit in the proposed method which achieves 51.5% when decreasing the activation precision to 1.4bit.  \n\nTherefore, it is not clear that the proposed methods improve over previous approaches. \n\n2. It is not clear to me: in which dimension of the tensors are we saving the scale factor?  If it is per feature map, or neuron, this eliminates the main benefits of quantization: doing efficient binarized operations when doing Weight*activation during the forward pass?\n\n3.  The review of the literature is inaccurate.  For example, it is not true that Courbariaux et al. (2016) \u201cfurther improved accuracy on small datasets\u201d: the main novelty there was binarizing the activations (which typically decreased the accuracy).  Also, it is not clear if the scale factors introduced by XNOR-Net indeed allowed \""a significant improvement over previous work\"" in ImageNet (e.g., see DoReFA and Hubara et al. who got similar results using binarized weigths and activations on ImageNet without scale factors).   Lastly, the statement \u201cTypical approaches include linearly placing the quantization points\u201d is inaccurate: it was observed that logarithmic quantization works better in various cases.  For example, see Miyashita, Lee and Murmann 2016, and Hubara et al. \n\n%%% After Author's Clarification %%%\nThis paper results seem more positive now, and I have therefore have increased my score, assuming the authors will revise the paper accordingly",1,1,1,1,1,1,1,1,1,-1
HJDV5YxCW-R2,"The paper tries to maintain the accuracy of 2bits network, while uses possibly less than 2bits weights. \n\n1.  The paper misses some more recent reference, e.g. [a,b].  The author should also have a discussion on them. \n\n2. Indeed, AlexNet is a good seedbed to test binary methods.  However, it is more interesting and important to test on more advanced networks.  So, I wish to see a section on testing with Resnet and GoogleNet. \n\nIndeed, the authors have commented: \""AlexNet with batch-normalization (AlexNet-BN) is the standard model ... acceptance that improvements made to accuracy transfer well to more modern architectures. \"" So, please show that. \n\n3. The paper wants to find a good trade-off on speed and accuracy.  The authors have plotted such trade-off on space v.s. accuracy in Figure 3(b), then how about speed v.s. accuracy? \n\nMy concern is that one-bit system is already complicated to implement.  Indeed, the authors have discussed their implementation in Section 3.3, so, how their method works in practice?  One example is Section 4 in [Courbariaux et al. 2016]. \n\n4. Is trade-off between 1 to 2 bits really important?  \n\nCompared with 2bits or ternary network, the proposed method at most achieving (1.4/2) compression ratio and (2/1.4) speedup (based on their Table 1).  Is such improvement really important? \n\nReference:\n[a]. Trained Ternary Quantization. ICLR 2017\n[b].  Extremely low bit neural network: Squeeze the last bit out with ADMM",1,1,1,1,1,1,1,1,1,-1
HJDV5YxCW-R3,"This paper presents an extension of binary networks, and the main idea is to use different bit rates for different layers so we can further reduce bitrate of the overall net, and achieve better performance (speed / memory).  The paper addresses a real problem which is meaningful, and provides interesting insights, but it is more of an extension. \n\nThe description of the Heterogeneous Bitwidth Binarization algorithm is interesting and simple, and potentially can be practical, However it also adds more complication to real world implementations, and might not be an elegant enough approach for practical usages.  \n\nExperiments wise, the paper has done solid experiments comparing with existing approaches and showed the gain.  Results are promising. \n\nOverall, I am leaning towards a rejection mostly due to limited novelty",1,1,1,1,-1,-1,1,1,1,-1
HJsk5-Z0W-R1,"This paper proposes to improve time complexity of factorization machine.  Unfortunately, the paper's claim that FM's time complexity is quadratic to feature size is wrong.  Specifically, the dot product can be computed as (which is linear to feature size)\n\n(\\sum x_i \\beta_i)^T (\\sum x_i \\beta_i) - \\sum_i x_i^2 beta_i^T beta_i \n\nThe projection of feature group into one embedded space proposed in the paper can be viewed as another form of representing the same model when group equals one.  When the number of feature groups do not equal one, they correspond to field aware factorization machine(FFM)",1,1,1,1,-1,-1,1,-1,-1,-1
HJsk5-Z0W-R2,"The authors introduce a novel novel for collaborative filtering.  The proposed model combines some of the strengths of factorization machines and of polynomial regression.  Another way to understand this model is that it's a feed forward neural network with a specific connection structure (i.e., not fully connected). \n\nThe paper is well written overall and relatively easy to understand.  The study seems fairly thorough (both vanilla and cold-start experiments are reported). \n\nOverall the paper feels a little bit incomplete .  This is particularly apparent in the empirical study.  Given the somewhat limited novelty of the model the potential impact of this work relies on more convincing experimental results.  Here are some suggestions about how to achieve that: \n\n1) Methodically report results for MF, FM, CTR (when meaningful), other strong baselines (maybe SLIM?) and all your methods for all datasets. \n\n2) Report results on well-known CF datasets.  Movielens comes to mind. \n\n3) Shed some light on some of the poor CTR results (last paragraph of Section 4.2.2)\n\n4) Explore the models and shed some lights on where the gains are coming from. \n\n\nMinor: \n\n- How do you deal with unobserved preferences in the implicit case?\ n\n- I found the idea of Figure 1 very good but in its current form I didn't find it particularly insightful (these \""clouds\"" are hard to interpret). \n\n- It may also be worth adding this reference when discussing neural factorization:\nhttp://www.cs.toronto.edu/~mvolkovs/nips2017_deepcf.pdf\n",1,1,1,1,1,1,1,1,1,-1
HJsk5-Z0W-R3,"This paper presents a method for matrix factorization using DNNs.  The suggestion is to make the factorization machine (eqn 1) deep, by grouping the features meaningfully (eqn 5), extracting nonlinear features from original inputs (deep-in, eqn 8), and adding additional nonlinearity after computing pairwise interactions (deep-out, eqn 7).  From the methodology point of view, such extensions are relatively straightforward.  As an example, from the experimental results, it seems the grouping of features is done mostly with domain knowledge (e.g., months of year) and not learned automatically . The authors claim the proposed method can circumvent the cold-start problem, and presented some experimental results on recommendation systems with text features. \n\nWhile the application problems look quite interesting, in my opinion, the paper needs to make the context and contribution clearer.  In particular, there is a huge literature in collaborative filtering, and I believe there is by now sufficient work on collaborative filtering with input features (and possibly dealing with the cold-start problem).  I think this paper does not connect very well with that literature.  When reading it, at times I felt the main purpose of this paper is to solve the application problems presented in experimental results, instead of proposing a general framework.  I suggest the authors to demonstrate their method on some well-known datasets (e.g., MovieLens, Netflix), to give the readers an idea if the proposed method is indeed advantageous over more classical methods,  or if the success of this paper is mostly due to clever processing of text features using DNNs. \n\nSome detailed comments:\n1. eqn 4 does not indicate any rank-r factors.  \n2. some statements do not seem straightforward/justified to me:  \n    -- the paper uses the word \""inference\"" several times without definition \n    -- \""if we were interested in interpreting the parameters, we could constrain w to be non-negative ... \"".  Is this easy to do, and can the authors demonstrate this in their experiments and show interpretable examples? \n    -- \""Note that if the dot product is replaced with a neural function, fast inference for cold-start ...\"" . \n3. the experimental setup seems quite unusual to me: \""since we only observe positive labels, for such tasks in the test set we sample a labels according to the label frequency\"".  This seems very problematic if most of the entries are not observed.  Why cannot you use the typical evaluation procedure for collaborative filtering,  where you hide some known entries during model training, and evaluate on these entries during test?",1,1,1,1,1,1,1,1,-1,-1
Hk0wHx-RW-R1,"This paper identifies and proposes a fix for a shortcoming of the Deep Information Bottleneck approach, namely that the induced representation is not invariant to monotonic transform of the marginal distributions (as opposed to the mutual information on which it is based).  The authors address this shortcoming by applying the DIB to a transformation of the data, obtained by a copula transform.  This explicit approach is shown on synthetic experiments to preserve more information about the target, yield better reconstruction and converge faster than the baseline.  The authors further develop a sparse extension to this Deep Copula Information Bottleneck (DCIB), which yields improved representations (in terms of disentangling and sparsity) on a UCI dataset. \n\n(significance) This is a promising idea.  This paper builds on the information theoretic perspective of representation learning, and makes progress towards characterizing what makes for a good representation.  Invariance to transforms of the marginal distributions is clearly a useful property, and the proposed method seems effective in this regard. \nUnfortunately, I do not believe the paper is ready for publication as it stands, as it suffers from lack of clarity and the experimentation is limited in scope. \n\n(clarity) While Section 3.3 clearly defines the explicit form of the algorithm (where data and labels are essentially pre-processed via a copula transform), details regarding the \u201cimplicit form\u201d are very scarce.  From Section 3.4, it seems as though the authors are optimizing the form of the gaussian information bottleneck I(x,t), in the hopes of recovering an encoder $f_\\beta(x)$ which gaussianizes the input (thus emulating the explicit transform) ?  Could the authors clarify whether this interpretation is correct, or alternatively provide additional clarifying details ?  There are also many missing details in the experimental section: how were the number of \u201cactive\u201d components selected ?  Which versions of the algorithm (explicit/implicit) were used for which experiments ?  I believe explicit was used for Section 4.1, and implicit for 4.2 but again this needs to be spelled out more clearly.  I would also like to see a discussion (and perhaps experimental comparison) to standard preprocessing techniques, such as PCA-whitening. \n\n(quality) The experiments are interesting and seem well executed.  Unfortunately, I do not think their scope (single synthetic, plus a single UCI dataset) is sufficient.  While the gap in performance is significant on the synthetic task, this gap appears to shrink significantly when moving to the UCI dataset.  How does this method perform for more realistic data, even e.g. MNIST ?  I think it is crucial to highlight that the deficiencies of DIB matter in practice, and are not simply a theoretical consideration.  Similarly, the representation analyzed in Figure 7 is promising,but again the authors could have targeted other common datasets for disentangling, e.g. the simple sprites dataset used in the beta-VAE paper.  I would have also liked to see a more direct and systemic validation of the claims made in the paper.  For example, the shortcomings of DIB identified in Section 3.1, 3.2 could have been verified more directly by plotting I(y,t) for various monotonic transformations of x.  A direct comparison of the explicit and implicit forms of the algorithms would also also make for a stronger paper in my opinion. \n\nPros:\n* Theoretically well motivated\n* Promising results on synthetic task\n* Potential for impact\n Cons:\n* Paper suffers from lack of clarity (method and experimental section) \n* Lack of ablative / introspective experiments\n* Weak empirical results (small or toy datasets only).",1,1,1,1,1,-1,1,1,1,-1
Hk0wHx-RW-R2,"This paper presents a sparse latent representation learning algorithm based on an information theoretic objective formulated through meta-Gaussian information bottleneck and solved via variational auto-encoder stochastic optimization.  The authors suggest Gaussianify the data using copula transformation and  further adopt a diagonal determinant approximation with justification of minimizing an upper bound of mutual information.   Experiments include both artificial data and real data.  \n\nThe paper is unclear at some places and writing gets confusing.  For example, it is unclear whether and when explicit or implicit transforms are used for x and y in the experiments, and the discussion at the end of Section 3.3 also sounds confusing.  It would be more helpful if the author can make those points more clear and offer some guidance about the choices between explicit and implicit transform in practice.  Moreover, what is the form of f_beta and how beta is optimized?   In the first equation on page 5, is tilde y involved?  How to choose lambda? \n\nIf MI is invariant to monotone transformations and information curves are determined by MIs, why \u201ctransformations basically makes information curve arbitrary\u201d?  Can you elaborate?   \n\nAlthough the experimental results demonstrate that the proposed approach with copula transformation yields higher information curves, more compact representation and better reconstruction quality, it would be more significant if the author can show whether these would necessarily lead to any improvements on other goals such as classification accuracy or robustness under adversarial attacks.  \n\nMinor comments: \n\n- What is the meaning of the dashed lines and the solid lines respectively in Figure 1?  \n- Section 3.3 at the bottom of page 4: what is tilde t_j? and x in the second term?  Is there a typo?  \n- typo, find the \u201cmost orthogonal\u201d representation if the inputs -> of the inputs  \n\nOverall, the main idea of this paper is interesting and well motivated and but the technical contribution seems incremental.  The paper suffers from lack of clarity at several places and the experimental results are convincing but not strong enough.  \n\n***************\nUpdates: \n***************\nThe authors have clarified some questions that I had and further demonstrated the benefits of copula transform with new experiments in the revised paper.  The new results are quite informative and addressed some of the concerns raised by me and other reviewers.  I have updated my score to 6 accordingly.",1,1,1,1,1,-1,1,1,1,-1
Hk0wHx-RW-R3,"[====================================REVISION ======================================================]\nOk so the paper underwent major remodel, which significantly improved the clarity.  I do agree now on Figure 5, which tips the scale for me to a weak accept.  \n[====================================END OF REVISION ================================================]\n\nThis paper explores the problems of existing Deep variational bottle neck approaches for compact representation learning.  Namely, the authors adjust deep variational bottle neck to conform to invariance properties (by making latent variable space to depend on copula only) - they name this model a  copula extension to dvib.  They then go on to explore the sparsity of the latent space \n\nMy main issues with this paper are experiments: The proposed approach is tested only on 2 datasets (one synthetic, one real but tiny - 2K instances) and some of the plots (like Figure 5) are not convincing to me.  On top of that, it is not clear how two methods compare computationally and how introduction of the copula  affects the convergence (if it does) \n\nMinor comments\nPage 1: forcing an compact -> forcing a compact\n\u201cand and\u201d =>and\nSection 2: mention that I is mutual information, it is not obvious for everyone\n\nFigure 3: circles/triangles are too small, hard to see \nFigure 5: not really convincing.  B does not appear much more structured than a, to me it looks like a simple transformation of a.",1,1,1,1,1,-1,1,1,1,-1
Hk0wHx-RW-R4,"The paper proposed a copula-based modification to an existing deep variational information bottleneck model, such that the marginals of the variables of interest (x, y) are decoupled from the DVIB latent variable model, allowing the latent space to be more compact when compared to the non-modified version.  The experiments verified the relative compactness of the latent space, and also qualitatively shows that the learned latent features are more 'disentangled'.  However, I wonder how sensitive are the learned latent features to the hyper-parameters and optimizations? \n\nQuality: Ok. The claims appear to be sufficiently verified in the experiments.  However, it would have been great to have an experiment that actually makes use of the learned features to make predictions.  I struggle a little to see the relevance of the proposed method without a good motivating example. \n\nClarity: Below average.  Section 3 is a little hard to understand.  Is q(t|x) in Fig 1 a typo?  How about t_j in equation (5)?  There is a reference that appeared twice in the bibliography (1st and 2nd). \n\nOriginality and Significance: Average. The paper (if I understood it correctly) appears to be mainly about borrowing the key ideas from Rey et. al. 2014 and applying it to the existing DVIB model.",1,1,1,1,1,1,1,1,1,-1
Hk2aImxAb-R1,"This work proposes a variation of the DenseNet architecture that can cope with computational resource limits at test time.  The paper is very well written, experiments are clearly presented and convincing and, most importantly, the research question is exciting (and often overlooked).  \n\nMy only major concern is the degree of technical novelty with respect to the original DenseNet paper of Huang et al. (2017).  The authors add a hierarchical, multi-scale structure and show that DenseNet can better cope with it than ResNet (e.g., Fig. 3).  They investigate pros and cons in detail adding more valuable analysis in the appendix.  However, this work is basically an extension of the DenseNet approach with a new problem statement and additional, in-depth analysis.    \n\nSome more minor comments: \n\n-\tPlease enlarge Fig. 4.  \n-\tI did not fully grasp the details in the first \""Solution\"" paragraph on P5.  Please extend and describe in more detail.  \n\nIn conclusion, this is a very well written paper that designs the network architecture (of DenseNet) such that it is optimized to include CPU budgets at test time.  I recommend acceptance to ICLR18",1,1,1,1,1,1,1,1,1,-1
Hk2aImxAb-R2,"This paper presents a method for image classification given test-time computational budgeting constraints.   Two problems are considered:  \""any-time\"" classification, in which there is a time constraint to evaluate a single example, and batched budgets, in which there is a fixed budget available to classify a large batch of images.   A convolutional neural network structure with a diagonal propagation layout over depth and scale is used, so that each activation map is constructed using dense connections from both same and finer scale features.   In this way, coarse-scale maps are constructed quickly, then continuously updated with feed-forward propagation from lower layers and finer scales, so they can be used for image classification at any intermediate stage.   Evaluations are performed on ImageNet and CIFAR-100. \n\nI would have liked to see the MC baselines also evaluated on ImageNet --- I'm not sure why they aren't there as well?  Also on p.6 I'm not entirely clear on how the \""network reduction\"" is performed ---  it looks like finer scales are progressively dropped in successive blocks,  but I don't think they exactly correspond to those that would be needed to evaluate the full model (this is \""lazy evaluation\"").   A picture would help here, showing where the depth-layers are divided between blocks.\ n\nI was also initially a bit unclear on how the procedure described for batched budgeted evaluation achieves the desired result:   It seems this relies on having a batch that is both large and varied, so that its evaluation time will converge towards the expectation.   So this isn't really a hard constraint (just an expected result for batches that are large and varied enough).   This is fine, but could perhaps be pointed out if that is indeed the case. \n\nOverall, this seems like a natural and effective approach, and achieves good results",1,1,1,1,1,-1,1,1,1,-1
Hk2aImxAb-R3,"This paper introduces a new model to perform image classification with limited computational resources at test time.  The model is based on a multi-scale convolutional neural network similar to the neural fabric (Saxena and Verbeek 2016),  but with dense connections (Huang et al., 2017) and with a classifier at each layer.  The multiple classifiers allow for a finer selection of the amount of computation needed for a given input image.  The multi-scale representation allows for better performance at early stages of the network.  Finally the dense connectivity allows to reduce the negative effect that early classifiers have on the feature representation for the following layers. \nA thorough evaluation on ImageNet and Cifar100 shows that the network can perform better than previous models and ensembles of previous models with a reduced amount of computation. \n\nPros:\n- The presentation is clear and easy to follow. \n- The structure of the network is clearly justified in section 4. \n- The use of dense connectivity to avoid the loss of performance of using early-exit classifier is very interesting. \n- The evaluation in terms of anytime prediction and budgeted batch classification can represent real case scenarios. \n- Results are very promising, with 5x speed-ups and same or better accuracy that previous models. \n- The extensive experimentation shows that the proposed network is better than previous approaches under different regimes. \n\nCons:\n- Results about the more efficient densenet* could be shown in the main paper \n\nAdditional Comments:\n- Why in training you used logistic loss instead of the more common cross-entropy loss?  Has this any connection with the final performance of the network? \n- In fig. 5 left for completeness I would like to see also results for DenseNet^MT and ResNet^MT \n- In fig. 5 left I cannot find the 4% and 8% higher accuracy with 0.5x10^10 to 1.0x10^10 FLOPs, as mentioned in section 5.1 anytime prediction results \n- How the budget in terms of Mul-Adds is actually estimated?\ n\nI think that this paper present a very powerful approach to speed-up the computational cost of a CNN at test time and clearly explains some of the common trade-offs between speed and accuracy and how to improve them.  The experimental evaluation is complete and accurate. \n\n",1,1,1,1,1,1,1,1,1,-1
Hk2MHt-3--R1,"This work proposed a reconfiguration of the existing state-of-the-art CNN model architectures including ResNet and DensNet.  By introducing new branching architecture, coupled ensembles, they demonstrate that the model can achieve better performance in classification tasks compared with the single branch counterpart with same parameter budget.  Additionally, they also show that the proposed ensemble method results in better performance than other ensemble methods (For example, ensemble over independently trained models)  not only in combined mode but also in individual branches. \n\nPaper Strengths:\n* The proposed coupled ensembles method truly show impressive results in classification benchmark (DenseNet-BC L = 118 k = 35 e = 3). \n* Detailed analysis on different ensemble fusion methods on both training time and testing time. \n* Simple but effective design to achieve a better result in testing time with same total parameter budget. \n\t\nPaper Weakness:\n* Some detail about different fusing method should be mentioned in the main paper instead of in the supplementary material. \n* In practice, how much more GPU memory is required to train the model with parallel branches (with same parameter budgets) because memory consumption is one of the main problems of networks with multiple branches. \n* At least one experiment should be carried out on a larger dataset such as ImageNet to further demonstrate the validity of the proposed method. \n* More analysis can be conducted on the training process of the model.  Will it converge faster?  What will be the total required training time to reach the same performance compared with single branch model with the same parameter budget",1,1,1,1,1,-1,1,1,1,-1
Hk2MHt-3--R2,"Strengths:\n* Very simple approach, amounting to coupled training of \""e\"" identical copies  of a chosen net architecture, whose predictions are fused during training.  This forces the different model instances to become more complementary. \n* Perhaps counterintuitively, experiments also show that coupled ensembling leads to individual nets that perform better than those produced by separate training. \n* The practical advantages of the proposed approach are twofold:\n1. Given a fixed parameter budget, coupled ensembling leads to better accuracy than a single net or an ensemble of disjointly-trained nets. \n2. For the same accuracy, coupled ensembling yields significant parameter savings. \n\nWeaknesses:\n* Although results are very strong, the proposed models do not outperform the state-of-the-art, except for the models reported in Table 4, which however were obtained by *traditional* ensembling of coupled ensembles.  \n* Coupled ensembling requires joint training of all nets in the ensemble and thus is limited by the size of the model that can be fit in memory.  Conversely, traditional ensembling involves separate training of the different instances and this enables the learning of an arbitrary number of individual nets.  \n* I am surprised by the results in Table 2, which suggest that the optimal number of nets in the ensemble is remarkably low (only 3!).  It'd be valuable to understand whether this kind of result holds for other network architectures or whether it is specific to this choice of net. \n* Strictly speaking it is correct to refer to the individual nets in the ensembles as \""branches\"" and \""basic blocks. \"" Nevertheless, I find the use of these terms confusing in the context of the proposed approach, since they are commonly used to denote concepts different from those represented here.   I would recommend refraining from using these terms here. \n\nOverall, the paper provides limited technical novelty.  Yet, it reveals some interesting empirical findings about the benefits of coordinated training of models in an ensemble",1,1,1,1,1,-1,1,1,1,-1
Hk2MHt-3--R3,"This paper presents a deep network architecture which processes data using multiple parallel branches and combines the posterior from these branches to compute the final scores; the network is trained in end-to-end, thus training the parallel branches jointly.  Existing literature with branching architecture either employ a 2 stage training approach, training branches independently and then training the fusion network, or the branching is restricted to local regions (set of contiguous layers).  In effect, this paper extends the existing literature suggesting end-to-end branching.  While the technical novelty, as described in the paper, is relatively limited, the thorough experimentation together with detailed comparisons between intuitive ways to combine the output of the parallel branches is certainly valuable to the research community. \n\n+ Paper is well written and easy to follow. \n+ Proposed branching architecture clearly outperforms the baseline network (same number of parameters with a single branch) and thus offer yet another interesting choice while creating the network architecture for a problem \n+ Detailed experiments to study and analyze the effect of various parameters including the number of branches as well as various architectures to combine the output of the parallel branches. \n+ [Ease of implementation] Suggested architecture can be easily implemented using existing deep learning frameworks. \n\n- Although joint end-to-end training of branches certainly brings value compared to independent training, but the increased resource requirements may limits the applicability to large benchmarks such as ImageNet.  While authors suggests a way to circumvent such limitations by training branches on separate GPUs but this would still impose limits on the number of branches as well as its ease of implementation. \n- Adding an overview figure of the architecture in the main paper (instead of supplementary) would be helpful. \n- Branched architecture serve as a regularization by distributing the gradients across different branches; however this also suggests that early layers on the network across branches would be independent.  It would helpful if authors would consider an alternate archiecture where early layers may be shared across branches, suggesting a delayed branching, with fusion at the final layer. \n- One of the benefits of architectures such as DenseNet is their usefulness as a feature extractor (output of lower layers) which generalizes even to domain other that the dataset; the branched architecture could potentially diminish this benefit. \n\nMinor edits: Page 1. 'significantly match and improve' => 'either match or improve'\n\nAdditional notes: \n- It would interesting to compare this approach with a conditional training pipeline that sequentially adds branches, keeping the previous branches fixed.  This may offer as a trade-off between benefits of joint training of branches vs being able to train deep models with several branches",1,1,1,1,1,1,1,1,1,-1
Hk3ddfWRW-R1,"The authors propose a new sampling based approach for inference in latent variable models.  They apply this approach to multi-modal (several \""intentions\"") imitation learning and demonstrate for a real visual robotics task that the proposed framework works better than deterministic neural networks and stochastic neural networks.  \n\nThe proposed objective is based upon sampling from the latent prior and truncating to the largest alpha-percentile likelihood values sampled.  The scheme is motivated by the fact that this estimator has a lower variance than pure sampling from the prior.  The objective to be maximized is a lower bound to 1/alpha * the likelihood.  \n\nQuality: The empirical results (including a video of an actual robotic arm system performing the task) looks good.  This reviewer is a bit sceptical to the methodology.  I am not convinced that the proposed bound will have low enough variance.  It is mentioned in a footnote that variational autoencoders were tested but that they failed.  Since the variational bound has much better sampling properties (due to recognition network, reparameterization trick and bounding to get log likelihoods instead of likelihoods) it is hard to believe that it is harder to get to work than the proposed framework.  Also, the recently proposed continuous relaxation of random variables seemed relevant.  \n\nClarity: The paper is fairly clearly written but there are many steps of engineering that somewhat dilutes the methodological contribution. \n\nSignificance: Hard to say. New method proposed and shown to work well in one case.  Too early to tell about significance. \n\nPro:\n1. Challenging and relevant problem solved better than other approaches. \n2. New latent variable model bound that might work better than classic approaches. \nCon:\n1. Not entirely convincing that it should work better than already existing methods. \n2. Missing some investigation of the properties of the estimator on simple problem to be compared to standard methods",1,1,1,1,-1,1,1,1,1,-1
Hk3ddfWRW-R2,"The authors provide a method for learning from demonstrations where several modalities of the same task are given.  The authors argue that in the case where several demonstrations exists and a deterministic (i.e., regular network) is given, the network learns some average policy from the demonstrations. \n\nThe paper begins with the authors stating the motivation and problem of how to program robots to do a task based only on demonstrations rather on explicit modeling or programming.  They put the this specific work in the right context of imitation learning and IRL.  Afterward, the authors argue that deterministic network cannot adequately several modalities.  The authors cover in Section 2 related topics, and indeed the relevant literature includes behavioral cloning, IRL , Imitation learning, GAIL, and VAEs.  I find that recent paper by Tamar et al 2016.  on Value Iteration Networks is highly relevant to this work: the authors there learn similar tasks (i.e., similar modalities) using the same network.  Even the control task is very similar to the current proposed task in this paper. \n\nThe authors argue that their contribution is 3-fold: (1) does not require robot  rollouts, (2) does not require label for a task, (3) work within raw image inputs.  Again, Tamar et al. 2016 deals with this 3 points. \n\nI went over the math.  It seems right and valid.  Indeed, SNN is a good choice for adding (Bayesian) context to a task.  Also, I see the advantage of referring only to the \""good\"" quantiles when needed.  It is indeed a good method for dealing with the variance.  \n\nI must say that I was impressed with the authors making the robot succeed in the tasks in hand (although reaching to an object is fairly simple task).  \n\nMy concerns are as follows:\n1) Seems like that the given trajectories are naturally divided with different tasks, i.e., a single trajectory consists only a single task.  For me, this is not the pain point in this tasks.  the pain point is knowing when tasks are begin and end.  \n2) I'm not sure, and I haven't seen evidence in the paper (or other references) that SNN is the only (optimal?) method for this context.  Why not adding (non Bayesian) context (not label) to the task will not work as well?  \n3) the robot task is impressive.  but proving the point, and for the ease of comparing to different tasks, and since we want to show the validity of the work on more than 200 trials, isn't showing the task on some simulation is better for understanding the different regimes that this method has advantage?  I know how hard is to make robotic tasks work...   \n4) I\u2019m not sure that the comparison of the suggested architecture to one without any underlying additional variable Z or context (i.e., non-Bayesian setup) is fair.  \""Vanilla\"" NN indeed may fail miserably .  So, the comparison should be to any other work that can deal with \""similar environment but different details\"". \n\nTo summarize, I like the work and I can see clearly the motivation.  But I think some more work is needed in this work: comparing to the right current state of the art, and show that in principal (by demonstrating on other simpler simulations domains) that this method is better than other methods",1,1,1,1,1,1,1,1,1,-1
Hk3ddfWRW-R3,"This paper focuses on imitation learning with intentions sampled \nfrom a multi-modal distribution.  The papers encode the mode as a hidden \nvariable in a stochastic neural network and suggest stepping around posterior \ninference over this hidden variable (which is generally required to \ndo efficient maximum likelihood) with a biased importance \nsampling estimator.  Lastly, they incorporate attention for large visual inputs.  \n\nThe unimodal claim for distribution without randomness is weak.  The distribution \ncould be replaced with a normalizing flow.  The use of a latent variable \nin this setting makes intuitive sense, but I don't think multimodality motivates it. \n\nMoreover, it really felt like the biased importance sampling approach should be \ncompared to a formal inference scheme.  I can see how it adds value over sampling \nfrom the prior, but it's unclear if it has value over a modern approximate inference \nscheme like a black box variational inference algorithm or stochastic gradient MCMC. \n\nHow important is using the pretrained weights from the deterministic RNN? \n\nFinally, I'd also be curious about how much added value you get from having \naccess to extra rollouts",1,1,1,1,1,-1,1,1,1,-1
Hk5elxbRW-R1,"The paper is clear and well written.  The proposed approach seems to be of interest and to produce interesting results.  As datasets in various domain get more and more precise, the problem of class confusing with very similar classes both present or absent of the training dataset is an important problem, and this paper is a promising contribution to handle those issues better. \n\nThe paper proposes to use a top-k loss such as what has been explored with SVMs in the past, but with deep models.  As the loss is not smooth and has sparse gradients, the paper suggests to use a smoothed version where maximums are replaced by log-sum-exps. \n\nI have two main concerns with the presentation. \n\nA/ In addition to the main contribution, the paper devotes a significant amount of space to explaining how to compute the smoothed loss.  This can be done by evaluating elementary symmetric polynomials at well-chosen values. \n\nThe paper argues that classical methods for such evaluations (e.g., using the usual recurrence relation or more advanced methods that compensate for numerical errors) are not enough when using single precision floating point arithmetic.  The paper also advances that GPU parallelization must be used to be able to efficiently train the network. \n\nThose claims are not substantiated, however, and the method proposed by the paper seems to add substantial complexity without really proving that it is useful. \n\nThe paper proposes a divide-and-conquer approach, where a small amount of parallelization can be achieved within the computation of a single elementary symmetric polynomial value.  I am not sure why this is of interest - can't the loss evaluation already be parallelized trivially over examples in a training/testing minibatch?  I believe the paper could justify this approach better by providing a bit more insights as to why it is required.  For instance:\n\n- What accuracies and train/test times do you get using standard methods for the evaluation of elementary symmetric polynomials? \n- How do those compare with CE and L_{5, 1} with the proposed method? \n- Are numerical instabilities making this completely unfeasible?  This would be especially interesting to understand if this explodes in practice, or if evaluations are just a slightly inaccurate without much accuracy loss. \n\n\nB/ No mention is made of the object detection problem, although multiple of the motivating examples in Figure 1 consider cases that would fall naturally into the object detection framework.  Although top-k classification considers in principle an easier problem (no localization), a discussion, as well as a comparison of top-k classification vs., e.g., discarding localization information out of object detection methods, could be interesting. \n\nAdditional comments:\n\n- Figure 2b: this visualization is confusing.  This is presented in the same figure and paragraph as the CIFAR results,  but instead uses a single synthetic data point in dimension 5, and k=1.  This is not convincing.  An actual experiment using full dataset or minibatch gradients on CIFAR and the same k value would be more interesting",1,1,1,1,1,-1,1,1,1,-1
Hk5elxbRW-R2,"This paper made some efforts in smoothing the top-k losses proposed in Lapin et al. (2015).   A family of smooth surrogate loss es was proposed, with the help of which the top-k error may be minimized directly.  The properties of the smooth surrogate losses were studied and the computational algorithms for SVM with these losses function were also proposed.  \n\nPros:\n1, The paper is well presented and is easy to follow. \n2, The contribution made in this paper is sound, and the mathematical analysis seems to be correct.  \n3, The experimental results look convincing.  \n\nCons:\nSome statements in this paper are not clear to me.  For example, the authors mentioned sparse or non-sparse loss functions.  This statement, in my view, could be misleading without further explanation (the non-sparse loss was mentioned in the abstract)",1,1,1,1,-1,-1,1,1,1,-1
Hk5elxbRW-R3,"This paper introduces a smooth surrogate loss function for the top-k SVM, for the purpose of plugging the SVM to the deep neural networks.  The idea is to replace the order statistics, which is not smooth and has a lot of zero partial derivatives, to the exponential of averages, which is smooth and is a good approximation of the order statistics by a good selection of the \""temperature parameter\"".  The paper is well organized and clearly written.  The idea deserves a publication. \n\nOn the other hand, there might be better and more direct solutions to reduce the combinatorial complexity.  When the temperature parameter is small enough, both of the original top-k SVM surrogate loss (6) and the smooth loss (9) can be computed precisely by sorting the vector s first, and take a good care of the boundary around s_{[k]}",1,1,-1,1,1,-1,1,1,1,-1
Hk6kPgZA--R1,"This paper proposes a principled methodology to induce distributional robustness in trained neural nets with the purpose of mitigating the impact of adversarial examples.  The idea is to train the model to perform well not only with respect to the unknown population distribution, but to perform well on the worst-case distribution in some ball around the population distribution.  In particular, the authors adopt the Wasserstein distance to define the ambiguity sets.  This allows them to use strong duality results from the literature on distributionally robust optimization and express the empirical minimax problem as a regularized ERM with a different cost.  The theoretical results in the paper are supported by experiments. \n\nOverall, this is a very well-written paper that creatively combines a number of interesting ideas to address an important problem.",1,1,-1,1,-1,-1,1,1,1,-1
Hk6kPgZA--R2,"This paper applies recently developed ideas in the literature of robust optimization, in particular distributionally robust optimization with Wasserstein metric,  and showed that under this framework for smooth loss functions when not too much robustness is requested,  then the resulting optimization problem is of the same difficulty level as the original one (where the adversarial attack is not concerned).  I think the idea is intuitive and reasonable, the result is nice.  Although it only holds when light robustness are imposed,  but in practice, this seems to be more of the case than say large deviation/adversary exists.  As adversarial training is an important topic for deep learning, I feel this work may lead to promising principled ways for adversarial training.",1,1,1,1,-1,-1,1,-1,1,-1
Hk6kPgZA--R3,"In this very good paper, the objective is to perform robust learning: to minimize not only the risk under some distribution P_0, but also against the worst case distribution in a ball around P_0.\ n\nSince the min-max problem is intractable in general,  what is actually studied here is a relaxation of the problem: it is possible to give a non-convex dual formulation of the problem.  If the duality parameter is large enough, the functions become convex given that the initial losses are smooth.  \n\nWhat follows are certifiable bounds for the risk for robust  learning and stochastic optimization over a ball of distributions.  Experiments show that this performs as expected, and gives a good intuition for the reasons why this occurs: separation lines are 'pushed away' from samples, and a margin seems to be increased with this procedure.",1,1,1,1,-1,-1,1,-1,1,-1
Hk8XMWgRb-R2,"\nIn this paper, the authors proposed an interesting algorithm for learning the l1-SVM and the Fourier represented kernel together.  The model extends kernel alignment with random feature dual representation and incorporates it into l1-SVM optimization problem.  They proposed algorithms based on online learning in which the Langevin dynamics is utilized to handle the nonconvexity.  Under some conditions about the quality of the solution to the nonconvex optimization, they provide the convergence and the sample complexity.  Empirically, they show the performances are better than random feature and the LKRF.  \n\nI like the way they handle the nonconvexity component of the model.  However, there are several issues need to be addressed.  \n\n1, In Eq. (6), although due to the convex-concave either min-max or max-min are equivalent, such claim should be explained explicitly.  \n\n2, In the paper, there is an assumption about the peak of random feature \""it is a natural assumption on realistic data that the largest peaks are close to the origin\"".  I was wondering where this assumption is used?  Could you please provide more justification for such assumption? \n\n3, Although the proof of the algorithm relies on the online learning regret bound, the algorithm itself requires visit all the data in each update, and thus, it is not suitable for online learning.  Please clarify this in the paper explicitly.  \n\n4, The experiment is weak.  The algorithm is closely related to boosting and MKL, while there is no such comparison.  Meanwhile, Since the proposed algorithm requires extra optimization w.r.t. random feature, it is more convincing to include the empirical runtime comparison.  \n\nSuggestion: it will be better if the author discusses some other model besides l1-SVM with such kernel learning",1,1,1,1,1,-1,1,1,1,-1
Hkc-TeZ0W-R1,"The paper seems clear enough and original enough.  The idea of jointly forming groups of operations to colocate and figure out placement on devices seems to hold merit.  Where the paper falls short is motivating the problem setting.  Traditionally, for determining optimal execution plans, one may resort to cost-based optimization (e.g., database management systems).  This paper's introduction provides precisely 1 statement to suggest that may not work for deep learning.  Here's the relevant phrase: \""the cost function is typically non-stationary due to the interactions between multiple devices\"" . Unfortunately, this statement raises more questions than it answers . Why are the cost functions non-stationary?  What exactly makes them dynamic?  Are we talking about a multi-tenancy setting where multiple processes execute on the same device ? Unlikely, because GPUs are involved.  Without a proper motivation, its difficult to appreciate the methods devised. \n\nPros:\n- Jointly optimizing forming of groups and placing these seems to have merit\ n- Experiments show improvements over placement by human \""experts \""\n- Targets an important problem\n\nCons:\n- Related work seems inadequately referenced . There exist other linear/tensor algebra engines/systems that perform such optimization including placing operations on devices in a distributed setting.  This paper should at least cite those papers and qualitatively compare against those approaches.  Here's one reference (others should be easy to find): \""SystemML's Optimizer: Plan Generation for Large-Scale Machine Learning Programs\"" by Boehm et al, IEEE Data Engineering Bulletin, 2014. \n- The methods are not well motivated.  There are many approaches to devising optimal execution plans, e.g., rule-based, cost-based, learning-based.  In particular, what makes cost-based optimization inapplicable?  Also, please provide some reasoning behind your hypothesis which seems to be that while costs may be dynamic, optimally forming groups and placing them is learn-able. \n- The template seems off . I don't see the usual two lines under the title (\""Anonymous authors\"", \""Paper under double-blind review\""). \n- The title seems misleading.  \"".... Device Placement\"" seems to suggest that one is placing devices when in fact, the operators are being placed.",1,1,1,1,1,1,1,1,1,1
Hkc-TeZ0W-R2,"In a previous work [1], an auto-placement (better model partition on multi GPUs) method was proposed to accelerate a TensorFlow model\u2019s runtime.  However, this method requires the rule-based co-locating step, in order to resolve this problem, the authors of this paper purposed a fully connect network (FCN) to replace the co-location step.  In particular, hand-crafted features are fed to the FCN and the output is the prediction of group id of this operation.  Then all the embeddings in each group are averaged to serve as the input of a seq2seq encoder.  \n\nOverall speaking, this work is quite interesting.  However, it also has several limitations, as explained below.\ n\nFirst, the computational cost of the proposed method seems very high.  It may take more than one day on 320-640 GPUs for training (I did not find enough details in this paper, but the training complexity will be no less than the in [1]).  This makes it very hard to reproduce the experimental results (in order to verify it), and its practical value becomes quite restrictive (very few organizations can afford such a cost). \n\nSecond, as the author mentioned, it\u2019s hard to compare the experimental results in this paper wit those in [1] because different hardware devices and software versions were used.  However, this is not a very sound excuse.  I would encourage the authors to implement colocRL [1] on their own hardware and software systems, and make direct comparison.  Otherwise, it is very hard to tell whether there is improvement, and how significant the improvement is . In addition, it would be better to have some analysis on the end-to-end runtime efficiency and the effectiveness of the placements .\n\n [1] Mirhoseini A, Pham H, Le Q V, et al. Device Placement Optimization with Reinforcement Learning[J]. arXiv preprint arXiv:1706.04972, 2017. https://arxiv.org/pdf/1706.04972.pdf \n",1,1,1,1,1,1,1,1,1,1
Hkc-TeZ0W-R3,"This paper proposes a device placement algorithm to place operations of tensorflow on devices.  \n\nPros:\n\n1. It is a novel approach which trains the placement end to end .\n2. The experiments are solid to demonstrate this method works very well .\n3. The writing is easy to follow. \n4. This would be a very useful tool for the community if open sourced. \n\nCons:\n\n1. It is not very clear in the paper whether the training happens for each model yielding separate agents, or a shared agent is trained and used for all kinds of models . The latter would be more exciting.  The adjacency matrix varies size for different graphs, so I guess a separate agent is trained for each graph?  However, if the agent is not shared, why not just use integer to represent each operation in the graph, since overfitting would be more desirable in this case. \n2. Averaging the embedding is hard to understand especially for the output sizes and number of outputs.\ n3. It is not clear how the adjacency information is used.\n",1,1,1,1,1,-1,1,1,1,-1
HkCsm6lRb-R1,"The authors propose a generative method that can produce images along a hierarchy of specificity, i.e. both when all relevant attributes are specified,  and when some are left undefined, creating a more abstract generation task.  \n\nPros:\n+ The results demonstrating the method's ability to generate results for (1) abstract and (2) novel/unseen attribute descriptions, are generally convincing . Both quantitative and qualitative results are provided.  \n+ The paper is fairly clea r.\n\nCons:\n- It is unclear how to judge diversity qualitatively, e.g. in Fig. 4(b) .\n- Fig. 5 could be more convincing; \""bushy eyebrows\"" is a difficult attribute to judge , and in the abstract generation when that is the only attribute specified, it is not clear how good the results are.\n",1,1,1,1,-1,-1,1,1,1,-1
HkCsm6lRb-R2,"This paper presented a multi-modal extension of variational autoencoder (VAE) for the task \""visually grounded imagination. \""  In this task,  the model learns a joint embedding of the images and the attributes . The proposed model is novel but incremental comparing to existing frameworks.   The author also introduced new evaluation metrics to evaluate the model performance concerning correctness, coverage, and compositionality.  \n\nPros:\n1. The paper is well-written, and the contribution (both the model and the evaluation metric) potentially can to be very useful in the community.   \n2. The discussion comparing the related work/baseline methods is insightful.  \n3. The proposed model addresses many important problems, such as attribute learning, disentanged representation learning, learning with missing values, and proper evaluation methods.  \n\nCons/questions:\n1. The motivation of the model choice of q is not clear.  \nComparing to BiVCCA, apart from the differences that the author discussed, a big difference is the choice of q.   BiVCCA uses two inference networks q(z|x) and q(z|y), while the proposed method uses three. q(z|x), q(z|y), and q(z|x,y).   How does such model choice affect the final performance?  \n\n2. Baselines are not necessarily sufficient . \nThe paper compared the vanilla version of BiVCCA but not the one with factorized representation version . In the original VAECCA paper, the extension of using factorized representation (private and shared) improved the performance] . The author should also compare this extension of VAECCA. \n\n3. Some details are not clear.  \na) How to set/learn the scaling parameter \\lambda_y and \\beta_y ? If it is set as hyper-parameter, how does the performance change concerning them?  \nb) Discussion of the experimental results is not sufficient . For example, why JMVAE performs much better than the proposed model when all attributes are given.   What is the conclusion from Figure 4(b)?  The JMVAE seems to generate more diverse (better coverage) results which are not consistent with the claims in the related work.   The same applies to figure 5.",1,1,1,1,1,1,1,1,1,-1
HkCsm6lRb-R3,"The paper proposes a method for generating images from attributes.  The core idea is to learn a shared latent space for images and attributes with variational auto-encoder using paired samples , and additionally learn individual inference networks from images or attributes to the latent space using unpaired samples.  During training the auto-encoder is trained on paired data (image, attribute) whereas during testing one uses the unpaired data to generate an image corresponding to an attribute or vice versa.  The authors propose handling missing data using a product of experts where the product is taken over available attributes, and it sharpens the prior distribution.  The authors evaluate their method using correctness i.e. if the generated images have the desired attributes, coverage i.e.  if the generated images sample unspecified attributes well, and compositionality i.e. if  images can be generated from unseen attributes.  Although the proposed method performs slightly poor compared to JMVAE in terms of concreteness when all attributes are provided,  it outperforms when some of the attributes are missing (Figure 4a).  It also outperforms existing methods in terms of coverage and compositionality. \n\nMajor comments:\n\nThe paper is well written, and summarizes its contribution succinctly. \n\nI did not fully understand the 'retrofitting' idea.  If I understood correctly, the authors first train \\theta and \\phi and then fix \\theta to train \\phi_x and \\phi_y.  If that is true, then is \\calL(\\theta, \\phi, \\phi_x, \\phi_y) are right cost function since one does not maximize all three ELBO terms when optimizing \\theta? Please clarify? \n\nMinor comments:\n\n- 'in order of increasing abstraction', does the order of gender-> smiling or not -> hair color matter? Or, is male, *, blackhair a valid option?\n\n-  what are the image sizes for the CelebA dataset\n\n- page 5: double the\n\n - Which multi-label classifier is used to classify images in attributes?",1,1,1,1,1,-1,1,1,1,-1
HkcTe-bR--R1,"The paper proposes a set of benchmarks for molecular design, and compares different deep models against them.  The main contributions of the paper are 19 molecular design benchmarks (with chembl-23 dataset), including two molecular design evaluation criterias and comparison of some deep models using these benchmarks.  The paper does not seem to include any method development. \n\nThe paper suffers from a lack of focus . Several existing models are discussed to some length, while the benchmarks are introduced quite shortly.  The dataset is not very clearly defined: it seems that there are 1.2 million training instance, does this apply for all benchmarks?  The paper's title also does not seem to fit: this feels like a survey paper, which is not reflected in the title . Biologically lots of important atoms are excluded from the dataset, for instance natrium, calcium and kalium . I don't see any reason to exlude these . What does \""biological activities on 11538 targets\"" mean?  \n\nThe paper discussed molecular generation and reinforcement learning, but it is somewhat unclear how it relates to the proposed dataset since a standard training/test setting is used.  Are the test molecules somehow generated in a directed or undirected fashion?  Shouldn't there also be experiments on comparing ways to generate suitable molecules, and how well they match the proposed criterion?   There should be benchmarks for predicting molecular properties (standard regression), and for generating molecules with certain properties.  Currently it's unclear which type of problems are solved here. \n\nTable 1 lists 5 models, while fig 3 contains 7, why the discrepancy?  In table 1 the plotted runs seem to differ a lot from average results (e.g. -0.43 to 0.15, or 0.32 to 0.83).  Variances should be added, and preferably more than 3 initialisations used. \n\nOverall this is an interesting paper , but does not have any methodological contribution, and there is also few insightful results about the compared methods, nor is there meaningful analysis of the problem domain of molecules eithe",1,1,1,1,1,-1,1,1,1,1
HkcTe-bR--R2,"Summary:\nThis work is about model evaluation for molecule generation and design.  19 benchmarks are proposed, small data sets are expanded to a large, standardized data set and it is explored how to apply new RL techniques effectively for molecular design.  \n\non the positive side: \nThe paper is well written, quality and clarity of the work are good.  The work provides a good overview about how to apply new reinforcement learning techniques for sequence generation.  It is investigated how several RL strategies perform on a large, standardized data set.  Different RL models like Hillclimb-MLE, PPO, GAN, A2C are investigated and discussed.   An implementation of 19 suggested benchmarks of relevance for de novo design will be provided as open source as an OpenAI Gym.  \n\n\non the negative side: \nThere is no new novel contribution on the methods side.   \n\n\n\nminor comments:\n\nSection 2.1. \nsee Fig.2 \u2014> see Fig.1\npage 4just before equation 8: the the",1,1,1,1,1,-1,1,1,1,1
HkcTe-bR--R3,"Summary: This paper studies a series of reinforcement learning (RL) techniques in combination with recurrent neural networks (RNNs) to model and synthesise molecules.  The experiments seem extensive, using many recently proposed RL methods,  and show that most sophisticated RL methods are less effective than the simple hill-climbing technique, with PPO is perhaps the only exception .  \n\nOriginality and significance:  \n\nThe conclusion from the experiments could be valuable to the broader sequence generation/synthesis field, showing that many current RL techniques can fail dramatically.  \n\nThe paper does not provide any theoretical contribution but nevertheless is a good application paper combining and comparing different techniques .\n\nClarity: The paper is generally well-written. However, I'm not an expert in molecule design, so might not have caught any trivial errors in the experimental set-up",1,1,1,1,-1,-1,1,1,1,-1
HkCvZXbC--R1,"Summary: This paper studied the conditional image generation with two-stream generative adversarial networks.  More specifically, this paper proposed an unsupervised learning approach to generate (1) foreground region conditioned on class label and (2) background region without semantic meaning in the label.  During training, two generators are competing against each other to hallucinate foreground region and background region with a physical gating operation.  An auxiliary \u201clabel difference cost\u201d was further introduced to encourage class information captured by the foreground generator.  Experiments on MNIST, SVHN, and CelebA datasets demonstrated promising generation results with the unsupervised two-stream generation pipeline. \n\n== Novelty/Significance ==\nControllable image generation is an important task in representation learning and computer vision.  I also like the unsupervised learning through gating function and label difference cost . However, considering many other related work mentioned by the paper, the novelty in this paper is quite limited. For example, layered generation (Section 2.2.1) has been explored in Yan et al 2016 (VAEs) and Vondrick et al 2016 (GANs). \n\n== Detailed comments ==\nThe proposed two-stream model is developed with the following two assumptions: (1) Single object in the scene; and (2) Class information is provided for the foreground/object region.  Although the proposed method learns to distinguish foreground and background in an unsupervised fashion, it is limited in terms of applicability and generalizability . For example, I am not convinced if the two-stream generation pipeline can work well on more challenging datasets such as MS-COCO, LSUN, and ImageNet.  \n\nGiven the proposed method is controllable image generation, I would assume to see the following ablation studies: keeping two latent variables from (z_u, z_l, z_v) fixed, while gradually changing the value of the other latent variable.  However, I didn\u2019t see such detailed analysis as in the other papers on controllable image generation. \n\nIn Figure 7 and Figure 10, the boundary between foreground and background region is not very sharp.  It looks like equation (5) and (6)  are insufficient for foreground and background separation (triplet/margin loss could work better) . Also, in CelebA experiment, it is not a well defined experimental setting since only binary label (smiling/non-smiling) is conditioned.  Is it possible to use all the binary attributes in the dataset. \n\nAlso, please either provide more qualitative examples or provide some type of quantitative evaluations (through user study , dataset statistics, or down-stream recognition tasks) .\n\nOverall, I believe the paper is interesting but not ready for publication.  I encourage authors to investigate (1) more generic layered generation process and (2) better unsupervised boundary separation . Hopefully, the suggested studies will improve the quality of the paper in the future submission. \n\n== Presentation ==\nThe paper is readable but not well polished . \n\n-- In Figure 1, the \u201cG1\u201d on the right should be \u201cG2\u201d;\n-- Section 2.2.1, \u201cX_f\u201d should be \u201cx_f\u201d;\n-- the motivation of having \u201cz_v\u201d should be introduced earlier;\n-- Section 2.2.4, please use either \u201calpha\u201d or \u201c\\alpha\u201d but not both;\n-- Section 3.3, the dataset information is incorrect: \u201c20599 images\u201d should be \u201c202599 images\u201d;\n\nMissing reference:\n- - Neural Face Editing with Intrinsic Image Disentangling, Shu et al. In CVPR 2017. \n-- Domain Separation Networks, Bousmalis et al. In NIPS 2016.\ n-- Unsupervised Image-to-Image Translation Networks, Liu et al. In NIPS 2017.\n",1,1,1,1,1,1,1,1,1,1
HkCvZXbC--R2,"[Overview]\n\nThis paper proposed a new generative adversarial network, called 3C-GAN for generating images in a composite manner.  In 3C-GAN, the authors exploited two generators, one (G1) is for generating context images, and the other one (G2) is for generating semantic contents.  To generate the semantic contents, the authors introduced a conditional GAN scheme, to force the generated images to match the annotations.  After generating both parts in parallel, they are combined using alpha blending to compose the final image.  This generated image is then sent to the discriminator.  The experiments were conducted on three datasets, MNIST, SVHN and MS-CelebA.  The authors showed qualitative results on all three datasets, demonstrating that AC-GAN could disentangle the context part from the semantic part in an image, and generate them separately. \n\n[Strenghts]\n\nThis paper introduced a layered-wise image generation, which decomposed the image into two separate parts: context part, and semantic part.  Corresponding to these two parts are two generators.  To ensure this, the authors introduced three strategies:\n\n1. Adding semantic labels: the authors used image semantic labels as the input and then exploited a conditional GAN to enforce one of the generators to generate semantic parts of images.  As usual, the label information was added as the input of generator and discriminator as well. \n\n2. Adding label difference cost: the intuition behind this loss is that changing the label condition should merely affect the output of G2.  Based on this, outputs of Gc should not change much when flipping the input labels. \n\n3. Adding exclusive prior: the prior is that the masks of context part (m1) and semantic part (m2) should be exclusive to each other.  Therefore, the authors added another loss to reduce the sum of component-wise multiplication between m1 and m2. \n\nDecomposing the semantic part from the context part in an image based on a generative model is an interesting problem.  However, to my opinion, completing it without any supervision is challenging and meaningless.  In this paper, the authors proposed a conditional way to generate images compositionally.  It is an interesting extension of previous works, such as Kwak & Zhang (2016) and Yang (2017). \n\n[Weaknesses]\n\nThis paper proposed an interesting and intuitive image generation model.  However, there are several weaknesses existed:\n\n1.  There is no quantitative evaluation and comparisons.  From the limited qualitative results shown in Fig.2-10, we can hardly get a comprehensive sense about the model performance.  The authors should present some quantitative evaluations in the paper, which are more persuasive than a number of examples.  To do that, I suggest the authors exploited evaluation metrics, such as Inception Score to evaluate the overall generation performance.  Also, in Yang (2017) the authors proposed adversarial divergence, which is suitable for evaluating the conditional generation.  Hence, I suggest the authors use a similar way to evaluate the classification performance of classification model trained on the generated images.  This should be a good indicator to show whether the proposed 3C-GAN could generate more realistic images which facilitate the training of a classifier. \n\n2. The authors should try more complicated datasets, like CIFAR-10.  Recently, CIFAR-10 has become a popular dataset as a testbed for evaluating various GANs.  It is easy to train since its low resolution, but also means a lot since it a relative complicated scene.  I would suggest the authors also run the experiments on CIFAR-10. \n\n3. The authors did not perform any ablation study.  Apart from several generation results based on 3C-GAN, iIcould not found any generation results from ablated models.  As such, I can hardly get a sense of the effects of different losses and know about the relative performance in the whole GAN spectrum.  I strongly suggest the authors add some ablation studies.  The authors should at least compare with one-layer conditional GAN.  \n\n4. The proposed model merely showed two-layer generation results.  There might be two reasons: one is that it is hard to extend it to more layer generation as I know, and the other one reason is the inflexible formulation to compose an image in 2.2.1 and formula (6).  The authors should try some datasets like MNIST-TWO in Yang (2017) for demonstration. \n\n5. Please show f1, m1, f2, m2 separately, instead of showing the blending results in Fig3, Fig4, Fig6, Fig7, Fig9, and Fig10.  I would like to see what kind of context image and foreground image 3C-GAN has generated so that I can compare it with previous works like Kwak & Zhang (2016) and Yang (2017). \n\n6. I did not understand very well the label difference loss in (5).  Reducing the different between G_c(z_u, z_v, z_l) and G_c(z_u, z_v, z_l^f) seems not be able to force G1 and G2 to generate different parts of an image.  G2 takes all the duty  can still obtain a lower L_ld.  From my point of view, the loss should be added to G1 to make G1 less prone to the variation of label information. \n\n7. Minor typos and textual errors. In Fig.1, should the right generator be G2 rather than G1? In 2.1.3 and 2.2.1, please add numbers to the equations. \n\n[Summary]\n\nThis paper proposed an interesting way of generating images, called 3C-GAN.  It generates images in a layer-wise manner.  To separate the context and semantic part in an image, the authors introduced several new techniques to enforce the generators in the model undertake different duties.  In the experiments, the authors showed qualitative results on three datasets, MNIST, SVHN and CelebA.  However, as I pointed out above, the paper missed quantitative evaluation and comparison, and ablation study.  Taking all these into account, I think this paper still needs more works to make it solid and comprehensive before being accepted",1,1,1,1,1,1,1,1,1,1
HkCvZXbC--R3,"\n- Paper summary\n\nThe paper proposes a label-conditional GAN generator architecture and a GAN training objective for the image modeling task.  The proposed GAN generator consists of two components where one focuses on generating foreground while the other focuses on generating background.  The GAN training objective function utilizing 3 conditional classifier.  It is shown that through combining the generator architecture and the GAN training objective function, one can learn a foreground--background decomposed generative model in an unsupervised manner . The paper shows results on the MNIST, SVHN, and Celebrity Faces datasets. \n\n- Poor experimental validation\n\nWhile it is interesting to know that a foreground--background decomposed generative model can be learned in an unsupervised manner,  it is clear how this capability can help practical applications, especially no such examples are shown in the paper.  The paper also fails to provide any quantitative evaluation of the proposed method.  For example, the paper will be more interesting if inception scores were shown for various challenging datasets.   In additional, there is no ablation study analyzing impacts of each design choices . As a result, the paper carries very little scientific value.",1,1,1,1,-1,-1,1,1,1,-1
HkCy2uqQM-R1,"The paper intends to show that complex and real valued neural network are different and lead to different results on similar tasks, the complex valued network being more appropriate to 'difficult' problems and datasets. \nThe work seems to have been written in a rush leading to a big number of typos and quickly filled experiment tables (7 and 8 are full of zeros ?).  The only valid conclusion is that real and complex valued neural network cannot be directly compared using the same number of parameters.  Some theoretical aspect or at least some intuition should be more in depth detailed to understand when one should be better than the other. \n\nConcerning the novelty the paper is in the same spirit as https://arxiv.org/abs/1705.09792 but with weaker experiments, theoretical justifications and no valid conclusion",1,1,1,1,1,1,1,1,1,-1
HkCy2uqQM-R2,"This work re-evaluates complex-valued neural networks: complex weights, complex activation functions. \n\nThe paper acknowledges that complex networks are not new, and that the findings of previous authors is that complex networks perform less well than real-valued alternatives. \n\nThe paper reports a comparison of real-valued and complex-valued neural networks, controlling for storage capacity (with an interesting discussion of controlling for capacity in terms of computational inference). \n\nThe paper concludes with \""Overall the complex-valued neural networks do not perform as well as expected .\"" I didn't understand this conclusion, because previous work found complex-valued neural networks to be inferior, which is consistent with the results reported here.  I did not see support in this paper for the claim in the abstract that special architectures make complex networks work better, or that they are well suited to particular data sets .\n\nThe empirical results are only presented in table-of-numbers format (graphical comparisons would be easier to understand), and tables 5-8 are all zero, which doesn't make sense for these classification tasks.",1,1,1,1,1,1,1,1,-1,-1
HkCy2uqQM-R3,"The authors show how techniques typically applied to real-valued networks (e.g. with real-valued inputs and parameters) can be straighforwardly generalized to complex-valued networks (e.g. with complex-valued inputs and parameters).   The authors then provide several evaluations of complex-valued networks on some standard ML benchmark tasks.  They find that the complex-valued networks do not in general perform better than real-valued networks.  \n\n====================\nClarity:  I found the paper clear and easy to understand.   In a number of places there are clear signs of sloppiness (e.g. undefined citations).   I found the undefined citations in the middle of page 2 frustrating, since I'd have liked to follow up on those citations as comparison points for this work . \n\nQuality: The mathematical formulas describing basic complex analysis ideas (e.g.  derivatives of complex functions, definitions of complex versions of standard activation functions) seem reasonable to me .   The general approach of assigning a parameter budget to ensure fairness in comparison between complex and real-valued networks seems reasonable -- - although of course, since all results should be reported on cross-validated testing subsets anyhow,   parameter equalization is not the only approach to fair evaluation .   \n\nOriginality:  It seems very unclear to me what is added in this paper in comparison to works like (e.g.) Trabelsi (2017).   That and other recent work have provided some systematic evaluations of complex-valued networks, and shown their utility in a number of cases.    The current paper's authors talk about previous work not being well-controlled for number of parameters.    However, at least in some key cases in the recent literature, parameter numbers *were* controlled (see e.g. Table 4 of Trabelsi (2017)).   So I'm not really sure what is being added here.  \n\nSignificance:   The paper does not make a great case for caring about complex-valued networks.   Of course, negative results are of value, but it doesn't seem like much is at stake in this work to begin with.    It's not like people expected complex-valued networks to somehow be extremely effective for the tasks discussed here - - so the failure to be better than the real-valued alternatives seems unremarkable.    The paper also doesn't illustrate any novel results on tasks for which it would be reasonable to assume that complex-valued inputs would be particularly important.   (The authors reference some signal processing tasks in the introduction , but don't actually show any results on such tasks.)    \n\",1,1,1,1,-1,1,1,1,1,1
HkeJVllRW-R1,"\n\nThis paper presented interesting ideas to reduce the redundancy in convolution kernels.  They are very close to existing algorithms .\n\n(1)\tThe SW-SC kernel (Figure 2 (a)) is an extension of the existing shaped kernel (Figure 1 (c)). \n(2)\tThe CW-SC kernel (Figure 2 (c)) is very similar to interleaved group convolutions.  The CW-SC kernel can be regarded as a redundant version of interleaved group convolutions [1].  \n\nI would like to see more discussions on the relation to these methods and more strong arguments for convincing reviewers to accept this paper.  \n\n[1] Interleaved Group Convolutions. Ting Zhang, Guo-Jun Qi, Bin Xiao, and Jingdong Wang. ICCV 2017.  http://openaccess.thecvf.com/content_ICCV_2017/papers/Zhang_Interleaved_Group_Convolutions_ICCV_2017_paper.pdf",1,-1,1,1,1,1,1,-1,-1,-1
HkeJVllRW-R2,"This paper introduces a new design of kernels in convolutional neural networks.  The idea is to have sparse but complementary kernels with predefined patterns, which altogether cover the same receptive field as dense kernels.  Because of the sparsity of such kernels, deeper or wider networks can be designed at the same computational cost as networks with dense kernels. \n\nStrengths:\n- The complementary kernels come at no loss compare to standard ones\n- The resulting wider networks can achieve better accuracies than the original ones\n\nWeaknesses:\n- The proposed patterns are clear for 3x3 kernels, but no solution is proposed for other dimensions\n- The improvement over the baseline is not very impressive\n- There is no comparison against other strategies, such as 1xk and kx1 kernels (e.g., Ioannou et al. 2016)\n\nDetailed comments:\n- The separation into + and x patterns is quite clear for 3x3 kernels.  However, two such patterns would not be sufficient for 5x5 or 7x7 kernels.  This idea would have more impact if it generalized to arbitrary kernel dimensions. \n\n- The improvement over the original models are of the order of less than 1 percent.  I understand that such improvements are not easy to achieve, but one could wonder if they are not due to the randomness of initialization/mini-batches.  It would be more meaningful to report average accuracies and standard deviations over several runs of each experiment. \n\n- Section 4.4 briefly discusses the comparison with using 3x1 and 1x3 kernels, mentioning that an empirical comparison is beyond the scope of this paper.  To me, this comparison is a must. In fact, the discussion in this section is not very clear to me, as it mentions additional experiments that I could not find (maybe I misunderstood the authors).  What I would like to see is the results of a model based on the method of Ioannou et al, 2016 with the same number of FLOPS. \n\n- In Section 2, the authors review ideas of so-called random kernel sparsity.  Note that the work of Wen et al., 2016, and that of Alvarez & Salzmann, NIPS 2016, do not really impose random sparsity, but rather aim to cancel out entire kernels, thus reducing the size of the model and not requiring implementation overhead.  They also do not require pre-training and re-training, but just a single training procedure.  Note also that these methods often tend not to decrease accuracy, but rather even increase it (by a similar magnitude to that in this paper), for a more compact model. \n\n- In the context of random sparsity, it would be worth citing the work of Collins & Kohli, 2014, Memory Bounded Deep Convolutional Networks. \n\n- I am not entirely convinced by the discussion of the grouped sparsity method in Section 3.1. In fact, the order of the channels is arbitrary, since the kernels are learnt.  Therefore, it seems to me that they could achieve the same result. Maybe the authors can clarify this? \n\n- Is there a particular reason why the central points appears in both complementary kernels (+ and x)? \n\n- Why did the authors change the training procedure of ResNets slightly compared to the original paper, i.e., 50k training images instead of 45k training + 5k validation?  Did the baseline (original model) reported here also use 50k?  What would the results be with 45k? \n\n- Fig. 5 is not entirely clear to me.  What was the width of each layer?  The original one or the modified one? \n\n- It would be interesting to report the accuracy of a standard ResNet with 1.325*width as a comparison, as well as the runtime of such a model. \n\n- In Table 4, I find it surprising that there is an actual speedup for the model with larger width.  I would have expected the same runtime.  How do the authors explain this",1,1,1,1,1,1,1,1,1,-1
HkeJVllRW-R3,"Summary:\nThis paper proposed a sparse-complementary convolution as an alternative to the convolution operation in deep networks.  In this method, two new types of kernels are developed, namely the spatial-wise and channel-wise sparse-complementary kernels.  The authors argue that the proposed kernels are able to cover the same receptive field as the regular convolution with almost half the parameters.  By adding more filters or layers in the model while keeping the same FLOPs and parameters, the models with the proposed method outperform the regular convolution models.  The paper is easy to follow and the idea is interesting.  However, the novelty of the paper is limited and the experiments are not sufficient. \n\nStrengths:\n1. The authors proposed the sparse-complementary convolution to cover the same receptive field as the regular convolution.  \n\n2. The authors implement the proposed sparse-complementary convolution on NVIDIA GPU and achieved competitive speed under the same computational load to regular convolution. \n\n3. The authors demonstrated that, given the same resource budget, the wider networks with the proposed method are more efficient than the deeper networks due to the nature of GPU parallel mechanism. \n\nWeak points:\n\n1. The novelty of this paper is limited.  The main idea is to design complementary kernels that cover the same receptive field as the regular convolution.  However, the performance improvement is marginal and may come from the benefit of wide networks rather than the proposed complementary kernels.  Moreover, the experiments are not sufficient to support the arguments.  For example, how is the performance of a model containing SW-SC or CW-SC without deepening or widening the networks?  Without such experiment, it is unclear whether the improved performance comes from the sparse-complementary kernels or the increased number of kernels. \n\n2. The relationship between the proposed spatial-wise kernels and the channel-wise kernels is not very clear.  Which kernel is better and how to choose between them in a deep network?  There is no experimental proof in the paper. \n\n3. The proposed two kernels introduce sparsity in the spatial and channel dimension, respectively.  The two methods are used separately.  Is it possible to combine them together? \n\n4. The proposed method only considers the \u201c+-shape\u201d and \u201cx-shape\u201d sparse pattern.  Given the same receptive field with multiple complementary kernels, is the kernel shape important for the training?  There is no experimental result to verify this. \n\n5. As mentioned in the paper, there are many methods which introduce sparsity in the convolution layer, such as \u201crandom kernels\u201d, \u201clow-rank approximated kernels\u201d and \u201cmixed-shape kernels\u201d.  However, there is no experimental comparison with these methods. \n\n6. In the paper, the author mentioned another sparse-complementary baseline (sc-seq), which applies sparse kernels sequentially.  It yields smaller receptive field than the proposed method when the model depth is very small.  Indeed, when the model goes deeper, the receptive field becomes very close to that of the proposed method.  In the experiments, it is strange that this method can also achieve comparable or better results.  So, what is the advantage of the proposed \u201csc\u201d method compared to the \u201csc-seq\u201d method? \n\n\n8. Figure 5 is hard to understand.  This figure only shows that training shallower networks is more effective than training the deeper networks on GPU.  However, it does not mean training the wider networks is more efficient than training the deeper ones",1,1,1,1,1,1,1,1,-1,-1
HkfXMz-Ab-R1,"The authors introduce an algorithm in the subfield of conditional program generation that is able to create programs in a rich java like programming language.  In this setting, they propose an algorithm based on sketches- abstractions of programs that capture the structure but discard program specific information that is not generalizable such as variable names.  Conditioned on information such as type specification or keywords of a method they generate the method's body from the trained sketches.  \n\u00a0\nPositives:\n\u00a0\n\t\u2022\tNovel algorithm and addition of rich java like language in subfield of 'conditional program generation' proposed\n\t\u2022\tVery good abstract:  It explains high level overview of topic and sets it into context plus gives a sketch of the algorithm and presents the positive results. \n\t\u2022\tExcellently structured and presented paper\n\u00a0\n\t\u2022\t  Motivation given in form of relevant applications and mention that it is relatively unstudied\n\t\u2022\t The hypothesis/ the papers goal is clearly stated.  It is introduced with 'We ask' followed by two well formulated lines that make up the hypothesis.  It is repeated multiple times throughout the paper.  Every mention introduces either a new argument on why this is necessary or sets it in contrast to other learners, clearly stating discrepancies. \n\t\u2022\tExplanations are exceptionally well done: terms that might not be familiar to the reader are explained.  This is true for mathematical aspects as well as program generating specific terms . Examples are given where appropriate in a clear and coherent manner\n\t\u2022\t Problem statement well defined mathematically and understandable for a broad audience\n\t\u2022\t Mentioning of failures and limitations demonstrates a realistic  view on the project\n\t\u2022\t Complexity and time analysis provided\n\t\u2022\t Paper written so that it's easy for a reader to implement the methods\n\t\u2022\t Detailed descriptions of all instantiations even parameters and comparison methods\n\t\u2022\tSystem specified\n\t\u2022\tValidation method specified\n\t\u2022\tData and repository,  as well as cleaning process provided\n\t\u2022\tEvery figure and plot is well explained and interpreted\n\t\u2022\t Large successful evaluation section provided\n\t\u2022\t Many different evaluation measures defined to measure different properties of the project\n\t\u2022\t Different observability modes\n\t\u2022\tEvaluation against most compatible methods from other sources \n\t\u2022\t  Results are in line with hypothesis\n\t\u2022\tThorough appendix clearing any open questions \n\u00a0\n It would have been good to have a summary/conclusion/future work section\n\u00a0\nSUMMARY: ACCEPT.   The authors present a very intriguing novel approach that  in a clear and coherent way.  The approach is thoroughly explained for a large audience. The task itself is interesting and novel.  The large evaluation section that discusses many different properties is a further indication that this approach is not only novel but also very promising.  Even though no conclusive section is provided,  the paper is not missing any information.\n",1,1,1,1,1,-1,1,1,1,1
HkfXMz-Ab-R2,"This paper aims to synthesize programs in a Java-like language from a task description (X) that includes some names and types of the components that should be used in the program.  The paper argues that it is too difficult to map directly from the description to a full program, so it instead formulates the synthesis in two parts.  First, the description is mapped to a \""sketch\"" (Y) containing high level program structure but no concrete details about,  e.g., variable names. Afterwards, the sketch is converted into a full program (Prog) by stochastically filling in the abstract parts of the sketch with concrete instantiations. \n\nThe paper presents an abstraction method for converting a program into a sketch, a stochastic encoder-decoder model for converting descriptions to trees, and rejection sampling-like approach for converting sketches to programs.  Experimentally, it is shown that using sketches as an intermediate abstraction outperforms directly mapping to the program AST.  The data is derived from an online repository of ~1500 Android apps,  and from that were extracted ~150k methods, which makes the data very respectable in terms of realisticness and scale.  This is one of the strongest points of the paper. \n\nOne point I found confusing is how exactly the Combinatorial Concretization step works.  Am I correct in understanding that this step depends only on Y, and that given Y,  Prog is conditionally independent of X?  If this is correct, how many Progs are consistent with a typical Y?  Some additional discussion of why no learning is required for the P(Prog | Y) step would be appreciated. \n\nI'm also curious whether using a stochastic latent variable (Z) is necessary . Would the approach work as well using a more standard encoder-decoder model with determinstic Z? \n\nSome discussion of Grammar Variational Autoencoder (Kusner et al) would probably be appropriate. \n\nOverall, I really like the fact that this paper is aiming to do program synthesis on programs that are more like those found \""in the wild\"".  While the general pattern of mapping a specification to abstraction with a neural net and then mapping the abstraction to a full program with a combinatorial technique is not necessarily novel,  I think this paper adds an interesting new take on the pattern (it has a very different abstraction than say, DeepCoder), and this paper is one of the more interesting recent papers on program synthesis using machine learning techniques, in my opinion",1,1,1,1,1,1,1,1,1,-1
HkfXMz-Ab-R3,"This is a very well-written and nicely structured paper that tackles the problem of generating/inferring code given an incomplete description (sketch) of the task to be achieved.  This is a novel contribution to existing machine learning approaches to automated programming that is achieved by training on a large corpus of Android apps.  The combination of the proposed technique and leveraging of real data are a substantial strength of the work compared to many approaches that have come previously. \n\nThis paper has many strengths:\n1) The writing is clear, and the paper is well-motivated\n2)  The proposed algorithm is described in excellent detail, which is essential to reproducibility\n3)  As stated previously, the approach is validated with a large number of real Android projects\n4)  The fact that the language generated is non-trivial (Java-like) is a substantial plus\n5) Good discussion of limitations\n\n Overall, this paper is a valuable addition to the empirical software engineering community, and a nice break from more traditional approaches of learning abstract syntax trees.",1,1,1,1,-1,1,1,-1,1,1
HkGJUXb0--R1,"The paper addresses the problem of tensor decomposition which is relevant and interesting.  The paper proposes Tensor Ring (TR) decomposition which improves over and bases on the Tensor Train (TT) decomposition method.  TT decomposes a tensor in to a sequences of latent tensors where the first and last tensors are a 2D matrices.  \n\nThe proposed TR method generalizes TT in that the first and last tensors are also 3rd-order tensors instead of 2nd-order.  I think such generalization is interesting but the innovation seems to be very limited.  \n\nThe paper develops three different kinds of solvers for TR decomposition, i.e., SVD, ALS and SGD.  All of these are well known methods.  \n\nFinally, the paper provides experimental results on synthetic data (3 oscillated functions) and image data (few sampled images).  I think the paper could be greatly improved by providing more experiments and ablations to validate the benefits of the proposed methods. \n\nPlease refer to below for more comments and questions. \n\n-- The rating has been updated. \n\nPros:\n1. The topic is interesting. \n2. The generalization over TT makes sense. \n\nCons:\n1. The writing of the paper could be improved and more clear: the conclusions on inner product and F-norm can be integrated into \""Theorem 5\"".  And those \""theorems\"" in section 4 are just some properties from previous definitions; they are not theorems.  \n2. The property of TR decomposition is that the tensors can be shifted (circular invariance).  This is an interesting property and it seems to be the major strength of TR over TT.  I think the paper could be significantly improved by providing more applications of this property in both theory and experiments. \n3. As the number of latent tensors increase, the ALS method becomes much worse approximation of the original optimization.  Any insights or results on the optimization performance vs. the number of latent tensors? \n4. Also, the paper mentions Eq. 5 (ALS) is optimized by solving d subproblems alternatively.  I think this only contains a single round of optimization.  Should ALS be applied repeated (each round solves d problems) until convergence? \n5. What is the memory consumption for different solvers? \n6. SGD also needs to update at least d times for all d latent tensors.  Why is the complexity O(r^3) independent of the parameter d? \n7. The ALS is so slow (if looking at the results in section 5.1), which becomes not practical.  The experimental part could be improved by providing more results and description about a guidance on how to choose from different solvers. \n8. What does \""iteration\"" mean in experimental results such as table 2?  Different algorithms have different cost for \""each iteration\"" so comparing that seems not fair.  The results could make more sense by providing total time consumptions and time cost per iteration.  also applies to table 4. \n9. Why is the \\epsion in table 3 not consistent?  Why not choose \\epsion = 9e-4 and \\epsilon=2e-15 for tensorization? \n10. Also, table 3 could be greatly improved by providing more ablations such as results for (n=16, d=8), (n=4, d=4), etc.  That could help readers to better understand the effect of TR. \n11. Section 5.3 could be improved by providing a curve (compression vs. error) instead of just providing a table of sampled operating points. \n12. The paper mentions the application of image representation but only experiment on 32x32 images.  How does the proposed method handle large images?  Otherwise, it does not seem to be a practical application. \n13. Figure 5: Are the RSE measures computed over the whole CIFAR-10 dataset or the displayed images? \n\nMinor:\n- Typo: Page 4 Line 7 \""Note that this algorithm use the similar strategy\"": use -> uses",1,1,1,1,1,-1,1,1,1,-1
HkGJUXb0--R2,"This paper proposes a tensor train decomposition with a ring structure for function approximation and data compression.  Most of the techniques used are well-known in the tensor community (outside of machine learning).  The main contribution of the paper is the introduce such techniques to the ML community and presents experimental results for support. \n\nThe paper is rather preliminary in its examination.  For example, it is claimed that the proposed decomposition provides \""enhanced representation ability\"", but this is not justified rigorously either via more comprehensive experimentation or via a theoretical justification.  Furthermore, the paper lacks in novelty aspect, as it is uses mostly well-known techniques",1,1,1,1,-1,1,1,1,1,-1
HkGJUXb0--R3,"This paper presents a tensor decomposition method called tensor ring (TR) decomposition.  The proposed decomposition approximates each tensor element via a trace operation over the sequential multilinear products of lower order core tensors.  This is in contrast with another popular approach based on tensor train (TT) decomposition which requires several constraints on the core tensors (such as the rank of the first and last core tensor to be 1). \n\nTo learn TR representations, the paper presents a non-iterative TR-SVD algorithm that is similar to TT-SVD algorithm.  To find the optimal lower TR-ranks, a block-wise ALS algorithms is presented, and an SGD algorithm is also presented to make the model scalable. \n\nThe proposed method is compared against the TT method on some synthetic high order tensors and on an image completion task, and shown to yield better results. \n\nThis is an interesting work.  TT decompositions have gained popularity in the tensor factorization literature recently and the paper tries to address some of their key limitations.  This seems to be a good direction.  The experimental results are somewhat limited but the overall framework looks appealing",1,1,1,1,-1,1,1,-1,1,-1
HkgNdt26Z-R1,"This paper deals with improving language models on mobile equipments\nbased on small portion of text that the user has ever input.  For this\npurpose, authors employed a linearly interpolated objectives between user\nspecific text and general English, and investigated which method (learning\nwithout forgetting and random reheasal) and which interepolation works better. \nMoreover, authors also look into privacy analysis to guarantee some level of\ndifferential privacy is preserved. \n\nBasically the motivation and method is good, the drawback of this paper is\nits narrow scope and lack of necessary explanations.  Reading the paper,\nmany questions arise in mind:\n\n- The paper implicitly assumes that the statistics from all the users must\n  be collected to improve \""general English\"".  Why is this necessary?  Why not\n  just using better enough basic English and the text of the target user? \n\n- To achieve the goal above, huge data (not the \""portion of the general English\"") should be communicated over the network.  Is this really worth doing?  If only\n  \""the portion of\"" general English must be communicated, why is it validated? \n\n- For measuring performance, authors employ keystroke saving rate.  For the\n  purpose of mobile input, this is ok: but the use of language models will\n  cover much different situation where keystrokes are not necessarily \n  available, such as speech recognition or machine translation.  Since this \n  paper is concerned with a general methodology of language modeling, \n  perplexity improvement (or other criteria generally applicable) is also\n  important. \n\n- There are huge number of previous work on context dependent language models,\n  let alone a mixture of general English and specific models.  Are there any\n  comparison with these previous efforts? \n\nFinally, this research only relates to ICLR in that the language model employed\nis LSTM: in other aspects, it easily and better fit to ordinary NLP conferences, such as EMNLP, NAACL or so.  I would like to advise the authors to submit\nthis work to such conferences where it will be reviewed by more NLP experts. \n\nMinor:\n- t of $G_t$ in page 2 is not defined so far.\n- What is \""gr\"" in Section 2.",1,1,1,1,1,1,1,1,1,1
HkgNdt26Z-R2,"my main concern is the relevance of this paper to ICLR. \nThis paper is much related not to representation learning but to user-interface. \nThe paper is NOT well organized and so the technical novelty of the method is unclear. \nFor example, the existing method and proposed method seems to be mixed in Section 2. \nYou should clearly divide the existing study and your work.  \nThe experimental setting is also unclear. \nKSS seems to need the user study. \nBut I do not catch the details of the user study, e.g., the number of users",1,1,1,1,1,-1,1,1,-1,-1
HkgNdt26Z-R3,"This paper discusses the application of word prediction for software keyboards.  The goal is to customize the predictions for each user to account for member specific information while adhering to the strict compute constraints and privacy requirements.  \n\nThe authors propose a simple method of mixing the global model with user specific data.  Collecting the user specific models and averaging them to form the next global model.  \n\nThe proposal is practical.  However, I am not convinced that this is novel enough for publication at ICLR.  \n\nOne major question.  The authors assume that the global model will depict general english.  However, it is not necessary that the population of users will adhere to general English and hence the averaged model at the next time step t+1 might be significantly different from general English.  It is not clear to me as how this mechanism guarantees that it will not over-fit or that there will be no catastrophic forgetting",1,1,-1,1,1,-1,1,1,1,-1
HkjL6MiTb-R1,"This paper introduces siamese neural networks to the competing risks framework of Fine and Gray. The authors optimize for the c-index by minimizing a loss function driven by the cumulative risk of competing risk m and correct ordering of comparable pairs.  While the idea of optimizing directly for the c-index directly is a good one (with an approximation and with useful complementary loss function terms),  the paper leaves something to be desired in quality and clarity. \n\nRelated works:\n- For your consideration: is multi-task survival analysis effectively a competing risks model, except that these models also estimate risk after the first competing event (i.e. in a competing risks model the rates for other events simply go to 0 or near-zero)? Please discuss.  Also, if the claim is that there are not deep learning survival analyses, please see, e.g. Jing and Smola. \n- It would be helpful to define t_k explicitly to alleviate determining whether it is the interval time between ordered events or the absolute time since t_0 (it's the latter).  Consider calling k a time index instead of t_k a time interval (\""subject x experiences cause m occurs [sic] in a time interval t_k\"")\n- Line after eq 8: do you mean accuracy term? \n- I would not call Reg a regularization term since it is not shrinking the coefficients . It is a term to minimize a risk not a parameter .\n- You claim to adjust for event imbalance and time interval imbalance but this is not mathematically shown nor documented in the experiments. \n- The results show only one form of comparison, and the results have confidence intervals that overlap with at least one competing method in all tasks.",1,1,1,1,1,1,1,1,1,-1
HkjL6MiTb-R2,"The authors tackle the problem of estimating risk in a survival analysis setting with competing risks.  They propose directly optimizing the time-dependent discrimination index using a siamese survival network.  Experiments on several real-world dataset reveal modest gains in comparison with the state of the art. \n\n- The authors should clearly highlight what is their main technical contribution.  For example, Eqs. 1-6 appear to be background material since the time-dependent discrimination index is taken from the literature, as the authors point out earlier . However, this is unclear from the writing.  \n\n- One of the main motivations of the authors is to propose a model that is specially design to avoid the nonidentifiability issue in an scenario with competing risks.  It is unclear why the authors solution is able to solve such an issue, specially given the modest reported gains in comparison with several competitive baselines.  In other words, the authors oversell their own work, specially in comparison with the state of the art. \n\n- The authors use off-the-shelf siamese networks for their settting and thus it is questionable there is any novelty there.  The application/setting may be novel,  but not the architecture of choice. \n\n- From Eq. 4 to Eq. 5, the authors argue that the denominator does not depend on the model parameters and can be ignored.  However, afterwards the objective does combine time-dependent discrimination indices of several competing risks, with different denominator values.  This could be problematic if the risks are unbalanced. \n\n- The competitive gain of the authors method in comparison with other competing methods is minor. \n\n- The authors introduce F(t, D | x) as cumulative incidence function (CDF) at the beginning of section 2, however, afterwards they use R^m(t, x), which they define as risk of the subject experiencing event m before t.  Is the latter a proxy for the former? How are they related?",1,1,1,1,1,1,1,1,1,-1
HkjL6MiTb-R3,"The paper entitled 'Siamese Survival Analysis' reports an application of a deep learning to three cases of competing risk survival analysis.  The author follow the reasoning that '... these ideas were not explored in the context of survival analysis', thereby disregarding the significant published literature based on the Concordance Index (CI).  \n\nBesides this deficit, the paper does not present a proper statistical setup (e.g. 'Is censoring assumed to be at random? ...) , and numerical results are only referring to some standard implementations, thereby again neglecting the state-of-the-art solution.  That being said, this particular use of deep learning in this context might be novel",1,1,1,1,-1,1,1,1,1,-1
HkL7n1-0b-R1,"This paper satisfies the following necessary conditions for\nacceptance.  The writing is clear and I was able to understand the\npresented method (and its motivation) despite not being too familiar\nwith the relevant literature.  Explicitly writing the auto-encoder(s)\nas pseudo-code algorithms was particular helpful.  I found no technical\nerrors.  The problem addressed is one worth solving - building a\ngenerative model of observed data.  There is some empirical testing\nwhich show the presented method in a good light. \n\nThe authors are careful to relate the presented method with existing\nones, most notably VAE and AAE.  I suppose one could argue that the\nclose connection to existing methods means that this paper is not\ninnovative enough.  I think that would be unfair - most new methods\nhave close relations with existing ones - it is just that sometimes\nthe authors do not flag this up as they should. \n\nWAE is a bit oversold.  The authors state that WAE generates \""samples\nof better quality\"" (than VAE) without any condition being put on when\nit does this.  There is no proof that it is always better, and I can't\nsee how there could be.  Any method of inferring a generative model\nfrom data must make some 'inductive' assumptions.  Surely one could\ndevise situations where VAE outperforms WAE.  I think this issue should\nhave been examined in more depth. \n\nI found no typo or grammatical errors which is unusual - good careful\njo",1,1,1,1,1,1,1,1,1,-1
HkL7n1-0b-R2,"This very well written paper covers the span between W-GAN and VAE.  For a reviewer who is not an expert in the domain, it reads very well, and would have been of tutorial quality if space had allowed for more detailed explanations.  The appendix are very useful, and tutorial paper material (especially A).  \n\nWhile I am not sure description would be enough to reproduce and no code is provided, every aspect of the architecture, if not described, if referred as similar to some previous work.  There are also some notation shortcuts (not explained) in the proof of theorems that can lead to initial confusion, but they turn out to be non-ambiguous.  One that could be improved is P(P_X, P_G) where one loses the fact that the second random variable is Y. \n\n\nThis work contains plenty of novel material, which is clearly compared to previous work:\n- The main consequence of the use of Wasserstein distance is the surprisingly simple and useful Theorem 1.  I could not verify its novelty, but this seems to be a great contribution .\n- Blending GAN and auto-encoders has been tried in the past, but the authors claim better theoretical foundations that lead to solutions that do not rquire min-max \n- The use of MMD in the context of GANs has also been tried.  The authors claim that their use in the latent space makes it more practival \n\nThe experiments are very convincing, both numerically and visually. \n\nSource of confusion: in algorithm 1 and 2, \\tilde{z} is \""sampled\"" from Q_TH(Z|xi), some one is lead to believe that this is the sampling process as in VAEs, while in reality Q_TH(Z|xi) is deterministic in the experiments",1,1,1,1,1,1,1,1,1,1
HkL7n1-0b-R3,"This paper provides a reasonably comprehensive generalization to VAEs and Adversarial Auto-encoders through the lens of the Wasserstein metric.  By posing the auto-encoder design as a dual formulation of optimal transport, the proposed work supports the use of both deterministic and random decoders under a common framework.  In my opinion, this is one of the crucial contributions of this paper.  While the existing properties of auto-encoders are preserved, stability characteristics of W-GANs are also observed in the proposed architecture.  The results from MNIST and CelebA datasets look convincing, though could include additional evaluation to compare the adversarial loss with the straightforward MMD metric and potentially discuss their pros and cons.  In some sense, given the challenges in evaluating and comparing closely related auto-encoder solutions, the authors could design demonstrative experiments for cases where Wassersterin distance helps and may be  its potential limitations. \n\nThe closest work to this paper is the adversarial variational bayes framework by Mescheder et.al.  which also attempts at unifying VAEs and GANs.  While the authors describe the conceptual differences and advantages over that approach, it will be beneficial to actually include some comparisons in the results section",1,1,1,1,1,1,1,1,1,-1
HklZOfW0W-R1,"Learning adjacency matrix of a graph with sparsely connected undirected graph with nonnegative edge weights is the goal of this paper.  A projected sub-gradient descent algorithm is used.  The UPS optimizer by itself is not new. \n\nGraph Polynomial Signal (GPS) neural network is proposed to address two shortcomings of GSP using linear polynomial graph filter.  First, a nonlinear function sigma in (8) is used, and second, weights are shared among neighbors of every data points.  There are some concerns about this network that need to be clarified:\n1. sigma is never clarified in the main context or experiments\n2.  the shared weights should be relevant to the ordering of neighbors, instead of the set of neighbors without ordering, in which case, the sharing looks random. \n3. another explanation about the weights as the rescaling to matrix A needs to further clarified.  As authors mentioned that the magnitude of |A| from L1 norm might be detrimental for the prediction.  What is the disagreement between L1 penalty and prediction quality?  Why not apply these weights to L1 norm as a weighted L1 norm to control the scaling of A? \n4. Authors stated that the last step is to build a mapping from the GPS features into the response Y.  They mentioned that linear fully connected layer or a more complex neural network can be build on top of the GPS features.  However, no detailed information is given in the paper.  In the experiments, authors only stated that \u201cwe fit the GPS architecture using UPS optimizer for varying degree of the neighborhood of the graph\u201d, and then the graph is used to train existing models as the input of the graph.  Which architecture is used for building the mapping ? \n\nIn the experimental results, detailed definition or explanation of the compared methods and different settings should be clarified.  For example, what is GPS 8, GCN_2 Eq. 9 in Table 1, and GCN_3 9 and GPS_1, GPS_2, GPS_3 and so on.  More explanations of Figure 2 and the visualization method can be great helpful to understand the advantages of the proposed algorithm",1,1,1,1,1,-1,1,1,-1,-1
HklZOfW0W-R2,"There are many language issues rendering the text hard to understand, e.g.,\n-- in the abstract: \""several convolution on graphs architectures\""\n-- in the definitions: \""Let data with N observation\"" (no verb, no plural, etc). \n-- in the computational section: \""Training size is 9924 and testing is 6695.  \""\nso part of my negative impression may be pure mis-understanding of what\nthe authors had to say.  \n\nStill, the authors clearly utilise basic concepts (c.f. \""utilize eigenvector \nbasis of the graph Laplacian to do filtering in the Fourier domain\"") in ways\nthat do not seem to have any sensible interpretation whatsoever, even allowing\nfor the mis-understanding due to grammar.  There are no clear insight, \nno theorems, and an empirical evaluation on an ill-defined problem in \ntime-series forecasting.  (How does it relate to graphs?  What is the graph \nin the time series or among the multiple time series?  How do the authors\nimplement the other graph-related approaches in this problem featuring\ntime series?)  My impression is hence that the only possible outcome is\n\nrejection",1,1,1,1,-1,-1,1,1,-1,-1
HklZOfW0W-R3,"The authors develop a novel scheme for backpropagating on the adjacency matrix of a neural network graph.   Using this scheme, they are able to provide a little bit of evidence that their scheme allows for higher test accuracy when learning a new graph structure on a couple different example problems. \n\nPros: \n-Authors provide some empirical evidence for the benefits of using their technique. \n-Authors are fairly upfront about how, overall, it seems their technique isn't doing *too* much--null results are still results, and it would be interesting to better understand *why* learning a better graph for these networks doesn't help very much. \n\nCons: \n-The grammar in the paper is pretty bad.   It could use a couple more passes with an editor. \n-For a, more or less, entirely empirical paper, the choices of experiments are...somewhat befuddling.   Considerably more details on implementation, training time/test time, and even just *more* experiment domains would do this paper a tremendous amount of good. \n-While I mentioned it as a pro, it also seems to be that this technique simply doesn't buy you very much as a practitioner.   If this is true--that learning better graph representations really doesn't help very much, that would be good to know, and publishable, but actually *establishing* that requires considerably more experiments. \n\nUltimately, I will have to suggest rejection, unless the authors considerably beef up their manuscript with more experiments, more details, and improve the grammar considerably",1,1,1,1,1,-1,1,1,1,-1
HkMCybx0--R1,"This paper introduces a new nonlinear activation function for  neural networks, i.e., Inverse Square Root Linear Units (ISRLU).  Experiments show that ISRLU is promising compared to competitors like ReLU and ELU. \n\nPros:\n(1) The paper is clearly written. \n\n(2) The proposed ISRLU function has similar curves with ELU and has a learnable parameter \\alpha (although only fixed value is used in the experiments) to control the negative saturation zone.  \n\nCons:\n(1) Authors claim that ISRLU is faster than ELU, while still achieves ELU\u2019s performance. However, they only show the reduction of computation complexity for convolution, and speed comparison between ReLU, ISRLU and ELU on high-end CPU . As far as I know, even though modern CNNs have reduced convolution\u2019s computation complexity,  the computation cost of activation function is still only a very small part (less than 1%) in the overall running time of training/inference . \n\n(2) Authors only experimented with two very simple CNN architectures and with three different nonlinear activation functions, i.e., ISRLU/ELU/ReLU and showed their accuracies on MNIST . They did not provide the comparison of running time which I believe is important here as the efficiency is emphasized a lot throughout the paper .\n\n(3) For ISRLU of CNN, experiments on larger scale dataset such as CIFAR or ImageNet would be more convincing . Moreover, authors also propose ISRU which is similar to tanh for RNN, but do not provide any experimental results. \n\nOverall, I think the current version of the paper is not ready for ICLR conference.  As I suggested above, authors need more experiments to show the effectiveness of their approach.\n",1,1,1,1,1,-1,1,1,1,-1
HkMCybx0--R2,"\nSummary:\n- The paper proposes a new activation function that looks similar to ELU but much cheaper by using the inverse square root function. \n\nContributions:\n- The paper proposes a cheaper activation and validates it with an MNIST experiment.  The paper also shows major speedup compared to ELU and TANH (unit-wise speedup) .\n\nPros:\n- The proposed function has similar behavior as ELU but 4x cheaper. \n- The authors also refer us to faster ways to compute square root functions numerically, which can be of general interests to the community for efficient network designs in the future. \n- The paper is clearly written and key contributions are well present. \n\nCons:\n- Clearly, the proposed function is not faster than ReLU.  In the introduction, the authors explain the motivation that ReLU needs centered activation (such as BN).  But the authors also need to justify that ISRLU (or ELU) doesn\u2019t need BN . In fact, in a recent study of ELU-ResNet (Shah et al., 2016) finds that ELU without BN leads to gradient explosion.  To my knowledge, BN (at least in training time) is much more expensive than the activation function itself, so the speedup get from ISRLU may be killed by using BN in deeper networks on larger benchmarks.  At inference time, all of ReLU, ELU, and ISRLU can fuse BN weights into convolution weights, so again ISRLU will not be faster than ReLU.  The core question here is, whether the smoothness and centered zero property of ELU can buy us any win, compared to ReLU?  I couldn\u2019t find it based on the results presented here .\n- The authors need to validate on larger datasets (e.g. CIFAR, if not ImageNet) so that their proposed methods can be widely adopted. \n- The speedup is only measured on CPU.  For practical usage, especially in computer vision, GPU speedup is needed to show an impact .\n\nConclusion:\n- Based on the comments above, I recommend weak reject. \n\nReferences:\n- Shah, A., Shinde, S., Kadam, E., Shah, H., Shingade, S.. Deep Residual Networks with Exponential Linear Unit.  In Proceedings of the Third International Symposium on Computer Vision and the Internet (VisionNet'16).",1,1,1,1,1,1,1,1,1,-1
HkMCybx0--R3,"Summary:\nThe contribution of this paper is an alternative activation function which is faster to compute than the Exponential Linear Unit, yet has similar characteristics. \nThe paper first presents the mathematical form of the proposed activation function (ISRLU), and then shows the similarities to ELU graphically.  It then argues that speeding up the activation function may be important since the convolution operations in CNNs are becoming heavily optimized and may form a lesser fraction of the overall computation.  The ISRLU is then reported to be 2.6x faster compared to ELU using AVX2 instructions.  The possibility of computing a faster approximation of ISRLU is also mentioned .\nPreliminary experimental results are reported which demonstrate that ISRLU can perform similar to ELU .\n\nQuality and significance:\nThe paper proposes an interesting direction for optimizing the computational cost of training and inference using neural networks . However, on one hand the contribution is rather narrow, and on the other the results presented do not clearly show that the contribution is of significance in practice. \nThe paper does not present clear benchmarks showing a) what is the fraction of CPU cycles spent in evaluating the activation function in any reasonably practical neural network,  b) and what is the percentage of cycles saved by employing the ISRLU. \nThe presented results using small networks on the MNIST dataset only show that networks with ISRLU can perform similar to those with other activation functions, but not the speed advantages of ISRLU.\nThe effect of using the faster approximation on performance also remains to be investigated. \n\nClarity:\nThe content of the paper is unclear in certain areas. \n- It is not clear what Table 2 is showing.  What is \""performance\"" measured in? In general the Table captions need to be clearer and more descriptive . The acronym pkeep in later Tables should be clarified. \n- Why is the final Cross-Entropy Loss so high even though the accuracy is >99% for the MNIST experiments ? It looks like the loss at initialization was reported instead?",1,1,1,1,1,-1,1,1,1,-1
HkMvEOlAb-R1,"This paper presents a method for clustering based on latent representations learned from the classification of transformed data after pseudo-labellisation corresponding to applied transformation.  Pipeline: -Data are augmented with domain-specific transformations . For instance, in the case of MNIST, rotations with different degrees are applied . All data are then labelled as \""original\"" or \""transformed by ...(specific transformation)\"". -Classification task is performed with a neural network on augmented dataset according to the pseudo-labels.  -In parallel of the classification, the neural network also learns the latent representation in an unsupervised fashion.  -k-means clustering is performed on the representation space observed in the hidden layer preceding the augmented softmax layer.  \n\nDetailed Comments:\n(*) Pros\n-The method outperforms the state-of-art regarding unsupervised methods for handwritten digits clustering on MNIST. \n-Use of ACOL and GAR is interesting, also the idea to make \""labeled\"" data from unlabelled ones by using data augmentation. \n\n(*) Cons\n-minor: in the title, I find the expression \""unsupervised clustering\"" uselessly redundant since clustering is by definition unsupervised. \n-Choice of datasets: we already obtained very good accuracy for the classification or clustering of handwritten digits . This is not a very challenging task. \nAnd just because something works on MNIST, does not mean it works in general.  \nWhat are the performances on more challenging datasets like colored images (CIFAR-10, labelMe, ImageNet, etc.)? \n-This is not clear what is novel here since ACOL and GAR already exist.  The novelty seems to be in the adaptation to GAR from the semi-supervised to the unsupervised setting with labels indicating if data have been transformed or not. \n\n\nMy main problem  was about the lack of novelty.  The authors clarified this point, and it turned out that ACOL and GAR have never published elsewhere except in ArXiv .  The other issue concerned the validation of the approach on databases other than MNIST.  The author also addressed this point, and I changed my scores accordingly.",1,1,1,1,1,1,1,1,1,-1
HkMvEOlAb-R2,"This paper utilizes ACOL algorithm for unsupervised learning.  ACOL can be considered a type of semi-supervised learning where the learner has access only to parent-class information (for example in digit recognition whether a digit is bigger than 5 or not) and not the sub-class information (number between 0-9) . Given that in many applications such parent-class supervised information is not available , the authors of this paper propose domain specific pseudo parent-class labels (for example transformed images of digits) to adapt ACOL for unsupervised learning . The authors also modified affinity and balance term utilized in GAR (as part of ACOL algorithm) to improve it.  The authors use multiple data sets to study different aspects of the proposed approach .\n\nI updated my scores based on the reviewers responses.  It turned out that ACOL and GAR are also originally proposed by the same authors and was only published in arxiv!  Because of the double-blind review nature of ICLR, I didn't know these ideas came from the same authors and is being published for the first time in a peer-reviewed venue (ICLR) . So my main problem with this paper, lack of novelty, is addressed and my score has changed.  Thanks to the reviewer for clarifying this.\n",1,1,1,1,-1,1,1,1,1,-1
HkMvEOlAb-R3,"The paper is well written and clear . The main idea is to exploit a schema of semisupervised learning based on ACOL and GAR for an unsupervised learning task . The idea is to introduce the notion of pseudo labelling.  \nPseudo labelling can be obtained by transformations of original input data .\nThe key point is the definition of the transformations . \nOnly whether the design of transformation captures the latent representation of the input data, the pseudo-labelling might improve the performance of the unsupervised learning task .\nSince it is not known in advance what might be a good set of transformations,  it is not clear what is the behaviour of the model when the large portion of transformations are not encoding the latent representation of clusters.",1,1,1,1,-1,-1,1,-1,1,-1
HknbyQbC--R1,"I thank the authors for the thoughtful response and rebuttal.  The authors have substantially updated their manuscript and improved the presentation. \n\nRe: Speed. I brought up this point because this was a bulleted item in the Introduction in the earlier version of the manuscript.  In the revised manuscript, this bullet point is now removed.  I will take this point to be moot. \n\nRe: High resolution.  The authors point to recent GAN literature that provides some first results with high resolution GANs but I do not see quantitative evidence in the high resolution setting for this paper.  (Figure 4 provides qualitative examples from ImageNet but no quantitative assessment. )\n\nBecause the authors improved the manuscript, I upwardly revised my score to 'Ok but not good enough - rejection'.  I am not able to accept this paper because of the latter point. \n==========================\n\nThe authors present an interesting new method for generating adversarial examples.  Namely, the author train a generative adversarial network (GAN) to adversarial examples for a target network.  The authors demonstrate that the network works well in the semi-white box and black box settings. \n\nThe authors wrote a clear paper with great references and clear descriptions. \n\nMy primary concern is that this work has limited practical benefit in a realistic setting.  Addressing each and every concern is quite important:\n\n1) Speed.  The authors suggest that training a GAN provides a speed benefit with respect to other attack techniques.  The FGSM method (Goodfellow et al, 2015) is basically 1 inference operation and 1 backward operation.  The GAN is 1 forward operation.  Granted this results in a small difference in timing 0.06s versus 0.01s, however it would seem that avoiding a backward pass is a somewhat small speed gain. \n \nFurthermore, I would want to question the practical usage of having an 'even faster' method for generating adversarial examples.  What is the reason that we need to run adversarial attacks 'even faster'?  I am not aware of any use-cases, but if there are some, the authors should describe the rationales at length in their paper. \n\n2) High spatial resolution images.  Previous methods, e.g. FGSM, may work on arbitrarily sized images.  At best, GANs generate reasonable images that are lower resolutions (e.g. < 128x128).  Building GAN's that operate above-and-beyond moderate spatial resolution is an open research topic.  The best GAN models for generating high resolution images are  difficult to train and it is not clear if they would work in this setting.  Furthermore, images with even higher resolutions, e.g. 512x512, which is quite common in ImageNet, are difficult to synthesizes using current techniques. \n\n3) Controlling the amount of distortion.  A feature of previous optimization based methods is that a user may specify the amount of perturbation (epsilon).  This is a key feature if not requirement in an adversarial perturbation because a user might want to examine the performance of a given model as a function of epsilon.  Performing such an analysis with this model is challenging (i.e. retraining a GAN) and it is not clear if a given image generated by a GAN will always achieve a given epsilon perturbation/\n\nOn a more minor note, the authors suggest that generating a *diversity* of adversarial images is of practical import.  I do not see the utility of being able to generate a diversity of adversarial images.  The authors need to provide more justification for this motivation",1,1,1,1,1,1,1,1,1,-1
HknbyQbC--R2,"This paper describes AdvGAN, a conditional GAN plus adversarial loss.  AdvGAN is able to generate adversarial samples by running a forward pass on generator.  The authors evaluate AdvGAN on semi-white box and black box setting. \n\nAdvGAN is a simple and neat solution to for generating adversary samples.  The author also reports state-of-art results. \n\nComment:\n\n1. For MNIST samples, we can easily find the generated sample is a mixture of two digitals. Eg, for digital 7 there is a light gray 3 overlap.  I am wondering this method is trying to mixture several samples into one to generate adversary samples.  For real color samples, it is harder to figure out the mixture. \n2. Based on mixture assumption, I suggest the author add one more comparison to other method,  which is relative change from original image, to see whether AdvGAN is the most efficient model to generate the adversary sample (makes minimal change to original image).\n\n\n\n",1,-1,1,1,1,-1,1,1,1,-1
HknbyQbC--R3,"The paper proposes a way of generating adversarial examples that fool classification systems. \nThey formulate it for a blackbox and a semi-blackbox setting (semi being, needed for training their own network, but not to generate new samples).\ n\nThe model is a residual gan formulation, where the generator generates an image mask M, and (Input + M) is the adversarial example.\ nThe paper is generally easy to understand and clear in their results. \nI am not awfully familiar with the literature on adversarial examples to know if other GAN variants exist . From this paper's literature survey, they dont exist.  \nSo this paper is innovative in two parts:\n- it applies GANs to adversarial example generation\ n- the method is a simple feed-forward network, so it is very fast to compute\ n\nThe experiments are pretty robust, and they show that their method is better than the proposed baselines .\nI am not sure if these are complete baselines or if the baselines need to cover other methods (again, not fully familiar with all literature here).\n",1,1,1,1,1,1,1,1,1,-1
HkNGsseC--R1,"The paper studies the expressive power provided by \""overlap\"" in convolution layers of DNNs.   Instead of ReLU networks with average/max pooling (as is standard in practice), the authors consider linear activations with product pooling.   Such networks, which have been known as convolutional arithmetic circuits, are easier to analyze (due to their connection to tensor decomposition), and provide insight into standard DNNs.\ n\nFor these networks, the authors show that overlap results in the overall function having a significantly higher rank (exponentially larger) than a function obtained from a network with non-overlapping convolutions (where the stride >= filter width).   The key part of the proof is showing a lower bound on the rank for networks with overlap .  They do so by an argument well-known in this space: showing a lower bound for some particular tensor, and then inferring the bound for a \""generic\"" tensor. \n\nThe results are interesting overall, but the paper has many caveats:\n1.  the results are only for ConvACs,  which are arguably quite different from ReLU networks (the non-linearity in successive non-pooling layers could be important).\ n2.  it's not clear if the importance of overlap is too surprising (or is a pressing question to understand, as in the case of depth) .\n3.  the rank of the tensor being high does not preclude approximation (to a very good accuracy) by tensors of much smaller rank. \n\nThat said, the results could be of interest to those thinking about minimizing the number of connections in ConvNets , as it gives some intuition about how much overlap might 'suffice'.  \n\nI recommend weak accept",1,1,1,1,-1,-1,1,1,1,-1
HkNGsseC--R2,"The paper analyzes the expressivity of convolutional arithmetic circuits (ConvACs), where neighboring neurons in a single layer have overlapping receptive fields.  To compare the expressivity of overlapping networks with non-overlapping networks, the paper employs grid tensors computed from the output of the ConvACs.  The grid tensors are matricized and the ranks of the resultant matrices are compared.  The paper obtains a lower bound on the rank of the resultant grid tensors , and uses them to show that an exponentially large number of non-overlapping ConvACs are required to approximate the grid tensor of an overlapping ConvACs.  Assuming that the result carries over to ConvNets, I find this result to be very interesting.   While overlapped convolutional layers are almost universally used, there has been very little theoretical justification for the same . This paper shows that overlapped ConvACs are exponentially more powerful than their non-overlapping counterparts.",1,1,1,1,-1,-1,1,1,1,-1
HkNGsseC--R3,"The paper studies convolutional neural networks where the stride is smaller than the convolutional filter size;  the so called overlapping convolutional architectures . The main object of study is to quantify the benefits of overlap in convolutional architectures. \n\nThe main claim of the paper is Theorem 1, which is that overlapping convolutional architectures are efficient with respect to non-overlapping architectures, i.e.,  there exists functions in the overlapping architecture which require an exponential increase in size to be represented in the non-overlapping architecture;  whereas overlapping architecture can capture within a linear size the functions represented by the non-overlapping architectures.  The main workhorse behind the paper is the notion of rank of matricized grid tensors following a paper of Cohen and Shashua which capture the relationship between the inputs and the outputs, the function implemented by the neural network.  \n\n(1) The results of the paper hold only for product pooling and linear activation function except for the representation layer, which allows general functions.  It is unclear why the generalized convolutional networks are stated with such generality when the results apply only to this special case . That this is the case should be made clear in the title and abstract . The paper makes a point that generalized tensor decompositions can be potentially applied to solve the more general case , but since it is left as future work, the paper should make it clear throughout. \n\n(2) The experiment is minimal and even the given experiment is not described well.  What data augmentation was used for the CIFAR-10 dataset?  It is only mentioned that the data is augmented with translations and horizontal flips. What is the factor of augmentation ? How much translation ? These are important because there maybe a much simpler explanation to the benefit of overlap: it is able to detect these translated patterns easily.  Indeed, this simple intuition seems to be why the authors chose to make the problem by introducing translations and flips.  \n\n(3) It is unclear if the paper resolves the mystery that they set out to solve, which is a reconciliation of the following two observations  (a) why are non-overlapping architectures so common?  (b) why only slight overlap is used in practice?   The paper seems to claim that since overlapping architectures have higher expressivity that answers (a) . It appears that the paper does not answer (b) well: it points out that since there is exponential increase, there is no reason to increase it beyond a particular point.  It seems the right resolution will be to show that after the overlap is set to a certain small value , there will be *only* linear increase with increasing overlap; i.e., the paper should show that small overlap networks are efficient with respect to *large* overlap networks; a comparison that does not seem to be made in the paper.  \n\n(4) Small typo: the dimensions seem to be wrong in the line below the equation in page 3 . \n\nThe paper makes important progress on a highly relevant problem using a new methodology (borrowed from a previous paper) . However, the writing is hurried and the high-level conclusions are not fully supported by theory and experiments.",1,1,1,1,1,-1,1,1,1,-1
Hko85plCW-R1,"This paper proposes a small modification to the monotonic attention in [1] by adding a soft attention to the segment predicted by the monotonic attention.  The paper is very well written and easy to follow . The experiments are also convincing. Here are a few suggestions and questions to make the paper stronger. \n\nThe first set of questions is about the monotonic attention.  Training the monotonic attention with expected context vectors is intuitive, but can this be justified further?  For example, how far does using the expected context vector deviate from marginalizing the monotonic attention?  The greedy step, described in the first paragraph of page 4, also has an effect on the produced attention . How does the greedy step affect training and decoding?  It is also unclear how tricks in the paragraph above section 2.4 affect training and decoding . These questions should really be answered in [1] . Since the authors are extending their work and since these issues might cause training difficulties,  it might be useful to look into these design choices .\n\nThe second question is about the window size $w$ . Instead of imposing a fixed window size, which might not make sense for tasks with varying length segments such as the two in the paper,  why not attend to the entire segment, i.e., from the current boundary to the previous boundary ?\n\nIt is pretty clear that the model is discovering the boundaries in the utterance shown in Figure 2.  (The spectrogram can be made more visible by removing the delta and delta-delta in the last subplot.)  How does the MoCha attention look like for words whose orthography is very nonphonemic, for example, AAA and WWW ?\n\nFor the experiments, it is intriguing to see that $w=2$ works best for speech recognition . If that's the case, would it be easier to double the hidden layer size and use the vanilla monotonic attention?  The latter should be a special case of the former, and in general you can always increase the size of the hidden layer to incorporate the windowed information.  Would the special cases lead to worse performance and if so why is there a difference?\ n\n[1] C Raffel, M Luong, P Liu, R Weiss, D Eck, Online and linear-time attention by enforcing monotonic alignments, 2017",1,1,1,1,1,-1,1,1,1,-1
Hko85plCW-R2,"The paper proposes an extension to a previous monotonic attention model (Raffel et al 2017) to attend to a fixed-sized window up to the alignment position.  Both the soft attention approximation used for training the monotonic attention model, and the online decoding algorithm is extended to the chunkwise model.  In terms of the model this is a relatively small extention of Raffel et al 2017. \n\nResults show that for online speech recognition the model matches the performance of an offline soft attention baseline, doing significantly better than the monotonic attention model.  Is the offline attention baseline unidirectional or bidirectional?  In case it is unidirectional it cannot really be claimed that the proposed model's performance is competitive with an offline model. \n\nMy concern with the statement that all hyper-parameters are kept the same as the monotonic model is that the improvement might partly be due to the increase in total number of parameters in the model.  Especially given that w=2 works best for speech recognition, it not clear that the model extension is actually helping.  My other concern is that in speech recognition the time-scale of the encoding is somewhat arbitrary, so possibly a similar effect could be obtained by doubling the time frame through the convolutional layer.  While the empirical result is strong it is not clear that the proposed model is the best way to obtain the improvement. \n\nFor document summarization the paper presents a strong result for an online model, but the fact that it is still less accurate than the soft attention baseline makes it hard to see the real significance of this.  If the contribution is in terms of speed (as shown with the synthetic benchmark in appendix B) more emphesis should be placed on this in the paper.  \nSentence summarization tasks do exhibit mostly monotonic alignment, and most previous models with monotonic structure were evaluated on that, so why not test that here? \n\nI like the fact that the model is truely online, but that contribution was made by Raffel et al 2017, and this paper at best proposes a slightly better way to train and apply that model. \n\n---\n The additional experiments in the new version gives stronger support in favour of the proposed model architecture (vs the effect of hyperparameter choices).  While I'm still on the fence on whether this paper is strong enough to be accepted for ICLR, this version is certainly improves the quality of the paper",1,1,1,1,1,1,1,1,1,-1
Hko85plCW-R3,"This paper extends a previously proposed monotonic alignment based attention mechanism by considering local soft alignment across features in a chunk (certain window).   \n\nPros.\n- the paper is clearly written.\ n- the proposed method is applied to several sequence-to-sequence benchmarks, and the paper show the effectiveness of the proposed method (comparable to full attention and better than previous hard monotonic assignments) .\nCons.\n- in terms of the originality, the methodology of this method is rather incremental from the prior study (Raffel et al),  but it shows significant gains from it. \n- in terms of considering a monotonic alignment, Hori et al, \""Advances in Joint CTC-Attention based End-to-End Speech Recognition with a Deep CNN Encoder and RNN-LM, \"" in Interspeech'17, also tries to solve this issue by combining CTC and attention-based methods.  The paper should also discuss this method in Section 4.\ n\nComments:\n- Eq. (16): $j$ in the denominator should be $t_j$.\n",1,1,1,1,1,1,1,1,1,-1
HkOhuyA6--R1,"The authors propose to use 2D CNNs for graph classification by transforming graphs to an image-like representation from its node embedding.  The approach uses node2vec to obtain a node embedding, which is then compacted using PCA and turned into a stack of discretized histograms.  Essentially the authors propose an approach to use a node embedding to achieve graph classification. \n\nIn my opinion there are several weak points:\n\n1) The approach to obtain the image-like representation is not well motivated .ther approaches how to  aggregate the set of node embeddings for graph classification are known, see, e.g., \""Representation Learning on Graphs: Methods and Applications\"", William L. Hamilton, Rex Ying, Jure Leskovec, 2017.  The authors should compare to such methods as a baseline. \n\n2) The experimental evaluation is not convincing:\n- the selection of competing methods is not sufficient.  I would like to suggest to add an approach similar to Duvenaud et al., \""Convolutional networks on graphs for learning molecular fingerprints\"", NIPS 2015.\ n- the accuracy results are taken from other publications and it is not clear that this is an authoritative comparison;  the accuracy results published for state-of-the-art graph kernels are superior to those obtained by the proposed method, cf., e.g., Kriege et al.,  \""On Valid Optimal Assignment Kernels and Applications to Graph Classification\"", NIPS 2016.\ n- it would be interesting to apply the approach to graphs with discrete and continuous labels .\n\n3) The authors argue that their method is preferable to graph kernels in terms of time complexity.  This argument is questionable.  Most graph kernels compute explicit feature maps and can therefore be used with efficient linear SVMs (unfortunately most publications use a kernelized SVM) . Moreover, the running of computing the node embedding must be emphasized: On page 2 the authors claim a \""constant time complexity at the instance level\"" , which is not true when also considering the running time of node2vec.  Moreover, I do not think that node2vec is more efficient than, e.g., Weisfeiler-Lehman refinement used by graph kernels. \n\nIn summary: Since the technical contribution is limited, the approach needs to be justified by an authoritative experimental comparison . This is not yet achieved with the results presented in the submitted paper.  Therefore, it should not be accepted in its current form.",1,1,1,1,1,1,1,1,1,-1
HkOhuyA6--R2,"The paper introduces a method for learning graph representations (i.e., vector representations for graphs).  An existing node embedding method is used to learn vector representations for the nodes . The node embeddings are then projected into a 2-dimensional space by PCA.  The 2-dimensional space is binned using an imposed grid structure.  The value for a bin is the (normalized) number of nodes falling into the corresponding region.  \n\nThe idea is simple and easily explained in a few minutes . That is an advantage.  Also, the experimental results look quite promising . It seems that the methods outperforms existing methods for learning graph representations . \n\nThe problem with the approach is that it is very ad-ho c. There are several (existing) ideas of how to combine node representations into a representation for the entire graph . For instance, averaging the node embeddings is something that has shown promising results in previous work . Since the methods is so ad-hoc (node2vec -> PCA -> discretized density map -> CNN architecure) and since a theoretical understanding of why the approach works is missing , it is especially important to compare your method more thoroughly to simpler methods.  Again, pooling operations (average, max, etc.) on the learned node2vec embeddings are examples of simpler alternatives.  \n\nThe experimental results are also not explained thoroughly enough.  For instance, since two runs of node2vec will give you highly varying embeddings (depending on the initialization) , you will have to run node2vec several times to reduce the variance of your resulting discretized density maps.  How many times did you run node2vec on each graph? \",1,1,1,1,1,1,1,1,1,-1
HkOhuyA6--R3,"The paper presents a novel representation of graphs as multi-channel image-like structures . These structures are extrapolated  by \n1) mapping the graph nodes into an embedding using an algorithm like node2vec\n2) compressing the embedding space using pca\n3) and extracting 2D slices from the compressed space and computing 2D histograms per slice. \nhe resulting multi-channel image-like structures are then feed into vanilla 2D CNN. \n  \nThe papers is well written and clear, and proposes an interesting idea of representing graphs as multi-channel image-like structures . Furthermore, the authors perform experiments with real graph datasets from the social science domain and a comparison with the SoA method both graph kernels and deep learning architectures.  The proposed algorithm in 3 out of 5 datasets, two of theme with statistical significant.",1,1,-1,1,-1,1,1,-1,1,-1
Hkp3uhxCW-R1,"*Summary*\n\nThe paper applies variational inference (VI) with the 'reparameterisation' trick for Bayesian recurrent neural networks (BRNNs).  The paper first considers the \""Bayes by Backprop\"" approach of Blundell et al. (2015) and then modifies the BRNN model with a hierarchical prior over the network parameters, which then requires a hierarchical variational approximation with a simple linear recognition model.  Several experiments demonstrate the quality of the prediction and the uncertainty over dropout.   \n\n*Originality + significance*\n\nTo my knowledge, there is no other previous work on VI with the reparameterisation trick for BRNNs.  However, one could say that this paper is, on careful examination, an application of reparameterisation gradient VI for a specific application.  \n\nNevertheless, the parameterisation of the conditional variational distribution q(\\theta | \\phi, (x, y)) using recognition model is interesting and could be useful in other models.  However, this has not been tested or concretely shown in this paper.  The idea of modifying the model by introducing variables to obtain a looser bound which can accommodate a richer variational family is also not new, see: hierarchical variational model (Ranganath et al., 2016) for example.  \n\n*Clarity*\n\nThe paper is, in general, well-written. However, the presentation in 4 is hard to follow.  I would prefer if appendix A3 was moved up front -- in this case, it would make it clear that the model is modified to contain \\phi, a variational approximation over both \\theta and \\phi is needed, and a q that couples \\theta, \\phi and and the gradient of the log likelihood term wrt \\phi is chosen.  \n\nAdditional comments:\n\nWhy is the variational approximation called \""sharpened\""?\n\nAt test time, normal VI just uses the fixed q(\\theta) after training.  It's not clear to me how prediction is done when using 'posterior sharpening' -- how is q(\\theta | \\phi, x) in eqs. 19-20 parameterised?  The first paragraph of page 5 uses q(\\theta | \\phi, (x, y)), but y is not known at test time. \n\nWhat is C in eq. 9? \n\nThis comment \""variational typically underestimate the uncertainty in the posterior...whereas expectation propagation methods are mode averaging and so tend to overestimate uncertainty...\"" is not precise.  EP can do mode averaging as well as mode seeking, depending on the underlying and approximate factor graphs.  In the Bayesian neural network setting when the likelihood is factorised point-wise and there is one factor for each likelihood, EP is just as mode-seeking as variational.  On the other hand, variational methods can avoid modes too, see the mixture of Gaussians example in the \""Two problems with variational EM... \"" paper by Turner and Sahani (2010). \n\nThere are also many hyperparameters that need to be chosen -- what would happen if these are optimised using the free-energy?  Was there any KL reweighting scheduling as done in the original BBB paper?  \n\nWhat is the significance of the difference between BBB and BBB with sharpening in the language modelling task?  Was sharpening used in the image caption generation task? \n\nWhat is the computational complexity of BBB with posterior sharpening?  Twice that BBB? If this is the case, would BBB get to the same performance if we optimise it for longer?  Would be interesting to see the time/accuracy frontier.",1,1,1,1,1,1,1,1,1,-1
Hkp3uhxCW-R2,"This paper proposes an interesting variational posterior approximation for the weights of an RNN.  The paper also proposes a scheme for assessing the uncertainty of the predictions of an RNN.  \n\npros:\n--I liked the posterior sharpening idea.  It was well motivated from a computational cost perspective hence the use of a hierarchical prior.  \n--I liked the uncertainty analysis.  There are many works on Bayesian neural networks but they never present an analysis of the uncertainty introduced in the weights.  These works can benefit from the uncertainty analysis scheme introduced in this paper. \n--The experiments were well carried through. \n\ncons:\n--Change the title! the title is too vague.  \""Bayesian recurrent neural networks\"" already exist and is rather vague for what is being described in this paper. \n--There were a lot of unanswered questions:\n (1) how does sharpening lead to lower variance?  This was a claim in the paper and there was no theoretical justification or an empirical comparison of the gradient variance in the experiment section \n(2) how is the level of uncertainty related to performance?  It would have been insightful to see effect of \\sigma_0 on the performance rather than report the best result.  \n(3) what was the actual computational cost for the BBB RNN and the baselines? \n--There were very minor typos and some unclear connotations.  For example there is no such thing as a \""variational Bayes model\"". \n\nI am willing to adjust my rating when the questions and remarks above get addressed.",1,1,1,1,1,1,1,1,1,-1
Hkp3uhxCW-R3,"The manuscript proposes a new framework for inference in RNN based upon the Bayes by Backprop (BBB) algorithm.   In particular, the authors propose a new framework to \""sharpen\"" the posterior. \n\nIn particular, the hierarchical prior in (6) and (7) frame an interesting modification to directly learning a multivariate normal variational approximation.   In the experimental results, it seems clear that this approach is beneficial, but it's not clear as to why.   In particular, how does the variational posterior change as a result of the hierarchical prior?   It seems that (7) would push the center of the variational structure back towards the MAP point and reduces the variance of the output of the hierarchical prior; however, with the two layers in the prior it's unclear what actually is happening.   Carefully explaining *what* the authors believe is happening and exploring how it changes the variational approximation in a classic modeling framework would be beneficial to understanding the proposed change and evaluating it.   As a final point, the authors state, \""as long as the improvement along the gradient is great than the KL loss incurred...this method is guaranteed to make progress towards optimizing L. \""  Do the authors mean that the negative log-likelihood will be improved in this case?   Or the actual optimization?   Improving the negative log-likelihood seems straightforward, but I am confused by what the authors mean by optimization. \n\nThe new evaluation metric proposed in Section 6.1.1 is confusing, and I do not understand what the metric is trying to capture.   This needs significantly more detail and explanation.   Also, it is unclear to me what would happen when you input data examples that are opposite to the original input sequence; in particular, for many neural networks the predictions are unstable outside of the input domain and inputting infeasible data leads to unusable outputs.   It's completely feasible that these outputs would just be highly uncertain, and I'm not sure how you can ascribe meaning to them.   The authors should not compare to the uniform prior as a baseline for entropy.   It's much more revealing to compare it to the empirical likelihoods of the words",1,1,1,1,1,-1,1,1,1,-1
HkPCrEZ0Z-R1,"The paper studies a combination of model-based and model-free RL.  The idea is to train a forward predictive model which provides multi-step estimates to facilitate model-free policy learning.   Some parts of the paper lack clarity and the empirical results need improvement to support the claims (see details below).    \n\nClarity \n- The main idea of the proposed method is clear.  \n- Some notations and equations are broken.  For example: \n(1) The definition of \\bar{A} in Section 4 is broken.  \n(2) The overall objective in Section 5 is broken.  \n(3) The computation of w in Algorithm 2 is problematic.  \n- Some details of the experiments/methods are confusing.  For example: \n(1) The step number k is dynamically determined by a short line search as in Section 4 ``Dynamic Rollout\u2019\u2019, but later in the experiments (Section 6) the value of k is set to be 2 uniformly.  \n(2) Only the policy and value networks specified.  The forward models are not specified.   \n(3) In algorithm 1, what exact method is used in determining if \\mu is converged or not?  \n\nOriginality\nThe proposed method can be viewed as a multi-step version of the stochastic value gradient algorithm.  An empirical comparison could be helpful but not provided.  \n\nThe idea of the proposed method is related to the classic Dyna methods from Sutton.  A discussion on the difference would be helpful.  \n\nSignificance\n- The paper could compare against other relevant baselines that combine model-based and model-free RL methods, such as SVG (stochastic value gradient).  \n- To make a fair comparison, the results in Table 1 should consider the amount of data used in pre-training the forward models.  Current results in Table 1 only compare the amount of data in policy learning.   \n- Figure 3 is plotted for just one random starting state.  The Figure could have been more informative if it was averaged over different starting states.   The same issue is found in Figure 2.   It would be helpful if the plots of other domains are provided.  \n- In Figure 2, even though the diff norm fluctuates, the cosine similarity remains almost constant.  Does it suggest the cosine similarity is not effective in measuring the state similarity?  \n- Figure 1, 4 and 5 need confidence intervals or standard errors.  \n\nPros:\n- The research direction in combining model-based and model-free RL is interesting. \n- The main idea of the proposed method is clear.  \n\nCons:\n- Parts of the paper are unclear and some details are missing.  \n- The paper needs more discussion and comparison to relevant baseline methods.   \n- The empirical results need improvement to support the paper\u2019s claims",1,1,1,1,1,1,1,1,1,-1
HkPCrEZ0Z-R2,"The main idea of the paper is to improve off-policy policy gradient estimates using control variates based on multi-step rollouts, and reduce the variance of those control variates using the reparameterization trick. This is laid out primarily in Equations 1-5, and seems like a nice idea, although I must admit I had some trouble following the maths in Equation 5.  They include results showing that their method has better sample efficiency than TRPO (which their method also uses under the hood to update value function parameters). \n\nMy main issue with this paper is that the empirical section is a bit weak, for instance only one run seems to be shown for both methods, there is no mention of hyper-parameter selection, and the measure used for generating Table 1 seems pretty arbitrary to me (how were those thresholds chosen?).  In addition, one thing I would have liked to get out of this paper is a better understanding of how much each component helps.  This could have been done via empirical work, for instance:\n- Explore the effect of the planning horizon, and implicitly compare to SVG(1), which as the authors point out is the same as their method with a horizon of 1. \n- Show the effect of the reparameterization trick on estimator variance.\n- Compare the bias and variance of TRPO estimates vs the proposed method",1,1,1,1,1,-1,1,1,1,1
HkPCrEZ0Z-R3,"This paper presents a model-based approach to variance reduction in policy gradient methods.   The basic idea is to use a multi-step dynamics model as a \""baseline\"" (more properly a control variate, as the terminology in the paper uses, but I think baselines are more familiar to the RL community) to reduce the variance of a policy gradient estimator, while remaining unbiased.   The authors also discuss how to best learn the type of multi-step dynamics that are well-suited to this problem (essentially, using off-policy data via importance weighting), and they demonstrate the effectiveness of the approach on four continuous control tasks. \n\nThis paper presents a nice idea, and I'm sure that with some polish it will become a very nice conference submission.  But right now (at least as of the version I'm reviewing), the paper reads as being half-finished.   Several terms are introduced without being properly defined, and one of the key formalisms presented in the paper (the idea of \""embedding\"" an \""imaginary trajectory\"" remains completely opaque to me.   Further, the paper seems to simply leave out some portions: the introduction claims that one of the contributions is \""we show that techniques such as latent space trajectory embedding and dynamic unfolding can significantly boost the performance of the model based control variates,\"" but I see literally no section that hints at anything like this (no mention of \""dynamic unfolding\"" or \""latent space trajectory embedding\"" ever occurs later in the paper). \n\nIn a bit more detail, the key idea of the paper, at least to the extent that I understood it, was that the authors are able to introduce a model-based variance-reduction baseline into the policy gradient term.   But because (unlike traditional baselines) introducing it alone would affect the actual estimate, they actually just add and subtract this term, and separate out the two terms in the policy gradient: the new policy gradient like term will be much smaller, and the other term can be computed with less variance using model-based methods and the reparameterization trick.   But beyond this, and despite fairly reasonable familiarity with the subject, I simply don't understand other elements that the paper is talking about. \n\nThe paper frequently refers to \""embedding\"" \""imaginary trajectories\"" into the dynamics model, and I still have no idea what this is actually referring to (the definition at the start of section 4 is completely opaque to me).   I also don't really understand why something like this would be needed given the understanding above, but it's likely I'm just missing something here.   But I also feel that in this case, it borders on being an issue with the paper itself, as I think this idea needs to be described much more clearly if it is central to the underlying paper. \n\nFinally, although I do think the extent of the algorithm that I could follow is interesting, the second issue with the paper is that the results are fairly weak as they stand currently.   The improvement over TRPO is quite minor in most of the evaluated domains (other than possibly in the swimmer task), even with substantial added complexity to the approach.   And the experiments are described with very little detail or discussion about the experimental setup. \n\nNor are either of these issues simply due to space constraints: the paper is 2 pages under the soft ICLR limit, with no appendix.   Not that there is anything wrong with short papers, but in this case both the clarity of presentation and details are lacking.   My honest impression is simply that this is still work in progress and that the write up was done rather hastily.  I think it will eventually become a good paper, but it is not ready yet",1,1,1,1,1,-1,1,1,1,-1
HkSZyinVG-R1,"The authors propose using piecewise linear activation functions with contraints to make it continous.  They report that, during training, tuning piecewise versions of the multiple activation functions such as ReLU, ELU, LReLU converge to shifted ELU termed ShELU in this article.  Authors claim to achive better performance when using ShELU  while learning an individual bias shift for each neuron. \n\nGiven a PReLU (learnable alpha) or ELU is applied on pre-activation wx+b at each neuron, one can achieve the same shift as that reported in ShELU if required.  Authors present no clear explanation on why the shift should result in improved performance.",1,-1,1,1,1,-1,1,-1,-1,-1
HkSZyinVG-R2,"The paper proposes a piecewise linear activation function that is build on ELU.  In general it was an OK paper and there are many to be improved.\n\n+ Novelty seems minor.  In my sense, the authors do not provide any evidence theoretically or analysis on why the shifted version of ELU (which does not pass the origin) is more favorable.  The idea proposed in the paper is just a stack of \""better\"" experiments.  Why the ultimate shape, irrelevant of their initialization (relu, lrelu, etc.) results in the same shape? \n\nSection 2 seems to provide a breakdown of how they formulate the piecewise linear function, which the difference from Alostinelli et al. 2014 is not clearly stated.  In  section 3, the shifted version, \\delta, is abruptly proposed only based on \""results presented in 4.1\"" could improve learning.  This is not a professional ML paper looks like.  \n\n+ Experiment not strong to support the idea.  Experiments are only conducted in CIFAR100.  This is obviously not enough.  In Table 4 I see SvELU is better for LeNet and ShELU is better for Clevert-11 network, which form (Sh or Sv) do you use as final candidate? (via the title name, sh wins).  And the performance seems to be trivial among each other (ELU, 44.96, shelu 44.77), the current SOTAs for cifar100 could reach below 30%. \n\nAlso, the paper needs to be re-organized in a better way (eg., state clearly the difference from previous methods, how to formulate the story, etc.) For now, I don't think it is ready to ICLR.",1,1,1,1,1,1,1,1,-1,-1
HkSZyinVG-R3,"This is an emergency review, as the replacement of an overdue review.  \n\n------------------------------------------------------------------------\n\nThis paper proposes three variants of the exponential linear unit (ELU) by adding a learnable shift variable for each hidden unit.  This modification to ELU is motivated by the claimed observation that a learned piecewise linear activation function appears to have the ELU shape despite a bias factor.  \n\nHowever, the motivation above is not justified well.  No theoretic results are present to support this design.  Figure 4 shows the only experimental results to \u201csupport\u201d the motivation.  However, it is a bit weird that 1) 100% tuned results are not shown, and 2) the learned activation goes up as the input goes negative, which is not the shape of ELU.  As a result, the motivation does not seem clear. \n\nThe shift variables seem only useful when they are not shared for different pixels.  Otherwise, the shift can be implemented by the bias term of the convolutional kernels and the bias term following batch normalization (if used).  The question is if it is worth adding so many pixel-wise parameters.  Moreover, the proposed formulation does not seem useful for the fully connected layer at any time.  \n\nThe experiments are limited.  Only the basic LeNet and another network are considered on Cifar-100.  The results are not as good as the state-of-the-art.  More importantly, the proposed activation functions reduce the errors only a bit (<0.5%).  Stronger results on more datasets are necessary to justify the usefulness of the proposed method.\n",1,1,1,1,1,-1,1,1,1,-1
HktJec1RZ-R1,"The paper introduces a neural translation model that automatically discovers phrases.   This idea is very interesting and tries to marry phrase-based statistical machine translation with neural methods in a principled way.  However, the clarity of the paper could be improved. \n\nThe local reordering layer has the ability to swap inputs, however, how do you ensure that it actually does swap inputs rather than ignoring some inputs and duplicating others? \n\nAre all segments translated independently, or do you carry over the hidden state of the decoder RNN between segments?  In Figure 1 both a BRNN and SWAN layer are shown, is there another RNN in the SWAN layer, or does the BRNN emit the final outputs after the segments have been determined?",1,1,1,1,1,-1,1,1,1,-1
HktJec1RZ-R2,"Authors proposed a new neural-network based machine translation method that generates the target sentence by generating multiple partial segments in the target sentence from different positions in the source information.  The model is based on the SWAN architecture which is previously proposed, and an additional \""local reordering\"" layer to reshuffle source information to adjust those positions to the target sentence. \n\nUsing the SWAN architecture looks more reasonable than the conventional attention mechanism when the ground-truth word alignment is monotone.  Also, the concept of local reordering mechanism looks well to improve the basic SWAN model to reconfigure it to the situation of machine translation tasks. \n\nThe \""window size\"" of the local reordering layer looks like the \""distortion limit\"" used in traditional phrase-based statistical machine translation methods, and this hyperparameter may impose a similar issue with that of the distortion limit into the proposed model; small window sizes may drop information about long dependency.  For example, verbs in German sentences sometimes move to the tail of the sentence and they introduce a dependency between some distant words in the sentence.  Since reordering windows restrict the context of each position to a limited number of neighbors, it may not capture distant information enough.  I expected that some observations about this point will be unveiled in the paper, but unfortunately, the paper described only a few BLEU scores with different window sizes which have not enough information about it.  It is useful for all followers of this paper to provide some observations about this point. \nIn addition, it could be very meaningful to provide some experimental results on linguistically distant language pairs, such as Japanese and English, or simply reversing word orders in either source or target sentences (this might work to simulate the case of distant reordering). \n\nAuthors argued some differences between conventional attention mechanism and the local reordering mechanism, but it is somewhat unclear that which ones are the definite difference between those approaches. \n\nA super interesting and mysterious point of the proposed method is that it achieves better BLEU than conventional methods despite no any global language models (Table 1 row 8),  and the language model options (Table 1 row 9 and footnote 4) may reduce the model accuracy as well as it works not so effectively.  This phenomenon definitely goes against the intuitions about developing most of the conventional machine translation models.  Specifically, it is unclear how the model correctly treats word connections between segments without any global language model.  Authors should pay attention to explain more detailed analysis about this point in the paper. \n\nEq. (1) is incorrect . According to Fig. 2, the conditional probability in the product operator should be revised to p(a_t | x_{1:t}, a_{1:t-1}), and the independence approximation to remove a_{1:t-1} from the conditions should also be noted in the paper. \nNevertheless, the condition x_{1:t} could not be reduced because the source position is always conditioned by all previous positions through an RNN.\n\n",1,1,1,1,1,-1,1,1,1,-1
HktJec1RZ-R3,"This paper introduces a new architecture for end to end neural machine translation.  Inspired by the phrase based approach, the translation process is decomposed as follows : source words are embedded and then reordered; a bilstm then encodes the reordered source; a sleep wake network finally generates the target sequence as a phrase sequence built from left to right.  \n\nThis kind of approach is more related to ngram based machine translation than conventional phrase based one.   \n\nThe idea is nice.  The proposed approach does not rely on attention based model.  This opens nice perpectives for better and faster inference.  \n\nMy first concern is about the architecture description.  For instance, the swan part is not really stand alone.  For reader who does not already know this net, I'm not sure this is really clear.  Moreover, there is no link between notations used for the swan part and the ones used in the reordering part.  \n\nThen, one question arises. Why don't you consider the reordering of the whole source sentence.  Maybe you could motivate your choice at this point.  This is the main contribution of the paper, since swan already exists. \n\nFinally, the experimental part shows nice improvements  but: 1/ you must provide baseline results with a well tuned phrase based mt system;  2/ the datasets are small ones, as well as the vocabularies, you should try with larger datasets and bpe for sake of comparison.",1,1,1,1,1,-1,1,1,1,-1
HktRlUlAZ-R1,"This paper presents a new convolutional network architecture that is invariant to global translations and equivariant to rotations and scaling.  The method is combination of a spatial transformer module that predicts a focal point, around which a log-polar transform is performed.  The resulting log-polar image is analyzed by a conventional CNN. \n\nI find the basic idea quite compelling.  Although this is not mentioned in the article, the proposed approach is quite similar to human vision in that people choose where to focus their eyes, and have an approximately log-polar sampling grid in the retina.  Furthermore, dealing well with variations in scale is a long-standing and difficult problem in computer vision, and using a log-spaced sampling grid seems like a sensible approach to deal with it. \n\nOne fundamental limitation of the proposed approach is that although it is invariant to global translations, it does not have the built-in equivariance to local translations that a ConvNet has.  Although we do not have data on this, I would guess that for more complex datasets like imagenet / ms coco, where a lot of variation can be reasonably well modelled by diffeomorphisms, this will result in degraded performance. \n\nThe use of the heatmap centroid as the prediction for the focal point is potentially problematic as well.  It would not work if the heatmap is multimodal, e.g. when there are multiple instances in the same image or when there is a lot of clutter.\n\nThere is a minor conceptual confusion on page 4, where it is written that \""Group-convolution requires integrability over a group and identification of the appropriate measure dg.  We ignore this detail as implementation requires application of the sum instead of integral. \""\nWhen approximating an integral by a sum, one should generally use quadrature weights that depend on the measure, so the measure cannot be ignored.  Fortunately, in the chosen parameterization, the Haar measure is equal to the standard Lebesque measure, and so when using equally-spaced sampling points in this parameterization, the quadrature weights should be one.  (Please double-check this - I'm only expressing my mathematical intuition but have not actually proven this). \n\nIt does not make sense to say that \""The above convolution requires computation of the orbit which is feasible with respect to the finite rotation group, but not for general rotation-dilations\"", and then proceed to do exactly that (in canonical coordinates).  Since the rotation-dilation group is 2D, just like the 2D translation group used in ConvNets, this is entirely feasible.  The use of canonical coordinates is certainly a sensible choice (for the reason given above),  but it does not make an infeasible computation feasible. \n\nThe authors may want to consider citing \n- Warped Convolutions: Efficient Invariance to Spatial Transformations, Henriques & Vedaldi. \nThis paper also uses a log-polar transform, but lacks the focal point prediction / STN. \nLikewise, although the paper makes a good effort to rewiev the literature on equivariance / steerability,  it missed several recent works in this area:\n- Steerable CNNs, Cohen & Welling\n- Dynamic Steerable Blocks in Deep Residual Networks, Jacobsen et al.  \n- Learning Steerable Filters for Rotation Equivariant CNNs, Weiler et al. \nThe last paper reports 0.71% error on MNIST-rot, which is slightly better than the PTN-CNN-B++ reported on in this paper. \n\nThe experimental results presented in this paper are quite good,  but both MNIST and ModelNet40 seem like simple / toyish datasets.  For reasons outlined above, I am not convinced that this approach in its current form would work very well on more complicated problems.  If the authors can show that it does (either in its current form or after improving it, e.g. with multiple saccades, or other improvements) I would recommend this paper for publication. \n\n\nMinor issues & typos\n- Section 3.1, psi_gh = psi_g psi_h.  I suppose you use psi for L and L', but this is not very clear. \n- L_h f = f(h^{-1}), p. 4\n- \""coordiantes\"", p. 5",1,1,1,1,1,1,1,1,1,-1
HktRlUlAZ-R2,"This paper proposes a method to learn networks invariant to translation and equivariant to rotation and scale of arbitrary precision.  The idea is to jointly train \n- a network predicting a polar origin, \n- a module transforming the image into a log-polar representation according to the predicted origin, \n- a final classifier performing the desired classification task. \nRotation and scale from the polar origin result in translation of the log-polar representation.  Rotation and scale can have arbitrary precision, which is novel to the best of my knowledge. \n\n(+) In my opinion, this is a simple, attractive approach to rotation and scale equivariant CNNs. \n\n(-) The evaluation, however, is quite limited.  The approach is evaluated on:\n 1) several variants of MNIST.  The authors introduce a new variant (SIM2MNIST), which is created by applying random similitudes to the images from MNIST.  This variant is of course very well suited to the proposed method, and a bit artificial.  \n 2) 3d voxel occupancy grids with a small resolution.   The objects can be rotated around the z-axis, and the method is used to be equivariant to this rotation.   \n\n(-) Since the method starts by predicting the polar origin, wouldn't it be possible to also predict rotation and scale?    Then the input image could be rectified to a canonical orientation and scale, without needing equivariance.    My intuition is that this simpler approach would work better",1,1,1,1,1,-1,1,1,1,-1
HktRlUlAZ-R3,"The authors introduce the Polar Transformer, a special case of the Spatial Transformer (Jaderberg et al. 2015) that achieves rotation and scale equivariance by using a log-polar sampling grid.  The paper is very well written, easy to follow and substantiates its claims convincingly on variants of MNIST.  A weakness of the paper is that it does not attempt to solve a real-world problem.  However, I think because it is a conceptually novel and potentially very influential idea, it is a valuable contribution as it stands. \n\nIssues:\n\n- The clutter in SIM2MNIST is so small that predicting the polar origin is essentially trivially solved by a low-pass filter.  Although this criticism also applies to most previous work using \u2018cluttered\u2019 variants of MNIST, I still think it needs to be considered.  What happens if predicting the polar origin is not trivial and prone to errors?  These presumably lead to catastrophic failure of the post-transformer network, which is likely to be a problem in any real-world scenario. \n\n- I\u2019m not sure if Section 5.5 strengthens the paper.  Unlike the rest of the paper, it feels very \u2018quick & dirty\u2019 and not very principled.  It doesn\u2019t live up to the promise of rotation and scale equivariance in 3D.  If I understand it correctly, it\u2019s simply a polar transformer in (x,y) with z maintained as a linear axis and assumed to be parallel to the axis of rotation.  This means that the promise of rotation and scale equivariance holds up only along (x,y).  I guess it\u2019s not possible to build full 3D rotation/scale equivariance with the authors\u2019 approach (spherical coordinates probably don\u2019t do the job), but at least the scale equivariance could presumably have been achieved by using log-spaced samples along z and predicting the origin in 3D.  So instead of showing a quick \u2018hack\u2019, I would have preferred an honest discussion of the limitations and maybe a sketch of a path forward even if no implemented solution is provided.\n",1,1,1,1,1,-1,1,1,1,-1
HkUR_y-RZ-R1,"This paper extends the concept of global rather than local optimization from the learning to search (L2S) literature to RNNs, specifically in the formation and implementation of SEARNN.  Their work takes steps to consider and resolve issues that arise from restricting optimization to only local ground truth choices, which traditionally results in label / transition bias from the teacher forced model. \n\nThe underlying issue (MLE training of RNNs) is well founded and referenced, their introduction and extension to the L2S techniques that may help resolve the issue are promising, and their experiments, both small and large, show the efficacy of their technique. \n\nI am also glad to see the exploration of scaling SEARNN to the IWSLT'14 de-en machine translation dataset.  As noted by the authors, it is a dataset that has been tackled by related papers and importantly a well scaled dataset.  For SEARNN and related techniques to see widespread adoption, the scaling analysis this paper provides is a fundamental component. \n\nThis reviewer, whilst not having read all of the appendix in detail, also appreciates the additional insights provided by it, such as including losses that were attempted but did not result in appreciable gains. \n\nOverall I believe this is a paper that tackles an important topic area and provides a novel and persuasive potential solution to many of the issues it highlights. \n\n(extremely minor typo: \""One popular possibility from L2S is go the full reduction route down to binary classification\"")",1,1,1,1,-1,1,1,-1,1,-1
HkUR_y-RZ-R2,"This paper proposes an adaptation of the SEARN algorithm to RNNs for generating text.  In order to do so, they discuss various issues on how to scale the approach to large output vocabularies by sampling which actions the algorithm to explore. \n\nPros:\n- Good literature review.  But the future work on bandits is already happening: \nPaper accepted at ACL 2017: Bandit Structured Prediction for Neural Sequence-to-Sequence Learning. Julia Kreutzer, Artem Sokolov, Stefan Riezler. \n\n\nCons:\n- The key argument of the paper is that SEARNN is a better IL-inspired algorithm than the previously proposed ones.  However there is no direct comparison either theoretical or empirical against them.  In the examples on spelling using the dataset of Bahdanau et al. 2017, no comparison is made against their actor-critic method.  Furthermore, given its simplicity, I would expect a comparison against scheduled sampling. \n\n- A lot of important experimental details are in the appendices and they differ among experiments.  Finally, in the NMT comparison while it is stated that similar architecture is used in order to compare fairly against previous work, this is not the case eventually, as it is acknowledged at least in the case of MIXER.  I would have expected the same encoder-decoder architecture to have been used for all the methods considered. \n \n- the two losses introduced are not really new.  The log-loss is just MLE, only assuming that instead of a fixed expert that always returns the same target, we have a dynamic one.  Note that the notion of dynamic expert is present in the SEARN paper too.  Goldberg and Nivre just adapted it to transition-based dependency parsing.  Similarly, since the KL loss is the same as XENT, why give it a new name? \n\n- the top-k sampling method is essentially the same as the targeted exploration of Goodman et al. (2016) which the authors cite.  Thus it is not a novel contribution. \n  \n- Not sure I see the difference between the stochastic nature of SEARNN and the online one of LOLS mentioned in section 7.  They both could be mini-batched similarly.  Also, not sure I see why SEARNN can be used on any task, in comparison to other methods.  They all seem to be equally capable. \n\nMinor comments:\n- Figure 1: what is the difference between \""cost-sensitive loss\"" and just \""loss\""? \n- local vs sequence-level losses: the point in Ranzato et al and Wiseman & Rush is that the loss they optimizise (BLEU/ROUGE) do not decompose over the the predictions of the RNNs. \n- Can't see why SEARNN can help with the vanishing gradient problem.  Seem to be rather orthogonal.\n",1,1,1,1,-1,1,1,1,1,-1
HkUR_y-RZ-R3,"The paper proposes new RNN training method based on the SEARN learning to search (L2S) algorithm and named as SeaRnn.  It proposes a way of overcoming the limitation of local optimization trough the exploitation of the structured losses by L2S.  It can consider different classifiers and loss functions, and a sampling strategy for making the optimization problem scalable is proposed.  SeaRnn improves the results obtained by MLE training in three different problems, including a large-vocabulary machine translation.  In summary, a very nice paper. \n\nQuality: SeaRnn is a well rooted and successful application of the L2S strategy to the RNN training that combines at the same time global optimization and scalable complexity.  \n\nClarity: The paper is well structured and written, with a nice and well-founded literature review. \n\nOriginality: the paper presents a new algorithm for training RNN based on the L2S methodology, and it has been proven to be competitive in both toy and real-world problems. \n\nSignificance: although the application of L2S to RNN training is not new",1,1,-1,1,-1,1,1,-1,1,-1
HkwBEMWCZ-R1,"The authors show that two types of singularities impede learning in deep neural networks: elimination singularities (where a unit is effectively shut off by a loss of input or output weights, or by an overly-strong negative bias), and overlap singularities, where two or more units have very similar input or output weights.  They then demonstrate that skip connections can reduce the prevalence of these singularities, and thus speed up learning. \n\nThe analysis is thorough: the authors explore alternative methods of reducing the singularities, and explore the skip connection properties that more strongly reduce the singularities, and make observations consistent with their overarching claims. \n\nI have no major criticisms. \n\nOne suggestion for future work would be to provide a procedure for users to tailor their skip connection matrices to maximize learning speed and efficacy.  The authors could then use this procedure to make highly trainable networks, and show that on test (not training) data, the resultant network leads to high performance.",1,1,1,1,1,-1,1,1,1,-1
HkwBEMWCZ-R2,"Paper examines the use of skip connections (including residual layers) in deep networks as a way of alleviating two perceived difficulties in training: 1) when a neuron does not contain any information, and  2) when two neurons in a layer compute the same function.  Both of these cases lead to singularities in the Hessian matrix, and this work includes a number of experiments showing the effect of skip connections on the Hessian during training.  \n\nThis is a significant and timely topic.  While I may not be the best one to judge the originality of this work, I appreciated how the authors presented clear and concise arguments with experiments to back up their claims.\n\n",1,1,-1,1,-1,-1,1,-1,1,-1
HkwBEMWCZ-R3,"This paper proposes to explain the benefits of skip connections in terms of eliminating the singularities of the loss function.  The discussion is largely based on a sequence of experiments, some of which are interesting and insightful.  The discussion here can be useful for other researchers.  \n\nMy main concern is that the result here is purely empirical, with no concrete theoretical justification.  What the experiments reveal is an empirical correlation between the Eigval index and training accuracy, which can be caused by lots of reasons (and cofounders), and does not necessarily establish a causal relation.  Therefore, i found many of the discussion to be questionable.  I would love to see more solid theoretical discussion to justify the hypothesis proposed in this paper. \n \nDo you have a sense how accurate is the estimation of the tail probabilities of the eigenvalues?  Because the whole paper is based on the approximation of the eigval indexes, it is critical to exam the estimation is accurate enough to draw the conclusions in the paper.  \n\nAll the conclusions are based on one or two datasets.  Could you consider testing the result on more different datasets to verify if the results are generalizable?",1,1,1,1,1,-1,1,1,1,-1
HkwZSG-CZ-R1,"The authors has addressed my concerns, so I raised my rating.  \n\nThe paper is grounded on a solid theoretical motivation and the analysis is sound and quite interesting. \n\nThere are no results on large corpora such as 1 billion tokens benchmark corpus, or at least medium level corpus with 50 million tokens.  The corpora the authors choose are quite small, the variance of the estimates are high, and similar conclusions might not be valid on a large corpus.  \n\n[1] provides the results of character level language models on Enwik8 dataset, which shows regularization doesn't have much effect and needs less tuning.  Results on this data might be more convincing. \n\nThe results of MOS is very good,  but the computation complexity is much higher than other baselines. In the experiments, the embedding dimension of MOS is slightly smaller, but the number of mixture is 15.  This will make it less usable, I think it's necessary to provide the training time comparison. \n\nFinally experiments on machine translation or speech recognition should be done and to see what improvements the proposed method could bring for BLEU or WER.  \n\n[1] Melis, G\u00e1bor, Chris Dyer, and Phil Blunsom.  \""On the state of the art of evaluation in neural language models. \"" arXiv preprint arXiv:1707.05589 (2017). \n\n[2] Joris Pelemans, Noam Shazeer, Ciprian Chelba, Sparse Non-negative Matrix Language Modeling,  Transactions of the Association for Computational Linguistics, vol. 4 (2016), pp. 329-342 \n\n[3] Shazeer et al. (2017). Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. ICLR 2017\n",1,1,1,1,1,1,1,1,1,-1
HkwZSG-CZ-R2,"The authors argue in this paper that due to the limited rank of the  context-to-vocabulary logit matrix in the currently used version of the softmax output layer, it is not able to capture the full complexity of language.  As a result, they propose to use a mixture of softmax output layers instead where the mixing probabilities are context-dependent, which allows to obtain a full rank logit matrix in complexity linear in the number of mixture components (here 15).  This leads to improvements in the word-level perplexities of the PTB and wikitext2 data sets, and Switchboard BLEU scores. \n\nThe question of the expressiveness of the softmax layer, as well as its suitability for word-level prediction, is indeed an important one which has received too little attention.  This makes a lot of the questions asked in this paper extremely relevant to the field.  However, it is unclear that the rank of the logit matrix is the right quantity to consider.  For example, it is easy to describe a rank D NxM matrix where up to 2^D lines have max values at different indices.  Further, the first two \""observations\"" in Section 2.2 would be more accurately described as \""intuitions\"" of the authors.  As they write themselves \""there is no evidence showing that semantic meanings are fully linearly correlated. \"" Why then try to link \""meanings\"" to basis vectors for the rows of A? \n\nTo be clear, the proposed model is undoubtedly more expressive than a regular softmax, and although it does come at a substantial computational cost (a back-of-the envelope calculation tells us that computing 15 components of 280d MoS takes the same number of operations as one with dimension 1084 = sqrt (280*280*15)), it apparently manages not to drastically increase overfitting, which is significant. \n\nUnfortunately, this is only tested on relatively small data sets, up to 2M tokens and a vocabulary of size 30K for language modeling.  They do constitute a good starting place to test a model,  but given the importance of regularization on those specific tasks, it is difficult to predict how the MoS would behave if more training data were available, and if one could e.g. simply try a 1084 dimension embedding for the softmax without having to worry about overfitting. \n\nAnother important missing experiment would consist in varying the number of mixture components (this could very well be done on WikiText2).  This could help validate the hypothesis: how does the estimated rank vary with the number of components?  How about the performance and pairwise KL divergence?  \n\nThis paper offers a promising direction for language modeling research,  but would require more justification, or at least a more developed experimental section. \n\nPros:\n- Important starting question \n- Thought-provoking approach \n- Experimental gains on small data sets \n\nCons:\n- The link between the intuition and reality of the gains is not obvious \n- Experiments limited to small data sets, some obvious questions remain",1,1,1,1,1,-1,1,1,1,-1
HkwZSG-CZ-R3,"Language models are important components to many NLP tasks.  The current state-of-the-art language models are based on recurrent neural networks which compute the probability of a word given all previous words using a softmax function over a linear function of the RNN's hidden state.  This paper argues the softmax is not expressive enough and proposes to use a more flexible mixture of softmaxes.  The use of a mixture of softmaxes is motivated from a theoretical point of view by translating language modeling into matrix factorization. \n\nPros:\n--The paper is very well written and easy to follow.  The ideas build up on each other in an intuitive way. \n--The idea behind the paper is novel: translating language modeling into a matrix factorization problem is new as far as I know. \n--The maths is very rigorous. \n--The experiment section is thorough. \n\nCons:\n--To claim SOTA all models need to be given the same capacity (same number of parameters).  In Table 2 the baselines have a lower capacity.  This is an unfair comparison \n--I suspect the proposed approach is slower than the baselines.  There is no mention of computational cost.  Reporting that would help interpret the numbers.  \n\nThe SOTA claim might not hold if baselines are given the same capacity.  But regardless of this, the paper has very strong contributions and deserves acceptance at ICLR.",1,1,1,1,-1,-1,1,1,1,-1
HkxF5RgC--R1,"The paper devises a sparse kernel for RNNs which is urgently needed because current GPU deep learning libraries (e.g., CuDNN) cannot exploit sparsity when it is presented and because a number of works have proposed to sparsify/prune RNNs so as to be able to run on devices with limited compute power (e.g., smartphones).  Unfortunately, due to the low-level and GPU specific nature of the work, I would think that this work will be better critiqued in a more GPU-centric conference.  Another concern is that while experiments are provided to demonstrate the speedups achieved by exploiting sparsity, these are not contrasted by presenting the loss in accuracy caused by introducing sparsity (in the main portion of the paper).  It may be the case by reducing density to 1% we can speedup by N fold but this observation may not have any value if the accuracy becomes  abysmal. \n\nPros:\n- Addresses an urgent and timely issue of devising sparse kernels for RNNs on GPUs \n- Experiments show that the kernel can effectively exploit sparsity while utilizing GPU resources well \n\nCons:\n- This work may be better reviewed at a more GPU-centric conference \n- Experiments (in main paper) only show speedups and do not show loss of accuracy due to sparsity",1,1,1,1,-1,1,1,1,1,-1
HkxF5RgC--R2,"This paper introduces sparse persistent RNNs, a mechanism to add pruning to the existing work of stashing RNN weights on a chip.  The paper describes the use additional mechanisms for synchronization and memory loading.  \n\nThe evaluation in the main paper is largely on synthetic workloads (i.e. large layers with artificial sparsity).   With evaluation largely over layers instead of applications, I was left wondering whether there is an actual benefit on real workloads.  Furthermore, the benefit over dense persistent RNNs for OpenNMT application (of absolute 0.3-0.5s over dense persistent rnns?) did not appear significant unless you can convince me otherwise.  \n\nStoring weights persistent on chip should give a sharp benefit when all weights fit on the chip.  One suggestion I have to strengthen the paper is to claim that due to pruning, now you can support a larger number of methods or method configurations and to provide examples of those. \n\nTo summarize, the paper adds the ability to support pruning over persistent RNNs.  However, Narang et. al., 2017 already explore this idea, although briefly.  Furthermore, the gains from the sparsity appear rather limited over real applications.  I would encourage the authors to put the NMT evaluation in the main paper (and perhaps add other workloads).  Furthermore, a host of techniques are discussed (Lamport timestamps, memory layouts) and implementing them on GPUs is not trivial.  However, these are well known and the novelty or even the experience of implementing these on GPUs should be emphasized.",1,1,1,1,1,1,1,1,1,-1
HkxF5RgC--R3,"The paper proposes improving performance of large RNNs by combing techniques of model pruning and persistent kernels.  The authors further propose model-pruning optimizations which are aware of the persistent implementation. \n\nIt's not clear if the paper is relevant to the ICLR audience due to its emphasize on low-level optimization which has little insight in learning representations.  The exposition in the paper is also not well-suited for people without a systems background, although I'll admit I'm mostly using myself as a proxy for the average machine learning researcher here.  For instance, the authors could do more to explain Lamport Timestamps than a 1974 citation. \n\nModulo problems of relevance and expected audience, the paper is well-written and presents useful improvements in performance of large RNNs, and the work has potential for impact in industrial applications of RNNs.  The work is clearly novel, and the contributions are clear and well-justified using experiments and ablations.",1,1,1,1,1,-1,1,1,1,-1
Hk__kGbCW-R1,"The article proposes to use dense skip-connections on the \""vertical\"" (between-layers) connections of recurrent networks.  Moreover, the article proposes to use separate attention-heads that run on the outputs of each encoder's layer, with each attention selecting other regions in the input to attend to. \n\nThe experiments demonstrate that the changes yield small BLEU score improvements on translation and summarization tasks. \n\nI am not convinced by the presented results for the following reasons:\n1) the paper introduces two concepts - the dense skip-connections and the multi-head attention.  Experiments only show their joint impact, yet claims are made about the effectiveness of the skip-connections - maybe what's helping is the multi-head attention? \n2) the results suggest that deeper model are better, with the densely connected networks being up to twice deeper than the baselines.  What happens for deeper and narrower baselines that have a similar number of parameters? \n3) looking at the training curves (thanks for including them), the densely connected model seems to converge faster by annealing the learning faster (I treat the \""jumps\"" in the training curves as signs of learning rate anneal).  Maybe this is what helps?  I know the authors use an automaton to anneal the learning rate, but maybe the impact of learning rates should be evaluated? \n\nQuality:\nGood\n\nClarity:\nThe paper is clearly written. \n\nOriginality:\nThe addition of dense connections to recurrent networks is trivial. \n\n\nPros&cons\n+ the proposed additions (dense skip connections) and multi-head attentions yield performance improvements\n- the impact of the two contributions is not disentangled in the paper\n- the two contributions are fairly obvious",1,1,1,1,1,-1,1,1,1,-1
Hk__kGbCW-R2,"This work proposes to densely connected layers to RNNs by concatenating previously constructed layers together as an input to the current layer.  In addition, attention context is computed for each layer, then, combined together as a single context.  Experimental results on English-French and English-German translation tasks and text summarization show comparable performance to a conventional non-densely connected layers with few number of parameters. \n\nMotivation is clear in that it applies the densely connected networks in vision to texts and the gains achieved by smaller number of parameters look reasonable.  However I have some concerns to this paper. \n\n- It is a combination of two techniques, dense connections and multiple attention and it is not clear where the actual gain come from.  I'd expect more ablation studies by isolating the effects of dense connection and the use of multiple attention mechanisms. \n\n- It is not clear why the experiments for dense sticked to a particular hidden size, e.g., 256 for machine translation, and varies only the number of layers.  Do you have experiments by fixing the number of layers and varying the hidden size? \n\nOther comment:\n\n- Section 3: sequence-to=sequence -> sequence-to-sequence\n\n- It is not clear why the concatenation of all layers is not experimented which is mentioned in section 3.2. Memory problem",1,1,1,1,1,-1,1,1,1,-1
Hk__kGbCW-R3,"This paper describes an attempt of improving information flow in deep networks (but is used and tested here with seq2seq models although it is reality unrelated to seq2seq models per se).  Slightly different from Resnet the information flow is improved by not just adding the outputs from previous layers but instead concatenating the outputs from previous layers with the current outputs.  The authors claim better convergence speed and better results for a similar number of parameters although the differences seems to be in the noise.   \n\nOverall this is an OK technique but in my opinion not really novel enough to justify a whole paper about it as it seems more like a relatively minor architecture tweak.  The results seem to indicate that there were some problems with getting deeper networks to work for the baseline (why is in Table 3 baseline-6L worse than baseline-4L?)  for which the reason could be a multitude of issues probably related to hyper-parameter tuning.  What is also missing is a an analysis of the negative consequences of this technique -- for example, doesn't the number of parameters increase with the depth of the network because of the concatenation?  Also, it would have been good to see more experiments with smaller baseline networks as well to match the smaller DenseNet networks in Table 1 and 2.  Finally, the writing of the paper could be improved a lot: The basic idea is not well described (however, many times repeated) and the grammar is often wrong and also there are some typos",1,1,1,1,1,-1,1,1,1,-1
Hy3MvSlRW-R1,"The paper aims to improve the accuracy of reading model on question answering dataset by playing against an adversarial agent (which is called narrator by the authors) that \""obfuscates\"" the document, i.e. changing words in the document.  The authors mention that word dropout can be considered as its special case which randomly drops words without any prior.  Then the authors claim that smartly choosing the words to drop can make a stronger adversarial agent, which in turn would improve the performance of the reader as well.  Hence the adversarial agent is trained and is architecturally similar to the reader but just has a different last layer, which predicts the word that would make the reader fail if the word is obfuscated..  While there have been numerous GAN-like approaches for language understanding, very few, if any, have shown worthy results.  So if this works, it could be an impactful achievement..  \n\nHowever, I am concerned with the experimental results. \n\nFirst, CBT: NE and CN numbers are too low.  Even a pure LSTM achieves (no attention, no memory) 44% and 45%, respectively (Yu et al., 2017).  These are 9% and 6% higher than the reported numbers for adversarial GMemN2N.  So it is very difficult to determine if the model is appropriate for the dataset in the first place, and whether the gain from the non-adversarial setting is due to the adversarial setup or not. \n\nSecond, Cambridge dialogs: the dataset's metric is not accuracy-based (while the paper reports accuracy), so I assume some preprocessing and altering have been done on the dataset.  So there is no baseline to compare.  Though I understand that the point of the paper is the improvement via the adversarial setting, it is hard to gauge how good the numbers are. \n\nThird, TripAdvisor: the dataset paper by Wang et al. (2010) is not evaluated on accuracy (rather on ranking, etc.).  Did you also make changes to the dataset?  Again, this makes the paper less strong because there is no baseline to compare. \n\nIn short, the only comparable dataset is CBT, which has too low accuracy compared to a very simple baseline. \nIn order to improve the paper, I recommend the authors to evaluate on more common datasets and/or use more appropriate reading models. \n\n---\n\nTypos:\npage 1 first para: \""One the first hand\"" -> \""On the first hand\""\npage 1 first para: \""minimize to probability\"" -> \""minimize the probability\""\npage 3 first para: \""compensate\"" -> \""compensated\""\npage 3 last para: \""softmaxis\"" -> \""softmax is\""\npage 4 sec 2.4: \""similar to the reader\"" -> \""similarly to the reader\""\npage 4 sec 2.4: \""unknow\"" -> \""unknown\ ""\npage 4 sec 3 first para: missing reference at \""a given dialog\""\npage 5 first para: \""Concretly\"" -> \""Concretely\""\nTable 1: \""GMenN2N\"" -> \""GMemN2N\""\nTable 1: what is difference between \""mean\"" and \""average\""?\npage 8 last para: missing reference at \""Iterative Attentive Reader\""\npage 9 sec 6.2 last para: several citations missing, e.g. which paper is by \""Tesauro\""? \n\n\n[Yu et al. 2017] Adams Wei Yu, Hongrae Kim, and Quoc V. Le. Learning to Skim Text. ACL 2017\n\n",1,1,1,1,1,1,1,1,1,-1
Hy3MvSlRW-R2,"Summary:\n\nThis paper proposes an adversarial learning framework for machine comprehension task.  Specifically, authors consider a reader network which learns to answer the question by reading the passage and a narrator network which learns to obfuscate the passage so that the reader can fail in its task.  Authors report results in 3 different reading comprehension datasets and the proposed learning framework results in improving the performance of GMemN2N. \n\n\nMy Comments:\n\nThis paper is a direct application of adversarial learning to the task of reading comprehension.  It is a reasonable idea and authors indeed show that it works. \n\n1. The paper needs a lot of editing.  Please check the minor comments. \n\n2. Why is the adversary called narrator network?  It is bit confusing because the job of that network is to obfuscate the passage. \n\n3. Why do you motivate the learning method using self-play?  This is just using the idea of adversarial learning (like GAN) and it is not related to self-play. \n\n4 In section 2, first paragraph, authors mention that the narrator prevents catastrophic forgetting.  How is this happening?  Can you elaborate more? \n\n5. The learning framework is not explained in a precise way.  What do you mean by re-initializing and retraining the narrator? Isn\u2019t it costly to reinitialize the network and retrain it for every turn?  How many such epochs are done?  You say that test set also contains obfuscated documents.  Is it only for the validation set?  Can you please explain if you use obfuscation when you report the final test performance too?  It would be more clear if you can provide a complete pseudo-code of the learning procedure. \n\n6. How does the narrator choose which word to obfuscate?  Do you run the narrator model with all possible obfuscations and pick the best choice? \n\n7. Why don\u2019t you treat number of hops as a hyper-parameter and choose it based on validation set?  I would like to see the results in Table 1 where you choose number of hops for each of the three models based on validation set. \n\n8. In figure 2, how are rounds constructed?  Does the model sees the same document again and again for 100 times or each time it sees a random document and you sample documents with replacement?  This will be clear if you provide the pseudo-code for learning. \n\n9. I do not understand author's\u2019 justification for figure-3.  Is it the case that the model learns to attend to last sentences for all the questions?  Or where it attends varies across examples?\n\n10. Are you willing to release the code for reproducing the results?\n\nMinor comments:\n\nPage 1, \u201cexploit his own decision\u201d should be \u201cexploit its own decision \u201d\nIn page 2, section 2.1, sentence starting with \u201cIndeed, a too low percentage \u2026\u201d needs to be fixed. \nPage 3, \u201cforgetting is compensate\u201d should be \u201cforgetting is compensated\u201d. \nPage 4, \u201cfor one sentences\u201d needs to be fixed. \nPage 4, \u201cunknow\u201d should be \u201cunknown\u201d.\nPage 4, \u201c?? \u201d needs to be fixed. \nPage 5, \u201cfor the two first datasets\u201d needs to be fixed. \nTable 1, \u201cGMenN2N\u201d should be \u201cGMemN2N\u201d.  In caption, is it mean accuracy or maximum accuracy? \nPage 6, \u201cdataset was achieves\u201d needs to be fixed. \nPage 7, \u201cdocument by obfuscated this word\u201d needs to be fixed.\ nPage 7, \u201coverall aspect of the two first readers\u201d needs to be fixed. \nPage 8, last para, references needs to be fixed. \nPage 9, first sentence, please check grammar. \nSection 6.2, last sentence is irrelevant.\n",1,1,1,1,1,-1,1,1,1,1
Hy3MvSlRW-R3,"The main idea of this paper is to automate the construction of adversarial reading comprehension problems in the spirit of Jia and Liang, EMNLP 2017.   In that work a \""distractor sentence\"" is manually added to a passage to superficially, but not logically, support an incorrect answer.   It was shown that these distractor sentences largely fool existing reading comprehension systems although they do not fool human readers.  \n\nThis paper replaces the manual addition of a distractor sentence with a single word replacement where a \""narrator\"" is trained adversarially to select a replacement to fool the question answering system.    This idea seems interesting but very difficult to evaluate.    An adversarial word replacement my in fact destroy the factual information needed to answer the question and there is no control for this.    The performance of the question answering system in the presence of this adversarial narrator is of unclear significance and the empirical results in the paper are very difficult to interpret.    No comparisons with previous work are given (and perhaps cannot be given).  \n\nA better model would be the addition of a distractor sentence as this preserves the information in the original passage.    A language model could probably be used to generate a compelling distractor.    But we want that the corrupted passage has the same correct answer as the uncorrupted passage and this difficult to guarantee.    A trained \""narrator\"" could learn to actually change the correct answer.",1,1,1,1,1,-1,1,1,1,-1
Hy6GHpkCW-R1,"This paper introduces a neural network architecture for generating sketch drawings.  The authors propose that this is particularly interesting over generating pixel data as it emphasises more human concepts I agree.  The contribution of this paper of this paper is two-fold.  I Firstly, the paper introduces a large sketch dataset that future papers can rely on.  Secondly, the paper introduces the model for generating sketch drawings.  I\n\nThe model is inspired by the variational autoencoder.  I However, the proposed method departs from the theory that justifies the variational autoencoder.  I believe the following things would be interesting points to discuss / follow up: \n- The paper preliminarily investigates the influence of the KL regularisation term on a validation data likelihood.  It seems to have a negative impact for the range of values that are discussed.  However, I would expect there to be an optimum.  Does the KL term help prevent overfitting at some stage?  Answering this question may help understand what influence variational inference has on this model. \n- The decoder model has randomness injected in it at every stage of the RNN.  Because of this, the latent state actually encodes a distribution over drawings, rather than a single drawing.  It seems plausible that this is one of the reasons that the model cannot obtain a high likelihood with a high KL regularisation term.  Would it help to rephrase the model to make the mapping from latent representation to drawing more deterministic?  This definitely would bring it closer to the way the VAE was originally introduced. \n- The unconditional generative model *only* relies on the \""injected randomness\"" for generating drawings, as the initial state is initialised to 0.  This also is not in the spirit of the original VAE, where unconditional generation involves sampling from the prior over the latent space. \n\nI believe the design choices made by the authors to be valid in order to get things to work.  But it would be interesting to see why a more straightforward application of theory perhaps *doesn't* work as well (or whether it works better).  This would help interesting applications inform what is wrong with current theoretical views. \n\nOverall, I would argue that this paper is a clear accept.",1,1,1,1,1,-1,1,1,1,-1
Hy6GHpkCW-R2,"The paper aims tackles the problem of generate vectorized sketch drawings by using a RNN-variational autoencoder.  Each node is represented with (dx, dy) along with one-hot representation of three different drawing status.  A bi-directional LSTM is used to encode latent space in the training stage.  Auto-regressive VAE is used for decoding.  \n\nSimilar to standard VAEs, log-likelihood has bee used as the data-term and the KL divergence between latent space and Gaussian prior is the regularisation term.  \n\nPros:\n- Good solution to an interesting problem.  \n- Very interesting dataset to be released. \n- Intensive experiments to validate the performance.  \n\nCons:\n- I am wondering whether the dataset contains biases regarding (dx, dy).  In the data collection stage, how were the points lists generated from pen strokes?   Did each points are sampled from same travelling distance or according to the same time interval?   Are there any other potential biases brought because the data collection tools? \n- Is log-likelihood a good loss here?  Think about the case where the sketch is exactly the same but just more points are densely sampled along the pen stroke.  How do you deal with this case? \n- Does the dataset contain more meta-info that could be used for other tasks beyond generation, e.g. segmentation, classification, identification, etc.?",1,1,1,1,1,-1,1,1,1,-1
Hy6GHpkCW-R3,"The paper presents both a novel large dataset of sketches and a new rnn architecture to generate new sketches. \n\n+ new and large dataset \n+ novel algorithm \n+ well written \n- no evaluation of dataset \n- virtually no evaluation of algorithm \n- no baselines or comparison \n\nThe paper is well written, and easy to follow.  The presented algorithm sketch-rnn seems novel and significantly different from prior work. \nIn addition, the authors collected the largest sketch dataset, I know of.  This is exciting as it could significantly push the state of the art in sketch understanding and generation.  \n\nUnfortunately the evaluation falls short.  If the authors were to push for their novel algorithm, I'd have expected them to compare to prior state of the art on standard metrics, ablate their algorithm to show that each component is needed, and show where their algorithm shines and where it falls short. \nFor ablation, the bare minimum includes: removing the forward and/or reverse encoder and seeing performance drop.  Remove the variational component, and phrasing it simply as an auto-encoder.  Table 1 is good,  but not sufficient.  Training loss alone likely does not capture the quality of a sketch. \nA comparison the Graves 2013 is absolutely required, more comparisons are desired. \nFinally, it would be nice to see where the algorithm falls short, and where there is room for improvement. \n\nIf the authors wish to push their dataset, it would help to first evaluate the quality of the dataset.  For example, how well do humans classify these sketches?  How diverse are the sketches?  Are there any obvious modes?  Does the discretization into strokes matter? \nAdditionally, the authors should present a few standard evaluation metrics they would like to compare algorithms on?  Are there any good automated metrics, and how well do they correspond to human judgement? \n\nIn summary, I'm both excited about the dataset and new architecture,  but at the same time the authors missed a huge opportunity by not establishing proper baselines, evaluating their algorithm, and pushing for a standardized evaluation protocol for their dataset.  I recommend the authors to decide if they want to present a new algorithm, or a new dataset and focus on a proper evaluation.",1,1,1,1,1,1,1,1,1,-1
Hy7fDog0b-R1,"Quick summary:\nThis paper shows how to train a GAN in the case where the dataset is corrupted by some measurement noise process.  They propose to introduce the noise process into the generation pipeline such that the GAN generates a clean image, corrupts its own output and feeds that into the discriminator.  The discriminator then needs to decide whether this is a real corrupted measurement or a generated one.   The method is demonstrated to the generate better results than the baseline on a variety of datasets and noise processes. \n\nQuality:\nI found this to be a nice paper - it has an important setting to begin with and the proposed method is clean and elegant albeit a bit simple.  \n\nOriginality:\nI'm pretty sure this is the first paper to tackle this problem directly in general. \n\nSignificance:\nThis is an important research direction as it is not uncommon to get noisy measurements in the real world under different circumstances.  \n\nPros:\n* Important problem \n* elegant and simple solution \n* nice results and decent experiments (but see below) \n\nCons:\n* The assumption that the measurement process *and* parameters are known is quite a strong one.  Though it is quite common in the literature to assume this, it would have been interesting to see if there's a way to handle the case where it is unknown (either the process, parameters or both). \n* The baseline experiments are a bit limited - it's clear that such baselines would never produce samples which are any better than the \""fixed\"" version which is fed into them.  I can't however, think of other baselines other than \""ignore\"" so I guess that is acceptable. \n* I wish the authors would show that they get a *useful* model eventually - for example, can this be used to denoise other images from the dataset? \n\nSummary:\nThis is a nice paper which deals with an important problem, has some nice results and while not groundbreaking, certainly merits a publication.",1,1,1,1,1,-1,1,1,1,-1
Hy7fDog0b-R2,"The paper explores GAN training under a linear measurement model in which one assumes that the underlying state vector $x$ is not directly observed but we do have access to measurements $y$ under a linear measurement model plus noise.  The paper explores in detail several practically useful versions of the linear measurement model, such as blurring, linear projection, masking etc. and establishes identifiability conditions/theorems for the underlying models. \nThe AmbientGAN approach advocated in the paper amounts to learning end-to-end differentiable Generator/Discriminator networks that operate in the measurement space.  The experimental results in the paper show that this works much better than reasonable baselines, such as trying to invert the measurement model for each individual training sample, followed by standard GAN training. \nThe theoretical analysis is satisfactory.  However, it would be great if the theoretical results in the paper were able to associate the difficulty of the inversion process with the difficulty of AmbientGAN training.  For example, if the condition number for the linear measurement model is high, one would expect that recovering the target real distribution is more difficult.  The condition in Theorem 5.4 is a step in this direction, showing that the required number of samples for correct recovery increases with the probability of missing data.  It would be great if Theorems 5.2 and 5.3 also came with similar quantitative bounds.",1,1,1,1,1,-1,1,1,1,-1
Hy7fDog0b-R3,"The paper proposes an approach to train generators within a GAN framework, in the setting where one has access only to degraded / imperfect measurements of real samples, rather than the samples themselves.  Broadly, the approach is to have a generator produce the \""full\"" real data, pass it through a simulated model of the measurement process, and then train the discriminator to distinguish between these simulated measurements of generated samples, and true measurements of real samples.  By this mechanism, the proposed method is able to train GANs to generate high-quality samples from only imperfect measurements. \n\nThe paper is largely well-written and well-motivated, the overall setup is interesting (I find the authors' practical use cases convincing---where one only has access to imperfect data in the first place), and the empirical results are convincing.  The theoretical proofs do make strong assumptions (in particular, the fact that the true distribution must be uniquely constrained by its marginal along the measurement).  However, in most theoretical analysis of GANs and neural networks in general, I view proofs as a means of gaining intuition rather than being strong guarantees---and to that end, I found the analysis in this paper to be informative. \n\nI would make a  suggestions for possible further experimental analysis: it would be nice to see how robust the approach is to systematic mismatches between the true and modeled measurement functions (for instance, slight differences in the blur kernels, noise variance, etc.).  Especially in the kind of settings the paper considers, I imagine it may sometimes also be hard to accurately model the measurement function of a device (or it may be necessary to use a computationally cheaper approximation for training).  I think a study of how such mismatches affect the training procedure would be instructive (perhaps more so than some of the quantitative evaluation given that they at best only approximately measure sample quality).",1,1,1,1,1,-1,1,1,1,-1
Hy8hkYeRb-R1,"Quality\n\nThe authors introduce a deep network for predictive coding.  It is unclear how the approach improves on the original predictive coding formulation of Rao and Ballard, who also use a hierarchy of transformations.  The results seem to indicate that all layers are basically performing the same.  No insight is provided about the kinds of filters that are learned. n\nClarity\n\nIn its present form it is hard to assess if there are benefits to the current formulation compared to already existing formulations. . The paper should be checked for typos. \n\nOriginality\n\nThere exist alternative deep predictive coding models such as https://arxiv.org/abs/1605.08104.  This work should be discussed and compared. \n\nSignificance \n\nIt is hard to see how the present paper improves on classical or alternative (deep) predictive coding results. \n\nPros\n\nRelevant attempt to develop new predictive coding architectures \n\nCons\n\nUnclear what is gained compared to existing work.",1,1,1,1,-1,1,1,1,1,-1
Hy8hkYeRb-R2,"The paper \""A Deep Predictive Coding Network for Learning Latent Representations\"" considers learning of a generative neural network.  The network learns unsupervised using a predictive coding setup.  A subset of the CIFAR-10 image database (1000 images horses and ships) are used for training.  Then images generated using the latent representations inferred on these images, on translated images, and on images of other objects are shown.  It is then claimed that the generated images show that the network has learned good latent representations. \n\nI have some concerns about the paper, maybe most notably about the experimental result and the conclusions drawn from them.  The numerical experiments are motivated as a way to \""understand the capacity of the network with regards to modeling the external environment\"" (abstract).  And it is concluded in the final three sentences of the paper that the presented network \""can infer effective latent representations for images of other objects\"" (i.e., of objects that have not been used for training); and further, that \""in this regards, the network is better than most existing algorithms [...]\"". \n\nI expected the numerical experiments to show results instructive about what representations or what abstractions are learned in the different layers of the network using the learning algorithm and objectives suggested.  Also some at least quantifiable (if not benchmarked) outcomes should have been presented given the rather strong claims/conclusions in abstract and discussion/conclusion sections.  As a matter of fact, all images shown (including those in the appendix) are blurred versions of the original images, except of one single image: Fig. 4 last row, 2nd image (and that is not commented on).  If these are the generated images, then some reconstruction is done by the network, fine, but also not unsurprising as the network was told to do so by the used objective function.  What precisely do we learn here?  I would have expected the presentation of experimental results to facilitate the development of an understanding of the computations going on in the trained network.  How can the reader conclude any functioning from these images?  Using the right objective function, reconstructions can also be obtained using random (not learned) generative fields and relatively basic models.  The fact that image reconstruction for shifted images or new images is evidence for a sophisticated latent representations is, to my mind, not at all shown here.  What would be a good measure for an \""effective latent representation\"" that substantiates the claims made?  The reconstruction of unseen images is claimed central but as far as I could see, Figures 2, 3, and 4 are not even referred to in the text, nor is there any objective measure discussed.  Studying the relation between predictive coding and deep learning makes sense, but I do not come to the same (strong) conclusions as the author(s) by considering the experimental results - and I do not see evidence for a sophisticated latent representation learned by the network.  I am not saying that there is none, but I do not see how the presented experimental results show evidence for this. \n\nFurthermore, the authors stress that a main distinguishing feature of their approach (top of page 3) is that in their network information flows from latent space to observed space (e.g. in contrast to CNNs).  That is a true statement but also one which is true for basically all generative models, e.g., of standard directed graphical models such as wake-sleep approaches (Hinton et al., 1995), deep SBNs and more recent generative models used in GANs (Goodfellow et al, 2014).  Any of these references would have made a lot of sense. \n\nWith my evaluation I do not want to be discouraging about the general approach.  But I can not at all give a good evaluation given the current experimental results (unless substantial new evidence which make me evaluate these results differently is provided in a discussion). \n\n\nMinor:\n\n- no legend for Fig. 1\n\n-notes -> noted\n\nhave focused\n\n\n\n\n",1,1,1,1,-1,1,1,1,-1,-1
Hy8hkYeRb-R3,"The paper attempts to extend the predictive coding model to a multilayer network.   The math is developed for a learning rule, and some demonstrations are shown for reconstructions of CIFAR-10 images. \n\nThe overall idea and approach being pursued here is a good one,  but the model needs further development",1,-1,1,1,1,-1,1,1,-1,-1
HydnA1WCb-R1,"This paper presents an interesting extension to Snell et al.'s prototypical networks, by introducing uncertainty through a parameterised estimation of covariance along side the image embeddings (means).  Uncertainty may be particularly important in the few-shot learning case this paper examines, when it is helpful to extract more information from limited number of input samples. \n\nHowever, several important concepts in the paper are not well explained or motivated.  For example, it is a bit misleading to use the word \""covariance\"" throughout the paper, when the best model only employs a scalar estimate of the variance.  A related, and potentially technical problem is in computing the prototype's mean and variance (section 3.3). Eq. 5 and 6 are not well motivated, and the claim of \""optimal\"" under eq.6 is not explained.  More importantly, eq. 5 and 6 do not use any covariance information (off-diagonal elements of S) --- as a result, the model is likely to ignore the covariance structure even when using full covariance estimate.  The distance function (eq. 4) is d Mahalanobis distance, instead of \""linear Euclidean distance\"".  While the paper emphasises the importance of the form of loss function, the loss function used in the model is given without explanation (and using cross-entropy over distances looks hacky). \n\nIn addition, the experiments are too limited to support the claimed benefits from encoding uncertainty.  Since the accuracies on omniglot data from recent models are already close to perfect, it is unclear whether the marginally improved number reported here is significant.  In addition, more analysis may better support existing claims.  For example, showing subsampled images indeed had higher uncertainty, rather than only the histogram for all data points. \n\nPros:\n-Interesting problem and interesting direction",1,1,1,1,1,-1,1,1,1,-1
HydnA1WCb-R2,"The paper extends the prototypical networks of Snell et al, NIPS 2017 for one shot learning.  Snell et al use a soft kNN classification rule, typically used in standard metric learning work (e.g. NCA, MCML), over learned instance projections, i.e. distances are computed over the learned projections.  Each class is represented by a class prototype which is given by the average of the projections of the class instances.  Classification is done with soft k-NN on the class prototypes.  The distance that is used is the Euclidean distance over the learned representations, i.e. (z-c)^T(z-c), where z is the projection of the x instance to be classified and c is a class prototype, computed as the average of the projections of the support instances of a given class. \n\nThe present paper extends the above work to include the learning of a Mahalanobis matrix, S, for each instance, in addition to learning its projection.  Thus now the classification is based on the Mahalanobis distance: (z-c)^T S_c (z-c).  On a conceptual level since S_c should be a PSD matrix it can be written as the square of some matrix, i.e. S_c = A_c^TA_c, then the Mahanalobis distance becomes (A_c z - A_c c)^T ( A_c z-A_c c), i.e. in addition to learning a projection as it is done in Snell et al, the authors now learn also a linear transformation matrix which is a function of the support points (i.e. the ones which give rise to the class prototypes).  The interesting part here is that the linear projection is a function of the support points.  I wonder though if such a transformation could not be learned by the vanilla prototypical networks simply by learning now a projection matrix A_z as a function of the query point z.  I am not sure I see any reason why the vanilla prototypical networks cannot learn to project x directly to A_z z and why one would need to do this indirectly through the use of the Mahalanobis distance as proposed in this paper. \n\nOn a more technical level the properties of the learned Mahalanobis matrix, i.e. the fact that it should be PSD, are not really discussed neither how this can be enforced especially in the case where S is a full matrix (even though the authors state that this method was not further explored).  If S is diagonal then the S generation methods a) b) c) in the end of section 3.1 will make sure that S is PSD, I do not think that this is the case with d) though. \n\nIn the definition of the prototypes the component wise weigthing (eq. 5) works when the Mahalanobis matrix is diagonal (even though the weighting should be done by the \\sqrt of it), how would it work if it was a full matrix is not clear. \n\nOn the experiments side the authors could have also experimented with miniImageNet and not only omniglot as is the standard practice in one shot learning papers.  \n\nI am not sure I understand figure 3 in which the authors try to see what happens if instead of learning the Mahalanobis matrix one would learn a projection that would have as many additional dimensions as free elements in the Mahalanobis matrix.  I would expect to see a comparison of the vanilla prototypical nets against their method for each one of the different scenarios of the free parameters of the S matrix, something like a ratio of accuracies of the two methods in order to establish whether learning the Mahalanobis matrix brings an improvement over the prototypical nets with an equal number of output parameters.  \n\n",1,1,1,1,1,1,1,1,1,-1
HydnA1WCb-R3,"SUMMARY: This work is about prototype networks for image classification.  The idea is to jointly embed an image and a \""confidence measure\"" into a latent space, and to use these embeddings to define prototypes together with confidence estimates.  A Gaussian model is used for representing these confidences as covariance matrices.  Within a class, the inverse covariance matrices of all corresponding images are averaged to for the inverse class-specific matrix S-C, and this S_C defines the tensor in the Mahalanobis metric for measuring the distances to the prototype.  \n\nEVALUATION:\nCLARITY: I found the paper difficult to read.  In principle, the idea seems to be clear, but then the description and motivation of the model remains very vague.  For instance, what is the the precise meaning of an image-specific covariance matrix (supported by just one point)?   What is the motivation to just average the inverse covariance matrices to compute S_C?   Why isn't the covariance matrix estimated in the usual way as the empirical covariance in the embedding space?   \nNOVELTY: Honestly, I had difficulties to see which parts of this work could be sufficiently novel.   The idea of using a Gaussian model and its associated Mahalanobis metric is certainly interesting,   but also a time-honored concept.   The experiments focus very specifically on the omniglot dataset, and it is not entirely clear to me what  should be concluded from the results presented.   Are you sure that there is any significant improvement over the models in (Snell et al, Mishra et al, Munkhandalai & Yu, Finn et al.)?   \n\n\n""",1,1,1,1,-1,1,1,1,1,-1
HyEi7bWR--R1,"This manuscript introduce a scheme for learning the recurrent parameter matrix in a neural network that uses the Cayley transform and a scaling weight matrix.  This scheme leads to good performance on sequential data tasks and requires fewer parameters than other techniques \n\nComments:\n-- It\u2019s not clear to me how D is determined for each test.  Given the definition in Theorem 3.1 it seems like you would have to have some knowledge of how many eigenvalues in W you expect to be close to -1.   \n-- For the copying and adding problem test cases, it might be useful to clarify or cite something clarifying that the failure mode RNNs run into with temporal ordering problems is an exploding gradient, rather than any other pathological training condition, just to make it clear why these experiments are relevant.  \n-- The ylabel in Figure 1 is \u201cTest Loss\u201d which I didn\u2019t see defined.  Is this test loss the cross entropy?  If so, I think it would be more effective to label the plot with that. \n-- The plots in figure 1 and 2 have different colors to represent the same set of techniques.  I would suggest keeping a  consistent color scheme \n-- It looks like in Figure 1 the scoRNN is outperformed by the uRNN in the long run in spite of the scoRNN convergence being smoother, which should be clarified. \n-- It looks like in Figure 2 the scoRNN is outperformed by the LSTM across the board, which should be clarified. \n-- How is test set accuracy defined in section 5.3?  Classifying digits? Recreating digits?  \n-- When discussing table 1, the manuscript mentions scoRNN and Restricted-capacity uRNN have similar performance for 16k parameters and then state that scoRNN has the best test accuracy at 96.2%.  However, there is no example for restricted-capacity uRNN with 69k parameters to show that the performance of restricted-capacity uRNN doesn't also increase similarly with more parameters. \n-- Overall it\u2019s unclear to me how to completely determine the benefit of this technique over the others because, for each of the tests, different techniques may have superior performance.  For instance, LSTM performs best in 5.2 and in 5.3 for the MNIST test accuracy.  scoRNN and Restricted-capacity uRNN perform similarly for permuted MNIST Test Accuracy in 5.3.  Finally, scoRNN seems to far outperform the other techniques in table 2 on the TIMIT speech dataset.  I don\u2019t understand the significance of each test and why the relative performance of the techniques vary from one to the other. \n-- For example, the manuscript seems to be making the case that the scoRNN gradients are more stable than those of a uRNN, but all of the results are presented in terms of network accuracy and not gradient stability.  You can sort of see that generally the convergence is more gradual for the scoRNN than the uRNN from the training graphs but it'd be nice if there was an actual comparison of the stability of the gradients during training (as in Figure 4 of the Arjovsky 2016 paper being compared to for instance) just to make it really clear.",1,1,1,1,1,-1,1,1,1,1
HyEi7bWR--R2,"This paper suggests an RNN reparametrization of the recurrent weights with a skew-symmetric matrix using Cayley transform to keep the recurrent weight matrix orthogonal.  They suggest that they reparametrization leads to superior performance compare to other forms of Unitary Recurrent Networks. \n\nI think the paper is well-written.   Authors have discussed previous works adequately and provided enough insight and motivation about the proposed method. \n\nI have two questions from authors:\n\n1- What are the hyperparameters that you optimized in experiments? \n\n2- How sensitive is the results to the number of -1 in the diagonal matrix? \n\n3- ince the paper is not about compression, it might be unfair to limit the number of hidden units in LSTMs just to match the number of parameters to RNNs.  In MNIST experiment, for example, better numbers are reported for larger LSTMs.  I think matching the number of hidden units could be helpful.  Also, one might want to know if the scoRNN is still superior in the regime where the number of hidden units is about 1000.  I appreciate if authors can provide more results in these settings.\n\n",1,1,1,1,1,1,1,1,1,-1
HyEi7bWR--R3,"The paper is clearly written, with a good coverage of previous relevant literature.  \nThe contribution itself is slightly incremental, as several different parameterization of orthogonal or almost-orthogonal weight matrices for RNN have been introduced. \nTherefore, the paper must show that this new method performs better in some way compared with previous methods.  They show that the proposed method is competitive on several datasets and a clear winner on one task: MSE on TIMIT. \n\nPros:\n1. New, relatively simple method for learning orthogonal weight matrices for RNN \n\n2. Clearly written\ n\n3. Quite good results on several relevant tasks. \n\nCons:\n1. Technical novelty is somewhat limited \n\n2. Experiments do not evaluate run time, memory use, computational complexity, or stability.  Therefore it is more difficult to make comparisons: perhaps restricted-capacity uRNN is 10 times faster than the proposed method?",1,1,1,1,1,1,1,1,1,-1
Hyg0vbWC--R1,"This paper considers the task of generating Wikipedia articles as a combination of extractive and abstractive multi-document summarization task where input is the content of reference articles listed in a Wikipedia page along with the content collected from Web search and output is the generated content for a target Wikipedia page.  The authors at first reduce the input size by using various extractive strategies and then use the selected content as input to the abstractive stage where they leverage the Transformer architecture with interesting modifications like dropping the encoder and proposing alternate self-attention mechanisms like local and memory compressed attention.    \n\nIn general, the paper is well-written and the main ideas are clear.  However, my main concern is the evaluation.  It would have been nice to see how the proposed methods perform with respect to the existing neural abstractive summarization approaches.  Although authors argue in Section 2.1 that existing neural approaches are applied to other kinds of datasets where the input/output size ratios are smaller,  experiments could have been performed to show their impact.  Furthermore, I really expected to see a comparison with Sauper & Barzilay (2009)'s non-neural extractive approach of Wikipedia article generation, which could certainly strengthen the technical merit of the paper. \n\nMore importantly, it was not clear from the paper if there was a constraint on the output length when each model generated the Wikipedia content.  For example, Figure 5-7 show variable sizes of the generated outputs.  With a fixed reference/target Wikipedia article, if different models generate variable sizes of output, ROUGE evaluation could easily pose a bias on a longer output as it essentially counts overlaps between the system output and the reference.  \n\nIt would have been nice to know if the proposed attention mechanisms account for significantly better results than the T-ED and T-D architectures.  Did you run any statistical significance test on the evaluation results?  \n\nAuthors claim that the proposed model can generate \""fluent, coherent\"" output, however, no evaluation has been conducted to justify this claim.  The human evaluation only compares two alternative models for preference, which is not enough to support this claim.   I would suggest to carry out a DUC-style user evaluation (http://www-nlpir.nist.gov/projects/duc/duc2007/quality-questions.txt) methodology to really show that the proposed method works well for abstractive summarization. \n\nDoes Figure 8 show an example input after the extractive stage or before?  Please clarify.\n\n---------------\nI have updated my scores as authors clarified most of my concerns.",1,1,1,1,1,1,1,1,1,-1
Hyg0vbWC--R2,"This paper proposes an approach to generating the first section of Wikipedia articles (and potentially entire articles).  \nFirst relavant paragraphs are extracted from reference documents and documents retrieved through search engine queries through a TD-IDF-based ranking.  Then abstractive summarization is performed using a modification of Transformer networks (Vasvani et al 2017).  A mixture of experts layer further improves performance.  \nThe proposed transformer decoder defines a distribution over both the input and output sequences using the same self-attention-based network.  On its own this modification improves perplexity (on longer sequences) but not the Rouge score; however the architecture enables memory-compressed attention which is more scalable to long input sequences.  It is claimed that the transformer decoder makes optimization easier but no complete explanation or justification of this is given.   Computing self-attention and softmaxes over entire input sequences will significantly increase the computational cost of training.  \n\nIn the task setup the information retrieval-based extractive stage is crucial to performance, but this contribution might be less important to the ICLR community.   It willl also be hard to reproduce without significant computational resources, even if the URLs of the dataset are made available.   The training data is significantly larger than the CNN/DailyMail single-document summarization dataset.  \n\nThe paper presents strong quantitative results and qualitative examples.   Unfortunately it is hard to judge the effectiveness of the abstractive model due to the scale of the experiments, especially with regards to the quality of the generated output in comparison to the output of the extractive stage.  \nIn some of the examples the system output seems to be significantly shorter than the reference, so it would be helpful to quantify this, as well how much the quality degrades when the model is forced to generate outputs of a given minimum length.   While the proposed approach is more scalable, it is hard to judge the extend of this.  \n\nSo while the performance of the overall system is impressive,   it is hard to judge the significance of the technical contribution made by the paper.  \n\n---\nThe additional experiments and clarifications in the updated version give substantially more evidence in support of the claims made by the paper, and I would like to see the paper accepted. \n",1,1,1,1,1,-1,1,1,1,1
Hyg0vbWC--R3,"The main significance of this paper is to propose the task of generating the lead section of Wikipedia articles by viewing it as a multi-document summarization problem.  Linked articles as well as the results of an external web search query are used as input documents, from which the Wikipedia lead section must be generated.  Further preprocessing of the input articles is required, using simple heuristics to extract the most relevant sections to feed to a neural abstractive summarizer.  A number of variants of attention mechanisms are compared, including the transofer-decoder, and a variant with memory-compressed attention in order to handle longer sequences.  The outputs are evaluated by ROUGE-L and test perplexity.   There is also a A-B testing setup by human evaluators to show that ROUGE-L rankings correspond to human preferences of systems, at least for large ROUGE differences. \n\nThis paper is quite original and clearly written.  The main strength is in the task setup with the dataset and the proposed input sources for generating Wikipedia articles.  The main weakness is that I would have liked to see more analysis and comparisons in the evaluation. \n\nEvaluation:\nCurrently, only neural abstractive methods are compared.  I would have liked to see the ROUGE performance of some current unsupervised multi-document extractive summarization methods, as well as some simple multi-document selection algorithms such as SumBasic.  Do redundancy cues which work for multi-document news summarization still work for this task? \n\nExtractiveness analysis:\nI would also have liked to see more analysis of how extractive the Wikipedia articles actually are, as well as how extractive the system outputs are.  Does higher extractiveness correspond to higher or lower system ROUGE scores?  This would help us understand the difficulty of the problem, and how much abstractive methods could be expected to help.  \n\nA further analysis which would be nice to do (though I have less clear ideas how to do it), would be to have some way to figure out which article types or which section types are amenable to this setup, and which are not.  \n\nI have some concern that extraction could do very well if you happen to find a related article in another website which contains encyclopedia-like or definition-like entries (e.g., Baidu, Wiktionary) which is not caught by clone detection.  In this case, the problem could become less interesting, as no real analysis is required to do well here. \n\nOverall, I quite like this line of work,  but I think the paper would be a lot stronger and more convincing with some additional work. \n\n----\nAfter reading the authors' response and the updated submission, I am satisfied that my concerns above have been adequately addressed in the new version of the paper.  This is a very nice contribution.\n",1,1,1,1,1,-1,1,1,1,-1
HyH9lbZAW-R1,"The authors adapts stochastic natural gradient methods for variational inference with structured inference networks.  The variational approximation proposed is similar to SVAE by Jonhson et al. (2016), but rather than directly using the global variable theta in the local approximation for x the authors propose to optimize a separate variational parameter.  The authors then extends and adapts the natural gradient method by Khan & Lin (2017) to optimize all the variational parameters.  In the experiments the authors generally show improved convergence over SVAE. \n\nThe idea seems promising  but it is still a bit unclear to me why removing dependence between global and local parameters that you know is there would lead to a better variational approximation.  The main motivation seems to be that it is easier to optimize. \n\n- In the last two sentences of the updates for \\theta_PGM you mention that you need to do SVI/VMP to compute the function \\eta_x\\theta.  Might this also suffer from non-convergence issues like you argue SVAE does?  Or do you simply mean that computation of this is exact using regular message passing/Kalman filter/forward-backward? \n- It was not clear to me why we should use a Gaussian approximation for the \\theta_NN parameters?  The prior might be Gaussian but the posterior is not?  Is this more of a simplifying assumption? \n- There has recently been interest in using inference networks as part of more flexible variational approximations for structured models.  Some examples of related work missing in this area is \""Variational Sequential Monte Carlo\"" by Naesseth et al. (2017) / \""Filtering Variational Objectives\"" by Maddison et al. (2017) / \""Auto-encoding sequential Monte Carlo\"" Le et al. (2017). \n-  Section 2.1, paragraph nr 5, \""algorihtm\"" -> \""algorithm\""\n",1,1,1,1,1,1,1,1,1,-1
HyH9lbZAW-R2,"This paper presents a variational inference algorithm for models that contain\ndeep neural network components and probabilistic graphical model (PGM)\ncomponents. \nThe algorithm implements natural-gradient message-passing where the messages\nautomatically reduce to stochastic gradients for the non-conjugate neural\nnetwork components.  The authors demonstrate the algorithm on a Gaussian mixture\nmodel and linear dynamical system where they show that the proposed algorithm\noutperforms previous algorithms.  Overall, I think that the paper proposes some\ninteresting ideas,  however, in its current form I do not think that the novelty\nof the contributions are clearly presented and that they are not thoroughly\nevaluated in the experiments. \n\nThe authors propose a new variational inference algorithm that handles models\nwith deep neural networks and PGM components.  However, it appears that the\nauthors rely heavily on the work of (Khan & Lin, 2017) that actually provides\nthe algorithm.  As far as I can tell this paper fits inference networks into\nthe algorithm proposed in (Khan & Lin, 2017) which boils down to i) using an\ninference network to generate potentials for a conditionally-conjugate\ndistribution  and ii) introducing new PGM parameters to decouple the inference\nnetwork from the model parameters.  These ideas are a clever solution to work\ninference networks into the message-passing algorithm of (Khan & Lin, 2017), \nbut I think the authors may be overselling these ideas as a brand new algorithm. \nI think if the authors sold the paper as an alternative to (Johnson, et al., 2016)\nthat doesn't suffer from the implicit gradient problem the paper would fit into\nthe existing literature better. \n\nAnother concern that I have is that there are a lot of conditiona-conjugacy\nassumptions baked into the algorithm that the authors only mention at the end\nof the presentation of their algorithm.  Additionally, the authors briefly state\nthat they can handle non-conjugate distributions in the model by just using\nconjugate distributions in the variational approximation.  Though one could do\nthis, the authors do not adequately show that one should, or that one can do this\nwithout suffering a lot of error in the posterior approximation.  I think that\nwithout an experiment the small section on non-conjugacy should be removed. \n\nFinally, I found the experimental evaluation to not thoroughly demonstrate the\nadvantages and disadvantages of the proposed algorithm.  The algorithm was applied\nto the two models originally considered in (Johnson, et al., 2016) and the\nproposed algorithm was shown to attain lower mean-square errors for the two\nmodels.  The experiments do not however demonstrate why the algorithm is\nperforming better.  For instance, is the (Johnson, et al., 2016) algorithm\nsuffering from the implicit gradient?  It also would have been great to have\nconsidered a model that the (Johnson, et. al., 2016) algorithm would not work\nwell on or could not be applied to show the added applicability of the proposed\nalgorithm. \n\nI also have some minor comments on the paper:\n- There are a lot of typos. \n- The first two sentences of the abstract do not really contribute anything to the paper.  What is a powerful model?  What is a powerful algorithm? \n- DNN was used in Section 2 without being defined. \n- Using p() as an approximate distribution in Section 3 is confusing notation \n  because p() was used for the distributions in the model",1,1,1,1,1,1,1,1,1,-1
HyH9lbZAW-R3,"The paper seems to be significant since it integrates PGM inference with deep models.  Specifically, the idea is to use the structure of the PGM to perform efficient inference.  A variational message passing approach is developed which performs natural-gradient updates for the PGM part and stochastic gradient updates for the deep model part.  Performance comparison is performed with an existing approach that does not utilize the PGM structure for inference. \nThe paper does a good job of explaining the challenges of inference, and provides a systematic approach to integrating PGMs with deep model updates.  As compared to the existing approach where the PGM parameters must converge before updating the DNN parameters, the proposed architecture does not require this, due to the re-parameterization which is an important contribution. \n\nThe motivation of the paper, and the description of its contribution as compared to existing methods can be improved. \ One of the main aspects it seems is generality, but the encodings are specific to 2 types PGMs. \ Can this be generalized to arbitrary PGM structures?  How about cases when computing Z is intractable?  Could the proposed approach be adapted to such cases.  I was not very sure as to why the proposed method is more general than existing approaches. \n\nRegarding the experiments, as mentioned in the paper the evaluation is performed on two fairly small scale datasets.  the approach shows that the proposed methods converge faster than existing methods.  However, I think there is value in the approach, and the connection between variational methods with DNNs is interesting.",1,1,1,1,1,-1,1,1,1,-1
HyHmGyZCZ-R1,"The paper suggests taking GloVe word vectors, adjust them, and then use a non-Euclidean similarity function between them.  The idea is tested on very small data sets (80 and 50 examples, respectively).  The proposed techniques are a combination of previously published steps, and the new algorithm fails to reach state-of-the-art on the tiny data sets. \n\nIt isn't clear what the authors are trying to prove, nor whether they have successfully proven what they are trying to prove . Is the point that GloVe is a bad algorithm?  That these steps are general?  If the latter, then the experimental results are far weaker than what I would find convincing.  Why not try on multiple different word embeddings?  What happens if you start with random vectors?  What happens when you try a bigger data set or a more complex problem?",1,1,1,1,1,-1,1,1,-1,-1
HyHmGyZCZ-R2,"This paper proposes a ranking-based similarity metric for distributional semantic models.  The main idea is to learn \""baseline\"" word embeddings, retrofitting those and applying localized centering, to then calculate similarity using a measure called \""Ranking-based Exponential Similarity Measure\"" (RESM), which is based on the recently proposed APSyn measure. \n\nI think the work has several important issues:\n\n1.  The work is very light on references.  There is a lot of previous work on evaluating similarity in word embeddings (e.g. Hill et al, a lot of the papers in RepEval workshops, etc.); specialization for similarity of word embeddings (e.g. Kiela et al., Mrksic et al., and many others); multi-sense embeddings (e.g. from Navigli's group); and the hubness problem (e.g. Dinu et al.). For the localized centering approach, Hara et al.'s introduced that method.  None of this work is cited, which I find inexcusable.\u2028\n\n2.  The evaluation is limited, in that the standard evaluations (e.g. SimLex would be a good one to add, as well as many others, please refer to the literature) are not used and there is no comparison to previous work.  The results are also presented in a confusing way, with the current state of the art results separate from the main results of the paper.  It is unclear what exactly helps, in which case, and why.\u2028\n\n3.  There are technical issues with what is presented, with some seemingly factual errors.  For example, \""In this case we could apply the inversion, however it is much more convinient [sic] to take the negative of distance.  Number 1 in the equation stands for the normalizing, hence the similarity is defined as follows\"" - the 1 does not stand for normalizing, that is the way to invert the cosine distance (put differently, cosine distance is 1-cosine similarity, which is a metric in Euclidean space due to the properties of the dot product).  Another example, \""are obtained using the GloVe vector, not using PPMI\"" - there are close relationships between what GloVe learns and PPMI, which the authors seem unaware of (see e.g. the GloVe paper and Omer Levy's work).\u2028\n\n4.  Then there is the additional question, why should we care?  The paper does not really motivate why it is important to score well on these tests: these kinds of tests are often used as ways to measure the quality of word embeddings, but in this case the main contribution is the similarity metric used *on top* of the word embeddings.  In other words, what is supposed to be the take-away, and why should we care? \n\nAs such, I do not recommend it for acceptance - it needs significant work before it can be accepted at a conference. \n\nMinor points:\n- Typo in Eq 10\n- Typo on page 6 (/cite instead of \\cite)",1,1,1,1,-1,1,1,1,1,1
HyHmGyZCZ-R3,"I hate to say that the current version of this paper is not ready, as it is poorly written.  The authors present some observations of the weaknesses of the existing vector space models and list a 6-step approach for refining existing word vectors (GloVe in this work), and test the refined vectors on 80 TOEFL questions and 50 ESL questions.  In addition to the incoherent presentation, the proposed method lacks proper justification.  Given the small size of the datasets, it is also unclear how generalizable the approach is. \n\nPros:\n  1. Experimental study on retrofitting existing word vectors for ESL and TOEFL lexical similarity datasets \n\nCons:\u000b  1. The paper is poorly written and the proposed methods are not well justified. \n  2. Results on tiny datasets\n",1,1,1,1,-1,-1,1,1,-1,-1
HyiAuyb0b-R1,"This paper includes several controlled empirical studies comparing MC and TD methods in predicting of value function with complex DNN function approximators.  Such comparison has been carried out both in theory and practice for simple low dimensional environments with linear (and RKHS) value function approximation showing how TD methods can have much better sample complexity and overall performance compared to pure MC methods.   This paper shows some results to the contrary when applying RL to complex perceptual observation space.  \n\nThe main results include:\n(1) In a rollout update a mix of MC and TD update (i.e. a rollout of > 1 and < horizon) outperforms either extreme.   This is inline with TD-lambda analysis in previous work.  \n(2) Pure MC methods can outperform TD methods when the rewards becomes noisy.  \n(3) TD methods can outperform pure MC methods when the return is mostly dominated by the reward in the terminal state. \n(4) MC methods tend to degrade less when the reward signal is delayed. \n(5) Somewhat surprising: MC methods seems to be on-par with TD methods when the reward is sparse and even longer than the rollout horizon. \n(6) MC methods can outperform TD methods with more complex and high dimensional perceptual inputs. \n\nThe authors conjecture that several of the above observations can be explained by the fact that the training target in MC methods is \""ground truth\"" and do not rely on bootstrapping from the current estimates as is done in a TD rollout.  They suggest that training on such signal can be beneficial when training deep models on complex perceptual input spaces. \n\nThe contributions of the paper are in parts surprising and overall interesting.  I believe there are far more caveats in this analysis than what is suggested in the paper and the authors should avoid over-generalizing the results based on a few domains and the analysis of a small set of algorithms.  Nonetheless I find the results interesting to the RL community and a starting point to further analysis of the MC methods (or adaptations of TD methods) that work better with image observation spaces.  Publishing the code, as the authors mentioned, would certainly help with that.  \n\nNotes:\n- I find the description of the Q_MC method presented in the paper very confusing and had to consult the reference to understand the details.  Adding a couple of equations on this would improve the readability of the paper. \n\n- The first mention of partial observability can be moved to the introduction. \n\n- Adding results for m=3 to table 2 would bring further insight to the comparison. \n\n- The results for the perceptual complexity experiment seem contradictory and inconclusive.  One would expect Q_MC to work well in Grid Map domain if the conjecture put forth by the authors was to hold universally. \n\n- In the study on reward sparsity, although a prediction horizon of 32 is less than the average steps needed to get to a rewarding state, a blind random walk might be enough to take the RL agent to a close-enough neighbourhood from which a greedy MC-based policy has a direct path to the goal.  What is missing from this picture is when a blind walk cannot reach such a state, e.g. when a narrow corridor is present in the environment.  Such a case cannot be resolved by a short horizon MC method.  In other words, a sparse reward setting is only \""difficult\"" if getting into a good neighbourhood requires long term planning and cannot be resolved by a (pseudo) blind random walk. \n\n- The extrapolation of the value function approximator can also contribute to why the limited horizon MC method can see beyond its horizon in a sparse reward setting.  That is, even if there is no way to reach a reward state in 32 steps, an MC value function approximation with horizon 32 can extrapolate from similar looking observed states that have a short path to a rewarding state, enough to be better than a blind random walk.  It would have been nice to experiment with increasing model complexity to study such effect.",1,1,1,1,1,1,1,1,1,1
HyiAuyb0b-R2,"This paper revisits a subject that I have not seen revisited empirically since the 90s: the relative performance of TD and Monte-Carlo style methods under different values for the rollout length.  Furthermore, the paper performs controlled experiments using the VizDoom environment to investigate the effect of a number of other environment characteristics, such as reward sparsity or perceptual complexity.  The most interesting and surprising result is that finite-horizon Monte Carlo performs competitively in most tasks (with the exception of problems where terminal states play a big role (it does not do well at all on Pong!), and simple gridworld-type representations), and outperforms TD approaches in many of the more interesting settings.   There is a really interesting experiment performed that suggests that this is the case due to finite-horizon MC having an easier time with learning perceptual representations.   They also show, as a side result, that the reward decomposition in Dosvitskiy & Koltun (oral presentation at ICLR 2017) is not necessary for learning a good policy in VizDoom.  \n\nOverall, I find the paper important for furthering the understanding of fundamental RL algorithms.   However, my main concern is regarding a confounding factor that may have influenced the results: Q_MC uses a multi-headed model, trained on different horizon lengths, whereas the other models seem to have a single prediction head.   May this helped Q_MC have better perceptual capabilities? \n\nA couple of other questions:\n- I couldn't find any mention of eligibility traces - why",1,1,1,1,1,-1,1,1,1,-1
HyiAuyb0b-R3,"The authors present a testing framework for deep RL methods in which difficulty can be controlled along a number of dimensions, including: reward delay, reward sparsity, episode length with terminating rewards, binary vs real rewards and perceptual complexity.  The authors then experiment with a variety of TD and MC based deep learners to explore which methods are most robust to increases in difficulty along these dimensions.  The key finding is that MC appears to be more robust than TD in a number of ways, and in particular the authors link this to domains with greater perceptual challenges.  \n\nThis is a well motivated and explained paper, in which a research agenda is clearly defined and evaluated carefully with the results reflected on thoughtfully and with intuition.  The authors discover some interesting characteristics of MC based Deep-RL which may influence future work in this area, and dig down a little to uncover the principles a little.  The testing framework will be made public too, which adds to the value of this paper.  I recommend the paper for acceptance and expect it will garner interest from the community. \n\nDetailed comments\n  \u2022 [p4, basic health gathering task] \""The goal is to survive and maintain as much health\nas possible by collecting health kits... The reward is +1 when the agent collects a health kit and 0 otherwise. \"" The reward suggests that the goal is to collect as many health kits as possible, for which surviving and maintaining health are secondary. \n  \u2022 [p4, Delayed rewards] It might be interesting to have a delay sampled from a distribution with some known mean.  Otherwise, the structure of the environment might support learning even when the reward delay would otherwise not. \n  \u2022 [p4, Sparse rewards] I am not sure it is fair to say that the general difficulty is kept fixed.  Rather, the average achievable reward for an oracle (that knows whether health packs are) is fixed. \n  \u2022 [p6] \""Dosovitskiy & Koltun (2017) have not tested DFP on Atari games. \"" Probably fairer/safer to say: did not report results on Atari games.\n",1,1,1,1,1,-1,1,1,1,1
HyiS6k-CW-R1,"OVERVIEW: The authors present results from several state-of-the-art generative models trained on a facial dataset for learning a general facial identity space.  \n\nSTRENGTHS: The paper in general is well written and easy to ready. I appreciate the idea of the Turing test and qualitative results presented are quite impressive. Also, the use of  diverse state-of-the-art generative models is also a strong point.   \n\nWEAKNESSES: While the strengths mentioned above are obvious I had the impression through the whole paper that a whole part is missing.  My list of concerns are the following: \n\n    The authors state as their first contribution the presentation of a novel dataset.  This is nice  but I see the data is actually already available as the photographic work of an artist.  So what's the authors' contribution?  As I understand from the paper it is just compiling this already available data. \n    The major problem of this paper in my opinion is the total lack of technical details.  In this sense the results cannot by any means by reproduced.  While the authors use a set of very novel generative models  there is absolutely no detail on how do they train them.  We are just shown some very impressive qualitative results which are indeed admirable but without further details I cannot judge them as true or not.  I strongly recommend to the authors, to provide technical details of topologies used, hyper parameters and any other important detail that would help a third party research to reproduce these results.  \n    Also it is really hard to understand how could they obtain such impressive result by doing an unsupervised training on a dataset containing 3353 samples taking into account the high capacity of the models they are using.  \n    In section 2.1 the authors mention that facial landmarks have been detected using a 'pre-trained ensemble-of-regresion-trees detector (Gerbrands, 1981)'.  I know very well the facial alignment literature and I do not understand this reference.  This I do not think is a reference to a facial alignment method bu t rather a set of general purpose linear algebra methods.  \n\nTaking into account this major weaknesses I cannot accept this paper and I do not think it is worth discussing results and applications in Sections 3,4,5 before authors detail, explain and clarify how exactly they have obtained these results.",1,1,1,1,1,-1,1,1,1,1
HyiS6k-CW-R2,"This paper proposes a new space for reasoning about human identity.  It proposes a new dataset based on an artist's work, and compares existing methods in terms of the realism of the synthetic faces they can create.  \n\nPros:\n+ The results are very pleasing visually. \n+ The authors show that one of the existing methods can fairly successfully fool humans to believe its synthetic results are actual human faces. \n\nCons:\n- There is no new methodology proposed. \n- If the main contribution is the dataset, perhaps the claim that it is \""uniquely diverse\"" could be justified with some quantitative arguments / statistics, comparing to other datasets.  \n- Since there are existing methods to generate images from a textual description (e.g. Zhang ICCV 2017, \""StackGAN\""), Fig. 10 merits a comparison to those. \n- It would have been convincing to see an experiment showing actual use of the proposed method for navigating the face space, e.g. for finding criminals based on a description. \n\nQuestions:\n- \""Inventing plausible fine details while preserving identity\"" -- since identity is created and there is no ground truth, where does the line between \""fine detail\"" and \""new identity\"" lie? \n- Some notation is not defined in the equation on the last page.",1,1,1,1,1,1,1,1,1,-1
HyiS6k-CW-R3,"This paper investigates identity space learning with well-controlled variations using an artistic portraits dataset.  Especially, the authors propose a visual Turing test to evaluate the synthesize quality of three generative models: WGAN-GP, DFC-VAE, and Pixel VAE. \n\nThe submission has following PROS:\n\n+ The proposed visual Turing test provides a novel solution to evaluate the generation quality.  The test not only distinguishes real from synthesized faces but also evaluates the observer ability by determining whether the observer is a human.  This is a merit compared with existing protocols used in generation evaluation.  \n\n+ The generated face images are very impressive, especially the improved 512x512-pixel outputs. \n\n+ The paper presents a promising application in police composite sketching, which can significantly improve human-in-the-loop search in face modeling.  \n\nHowever, the submission also suffers from multiple CONS:\n\n- The novelty of this paper is limited.  The only novelty I can pinpoint is the proposed visual Turing test.  The dataset, as well as all investigated models/approaches, are existing work.  The visual Turing test is interesting but not concrete enough to support an ICLR publication. \n\n- A very small dataset (3,300 subjects and 3,353 images) is used in the whole investigation.  It is doubtful that the conclusion or results obtained in this small dataset could be scaled up to real-world applications or datasets (millions of subjects and images).  It would be favorable to empirically prove this by designing additional experiments. \n\n- Missing details.  \n(a)In section 4, how to use formal method (Ledig et al., 2016) to enlarge the portrait from 64x64 to 512x512 is unclear. \n(b) Lacking details of the model setups and training strategies.  The generation models are usually highly sensitive to details settings.  The readers can hardly reproduce the results or evaluate possible performance by reading the paper.  \n(c) If the paper length is limited, a supplementary material about those details would be preferred. \n\n- Typos.\n(a) Page 7, \""Figure 8 shows the seeds and example images for 10 rounds...\""  \n----> Figure 8 should be Figure 10\n(b) Page 4, \""yet is is unclear how many pixels are required...\""\n----> yet is is",1,1,1,1,1,1,1,1,1,1
HyKZyYlRZ-R1,"The paper presents a multi-task, multi-domain model based on deep neural networks.  The proposed model is able to take inputs from various domains (image, text, speech) and solves multiple tasks, such as image captioning, machine translation or speech recognition.  The proposed model is composed of several features learning blocks (one for each input type) and of an encoder and an auto-regressive decoder, which are domain-agnostic.  The model is evaluated on 8 different tasks and is compared with a model trained separately on each task, showing improvements on each task. \n\nThe paper is well written and easy to follow. \n\nThe contributions of the paper are novel and significant.  The approach of having one model able to perform well on completely different tasks and type of input is very interesting and inspiring.  The experiments clearly show the viability of the approach and give interesting insights.  This is surely an important step towards more general deep learning models.  \n\nComments:\n\n* In the introduction where the 8 databases are presented, the tasks should also be explained clearly, as several domains are involved and the reader might not be familiar with the task linked to each database.  Moreover, some databases could be used for different tasks, such as WSJ or ImageNet. \n\n* The training procedure of the model is not explained in the paper.  What is the cost function and what is the strategy to train on multiple tasks ?  The paper should at least outline the strategy. \n\n* The experiments are sufficient to demonstrate the viability of the approach,  but the experimental setup is not clear.  Specifically, there is an issue about the speech recognition part of the experiment.  It is not clear what the task exactly is: continuous speech recognition, isolated word recognition ?  The metrics used in Table 1 are also not clear, they should be explained in the text.  Also, if the task is continuous speech recognition, the WER (word error rate) metric should be used.  Information about the detailed setup is also lacking, specifically which test and development sets are used (the WSJ corpus has several sets). \n\n* Using raw waveforms as audio modality is very interesting,  but this approach is not standard for speech recognition,  some references should be provided, such as:\nP. Golik, Z. Tuske, R. Schluter, H. Ney, Convolutional Neural Networks for Acoustic Modeling of Raw Time Signal in LVCSR, in: Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015, pp. 26\u201330. \nD. Palaz, M. Magimai Doss and R. Collobert, (2015, April). Convolutional neural networks-based continuous speech recognition using raw speech signal.  In Acoustics, Speech and Signal Processing (ICASSP), 2015 IEEE International Conference on (pp. 4295-4299). IEEE. \nT. N. Sainath, R. J. Weiss, A. Senior, K. W. Wilson, and O. Vinyals.  Learning the Speech Front-end With Raw Waveform CLDNNs.  Proceedings of the Annual Conference of the International Speech Communication Association (INTERSPEECH), 2015. \n\nRevised Review:\nThe main idea of the paper is very interesting and the work presented is impressive.  However, I tend to agree with Reviewer2, as a more comprehensive analysis should be presented to show that the network is not simply multiplexing tasks.  The experiments are interesting,  except for the WSJ speech task, which is almost meaningless.  Indeed, it is not clear what the network has learned given the metrics presented, as the WER on WSJ should be around 5% for speech recognition. \nI thus suggest to either drop the speech experiment, or the modify the network to do continuous speech recognition.  A simpler speech task such as Keyword Spotting could also be investigated.\n",1,1,1,1,1,-1,1,1,1,-1
HyKZyYlRZ-R2,"The paper describes a neural end-to-end architecture to solve multiple tasks at once.   The architecture consists of an encoder, a mixer, a decoder, and many modality networks to cover different types of input and output pairs for different tasks.   The engineering endeavor is impressive,  but the paper has little scientific value.   Below are a few suggestions to make the paper stronger. \n\nIt is possible that the encoder, mixer, and decoder are just multiplexing tasks based on the input.   One way to analyze whether this happens is to predict the identity of the task from the hidden vectors.   If this is the case, how to prevent it from happening?   If this does not happen, what is being shared across tasks?   This can be analyzed by embedding the inputs from different tasks and looking for inputs from other tasks within a neighborhood in the embedding space. \n\nWhy multitask learning help the model perform better is still unclear.   If the model is able to leverage knowledge learned from one task to perform another task, then we expect to see either faster convergence or good performance with fewer samples.   The authors should analyze if this is the case, and if not, what are we actually benefiting from multitask learning? \n\nIf the modality network is shared across multiple tasks, we expect the learned hidden representation produced by the modality network is more universal.   If that is the case, what information of the input is being retained when training with multiple tasks and what information of the input is being discarded when training with a single task? \n\nReporting per-token accuracies, such as those in Table 2, is problematic.   It's unclear how to compute per-token accuracies for structured prediction tasks, such as speech recognition, parsing, and translation.   Furthermore, based on the results in Table 2, the model clearly fails on the speech recognition task.   The author should also report the standard speech recognition metric, word error rates (WER), for the speech recognition task in Table 1.\n",1,1,1,1,1,-1,1,1,1,-1
HyKZyYlRZ-R3,"The paper presents a multi-task architecture that can perform multiple tasks across multiple different domains.  The authors design an architecture that works on image captioning, image classification, machine translation and parsing.\ n\nThe proposed model can maintain performance of single-task models and in some cases show slight improvements.  This is the main take-away from this paper.  \n\nThere is a factually incorrect statement - depthwise separable convolutions were not introduced in Chollet 2016.  Section 2 of the same paper also notes it (depthwise convolutions can be traced back to at least 2012).",1,1,-1,1,-1,-1,1,-1,1,-1
HylgYB3pZ-R1,"This paper studies the impact of angle bias on learning deep neural networks, where angle bias is defined to be the expected value of the inner product of a random vectors (e.g., an activation vector) and a given vector (e.g., a weight vector).   The angle bias is non-zero as long as the random vector is non-zero in expectation and the given vector is non-zero.   This suggests that the some of the units in a deep neural network have large values (either positive or negative) regardless of the input, which in turn suggests vanishing gradient.   The proposed solution to angle bias is to place a linear constraint such that the sum of the weight becomes zero.   Although this does not rule out angle bias in general, it does so for the very special case where the expected value of the random vector is a vector consisting of a common value.   Nevertheless, numerical experiments suggest that the proposed approach can effectively reduce angle bias and improves the accuracy for training data in the CIFAR-10 task.   Test accuracy is not improved, however. \n\nOverall, this paper introduces an interesting phenomenon that is worth studying to gain insights into how to train deep neural networks, but the results are rather preliminary both on theory and experiments. \n\nOn the theoretical side, the linearly constrained weights are only shown to work for a very special case.   There can be many other approaches to mitigate the impact of angle bias.   For example, how about scaling each variable in a way that the mean becomes zero, instead of scaling it into [-1,+1] as is done in the experiments?   When the mean of input is zero, there is no angle bias in the first layer.   Also, what about if we include the bias term so that b + w a is the preactivation value? \n\nOn the experimental side, it has been shown that linearly constrained weights can mitigate the impact of angle bias on vanishing gradient and can reduce the training error, but the test error is unfortunately increased for the particular task with the particular dataset in the experiments.   It would be desirable to identify specific tasks and datasets for which the proposed approach outperforms baselines.   It is intuitively expected that the proposed approach has some merit in some domains, but it is unclear exactly when and where it is. \n\nMinor comments:\n\nIn Section 2.2, is Layer 1 the input layer or the next?",1,1,1,1,1,-1,1,1,1,-1
HylgYB3pZ-R2,"The authors introduce the concept of angle bias (angle between a weight vector w and input vector x)  by which the resultant pre-activation (wx) is biased if ||x|| is non-zero or ||w|| is non-zero (theorm 2 from the article).  The angle bias results in almost constant activation independent of input sample resulting in no weight updates for error reduction.    Authors chose to add an additional optimization constraint LCW (|w|=0) to achieve zero-mean pre-activation while, as mentioned in the article, other methods like batch normalization BN tend to push for |x|=0 and unit std to do the same.  \n\nClearly, because of lack of scaling factor incase of LCW, like that in BN, it doesnot perform well when used with ReLU.  When using with sigmoid the activation being bouded (0,1) seems to compensate for the lack of scaling in input.  While BN explicitly makes the activation zero-mean LCW seems to achieve it through constraint on the weight features.  Though it is shown to be computationally less expensive LCW seems to work in only specific cases unlike BN.",1,1,1,1,-1,-1,1,-1,1,-1
HylgYB3pZ-R3,"Pros:\nThe paper is easy to read.  Logic flows naturally within the paper.\n\n Cons:\n\n1. Experimental results are neither enough nor convincing.  \n\nOnly one set of data is used throughout the paper: the Cifar10 dataset, and the architecture used is only a 100 layered MLP.  Even though LCW performs better than others in this circumstance,  it does not prove its effectiveness in general or its elimination of the gradient vanishing problem.  For the 100 layer MLP, it's very hard to train a simple MLP and the training/testing accuracy is very low for all the methods.  More experiments with different number of layers and different architecture like ResNet should be tried to show better results.  \n\nIn Figure (7), LCW seems to avoid gradient vanishing but introduces gradient exploding problem. \n\nThe proposed concept is only analyzed in MLP with Sigmoid activation function.  In the experimental parts, the authors claim they use both ReLU and Sigmoid function, but no comparisons are reflected in the figures.  \n\n2. The whole standpoint of the paper is quite vague and not very convincing. \nIn section 2, the authors introduce angle bias and suggest its effect in MLPs that with random weights, showing that different samples may result in similar output in the second and deeper layers.  However, the connection between angle bias and the issue of gradient vanishing lacks a clear analytical connection.  The whole analysis of the connection is built solely on this one sentence \""At the same time, the output does not change if we adjust the weight vectors in Layer 1\"", which is nowhere verified.  \n\nFurther, the phenomenon is only tested on random initialization. When the network is trained for several iterations and becomes more settled, it is not clear how \""angle affect\"" affects gradient vanishing problem. \n\n\nMinors:\n1. Theorem 1,2,3 are direct conclusions from the definitions and are mis-stated as Theorems. . In section 2.3, reasons 1 and 2 state the similar thing that output of MLP has relatively small change with different input data when angle bias occurs.  Only reason 1 mentions the gradient vanishing problem, even though the title of this section is \""Relation to Vanishing Gradient Problem\"". \n",1,1,1,1,1,-1,1,1,1,-1
HymuJz-A--R1,"Quality\n\nThis paper demonstrates that convolutional and relational neural networks fail to solve visual relation problems by training networks on artificially generated visual relation data.  This points at important limitations of current neural network architectures where architectures depend mainly on rote memorization. \n\nClarity\n\nThe rationale in the paper is straightforward.  I do think that breakdown of networks by testing on increasing image variability is expected given that there is no reason that networks should generalize well to parts of input space that were never encountered before. \n\nOriginality\n\nWhile others have pointed out limitations before, this paper considers relational networks for the first time. \n\nSignificance \n\nThis work demonstrates failures of relational networks on relational tasks, which is an important message.  At the same time, no new architectures are presented to address these limitations. \n\nPros\n\nImportant message about network limitations. \n\nCons\n\nStraightforward testing of network performance on specific visual relation tasks.  Conclusions drawn by testing on out of sample data may not be completely valid.",1,1,1,1,-1,1,1,1,1,-1
HymuJz-A--R2,"The authors introduce a set of very simple tasks that are meant to illustrate the challenges of learning visual relations.   They then evaluate several existing network architectures on these tasks, and show that results are not as impressive as others might have assumed they would be.    They show that while recent approaches (e.g. relational networks) can generalize reasonably well on some tasks, these results do not generalize as well to held-out-object scenarios as might have been assumed.  \n\nClarity:  The paper is fairly clearly written.    I think I mostly followed it.    \n\nQuality:  I'm intrigued by but a little uncomfortable with the generalization metrics that the authors use.    The authors estimate the performance of algorithms by how well they generalize to new image scenarios when trained on other image conditions.    The authors state that \"". . . the effectiveness of an architecture to learn visual-relation problems should be measured in terms of generalization over multiple variants of the same problem, not over multiple splits of the same dataset.  \""  Taken literally, this would rule out a lot of modern machine learning, even obviously very good work.   On the other hand, it's clear that at some point, generalization needs to occur in testing ability to understand relationships.    I'm a little worried that it's \""in the eye of the beholder\"" whether a given generalization should be expected to work or not.   \n\nThere are essentially three scenarios of generalization discussed in the paper:\n        (a) various generalizations of image parameters in the PSVRT dataset\n          (b) various hold-outs of the image parameters in the sort-of-CLEVR dataset\n          (c) from sort-of-CLEVR \""objects\"" to PSVRT bit patterns  \n\nThe result that existing architectures didn't do very well at these generalizations (especially b and c) *may* be important -- or it may not.      Perhaps if CNN+RN were trained on a quite rich real-world training set with a variety of real-world three-D objects beyond those shown in sort-of-CLEVR, it would generalize to most other situations that might be encountered.      After all, when we humans generalize to understanding relationships, exactly what variability is present in our \""training sets\"" as compared to our \""testing\"" situations?     How do the authors know that humans are effectively generalizing rather than just \""interpolating\"" within their (very rich) training set?    It's not totally clear to me that if totally naive humans (who had never seen spatial relationships before) were evaluated on exactly the training/testing scenarios described above, that they would generalize particularly well either.     I don't think it can just be assumed a priori that humans would be super good this form of generalization.    \n\nSo how should authors handle this criticism?    What would be useful would either be some form of positive control.    Either human training data showing very effective generalization (if one could somehow make \""novel\"" relationships unfamiliar to humans), or a different network architecture that was obviously superior in generalization to CNN+RN.   If such were present, I'd rate this paper significantly higher.  \n\nAlso, I can't tell if I really fully believe the results of this paper.    I don't doubt that the authors saw the results they report.    However, I think there's some chance that if the same tasks were in the hands of people who *wanted* CNNs or CNN+RN to work well, the results might have been different.    I can't point to exactly what would have to be different to make things \""work\"", because it's really hard to do that ahead of actually trying to do the work.    However, this suspicion on my part is actually a reason I think it might be *good* for this paper to be published at ICLR.   This will give the people working on (e.g.) CNN+RN somewhat more incentive to try out the current paper's benchmarks and either improve their architecture or show that the the existing one would have totally worked if only tried correctly.   I myself am very curious about what would happen and would love to see this exchange catalyzed.  \n\nOriginality and Significance:  The area of relation extraction seems to me to be very important and probably a bit less intensively worked on that it should be.   However, as the authors here note, there's been some recent work (e.g. Santoro 2017) in the area.    I think that the introduction of baselines  benchmark challenge datasets such as the ones the authors describe here is very useful, and is a somewhat novel contribution.",1,1,1,1,1,1,1,1,1,-1
HymuJz-A--R3,"Strengths:\n\n-\tThere is an interesting analysis on how CNN\u2019s perform better Spatial-Relation problems in contrast to Same-Different problems, and how Spatial-Relation problems are less sensitive to hyper parameters. \n\n-\tThe authors bring a good point on the limitations of the SVRT dataset \u2013 mainly being the difficulty to compare visual relations due to the difference of image structures on the different relational tasks and the use of simple closed curves to characterize the relations, which make it difficult to quantify the effect of image variability on the task.  And propose a challenge that addresses these issues and allows controlling different aspects of image variability. \n\n-\tThe paper shows how state of the art relational networks, performing well on multiple relational tasks, fail to generalize to same-ness relationships, \n\nWeaknesses:\n\n-\tWhile the proposed PSVRT dataset addresses the 2 noted problems in SVRT, using only 2 relations in the study is very limited. \n\n-\tThe paper describes two sets of relationships, but it soon suggests that current approaches actually struggle in Same-Different relationships.  However, they only explore this relationship under identical objects.  It would have been interesting to study more kinds of such relationships, such as equality up to translation or rotation, to understand the limitation of such networks.  Would that allow improving generalization to varying item or image sizes? \n\nComments:\n\n-\tIn page 2, authors suggest that from that G\u00fcl\u00e7ehre, Bengio (2013) that for visual relations \u201cfailure of feed-forward networks [\u2026] reflects a poor choice of hyper parameters.  This seems to contradict the later discussion, where they suggest that probably current architectures cannot handle such visual relationships.  \n\n-\tThe point brought about CNN\u2019s failing to generalize on same-ness relationships on sort-of-CLEVR is interesting, but it would be good to know why PSVRT provides better generalization.  What would happen if shapes different than random squared patterns were used at test time? \n\n-\tAuthors reason about biological inspired approaches, using Attention and Memory, based on existing literature.  While they provide some good references to support this statement it would have been interesting to show whether they actually improve TTA under image parameter variations\n",1,1,1,1,1,-1,1,1,1,-1
HymYLebCb-R1,,1,1,1,1,1,-1,1,1,1,-1
HymYLebCb-R2,"The paper proposed a subgraph image representation and validate it in image classification and transfer learning problems.  The image presentation is a minor extension based on a method of producing permutation-invariant adjacency matrix.  The experimental results supports the claim. \n\nIt is very positive that the figures are very helpful for delivering the information. \n\nThe work seems to be a little bit incremental.  The proposed image representation is mainly based on a previous work of permutation-invariant adjacency matrix.  A novelty of this work seems to be transforming a graph into an image.  \n\nIt will be better if the authors could provide more details in the methodology or framework section. \n\nThe experiments on 9 networks support the claims that the image embedding approaches with their image representation of the subgraph outperform the graph kernel and classical features based methods.  It seem to be promising when using transfer learning.  No caption or figure number is provided. \n\nIt will be better to make the notations easy to understand and avoid any notation in a sentence without explanation nearby. \nFor example:\n\""the test example is correctly classified if and only if its ground truth matches C.\""(P5) \n\""We carry out this exercise 4 times and set n to 8, 16, 32 and 64 respectively. \""(P6)\n\nSome minor issues:\n\""Zhu et al.(2011) discuss heterogeneous transfer learning where in they use...\""(P3)\n\""Each label vector (a tuple of label, label-probability pairs).\"" (incomplete sentence?P5)",1,1,1,1,1,1,1,1,1,-1
HymYLebCb-R3,"This paper views graph classification as image classification, and shows that the CNN model adapted from image net can be effectively adapted to the graph classification.  The idea is interesting and the result looks promising,  but I do not understand the intuition behind the success of analogizing graph with images. \n\nFundamentally, a convolutional filter stands for a operation within a small neighborhood on the image.  However, it is unclear how it means for the graph representation.  Is the neighborhood predefined?  Are the graph nodes pre-ordered?  \n\nI am also curious with the effect of pre-trained model from ImageNet.  Since the graph presentation does not use color channels,  pre-trained model is used different from what it was designed to.  I would imagine the benefit of using ImageNet is just to bring a random, high-dimensional embedding.   In addition, I wonder whether it will help to fine-tune the model on the graph classification data.  Could this submission show some fine-tune experiments?",1,1,1,1,1,-1,1,1,1,-1
HyN-ZvlC--R1,"This paper presents two methods for imposing a margin on discriminative loss functions, one which uses the margin between the reference transcription and alternatively hypothesized transcriptions (LMLM), and another which compares all alternative candidates and uses a margin between those with a better system objective (WER or bleu) and those with a worse system objective (rank-LMLM).   Some interesting results on the development set show the importance of things like warm starting on large language model training data.   The methods presented here could be of interest to those training language models for use in specific systems, and the paper reads reasonably clearly .\n\nThe principal shortcoming of the paper is that there was essentially no effort to establish that the baseline systems that are being improved through reranking via these methods are decent baselines for such a use, or to really specify these systems in a way that would allow for replication of the results being presented in the paper.   Sufficient specification of the exact training data and procedure is standard in papers that purport to establish methods to improve upon such baselines,  yet such information is sorely lacking in this paper.   Further, the speech data sets, Fisher and Wall St. Journal, have what would seem to be very high word error rates versus what should be possible with standard open-source speech recognizers such as Kaldi.    For example, by referencing a page that attempts to establish the state of the art on standard data sets (https://github.com/syhw/wer_are_we), we can find links to papers by Povey et al (http://www.danielpovey.com/files/2016_interspeech_mmi.pdf) and the Deep 2 paper in your citations, which themselves include baselines from other papers that cut the error rate in half versus even your best scoring systems, let alone your baselines.   Similarly, your Bleu score on Vietnamese to English translation is way below what were reported (even by the organizer baseline) for the IWSLP conference where the data became available: https://github.com/magizbox/underthesea/wiki/SOTA-Machine-Translation:-IWSLT-2015\n\nGranted, the competing systems also were outperformed by the organizer baseline for that task at IWSLT 2015, but not by the degree to which your system is.    Again, your best performing system (using your new methods) has performance far below the worst reported competing system.    The cavalier presentation of specific details regarding your baseline systems (which is critical for any sort of replicability) and the uniformly weak performance of these systems relative to widely reported results, leads me to discount the probability that your methods would actually result in improvements on truly solid baselines.   I would have preferred one domain experiment carried out with appropriately rock solid documentation of the ball-park competitive baseline system to these results. \n\nOverall, the method is interesting and the dev set experiments were informative,  but ultimately the experiments were not. \n\nRevision:  Having read the author response for this paper, I am encouraged by the updated baseline for WSJ and the additional explication about the competing Fisher systems.   The additional information about the systems included in section 4.4 is pretty nominal, though, and I worry about the ability of others to replicate these results.   I would have felt better about the results if there were reported results from other papers included here, instead of the authors' attempt to create a baseline from the given data, which may or may not (as we have seen) represent a strong enough baseline from which to draw conclusions.   That is to say that I still have some reservations (though less than I had before).   I am including this additional information in my review, for use by the area chair, but otherwise leaving my assessment as-is.\n",1,1,1,1,-1,1,1,1,1,1
HyN-ZvlC--R2,"The main contribution of this paper are:\n(a) replacing the typical maximum likelihood criterion in neural language model training with a discriminative criterion, \n(b) propose two large margin criterion -- difference in likelihood and difference in rank (WER or BLUE ordered) hypotheses, \n(c) demonstrate performance gains two standard tasks -- an ASR task on Wall Street Journal (small task) and an MT task. \n\nIn addition, they provide examples in Figure (1) and (2) that illustrate the effect of the cost function on training.  Their illustration in Figure 4 is also helpful in seeing the impact of using a warm start with a generative model.\n",1,-1,1,1,-1,-1,1,-1,1,-1
HyN-ZvlC--R3,"A large margin , end to end language model that uses a discriminative objective function is proposed.  The proposed objective imposes a hinge loss on the margin to ensure that the ground truth is at least  some fixed amount larger than the imposter.  A variant on this, which also incorporates the ranks of the imposters sorted by a metric such as edit distance or BLEU metric with respect to the ground truth is also introduced. \n\nThe paper is missing some of the original references to a discriminative LM (DLM) as well as  references to the use of a NN LM directly in decoding (presented in ICASSP and Interspeech conferences over the last 5 years).  For example, H.-K. J. Kuo, E. Fosler-Lussier, H. Jiang, and C.-H. Lee, \u201cDiscriminative training of language models for speech recognition,\u201d in Proc. ICASSP,\nvol. 1, 2002, pp. 325\u2013328.  \n\nHave you considered the widely-used NCE method to handle the large vocabulary?  \n\nThe dev perplexity quoted in Section 4 for a 5 gram LM is very high.    Also Table 4 and Table 5 on WSJ and  FIsher  show baseline experiments that are quite far away from the state-of-the-art in these tasks.   Even if you assume that you use the simplest possible acoustic model and/or an open source tool kit for the decoder,  these error rates are high (WSJ error rates are lower than 10%, not 16.7%).   Even if the LM is trained on the common-crawl corpus, it has  a very low OOV rate, and fine tuning on the tasks only lowers it b t 1%.    For reference,  please see papers from Saon et al., Seide et al, Povey et al, Yajie Miao et al in various ICASSP, Interspeech and arXiv papers. Comparisons with weak baselines can significantly color the conclusions.   On the Fisher test set, the interpolated LM offers very little over the baseline LM in Table 5.   This is contrary to what is observed in the literature.    There is not much difference between rankLM and LMLM as well to draw a clear conclusion between the two.   Given that this is n-best rescoring, how are the N-best lists generated?    You state that they are extracted from  64 beam candidates, are they unique N-best lists?    Can this method be applied to lattices?  What is the perplexity of all the language models corresponding to  Tables 4 and 5?  This would have been useful to study in itself. \n\nIn the SMT tasks, the baselines reported seem to be far away from results presented in the literature on the IWSLT task (see http://workshop2015.iwslt.org/downloads/IWSLT_2015_EP_3.pdf) \n\nWhile the proposed objective is interesting and meaningful for several conversational applications, as well as sentence modeling, the presented experimental results are not convincing.",1,1,1,1,1,1,1,1,1,1
Hyp-JJJRW-R1,"This paper proposes to train a classifier neural network not just to classifier, but also to reconstruct a representation of its input, in order to factorize the class information from the appearance (or \""style\"" as used in this paper).  This is done by first using unsupervised pretraining and then fine-tuning using a weighted combination of the regular multinomial NLL loss and a reconstruction loss at the last hidden layer.  Experiments on MNIST are provided to analyse what this approach learns. \n\nUnfortunately, I fail to see a significantly valuable contribution from this work.  First, the paper could do a better job at motivating the problem being addressed . Why is it important to separate class from style?  Should it allow better classification performance?  If so, it's never measured in this work.  If that's not the motivation, then what is it? \n\nSecond, all experiments were conducted on the MNIST dataset.  In 2017, most would expect experiments on at least one other, more complex dataset, to trust any claims on a method. \n\nFinally, the results are not particularly impressive.  I don't find the reconstructions demonstrated particularly compelling (they are generally pretty different from the original input).  Also, that the \""style\"" representation contain less (and I'd say slightly less, in Figure 7 b and d, we still see a lot of same class nearest neighbors) is not exactly a surprising result.  And the results of figure 9, showing poor reconstructions when changing the class representation essentially demonstrates that the method isn't able to factorize class and style successfully.  The interpolation results of Figure 11 are also underwhelming, though possibly mostly because the reconstructions are in general not great.  But most importantly, none of these results are measured in a quantitative way: they are all qualitative, and thus subjective. \n\nFor all these reasons, I'm afraid I must recommend this paper be rejected.",1,1,1,1,-1,-1,1,1,1,-1
Hyp-JJJRW-R2,"The paper proposes training an autoencoder such that the middle layer representation consists of the class label of the input and a hidden vector representation called \""style memory\"", which would presumably capture non-class information.  The idea of learning representations that decompose into class-specific and class-agnostic parts, and more generally \""style\"" and \""content\"", is an interesting and long-standing problem.  The results in the paper are mostly qualitative and only on MNIST.  They do not show convincingly that the network managed to learn interesting class-specific and class-agnostic representations.  It's not clear whether the examples shown in figures 7 to 11 are representative of the network's general behavior.  The tSNE visualization in figure 6 seems to indicate that the style memory representation does not capture class information as well as the raw pixels, but doesn't indicate whether that representation is sensible. \n\nThe use of fully connected networks on images may affect the quality of the learned representations, and it may be necessary to use convolutional networks to get interesting results.  It may also be interesting to consider class-specific representations that are more general than just the class label.  For example, see \""Learning a Nonlinear Embedding by Preserving Class Neighbourhood Structure\"" by Salakhutdinov and Hinton, 2007, which learns hidden vector representations for both class-specific and class-agnostic parts. (This paper should be cited.)",1,1,1,1,1,1,1,1,1,-1
Hyp-JJJRW-R3,"The paper proposes combining classification-specific neural networks with auto-encoders.  This is done in a straightforward manner by designating a few nodes in the output layer for classification and few for reconstruction.  The training objective is then changed to minimize the sum of the classification loss (as measured by cross-entropy for instance) and the reconstruction error (as measured by ell-2 error as is done in training auto-encoders).  \n\nThe authors minimize the loss function by greedy layer-wise training as is done in several prior works.  The authors then perform other experiments on the learned representations in the output layer (those corresponding to classification + those corresponding to reconstruction).  For example, the authors plot the nearest-neighbors for classification-features and for reconstruction-features and observe that the two are very different.  The authors also observe that interpolating between two reconstruction-feature vectors (by convex combinations) seems to interpolate well between the two corresponding images. \n\nWhile the experimental results are interesting  they are not striking especially when viewed in the context of the tremendous amount of work on auto-encoders.  Training the classification-features along with reconstruction-features does not seem to give any significantly new insights.",1,1,1,1,-1,1,1,1,1,-1
Hyp3i2xRb-R1,"The paper investigates the iterative estimation view on gated recurrent networks (GNN).  Authors observe that the average estimation error between a given hidden state and the last hidden state  gradually decreases toward zeros.  This suggest that GNN are bias toward an identity mapping and learn to preserve the activation through time. \nGiven this observation, authors then propose RIN, a new RNN parametrization where the hidden to hidden matrix is decomposed as a learnable weight matrix plus the identity matrix. \nAuthors evaluate their RIN on the adding, sequential MNIST and the baby tasks and show that their IRNN outperforms the IRNN and LSTM models. \n\nQuestions:\n- Section 2 suggests that use of the gate  in GNNs encourages to learn an identity mapping.  Does the average iteration error behaves differently in case of a tanh-RNN ?  \n- It seems from Figure 4 (a) that the average estimation error is higher for RIN than IRNN and LSTM and only decrease toward zero at the very end.  What could explain this phenomenon?  \n- While the LSTM baseline matches the results of Le et al., later work such as Recurrent Batch Normalization or Unitary Evolution RNN have demonstrated much better performance with a vanilla LSTM on those tasks (outperforming both IRNN and RIN).  What could explain this difference in the performances?  \n- Unless I am mistaken, Gated Orthogonal Recurrent Units: On Learning to Forget from Jing et al. also reports better performances for the LSTM (and GRU) baselines that outperform RIN on the baby tasks with mean performances of 58.2 and 56.0 for GRU and LSTM respectively? \n\n- Quality/Clarity:\nThe paper is well written and pleasant to read\ n\n- Originality:\nLooking at RNN from an iterative refinement point of view seems novel. \n\n- Significance:\nWhile looking at RNN from an iterative estimation is interesting,  the experimental part does not really show what are the advantages of the propose RIN.  In particular, the LSTM baseline seems to weak compared to other works.",1,1,1,1,-1,1,1,1,1,-1
Hyp3i2xRb-R2,"Here are my main critics of the papers:\n\n1. Equation (1), (2), (3) are those expectations w.r.t. the data distribution (otherwise I can't think of any other stochasticity)?  If so your phrase \""is zero given a sequence of inputs X1, ...,T\"" is misleading.  \n2. Lack of motivation for IE or UIE.  Where is your background material?  I do not understand why we would like to assume (1), (2), (3).  Why the same intuition of UIE can be applied to RNNs?  \n3. The paper proposed the new architecture RIN, but it is not much different than a simple RNN with identity initialization.  Not much novelty. \n4. The experimental results are not convincing.  It's not compared against any previous published results.  E.g. the addition tasks and sMNIST tasks are not as good as those reported in [1].  Also it only has been tested on very simple datasets. \n\n\n[1] Path-Normalized Optimization of Recurrent Neural Networks with ReLU Activations.  Behnam Neyshabur, Yuhuai Wu, Ruslan Salakhutdinov, Nathan Srebro.",1,1,1,1,-1,1,1,1,1,-1
Hyp3i2xRb-R3,"Summary: \nThe authors present a simple variation of vanilla recurrent neural networks, which use ReLU hiddens and a fixed identity matrix that is added to the hidden-to-hidden weight matrix.  This identity connection acts as a \u201csurrogate memory\u201d component, preserving hidden activations over time steps.  \nThe experiments demonstrate that this architecture reliably solves the addition task for up to 400 input frames.  It also achieves a very good performance on sequential and permuted MNIST and achieves SOTA performance on bAbI. \nThe authors observe that the proposed recurrent identity network (RIN) is relatively robust to hyperparameter choices.  After Le et al. (2015), the paper presents another convincing case for the application of ReLUs in RNNs. \n\nReview: \nI very much like the paper.  The motivation and architecture is presented very clearly and I am happy to also see explorations of simpler recurrent architectures in parallel to research of gated architectures! \nI have a few comments and questions:\n1) Clarification: In Section 2.2, do you really mean bit-wise multiplication or element-wise?  If bit-wise, can you elaborate why?  I might have missed something. \n2) Why does the learning curve of the IRNN stop around epoch 270 in Figure 2c?  Also some curves in the appendix stop abruptly without visible explosions.  Were these experiments run until completion?  If so, would it be possible to plot the complete curves? \n3) I think for a fair comparison with LSTMs and IRNNs a limited hyperparameter search should be performed separately on all three architectures at least for the addition task.  Optimal hyperparameters are usually model-specific.  Admittedly, the authors mention that they do not intend to make claims about superior performance to LSTMs, however the competitive performance of small RINs is mentioned a couple of times in the manuscript. \nLe et al. (2015) for instance perform a coarse grid search for each model. \n5) I think what enables the training of very deep networks or LSTMs on long sequences is the presence of a (close-to-)identity component in forward/backward propagation, not the gating.  The use of ReLU activations in IRNNs (with identity initialization of the hidden-to-hidden weights) and RINs (effectively initialized with identity plus some noise) makes the recurrence more linear than with squashing activation functions. \n6) Regarding the absence of gating in RINs: What is your intuition on how the model would perform in tasks for which conditional forgetting is useful.  Consider for example a task with long sequences, outputs at every time step and hidden activations not necessarily being encouraged to estimate last step hidden activations.  Would RINs readily learn to reset parts of the hidden state? \n7) Henaff et al. (2016) might be related, as they are also looking into the addition task with long sequences.\n\nOverall, the presented idea is novel to the best of my knowledge and the manuscript is well-written. I would recommend it for acceptance, but would like to see the above points addressed (especially 1-3 and some comments on 4-6).  After a revision I would consider to increase the score. \n\nReferences:\nHenaff, Mikael, Arthur Szlam, and Yann LeCun.  \""Recurrent orthogonal networks and long-memory tasks. \"" In International Conference on Machine Learning, pp. 2034-2042. 2016. \nLe, Quoc V., Navdeep Jaitly, and Geoffrey E. Hinton.  \""A simple way to initialize recurrent networks of rectified linear units.\"" arXiv preprint arXiv:1504.00941 (2015).",1,1,1,1,1,1,1,1,1,-1
HyPpD0g0Z-R1,"The paper discusses ways to guard against adversarial domain shifts with so-called counterfactual regularization.  The main idea is that in several datasets there are many instances of images for the same object/person, and that taking this into account by learning a classifier that is invariant to the superficial changes (or \u201cstyle\u201d features, e.g. hair color, lighting, rotation etc.) can improve the robustness and prediction accuracy.  The authors show the benefit of this approach, as opposed to the naive way of just using all images without any grouping, in several toy experimental settings. \n\nAlthough I really wanted to like the paper, I have several concerns, First and most importantly, the paper is not citing several important related work.  Especially, I have the impression that the paper is focusing on a very similar setting (causally) to the one considered in  [Gong et al. 2016] (http://proceedings.mlr.press/v48/gong16.html), as can be seen from Fig. 1. Although not focusing on classification directly, this paper also tries to a function T(X) such that P(Y|T(X)) is invariant to domain change.  Moreover, in that paper, the authors assume that even the distribution of the class can be changed in the different domains (or interventions in this paper).33.07 \nBesides, there are also other less related papers, e.g. http://proceedings.mlr.press/v28/zhang13d.pdf, https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/view/10052/0, https://arxiv.org/abs/1707.09724, (or potentially https://arxiv.org/abs/1507.05333 and https://arxiv.org/abs/1707.06422), that I think may be mentioned for a more complete picture.  Since there is some related work, it may be also worth to compare with it, or use the same datasets. \n\nI\u2019m also not very happy with the term \u201ccounterfactual\u201d. As the authors mention in footnote, this is not the correct use of the term, since counterfactual means \u201cagainst the fact\u201d.  For example, a counterfactual query is \u201cwe gave the patient a drug and the patient died, what would have happened if we didn\u2019t give the drug? \u201d In this case, these are just different interventions on possibly the same object.  I\u2019m not sure that in the practical applications one can assure that the noise variables stay the same, which, as the authors correctly mention, would make it a bit closer to counterfactuals.  It may sound pedantic, but I don\u2019t understand why use the wrong and confusing terminology for no specific reason, also because in practice the paper reduces to the simple idea of finding a classifier that doesn\u2019t vary too much in the different images of the single object. \n\n**EDIT**: I was satisfied with the clarifications from the authors and I appreciated the changes that they did with respect to the related work and terminology, so I changed my evaluation from a 5 (marginally below threshold) to a 7 (good paper, accept).",1,1,1,1,1,1,1,1,1,1
HyPpD0g0Z-R2,"Proposal is to restrict the feasible parameters to ones that have produce a function with small variance over pre-defined groups of images that should be classified the same.  As authors note, this constraint can be converted into a KKT style penalty with KKT multiplier lambda.    Thus this is very  similar to other regularizers that increase smoothness of the function, such as total variation or a graph Laplacian defined with graph edges connecting the examples in each group, as well as manifold regularization (see e.g. Belkin, Niyogi et al. JMLR).    Heck, in practie ridge regularization will also do something similar for many function classes.   \n\nExperiments didn't compare to any similar smoothness regularization (and my preferred would have been a comparison to graph Laplacian or total variation on graphs formed by the same clustered examples).   It's also not clear either how important it is that they hand-define the groups over which to minimize variance or if just generally adding smoothness regularization would have achieved the same results.     That made it hard to get excited about the results in a vacuum.    \n\nWould this proposed strategy have thwarted the Russian tank legend problem?    Would it have fixed the Google gorilla problem?Why or why not?  \n\nOverall, I found the writing a bit bombastic for a strategy that seems to require the user to hand-define groups/clusters of examples.   \n\nPage 2: calling additional instances of the same person \u201ccounterfactual observations\u201d didn\u2019t seem consistent with the usual definition of that term\u2026 maybe I am just missing the semantic link here, but this isn't how we usually use the term counterfactual in my corner of the field.  \n\nRe: \u201cone creates additional samples by modifying\u2026\u201d be nice to quote more of the early work doing this, I believe the first work of this sort was Scholkopf\u2019s, he called it \u201cvirtual examples\u201d and I\u2019m pretty sure he specifically did it for rotation MNIST images (and if not exactly that, it was implied).    I think the right citation is \u201cIncorporating invariances in support vector learning machines\n\u201c Scholkopf, Burges, Vapnik 1996, but also see Decoste * Scholkopf 2002 \u201cTraining invariant support vector machines.\u201d """,1,1,1,1,1,1,1,1,1,1
HyPpD0g0Z-R3,"This paper aims at robust image classification against adversarial domain shifts.  In the used model, there are two types of latent features, \""core\"" features and \""style\"" features, and the goal is to achieved by avoiding using the changing style features.  The proposed method, which makes use of grouping information, seems reasonable and useful.  \n\nIt is nice that the authors use \""counterfactual regularization\"".  But I failed to see a clear, new contribution of using this causal regularization, compared to some of the previous methods to achieve invariance (e.g., relative to translation or rotation).  For examples of such methods, one may see the paper \""Transform Invariant Auto-encoder\"" (by Matsuo et al.) and references therein. \n\nThe data-generating process for the considered model, given in Figure 2, seems to be consistent with Figure 1 of the paper \""Domain Adaptation with Conditional Transferable Components\"" (by Gong et al.). Perhaps the authors can draw the connection between their work and Gong et al.'s work and the related work discussed in that paper. \n\nBelow are some more detailed comments, In Introduction, it would be nice if the authors made it clear that \""Their high predictive accuracy might suggest that the extracted latent features and learned representations resemble the characteristics our human cognition uses for the task at hand. \"" Why do the features human cognition uses give an optimal predictive accuracy?  On page 2, the authors claimed that \""These are arguably one reason why deep learning requires large sample sizes as large sample size is clearly not per se a guarantee that the confounding effect will become weaker. \"" Could the authors give more detail on this? A reference would be appreciated.",1,1,1,1,1,1,1,1,1,-1
HyrCWeWCb-R1,"Clarity \nThe paper is well-written and clear.  \n\nOriginality\nThe paper proposes a path consistency learning method with a new combination of entropy regularization and relative entropy.  The paper leverages a novel method in determining the coefficient of relative entropy.   \n\nSignificance\n- Trust-PCL achieves overall competitive with state-of-the-art external implementations. \n- Trust-PCL (off-policy) significantly outperform TRPO in terms of data efficiency and final performance.   \n- Even though the paper claims Trust-PCL (on-policy) is close to TRPO, the initial performance of TRPO looks better in HalfCheetah, Hopper, Walker2d and Ant.   \n- Some ablation studies (e.g., on entropy regularization and relative entropy) and sensitivity analysis on parameters (e.g. \\alpha and update frequency on \\phi) would be helpful.  \n\nPros:\n- The paper is well-written and clear.   \n- Competitive with state-of-the-art external implementations  \n- Significant empirical advantage over TRPO.  \n-  Open source codes.  \n\nCons:\n- No ablation studies. \n",1,1,1,1,1,-1,1,1,1,1
HyrCWeWCb-R2,"This paper presents a policy gradient method that employs entropy regularization and entropy constraint at the same time.  The entropy regularization on action probability is to encourage the exploration of the policy, while the entropy constraint is to stabilize the gradient. \n\nThe major weakness of this paper is the unclear presentation.  For example, the algorithm is never fully described, though a handful variants are discussed.   How the off-policy version is implemented is missing.  \n\nIn experiments, why the off-policy version of TRPO is not compared.  Comparing the on-policy results, PCL does not show a significant advantage over TRPO.  Moreover, the curves of TRPO is so unstable, which is a bit uncommon.  \n\nWhat is the exploration strategy in the experiments?  I guess it was softmax probability.  However, in many cases, softmax does not perform a good exploration, even if the entropy regularization is added. \n\nAnother issue is the discussion of the entropy regularization in the objective function.  This regularization, while helping exploration, do changes the original objective.  When a policy is required to pass through a very narrow tunnel of states, the regularization that forces a wide action distribution could not have a good performance.  Thus it would be more interesting to see experiments on more complex benchmark problems like humanoids.",1,1,1,1,1,-1,1,1,1,-1
HyrCWeWCb-R3,"The paper extends softmax consistency by adding in a relative entropy term to the entropy regularization and applying trust region policy optimization instead of gradient descent.   I am not an expert in this area.  It is hard to judge the significance of this extension. \n\nThe paper largely follows the work of Nachum et al 2017.  The differences (i.e., the claimed novelty) from that work are the relative entropy and trust region method for training.  However, the relative entropy term added seems like a marginal modification",1,-1,1,1,-1,1,1,1,1,-1
HyRnez-RW-R1,"The authors present a scalable model for questioning answering that is able to train on long documents.  On the TriviaQA dataset, the proposed model achieves state of the art results on both domains (wikipedia and web).  The formulation of the model is straight-forward,  however I am skeptical about whether the results prove the premise of the paper (e.g. multi-mention reasoning is necessary).  Furthermore, I am slightly unconvinced about the authors' claim of efficiency.  Nevertheless, I think this work is important given its performance on the task. \n\n1. Why is this model successful?  Multi-mention reasoning or more document context? \nI am not convinced of the necessity of multi-mention reasoning, which the authors use as motivation, as shown in the examples in the paper.  For example, in Figure 1, the answer is solely obtained using the second last passage.  The other mentions provide signal, but does not provide conclusive evidence.  Perhaps I am mistaken, but it seems to me that the proposed model cannot seem to handle negation, can the authors confirm/deny this?  I am also skeptical about the computation efficiency of a model that scores all spans in a document (which is O(N^2), where N is the document length).  Can you show some analysis of your model results that confirm/deny this hypothesis? \n\n2. Why is the computational complexity not a function of the number of spans? \nIt seems like the derivations presents several equations that score a given span.  Perhaps I am mistaken, but there seems to be n^2 spans in the document that one has to score.  Shouldn't the computational complexity then be at least O(n^2), which makes it actually much slower than, say, SQuAD models that do greedy decoding O(2n + nm)? \n\nSome minor notes\n- 3.3.1 seems like an attention computation in which the attention context over the question and span is computed using the question.  Explicitly mentioning this may help the reading grasp the formulation. \n- Same for 3.4, which seems like the biattention (Seo 2017) or coattention (Xiong 2017) from previous squad work. \n- The sentence \""We define ... to be the embeddings of the l words of the sentence that contains s.\"" is not very clear.  Do you mean that the sentence contains l words?  It could be interpreted that the span has l words. \n- There is a typo in your 3.7 \""level 1 complexity\"": there is an extra O inside the big O notation.",1,1,1,1,1,-1,1,1,1,-1
HyRnez-RW-R2,"This paper proposes a method that scales reading comprehension QA to large quantities of text with much less document truncation than competing approaches.  The model also does not consider the first mention of the answer span as gold, instead formulating its loss function to incorporate multiple mentions of the answer within the evidence.  The reported results were state-of-the-art(*) on the TriviaQA dataset at the time of the submission deadline.  It's interesting that such a simple model, relying mainly on (weighted) word embedding averages, can outperform more complex architectures; however, these improvements are likely due to decreased truncation as opposed to bag-of-words architectures being superior to RNNs.  \n\nOverall, I found the paper interesting to read, and scaling QA up to larger documents is definitely an important research direction.  On the other hand, I'm not quite convinced by its experimental results (more below) and the paper is lacking an analysis of what the different sub-models are learning.  As such, I am borderline on its acceptance. \n\n* The TriviaQA leaderboard shows a submission from 9/24/17 (by \""chrisc\"") that has significantly higher EM/F1 scores than the proposed model.  Why is this result not compared to in Table 1?  \n\nDetailed comments:\n- Did you consider pruning spans as in the end-to-end coreference paper of Lee et al., EMNLP 2017?  This may allow you to avoid truncation altogether.  Perhaps this pruning could occur at level 1, making subsequent levels would be much more efficient. \n- How long do you estimate training would take if instead of bag-of-words, level 1 used a biLSTM encoder for spans / questions? \n- What is the average number of sentences per document?  It's hard to get an idea of how reasonable the chosen truncation thresholds are without this. \n- In Figure 3, it looks like the exact match score is still increasing as the maximum tokens in document is increased.  Did the authors try truncating after more words (e.g., 10k)? \n- I would have liked to see some examples of questions that are answered correctly by level 3 but not by level 2 or 1, for example, to give some intuition as to how each level works. \n- \""Krasner\"" misspelled multiple times as \""Kramer\""",1,1,1,1,1,-1,1,1,1,-1
HyRnez-RW-R3,"This paper proposes a lightweight neural network architecture for reading comprehension, which 1) only consists of feed-forward nets; 2) aggregates information from different occurrences of candidate answers, and demonstrates good performance on TriviaQA (where documents are generally pretty long). \n\nOverall, I think it is a nice demonstration that non-recurrent models can work so well,  but I also don\u2019t find the results strikingly surprising.  It is also a bit hard to get the main takeaway messages.  It seems that multi-loss is important (highlight that!), summing up multiple mentions of the same candidate answers seems to be important (This paper should be cited: Text Understanding with the Attention Sum Reader Network https://arxiv.org/abs/1603.01547).  But all the other components seem to have been demonstrated previously in other papers.  \n\nAn important feature of this model is it is easier to parallelize and speed up the training/testing processes.  However, I don\u2019t see any demonstration of this in the experiments section. \n\nAlso, I am a bit disappointed by how \u201ccascades\u201d are actually implemented.  I was expecting some sophisticated ways of combining information in a cascaded way (finding the most relevant piece of information, and then based on what it is obtained so far trying to find the next piece of relevant information and so on).  The proposed model just simply sums up all the occurrences of candidate answers throughout the full document.  3-layer cascade is really just more like stacking several layers where each layer captures information of different granularity.  \n\nI am wondering if the authors can also add results on other RC datasets (e.g., SQuAD) and see if the model can generalize or not. \n",1,1,1,1,1,1,1,1,1,-1
HyRVBzap--R1,"The authors proposed to supplement adversarial training with an additional regularization that forces the embeddings of clean and adversarial inputs to be similar.  The authors demonstrate on MNIST and CIFAR that the added regularization leads to more robustness to various kinds of attacks.  The authors further propose to enhance the network with cascaded adversarial training, that is, learning against iteratively generated adversarial inputs, and showed improved performance against harder attacks.  \n\nThe idea proposed is fairly straight-forward.  Despite being a simple approach, the experimental results are quite promising.   The analysis on the gradient correlation coefficient and label leaking phenomenon provide some interesting insights.   \n\nAs pointed out in section 4.2, increasing the regularization coefficient leads to degenerated embeddings.  Have the authors consider distance metrics that are less sensitive to the magnitude of the embeddings, for example, normalizing the inputs before sending it to the bidirectional or pivot loss, or use cosine distance etc.? \n\nTable 4 and 5 seem to suggest that cascaded adversarial learning have more negative impact on test set with one-step attacks than clean test set, which is a bit counter-intuitive.  Do the authors have any insight on this? \n2. Arrow in Figure 3 are not quite readable;\ n3. The paper is over 11 pages.  The authors might want to consider shrink it down the recommended length.",1,1,1,1,1,-1,1,1,1,-1
HyRVBzap--R2,"This paper improves adversarial training by adding to its traditional objective a regularization term forcing a clean example and its adversarial version to be close in the embedding space.  This is an interesting idea which, from a robustness point of view (Xu et al, 2013) makes sense.  Note that a similar strategy has been used in the recent past under the name of stability training.  The proposed method works well on CIFAR and MNIST datasets.  My main concerns are:\n\n\t- The adversarial objective and the stability objective are potentially conflicting.  Indeed when the network misclassifies an example, its adversarial version is forced to be close to it in embedding space while the adversarial term promotes a different prediction from the clean version (that of the ground truth label).  Have the authors considered this issue?  Can they elaborate more on how they with this? \n\n\t- It may be significantly more difficult to make this work in such setting due to the dimensionality of the data.  Did the authors try such experiment?  It would be interesting to see these results.  \n\nLastly, The insights regarding label leaking are not compelling.   Label leaking is not a mysterious phenomenon.  An adversarially trained model learns on two different distributions.  Given the fixed size of the hypothesis space explored (i.e., same architecture used for vanilla and adversarial training), It is natural that the statistics of the simpler distribution are captured better by the model.  Overall, the paper contains valuable information and a method that can contribute to the quest of more robust models.  I lean on accept side.  \n\n\n",1,1,1,1,1,1,1,1,1,-1
HyTrSegCb-R1,"The paper is a pain to read.  Most of the citation styles are off (i.e., without parentheses).  Most of the sentences are not grammatically correct.  Most, if not all, of the determiners are missing.  It is ironic that the paper is proposing a model to generate grammatically correct sentences, while most of the sentences in the paper are not grammatically correct. \n\nThe experimental numbers look skeptical.  For example, 1/3 of the training results are worse than the test results in Table 1.  It also happens a few times in Table 5.  Either the models are not properly trained, or the models are heavily tuned on the test set. \n\nThe running times in Table 9 are also skeptical.  Why are the Concorde models faster than unigrams and bigrams?  Maybe this can be attributed to the difference in the size of the vocabulary, but why is the unigram model slower than the bigram model?",1,1,1,1,-1,-1,1,1,-1,1
HyTrSegCb-R2,"In this work, the authors propose a sequence-to-sequence architecture that learns a mapping from a normalized sentence to a grammatically correct sentence.  The proposed technique is a simple modification to the standard encoder-decoder paradigm which makes it more efficient and better suited to this task.  The authors evaluate their technique using three morphologically rich languages French, Polish and Russian and obtain promising results. \n\nThe morphological agreement task would be an interesting contribution of the paper, with wider potential.  But one concern that I have is regarding the evaluation metrics used for it.  Firstly, word accuracy rate doesn't seem appropriate, as it does not measure morphological agreement.  Secondly, sentence accuracy (w.r.t. the sentences from which the normalized sentences are derived) is not indicative of morphological agreement: even \""wrong\"" sentences in the output could be perfectly valid in terms of agreement.  A grammatical error rate (fraction of grammatically wrong sentences produced) would probably be a better measure. \n\nAnother concern I have is regarding the quality of the baseline: Additional variants of the baseline models should be considered and the best one reported.  Specifically, in the conversation task, have the authors considered switching the order of normalized answer and context in the input?  Also, the word order of the normalized answer and/or context could be reversed (as is done in sequence-to-sequence translation models). \n\nAlso, many experimental details are missing from the draft: \n-- What are the sizes of the train/test sets derived from the OpenSubtitles database? \n-- Details of the validation sets used to tune the models. \n-- In Section 5.4, no details of the question-answer corpus are provided.  How many pairs were extracted?  How many were used for training and testing? \n-- In Section 5.4.1, how many assessors participated in the evaluation and how many questions were evaluated? \n-- In some of the tables (e.g. 6, 7, 8) which show example sentences from Polish, Russian and French, please provide some more information in the accompanying text on how to interpret these examples (since most readers may not be familiar with these languages). \n\nPros:\n-- Efficient model \n-- Proposed architecture is general enough to be useful for other sequence-to-sequence problems \n\nCons:\n-- Evaluation metrics for the morphological agreement task are unsatisfactory \n-- It would appear that the baselines could be improved further using standard techniques",1,1,1,1,1,-1,1,1,1,-1
HyTrSegCb-R3,"The key contributions of this paper are:\n(a) proposes to reduce the vocabulary size in large sequence to sequence mapping tasks (e.g., translation) by first mapping them into a \""standard\"" form and then into their correct morphological form, \n(b) they achieve this by clever use of character LSTM encoder / decoder that sandwiches a bidirectional LSTM which captures context, \n(c) they demonstrate clear and substantial performance gains on the OpenSubtitle task,  and\n(d) they demonstrate clear and substantial performance gains on a dialog question answer task.  \n\nTheir analysis in Section 5.3 shows one clear advantage of this model in the context of long sequences.   \n\nAs an aside, the authors should correct the numbering of their Figures (there is no Figure 3) and provide better captions to the Tables so the results shown can easily understood at a glance.   \n\nThe only drawback of the paper is that this does not advance representation learning per se though a nice application of current models.",1,1,1,1,1,-1,1,1,1,-1
HytSvlWRZ-R1,"This work proposes a multi task learning framework for the modeling of clinical data in neurodegenerative diseases.  \nDifferently from previous applications of machine learning in neurodegeneration modeling, the proposed approach models the clinical data accounting for the bounded nature of cognitive tests scores.  The framework is represented by a feed-forward deep architecture analogous to a residual network.  At each layer a low-rank constraint is enforced on the linear transformation, while the cost function is specified in order to differentially account for the bounds of the predicted variables. \n\nThe idea of explicitly accounting for the boundedness of clinical scores is interesting,  although the assumption of the proposed model is still incorrect: clinical scores are defined on discrete scales.  For this reason the Gaussian assumption for the cost function used in the method is still not appropriate for the proposed application.  \nFurthermore, while being the main methodological drive of this work, the paper does not show evidence about improved predictive performance and generalisation when accounting for the boundedness of the regression targets.  \nThe proposed algorithm is also generally compared with respect to linear methods, and the authors could have provided a more rigorous benchmark including standard non-linear prediction approaches (e.g. random forests, NN, GP, \u2026).  \n\nOverall, the proposed methods seems to provide little added value to the large amount of predictive methods proposed so far for prediction in neurodegenerative disorders.  Moreover, the proposed experimental paradigm appears flawed.  What is the interest of predicting baseline (or 6 months at best) cognitive scores (relatively low-cost and part of any routine clinical assessment) from brain imaging data (high-cost and not routine)? \n\nOther remarks. \n\n- In section 2.2 and 4 there is some confusion between iteration indices and samples indices \u201ci\u201d.  \n\n- Contrarily to what is stated in the introduction, the loss functions proposed in page 3 (first two formulas) only accounts for the lower bound of the predicted variables.   \n\n-  Figure 2, synthetic data.  The scale of the improvement of the subspace difference is quite tiny, in the order of 1e-2 when compared to U, and of 1e-5 across iterations.  The loss function of Figure 2. b also does not show a strong improvement across iterations, while indicating a rather large instability of the optimisation procedure.  These aspects may be a sign of convergence issues.  \n\n- The dimensionality of the subspace representation importantly depends on the choice of the rank R of U and V.  This is a crucial parameters that is however not discussed nor analysed in the paper.  \n\n- The synthetic example of page 7 is quite misleading and potentially biased towards the proposed model.  The authors are generating the synthetic data according to the model, and it is thus not surprising that they managed to obtain the best performance.   In particular, due to the nonlinear nature of (1), all the competing linear models are expected to perform poorly in this kind of setting. \n\n- The computation time for the linear model shown in Table 3 is quite surprising (~20 minutes for linear regression of 5k observations).  Is there anything that I am missing?\n",1,1,1,1,1,1,1,1,1,-1
HytSvlWRZ-R2,"The authors propose a DNN, called subspace network, for nonlinear multi-task censored regression problem.  The topic is important. Experiments on real data show improvements compared to several traditional approaches. \n\nMy major concerns are as follows. \n\n1. The paper is not self-contained.  The authors claim that they establish both asymptotic and non-asymptotic convergence properties for Algorithm 1.  However, for some key steps in the proof, they refer to other references.  If this is due to space limitation in the main text, they may want to provide a complete proof in the appendix. \n\n2. The experiments are unconvincing.  They compare the proposed SN with other traditional approaches on a very small data  set with 670 samples and 138 features.  A major merit of DNN is that it can automatically extract useful features.  However, in this experiment, the features are handcrafted before they are fed into the models.  Thus, I would like to see a comparison between SN with vanilla DNN.",1,1,1,1,1,-1,1,1,-1,-1
HytSvlWRZ-R3,"This paper presents a new multi-task network architecture within which low-rank parameter spaces were found using matrix factorization.  As carefully proved and tested, only one pass of the training data would help recover the parametric subspace, thus network could be easily trained layer-wise and expanded. \n\nSome novel contributions:\n1. Layer by layer feedforward training process, no back-prop. \n2. On-line settings to train parameters ( guaranteed convergence in a single pass of the data) \n\nWeakness :\n1. The assumption that a low-rank parameter space exists among tasks rather than original feature spaces is not new and widely used in literature. \n2. The proof part(Section 2.2) can be extended with more details in Appendix. \n3. In synthetic data experiments (Table1), only small margins could be observed between SN, f-MLP and rf-MLP, and only Layer 1 of SN performs better above all others.  \n4. Typo: In Table2,3,5, Multi-l_{2,1} (denotes the L2,1 norm) were written wrong. \n5. In the synthetic data experiments on comparison with single-task and multi-task models, counter-intuitive results (with larger training data split, ANMSE raises instead of decreases) of multi-task models may need further explanation.  \n6. Extra models like Deep Networks with/without matrix factorization could be added.  ( As proposed model is a deep model, the lack of comparison with deep methods is dubious)  \n7. In Section 4.2, the real dataset is rather small thus the results on this small dataset were not convincing enough. SN model outperforms the state-of-the-art with only small margin.   Extensive experiments could be added.  \n8. The performance on One-Layer Subspace Network (with only the input features) could be added.  \n\nConclusion:\nThough with a quite novel idea on solving multi-task censored regression problem,  the experiments conducted on synthetic data and real data are not convincing enough to ensure the contribution of the Subspace Network. \n",1,1,1,1,1,1,1,1,1,-1
HyUNwulC--R1,This paper focuses on accelerating RNN by applying the method from Blelloch (1990).  The application is straightforward and thus technical novelty of this paper is limited.  But the results are impressive.  \n\nOne concern is the proposed technique is only applied for few types of RNNs which may limit its applications in practice.  Could the authors comment on this potential limitation,1,1,1,1,1,-1,1,1,1,-1
HyUNwulC--R2,"# Summary and Assessment\n\nThe paper addresses an important issue\u2013that of making learning of recurrent networks tractable for sequence lengths well beyond 1\u2019000s of time steps.  A key problem here is that processing such sequences with ordinary RNNs requires a reduce operation, where the output of the net at time step t depends on the outputs of *all* its predecessor.  \nThe authors now make a crucial observation, namely that a certain class of RNNs allows evaluation in a non-linear fashion through a so-called SCAN operator.  Here, if certain conditions are satisfied, the calculation of the output   can be parallelised massively. \nIn the following, the authors explore the landscape of RNNs satisfying the necessary conditions.  The performance is investigated in terms of wall clock time.  Further, experimental results of problems with previously untacked sequence lengths are reported. \n\nThe paper is certainly relevant, as it can pave the way towards the application of recurrent architectures to problems that have extremely long term dependencies. \nTo me, the execution seems sound.  The experiments back up the claim. \n\n## Minor\n- I challenge the claim that thousands and millions of time steps are a common issue in \u201crobotics, remote sensing, control systems, speech recognition, medicine and finance\u201d, as claimed in the first paragraph of the introduction.  IMHO, most problems in these domains get away with a few hundred time steps; nevertheless, I\u2019d appreciate a few examples where this is a case to better justify the method",1,1,1,1,1,-1,1,1,1,-1
HyUNwulC--R3,"This paper abstracts two recently-proposed RNN variants into a family of RNNs called the Linear Surrogate RNNs which satisfy  Blelloch's criteria for parallelizable sequential computation.  The authors then propose an efficient parallel algorithm for this class of RNNs, which produces speedups over the existing implements of Quasi-RNN, SRU, and LSTM.  Apart from efficiency results, the paper also contributes a comparison of model convergence on a long-term dependency task due to (Hochreiter and Schmidhuber, 1997).  A novel linearized version of the LSTM outperforms traditional LSTM on this long-term dependency task, and raises questions about whether RNNs and LSTMs truly need the nonlinear structure. \n\nThe paper is written very well, with explanation (as opposed to obfuscation) as the goal.  Linear Surrogate RNNs is an important concept that is useful to understand RNN variants today, and potentially other future novel architectures. \n\nThe paper provides argument and experimental evidence against the rotation used typically in RNNs.  While this is an interesting insight, and worthy of further discussion, such a claim needs backing up with more large-scale experiments on real datasets. \n\nWhile the experiments on toy tasks is clearly useful, the paper could be significantly improved by adding experiments on real tasks such as language modelling",1,1,1,1,1,-1,1,1,1,-1
HyWrIgW0W-R1,"The paper takes a closer look at the analysis of SGD as variational inference, first proposed by Duvenaud et al. 2016\nand Mandt et al. 2016.  In particular, the authors point out that in general, SGD behaves quite differently from Langevin diffusion due to the multivariate nature of the Gaussian noise.   As the authors show based on the Fokker-Planck equation of the underlying stochastic process, there exists a conservative current (a gradient of an underlying potential) and a non-conservative current (which might induce stationary persistent currents at long times).  The non-conservative part leads to the fact that the dynamics of SGD\tmay show oscillations, and these oscillations may even prevent the algorithm from converging to the 'right' local optima.  The theoretical analysis is carried-out very nicely, and the theory is supported by experiments on two-dimensional toy examples, and Fourier-spectra of the iterates of SGD. \n\nThis is a nice paper which I would like to see accepted.  In particular I appreciate that the authors stress the importance\nof 'non-equilibrium physics' for understanding the SGD process.  Also, the presentation is quite clear and the paper well written. \n\nThere are a few minor points which I would like to ask the authors to address:\n\n1. Why cite Kingma and Welling as a source for variational inference in\tsection 3.1?  VI is a much older\tfield, and Kingma and Welling proposed a very special form of VI, namely amortized VI with inference networks.  A better citation would be Jordan et\tal 1999. \n\n2. I'm not sure how much to trust the Fourier-spectra.  In particular, perhaps the deviations from Brownian motion could also be due to the discrete\tnature of SGD (i.e. that the continuous-time formalism is only an approximation of a discrete process).   Could you elaborate on this?  \n\n3. Could you give the reader more details on how the uncertainty estimates on the Fourier transformations were obtained?\n\nThanks.",1,1,1,1,1,1,1,1,1,1
HyWrIgW0W-R2,"The authors discuss the regularized objective function minimized by standard SGD in the context of neural nets, and provide a variational inference perspective using the Fokker-Planck equation.  They note that the objective can be very different from the desired loss function if the SGD noise matrix is low rank, as evidenced in their experiments. \n\nOverall the paper is written quite well, and the authors do a good job of explaining their thesis.  However I was unable to identify any real novelty in the theory: the Fokker-Planck equation has been widely used in analysis of stochastic noise in MCMC samplers in recent years, and this paper mostly rephrases those results.  Also the fact that SGD theory only works for isotropic noise is well known, and that there is divergence from the true loss function in case of low rank noise is obvious.  Thus I found most of section 3 to be a reformulation of known results, including Theorem 5 and its proof.\n\nSame goes for section 5; the symmetric- anti symmetric split is a common technique used in the stochastic MCMC literature over the last few years, and I did not find any new insight into those manipulations of the Fokker-Planck equation from this paper. \n\nThus I think that although this paper is written well,  the theory is mostly recycled and the empirical results in Section 4 are known; thus it is below acceptance threshold due to lack of novelty.",1,1,1,1,-1,1,1,1,1,-1
HyWrIgW0W-R3,"This paper develop theory to study the impact of stochastic gradient noise for SGD, especially for deep neural network models.  It is shown that when the gradient noise is isotropic normal, SGD converges to a distribution tilted by the original objective function.  However, when the gradient noise is non isotropic normal, which is shown common in many models especially in deep neural network models, the behavior of SGD is intriguing, which will not converge to the tilted distribution by the original objective function, sometimes more interestingly, will converge to limit cycles around some critical points of the original objective function.  The paper also provides some hints on why using SGD can get good generalization ability than gradient descend.\n\nI think the finding of this paper is interesting, and the technical details are correct. I still have the following comments.\n\nFirst, Assumption 4 seems a bit too abstract.  It is not easy to see what the assumption means.  It would be better if an example is given, which is verified to satisfy the assumption. \n\nAnother comment is related to the overall content of this paper.  Thought the paper point out that SGD will have the out-of-equilibrium behavior when the gradient noise is non isotropic normal, it remains to show how far away this stationary distribution is from the original distribution defined by the objective function.",1,1,1,1,1,-1,1,1,1,-1
HyXBcYg0b-R1,"The authors revised the paper according to all reviewers suggestions, I am satisfied with the current version. \n\nSummary: this works proposes to employ recurrent gated convnets to solve graph node labeling problems on arbitrary graphs.  It build upon several previous works, successively introducing convolutional networks, gated edges convnets on graphs, and LSTMs on trees.  The authors extend the tree LSTMs formulation to perform graph labeling on arbitrary graphs, merge convnets with residual connections and edge gating mechanisms.  They apply the 2 proposed models to 3 baselines also based on graph neural networks on two problems: sub-graph matching (expressing the problem of sub-graph matching as a node classification problem), and semi supervised clustering.   \n\nMain comments:\nIt would strengthen the paper to also compare all these network learning based approaches to variational ones.  For instance, to a spectral clustering method for the semi supervised clustering, or\nsolving the combinatorial Dirichlet problem as in Grady: random walks for image segmentation, 2006. \n\nThe abstract and the conclusion should be revised, they are very vague. \n- The abstract should be self contained and should not contain citations. \n- The authors should clarify which problem they are dealing with. \n- instead of the \""numerical result show the performance of the new model\"", give some numerical results here, otherwise, this sentence is useless. \n- we propose ... as propose -> unclear: what do you propose? \n \n\nMinor comments:\n- You should make sentences when using references with the author names format.  Example: ... graph theory, Chung (1997) -> graph theory by Chung (1997) \n- As Eq 2 -> As the minimization of Eq 2 (same with eq 4)\n- Don't start sentences with And, or But\n\n",1,1,1,1,1,1,1,1,1,-1
HyXBcYg0b-R2,"The paper proposes an adaptation of existing Graph ConvNets and evaluates this formulation on a several existing benchmarks of the graph neural network community.  In particular, a tree structured LSTM is taken and modified.   The authors describe this as adapting it to general graphs, stacking, followed by adding edge gates and residuality.  \n\nMy biggest concern is novelty, as the modifications are minor.  In particular, the formulation can be seen in a different way.  As I see it, instead of adapting Tree LSTMs to arbitary graphs, it can be seen as taking the original formulation by Scarselli and replacing the RNN by a gated version, i.e. adding the known LSTM gates (input, output, forget gate).   This is a minor modification.  Adding stacking and residuality are now standard operations in deep learning, and edge-gates have also already been introduced in the literature, as described in the paper. \n\nA second concern is the presentation of the paper, which can be confusing at some points.  A major example is the mathematical description of the methods.  When reading the description as given, one should actually infer that Graph ConvNets and Graph RNNs are the same thing, which can be seen by the fact that equations (1) and (6) are equivalent.  \n\nAnother example, after (2), the important point to raise is the difference to classical (sequential) RNNs, namely the fact that the dependence graph of the model is not a DAG anymore, which introduces cyclic dependencies.  \n\nGenerally, a clear introduction of the problem is also missing.  What are the inputs, what are the outputs, what kind of problems should be solved?   The update equations for the hidden states are given for all models, but how is the output calculated given the hidden states from variable numbers of nodes of an irregular graph?  \n\nThe model has been evaluated on standard datasets with a performance, which seems to be on par, or a slight edge, which could probably be due to the newly introduced residuality.  \n\nA couple of details :\n\n- the length of a graph is not defined.   The size of the set of nodes might be meant. \n\n- at the beginning of section 2.1 I do not understand the reference to word prediction and natural language processing.  RNNs are not restricted to NLP and I think there is no need to introduce an application at this point. \n\n- It is unclear what does the following sentence means: \""ConvNets are more pruned to deep networks than RNNs\""? \n\n- What are \""heterogeneous graph domains\""?\n",1,1,1,1,1,1,1,1,1,-1
HyXBcYg0b-R3,"The paper proposes a new neural network model for learning graphs with arbitrary length, by extending previous models such as graph LSTM (Liang 2016), and graph ConvNets.  There are several recent studies dealing with similar topics, using recurrent and/or convolutional architecture.  The Related work part of this paper makes a good description of both topics.   \n\nI would expect the paper elaborate more (at least in a more explicit way) about the relationship between the two models (the proposed graph LSTM and the proposed Gated Graph ConvNets).   The authors claim that the innovative of the graph Residual ConvNets architecture, but experiments and the model section do not clearly explain the merits of Gated Graph ConvNets over Graph LSTM.  The presentation may raise some misunderstanding.  A thorough analysis or explanation of the reasons why the ConvNet-like architecture is better than the RNN-like architecture would be interesting.  \n\nIn the section of experiments, they compare 5 different methods on two graph mining tasks.  These two proposed neural network models seem performing well empirically.  \n\nIn my opinion, the two different graph neural network models are both suitable for learning graphs with arbitrary length, \nand both models worth future stuies for speicific problems.",1,1,1,1,1,1,1,1,1,-1
Hy_o3x-0b-R1,"The paper combines several recent advances on generative modelling including a ladder variational posterior and a PixelCNN decoder together with the proposed convolutional stochastic layers to boost the NLL results of the current VAEs.  The numbers in the tables are good but I have several comments on the motivation, originality and experiments. \n\nMost parts of the paper provide a detailed review of the literature.  However, the resulting model is quite like a combination of the existing advances and the main contribution of the paper, i.e. the convolution stochastic layer, is not well discussed.  Why should we introduce the convolution stochastic layers?  Could the layers encode the spatial information better than a deterministic convolutional layer with the same architecture?   What's the exact challenge of training VAEs addressed by the convolution stochastic layer?   Please strengthen the motivation and originality of the paper. \n\nThough the results are good,  I still wonder what is the exact contribution of the convolutional stochastic layers to the NLL results?   Can the authors provide some results without the ladder variational posterior and the PixelCNN decoder on both the gray-scaled and the natural images? \n\nAccording to the experimental setting in the Section 3 (Page 5 Paragraph 2), \""In case of gray-scaled images the stochastic latent layers are dense with sizes 64, 32, 16, 8, 4 (equivalent to S\u00f8nderby et al. (2016)) and for the natural images they are spatial (cf. Table 1).  There was no significant difference when using feature maps (as compared to dense layers) for modelling gray-scaled images ""there is no stochastic convolutional layer.    Then is there anything new in FAME on the gray images?   Furthermore, how could FAME advance the previous state-of-the-art?   It seems because of other factors instead of the stochastic convolutional layer.   \n\nThe results on the natural images are not complete.   Please present the generation results on the ImageNet dataset and the reconstruction results on both the CIFAR10 and ImageNet datasets.  The quality of the samples on the CIFAR10 dataset seems not competitive to the baseline papers listed in the table.  Though the visual quality does not necessarily agree with the NLL results but such large gap is still strange.  Besides, why FAME can obtain both good NLL and generation results on the MNIST and OMNIGLOT datasets when there is no stochastic convolutional layer?  Meanwhile, why FAME cannot obtain good generation results on the CIFAR10 dataset?  Is it because there is a lot randomness in the stochastic convolutional layer?  It is better to provide further analysis and it is not safe to say that the stochastic convolutional layer helps learn better latent representations based on only the NNL results. \n\nMinor things:\n\nPlease rewrite the sentence \""When performing reconstructions during training ... while also using the stochastic latent variables z = z 1 , ..., z L",1,1,1,1,1,1,1,1,1,-1
Hy_o3x-0b-R2,"The description of the proposed method is very unclear.  From the paper it is very difficult to make out exactly what architecture is proposed.  I understand that the prior on the z_i in each layer is a pixel-cnn, but what is the posterior?  Equations 8 and 9 would suggest it is of the same form (pixel-cnn) but this would be much too slow to sample during training.  I'm guessing it is just a factorized Gaussian, with a separate factorized Gaussian pseudo-prior?  That is, in figure 1 all solid lines are factorized Gaussians and all dashed lines are pixel-cnns? \n\n* The word \""layers\"" is sometimes used to refer to latent variables z, and sometimes to parameterized neural network layers in the encoder and decoder.  E.g. \""The top stochastic layer z_L in FAME is a fully-connected dense layer\"".  No, z_L is a vector of latent variables.  Are you saying the encoder produces it using a fully-connected layer? \n* Section 2.2 starts talking about \""deterministic layers h\"".  Are these part of the encoder or decoder?  What is meant by \""number of layers connecting the stochastic latent variables\""? \n* Section 2.3: What is meant by \""reconstruction data\""? \n\nIf my understanding of the method is correct, the novelty is limited.  Autoregressive priors were used previously in e.g. the Lossy VAE by Chen et al. and IAF-VAE by Kingma et al.  The reported likelihood results are very impressive though, and would be reason for acceptance if correct.  However, the quality of the sampled images shown for CIFAR-10 doesn't match the reported likelihood.  There are multiple possible reasons for this, but after skimming the code I believe it might be due to a faulty implementation of the variational lower bound.  Instead of calculating all quantities in the log domain, the code takes explicit logs and exponents and stabilizes them by adding small quantities \""eps\"": this is not guaranteed to give the right result.  Please fix this and re-run your experiments. (I.e. in _loss.py don't use x/(exp(y)+eps) but instead use x*exp(-y).  Don't use log(var+eps) with var=softplus(x), but instead use var=softplus(x)+eps or parameterize the variance directly in the log domain).",1,1,1,1,1,1,1,1,1,1
Hy_o3x-0b-R3,"Update:  In light of Yoon Kim's retraction of replication, I've downgraded my score until the authors provide further validation (i.e. CIFAR and ImageNet samples). \n\nSummary\n\nThis paper proposes VAE modifications that allow for the use multiple layers of latent variables.   The modifications are: (1) a shared en/decoder parametrization as used in the Ladder VAE [1],  (2) the latent variable parameters are functions of a CNN,  and (3) use of a PixelCNN decoder [2] that is fed both the last layer of stochastic variables and the input image, as done in [3].   Negative log likelihood (NLL) results on CIFAR 10, binarized MNIST (dynamic and static), OMNIGLOT, and ImageNet (32x32) are reported.   Samples are shown for CIFAR 10, MNIST, and OMNIGLOT.         \n\n\nEvaluation\n\nPros:  The paper\u2019s primary contribution is experimental: SOTA results are achieved for nearly every benchmark image dataset (the exception being statically binarized MNIST, which is only .28 nats off).   This experimental feat is quite impressive, and moreover, in the comments on OpenReview, Yoon Kim claims to have replicated the CIFAR result.   I commend the authors for making their code available already via DropBox.   Lastly, I like how the authors isolated the effect of the concatenation via the \u2018FAME No Concatenation\u2019 results.                  \n\nCons:  The paper provides little novelty in terms of model or algorithmic design, as using a CNN to parametrize the latent variables is the only model detail unique to this paper.   In terms of experiments, the CIFAR samples look a bit blurry for the reported NLL (as others have mentioned in the OpenReview comments).   I find the authors\u2019 claim that FAME is performing superior global modeling interesting.   Is there a way to support this experimentally?   Also, I would have liked to see results w/o the CNN parametrization; how important was this choice?   \n\n\nConclusion\n\nWhile the paper's conceptual novelty is low,  the engineering and experimental work required (to combine the three ideas discussed in the summary and evaluate the model on every benchmark image dataset) is commendable.   I recommend the paper\u2019s acceptance for this reason. \n\n\n[1]  C. Sonderby et al., \u201cLadder Variational Autoencoders.\u201d  NIPS 2016.\n[2]  A. van den Oord et al., \u201cConditional Image Generation with PixelCNN Decoders.\u201d ArXiv 2016.\n[3]  I. Gulrajani et al., \u201cPixelVAE: A Latent Variable Model for Natural Images.\u201d  ICLR 2017.\n",1,1,1,1,1,1,1,1,1,1
SkYXvCR6W-R1,"The paper proposed to encode text into a binary matrix by using a compressing code for each word in each matrix row.  The idea is interesting, and overall introduction is clear. \n\nHowever, the work lacks justification for this particular way of encoding, and no comparison for any other encoding mechanism is provided except for the one-hot encoding used in Zhang & LeCun 2015.  The results using this particular encoding are not better than any previous work. \n\nThe network architecture seems to be arbitrary and unusual.  It was designed with 4 convolutional layers stacked together for the first layer, while a common choice is to just make it one convolutional layer with 4 times the output channels.  The depth of the network is only 5, even with many layers listed in table 5. \n\nIt uses 1-D convolution across the word dimension (inferred from the feature size in table 5), which means the convolutional layers learn intra-word features for the entire text but not any character-level features.  This does not seem to be reasonable. \n\nOverall, the lack of comparisons and the questionable choices for the networks render this work lacking significance to be published in ICLR 2018.",1,1,1,1,-1,1,1,1,1,-1
SkYXvCR6W-R2,"The manuscript proposed to use prefix codes to compress the input to a neural network for text classification.  It builds upon the work by Zhang & LeCun (2015) where the same tasks are used. \n\n\nThere are several issues with the paper and I cannot recommend acceptance of the paper in the current state.  \n- It looks like it is not finished. \n- the datasets are not described properly.  \n- It is not clear to me where the baseline results come from. \n They do not match up to the Zhang paper (I have tried to find the matching accuracies there). \n- It is not clear to me what the baselines actually are or how I can found more info on those. \n- the results are not remarkable.  \n\nBecause of this, the paper needs to be updated and cleaned up before it can be properly reviewed.  \n\nOn top of this, I do not enjoy the style the paper is written in, the language is convoluted.  \nFor example: \u201cThe effort to use Neural Convolution Networks for text classification tasks is justified by the possibility of appropriating tools from the recent developments of techniques, libraries and hardware used especially in the image classification \u201c \nI do not know which message the paper tries to get across here.  \nAs a reviewer my impression (which is subjective) is that the authors used difficult language to make the manuscript look more impressive. \nThe acknowledgements should not be included here either. \n\n",1,1,1,1,-1,1,1,1,-1,-1
SkYXvCR6W-R3,"This paper proposes a new character encoding scheme for use with character-convolutional language models.  This is a poor quality paper, is unclear in the results (what metric is even reported in Table 6), and has little significance (though this may highlight the opportunity to revisit the encoding scheme for characters).",1,1,-1,1,-1,-1,1,1,1,-1
Skz_WfbCZ-R1,"The authors prove a generalization guarantee for deep\nneural networks with ReLU activations, in terms of margins of the\nclassifications and norms of the weight matrices.   They compare this\nbound with a similar recent bound proved by Bartlett, et al.   While,\nstrictly speaking, the bounds are incomparable in strength, the\nauthors of the submission make a convincing case that their new bound\nmakes stronger guarantees under some interesting conditions. \n\nThe analysis is elegant.   It uses some existing tools, but brings them\nto bear in an important new context, with substantive new ideas needed.\nThe mathematical writing is excellent. \n\nVery nice paper. \n\nI guess that networks including convolutional layers are covered by\ntheir analysis.   It feels to me that these tend to be sparse,  but that\ntheir analysis still my provides some additional leverage for such\nlayers.   Some explicit discussion of convolutional layers may be\nhelpful.",1,1,1,1,1,1,1,1,1,-1
Skz_WfbCZ-R2,"This paper provides a new generalization bound for feed forward networks based on a PAC-Bayesian analysis.  The generalization bound depends on the spectral norm of the layers and the Frobenius norm of the weights.  The resulting generalization bound is similar (though not comparable) to a recent result of Bartlett et al (2017),  however the technique is different since this submission uses PAC-Bayesian analysis.  The resulting proof is more simple and streamlined compared to that of Bartlett et al (2017). \n\nThe paper is well presented, the result is explained and compared to other results, and the proofs seem correct.  The result is not particularly different from previous ones,  but the different proof technique might be a good enough reason to accept this paper.  \n\n\n\n\nTypos: Several citations are unparenthesized when they should be.  Also, after equation (6) there is a reference command that is not compiled properly.\n\n",1,1,1,1,1,1,1,1,1,-1
Skz_WfbCZ-R3,"This paper combines a simple PAC-Bayes argument with a simple perturbation analysis (Lemma 2) to get a margin based generalization error bound for ReLU neural networks (Theorem 1) which depends on the product of the spectral norms of the layer parameters as well as their Frobenius norm.  The main contribution of the paper is the simple proof technique to derive Theorem 1, much simpler than the one use in the very interesting work [Bartlett et al. 2017] (appearing at NIPS 2017) which got an analogous bound but with a dependence on the l1-norm of the layers instead of the Frobenius norm.  The authors make a useful comparison between these bounds in Section 3 showing that none is dominating the others, but still analyzing their properties in terms of structural properties of the weight matrices. \n\nI enjoyed reading this paper.  One could think that it makes a somewhat incremental contribution with respect to the more complete work (both theory and practice) from [Bartlett et al. 2017].  Nevertheless, the simplicity and elegance of the proof as well as the result might be useful for the community to get progress on the theoretical analysis of NNs. \n\nThe paper is well written,  though I make some suggestions for the camera ready version below to improve clarity. \n\nI verified most of the math. \n\n== Detailed suggestions ==\n\n1) The authors should specify in the abstract and in the introduction that they are analyzing feedforward neural networks *with ReLU activation functions* so that the current context of the result is more transparent.  It is quite unclear how one could generalize the Theorem 1 to arbitrary activation functions phi given the crucial use of the homogeneity of the ReLU at the beginning of p.4.  Though the proof of Lemma 2 only appears to be using the 1-Lipschitzness property of phi as well as phi(0) =0. (Unless they can generalize further; I also suggest that they explicitly state in the (interesting) Lemma 2 that it is for the ReLU activations (like they did in Theorem 1)). \n\n2) A footnote (or citation) could be useful to give a hint on how the inequality 1/e beta^(d-1) <= tilde{beta}^(d-1) <= e beta^(d-1) is proven from the property |beta-tilde{beta}|<= 1/d beta (middle of p.4). \n\n3) Equation (3) -- put the missing 2 subscript for the l2 norm of |f_(w+u)(x) - f_w(x)|_2 on the LHS (for clarity). \n\n4) One extra line of derivation would be helpful for the reader to rederive the bound|w|^2/2sigma^2  <= O(...) just above equation (4).  I.e. first doing the expansion keeping the beta terms and Frobenius norm sum, and then going directly to the current O(...) term. \n\n5) bottom of p.4: use hat{L}_gamma = 1 instead of L_gamma =1 for more clarity. \n\n6) Top of p.5: the sentence \""Since we need tilde{beta} to satisfy (...)\"" is currently awkwardly stated.  I suggest instead to say that \""|tilde{beta}- beta| <= 1/d (gamma/2B)^(1/d) is a sufficient condition to have the needed condition |tilde{beta}-beta| <= 1/d beta over this range, thus we can use a cover of size dm^(1/2d). \""\n\n7) Typo below (6): citetbarlett2017...\n\n8) Last paragraph p.5: \""Recalling that W_i is *at most* a hxh matrix\"" (as your result do not require constant size layers and covers the rectangular case). \n",1,1,1,1,1,1,1,1,1,-1
Sy-tszZRZ-R1,"This paper investigates the complexity of neural networks with piecewise linear activations by studying the number of linear regions of the representable functions.  It builds on previous works Montufar et al. (2014) and Raghu et al. (2017) and presents improved bounds on the maximum number of linear regions.  It also evaluates the number of regions of small networks during training.  \n\nThe improved upper bound given in Theorem 1 appeared in SampTA 2017 - Mathematics of deep learning ``Notes on the number of linear regions of deep neural networks'' by Montufar.  \n\nThe improved lower bound given in Theorem 6 is very modest but neat.  Theorem 5 follows easily from this.  \n\nThe improved upper bound for maxout networks follows a similar intuition but appears to be novel.  \n\nThe paper also discusses the exact computation of the number of linear regions in small trained networks.  It presents experiments during training and with varying network sizes.  These give an interesting picture, consistent with the theoretical bounds, and showing the behaviour during training.  \n\nHere it would be interesting to run more experiments to see how the number of regions might relate to the quality of the trained hypotheses. \n\n\n\n",1,1,1,1,1,1,1,1,1,-1
Sy-tszZRZ-R2,"Paper Summary:\n\nThis paper looks at providing better bounds for the number of linear regions in the function represented by a deep neural network.  It first recaps some of the setting: if a neural network has a piecewise linear activation function (e.g. relu, maxout), the final function computed by the network (before softmax) is also piecewise linear and divides up the input into polyhedral regions which are all different linear functions.  These regions also have a correspondence with Activation Patterns, the active/inactive pattern of neurons over the entire network.  Previous work [1], [2], has derived lower and upper bounds for the number of linear regions that a particular neural network architecture can have.  This paper improves on the upper bound given by [2] and the lower bound given by [1].  They also provide a tight bound for the one dimensional input case.  Finally, for small networks, they formulate finding linear regions as solving a linear program, and use this method to compute the number of linear regions on small networks during training on MNIST \n\nMain Comments:\nThe paper is very well written and clearly states and explains the contributions.  However, the new bounds proposed (Theorem 1, Theorem 6), seem like small improvements over the previously proposed bounds,  with no other novel interpretations or insights into deep architectures.  (The improvement on Zaslavsky's theorem is interesting.)  The idea of counting the number of regions exactly by solving a linear program is interesting,  but is not going to scale well,  and as a result the experiments are on extremely small networks (width 8), which only achieve 90% accuracy on MNIST.  It is therefore hard to be entirely convinced by the empirical conclusions that more linear regions is better.  I would like to see the technique of counting linear regions used even approximately for larger networks, where even though the results are an approximation, the takeaways might be more insightful. \n\nOverall, while the paper is well written and makes some interesting points,  it presently isn't a significant enough contribution to warrant acceptance. \n\n[1] On the number of linear regions of Deep Neural Networks, 2014, Montufar, Pascanu, Cho, Bengio \n[2] On the expressive power of deep neural networks, 2017, Raghu, Poole, Kleinberg, Ganguli, Sohl-Dickstein",1,1,1,1,1,1,1,1,1,-1
Sy-tszZRZ-R3,"This is quite an interesting paper. Thank you.  Here are a few comments:\n\nI think this style of writing theoretical papers is pretty good, where the main text aims of preserving a coherent story while the technicalities of the proofs are sent to the appendix.  \nHowever I would have appreciated a little bit more details about the proofs in the main text (maybe more details about the construct that is involved).  I can appreciate though that this a fine line to walk.  Also in the appendix, please restate the lemma that is being proven.  Otherwise one will have to scroll up and down all the time to understand the proof.  \n\nI think the paper could also discuss a bit more in detail the results provided.  For example a discussion of how practical is the algorithm proposed for exact counting of linear regions would be nice.  Though regardless, I think the findings speak for themselves and this seems an important step forward in understanding neural nets.  \n\n****************\nI had reduced my score based on the observation made by Reviewer 1 regarding the talk Montufar at SampTA.  Could the authors prioritize clarification to that point !  \n - Thanks for the clarification and adding this citation.",1,1,1,1,1,-1,1,1,1,1
Sy1f0e-R--R1,"In the paper, the authors discuss several GAN evaluation metrics. \nSpecifically, the authors pointed out some desirable properties that GANS evaluation metrics should satisfy. \nFor those properties raised, the authors experimentally evaluated whether existing metrics satisfy those properties or not. \nSection 4 summarizes the results, which concluded that the Kernel MMD and 1-NN classifier in the feature space are so far recommended metrics to be used. \n\nI think this paper tackles an interesting and important problem, what metrics are preferred for evaluating GANs. \nIn particular, the authors showed that Inception Score, which is one of the most popular metric, is actually not preferred for several reasons. \nThe result, comparing data distributions and the distribution of the generator would be the preferred choice (that can be attained by Kernel MMD and 1-NN classifier), seems to be reasonable. \nThis would not be a surprising result as the ultimate goal of GAN is mimicking the data distribution. \nHowever, the result is supported by exhaustive experiments making the result highly convincing. \n\nOverall, I think this paper is worthy for acceptance as several GAN methods are proposed and good evaluation metrics are needed for further improvements of the research field.\n",1,1,1,1,-1,1,1,1,1,-1
Sy1f0e-R--R2,"Thanks for an interesting paper.  \n\nThe paper evaluates popular GAN evaluation metrics to better understand their properties.  The \""novelty\"" of this paper is a bit hard to assess.  However, I found their empirical evaluation and experimental observations to be very interesting.  If the authors release their code as promised, the off-the-shelf tool would be a very valuable contribution to the GAN community.  \n\nIn addition to existing metrics, it would be useful to add Frechet Inception Distance (FID) and Multi-scale structural similarity (MS-SSIM).  \n\nHave you considered approximations to Wasserstein distance? E.g. Danihelka et al proposed using an independent Wasserstein critic to evaluate GANs:  \nComparison of Maximum Likelihood and GAN-based training of Real NVPs\nhttps://arxiv.org/pdf/1705.05263.pdf \n\nHow sensitive are the results to hyperparameters?  It would be interesting to see some sensitivity analysis as well as understand the correlations between different metrics for different hyperparameters (cf. Appendix G in https://arxiv.org/pdf/1706.04987.pdf) \n\nDo you think it would be useful to compare other generative models (e.g. VAEs) using these evaluation metrics?  Some of the metrics don't capture perceptual similarity, but I'm curious to hear what you think. \n",1,1,1,1,1,1,1,1,1,1
Sy1f0e-R--R3,"The paper describes an empirical evaluation of some of the most common metrics to evaluate GANs (inception score, mode score, kernel MMD, Wasserstein distance and LOO accuracy).  \n\nThe paper is well written, clear, organized and easy to follow. \n\nGiven that the underlying application is image generation, the authors move from a pixel representation of images to using the feature representation given by a pre-trained ResNet, which is key in their results and further comparisons.  They analyzed discriminability, mode collapsing and dropping, robustness to transformations, efficiency and overfitting.  \n\nAlthough this work and its results are very useful for practitioners,  it lacks in two aspects.  First, it only considers a single task for which GANs are very popular.  Second, it could benefit from a deeper (maybe theoretical analysis) of some of the questions.  Some of the conclusions could be further clarified with additional experiments (e.g., Sec 3.6 \u2018while the reason that RMS also fails to detect overfitting may again be its lack of generalization to datasets with classes not contained in the ImageNet dataset\u2019).\n",1,1,1,1,1,-1,1,1,1,-1
Sy21R9JAW-R1,"This paper discusses several gradient based attribution methods, which have been popular for the fast computation of saliency maps for interpreting deep neural networks.  The paper provides several advances:\n- \\epsilon-LRP and DeepLIFT are formulated in a way that can be calculated using the same back-propagation as training. \n- This gives a more unified way of understanding, and implementing the methods. \n- The paper points out situations when the methods are equivalent \n- The paper analyses the methods' sensitivity to identifying single and joint regions of sensitivity \n- The paper proposes a new objective function to measure joint sensitivity \n\nOverall, I believe this paper to be a useful contribution to the literature.  It both solidifies understanding of existing methods and provides new insight into quantitate ways of analysing methods.  Especially the latter will be appreciated.",1,1,1,1,-1,-1,1,1,1,-1
Sy21R9JAW-R2,"The paper summarizes and compares some of the current explanation techniques for deep neural networks that rely on the redistribution of relevance / contribution values from the output to the input space. \n\nThe main contributions are the introduction of a unified framework that expresses 4 common attribution techniques (Gradient * Input, Integrated Gradient, eps-LRP and DeepLIFT) in a similar way as modified gradient functions and the definition of a new evaluation measure ('sensitivity n') that generalizes the earlier defined properties of 'completeness' and 'summation to delta'. \n\nThe unified framework is very helpful since it points out equivalences between the methods and makes the implementation of eps-LRP and DeepLIFT substantially more easy on modern frameworks.  However, as correctly stated by the authors some of the unification (e.g. relation between LRP and Gradient*Input) has been already mentioned in prior work. \n\nSensitivity-n as a measure tries to tackle the difficulty of estimating the importance of features that can be seen either separately or in combination.  While the measure shows interesting trends towards a linear behaviour for simpler methods,  it does not persuade me as a measure of how well the relevance attribution method mimics the decision making process and does not really point out substantial differences between the different methods.  Furthermore, The authors could comment on the relation between sensitivity-n and region perturbation techniques (Samek et al., IEEE TNNLS, 2017).  Sensitivtiy-n seems to be an extension of the region perturbation idea to me. \n\nIt would be interesting to see the relation between the \""unified\"" gradient-based explanation methods and approaches (e.g. Saliency maps, alpha-beta LRP, Deep Taylor, Deconvolution Networks, Grad-CAM, Guided Backprop ...) which do not fit into the unification framework.  It's good that the author mention these works, still it would be great to see more discussion on the advantages/disadvantages, because these methods may have some nice theoretically properties (see e.g. the discussion on gradient vs. decompositiion techniques in Montavon et al., Digital Signal Processing, 2017) which can not be incorporated into the unified framework.",1,1,1,1,1,1,1,1,1,-1
Sy21R9JAW-R3,"The paper shows that several recently proposed interpretation techniques for neural network are performing similar processing and yield similar results.  The authors show that these techniques can all be seen as a product of input activations and a modified gradient, where the local derivative of the activation function at each neuron is replaced by some fixed function. \n\nA second part of the paper looks at whether explanations are global or local.  The authors propose a metric called sensitivity-n for that purpose, and make some observations about the optimality of some interpretation techniques with respect to this metric in the linear case.  The behavior of each explanation w.r.t. these properties is then tested on multiple DNN models tested on real-world datasets.  Results further outline the resemblance between the compared methods. \n\nIn the appendix, the last step of the proof below Eq. 7 is unclear.  As far as I can see, the variable g_i^LRP wasn\u2019t defined, and the use of Eq. 5 to achieve this last could be better explained.  There also seems to be some issues with the ordering i,j, where these indices alternatively describe the lower/higher layers, or the higher/lower layers.",1,1,1,1,1,-1,1,1,1,-1
Sy3fJXbA--R1,"The authors extend the ResNeXt architecture.  They substitute the simple add operation with a selection operation for each input in the residual module.  The selection of the inputs happens through gate weights, which are sampled at train time.  At test time, the gates with the highest values are kept on, while the other ones are shut.  The authors fix the number of the allowed gates to K out of C possible inputs (C is the multi-branch factor in the ResNeXt modules).  They show results on CIFAR-100 and ImageNet (as well as mini ImageNet).  They ablate the choice of K, the binary nature of the gate weights. \n\nPros:\n(+) The paper is well written and the method is well explained \n(+) The authors ablate and experiment on large scale datasets \n\nCons:\n(-) The proposed method is a simple extension of ResNeXt  \n(-) The gains are reasonable,  yet not SOTA, and come at a price of more complex training protocols (see below) \n(-) Generalization to other tasks not shown \n\nThe authors do a great job walking us through the formulation and intutition of their proposed approach.  They describe their training procedure and their sampling approach for the gate weights.  However, the training protocol gets complicated with the introduction of gate weights.  In order to train the gate weights along with the network parameters, the authors need to train the parameters jointly followed by the training of only the network parameters while keeping the gates frozen.  This makes training of such networks cumbersome. \n\nIn addition, the authors report a loss in performance when the gates are not discretized to {0,1}.  This means that a liner combination with the real-valued learned gate parameters is suboptimal.  Could this be a result of suboptimal, possibly compromised training?  \n\nWhile the CIFAR-100 results look promising, the ImageNet-1k results are less impressive.  The gains from introducing gate weights in the input of the residual modules vanish when increasing the network size.  \n\nLast, the impact of ResNeXt/ResNet lies in their ability to generalize to other tasks.  Have the authors experimented with other tasks, e.g. object detection, to verify that their approach leads to better performance in a more diverse set of problems?\n",1,1,1,1,1,-1,1,1,1,-1
Sy3fJXbA--R2,"The paper is clear and well written. \nIt is an incremental modification of prior work (ResNeXt) that performs better on several experiments selected by the author; comparisons are only included relative to ResNeXt. \n\nThis paper is not about gating (c.f., gates in LSTMs, mixture of experts, etc) but rather about masking or perhaps a kind of block sparsity, as the \""gates\"" of the paper do not depend upon the input: they are just fixed masking matrices (see eq (2)). \n\nThe main contribution appears to be the optimisation procedure for the binary masking tensor g.  But this procedure is not justified: does each step minimise the loss?  This seems unlikely due to the sampling.  Can the authors show that the procedure will always converge?  It would be good to contrast this with other attempts to learn discrete random variables (for example, The Concrete Distribution: Continuous Relaxation of Continuous Random Variables, Maddison et al, ICLR 2017).\n",1,1,1,1,1,1,1,1,-1,-1
Sy3fJXbA--R3,"The paper proposes replacing each layer in a standard (residual) convnet with a set of convolutional modules which are run in parallel.   The input to each model is a sparse sum of the outputs of modules in the previous set.  The paper shows marginal improvements on image classification datasets (2% on CIFAR, .2% on ImageNet) over the ResNeXt architecture that they build on.   \n\nPros:\n- The connectivity is constrained to be sparse between modules, and it is somewhat interesting that this connectivity can be learned with algorithms similar to those previously proposed to learn binary weights.   Furthermore, this learning extends to large-scale image datasets. \n- There is indeed a boost in classification performance, and the approach shows promise for automatically reducing the number of parameters in the network. \n\nCons:\n- Overall, the approach seems to be an incremental improvement over the previous work ResNeXt. \n- The datasets used are not very interesting: Cifar is too small, and ImageNet is essentially solved.   From the standpoint of the computer vision community, increasing performance on these datasets is no longer a meaningful objective.  \n- The modifications add complexity.  \n\nThe paper is well written and conceptually simple.   However, I feel the paper demonstrates neither enough novelty nor enough of a performance gain for me to advocate acceptance.",1,1,1,1,-1,1,1,1,1,-1
Sy3nGCYXz-R1,"This paper gives an empirical estimation of the intrinsic dimensionality of the convolutional neural network VGG19 due to Simonyan and Zisserman.   To estimate the ID, the authors apply singular value decomposition to the matrix of activation vectors at each of the layers of the network, in which the intrinsic dimension is determined (in a more or less standard way) by the rank at which two consecutive singular values has a ratio exceeding some threshold.  For the convolutional layers of VGG19, they observe that the sum of IDs for each feature map is roughly equal to the ID of the matrix formed by concatenating the vectors over all feature maps.  They also observe that the ID drops with each successive layer. \n\nThe authors' findings are intuitively obvious, and certainly not surprising.  Although a thorough and careful empirical investigation of this phenomenon would be a welcome addition to the research literature, this paper does not yet reach this standard.  First and foremost, a result for a single neural network does not constitute enough evidence to justify the authors' conclusions.  Second, the latter half of the paper is concerned with details of the experimental results, without offering any insights as to the implications for deep learning.  Third, the paper is not well presented and organized: the introduction is scant; the notational formulism is not at all clear, rigorous, or consistent; the paper overall lacks polish, with many grammatical errors.  \n\nOverall, I feel that while this line of research is worthwhile, at this stage the work is not yet ready for publication.",1,1,1,1,-1,-1,1,1,1,-1
Sy3nGCYXz-R2,"This paper analyzed the dimensionality of feature maps and fully connected layers of pre-trained CNN on images within a same category.  The local dimensionality of this paper is the SVD dimensionality on the augmented images of the same class images which classified by the neural network as high probability.  However, the motivation of the analysis of this paper is unclear.  I could not understand how such analysis contributes advance of representation learning.  \n\nFurther, the analysis of this paper is not convincing.   \n-      I cannot believe the sum of SVD dimensionality of each feature maps becomes equals to the dimensionality to concatenated feature maps.  As shown in in Fig.8, the estimated dimensions and original dimensions are very different.  Although by looking some features maps, this rule might be hold as shown in Fig.5.  However, the analysis is done on small examples without any theoretical analysis.  \n\n-\tThe authors experimented one trained CNN and tested on images on only three categories (Persian Cat, Container Ship, and Volcao).  It is not clear if the same rules holds to other CNNs, and images of other categories. \n",1,1,1,1,-1,-1,1,1,1,-1
Sy3nGCYXz-R3,"This paper try to analyze the intrinsic structure of VGG19 and give a new insight of deep neural networks.  The authors propose to use SVD tools to estimate the dimension of the deep manifolds, and conduct experiments on three categories of ImageNet.  The papers are written well and easy to follow.  The analysis of manifold structure of DNN is important direction, but I am afraid novelty and insight of this work is not enough for acceptance. \npros:\n 1. The paper is well written and easy to follow. \n 2. Manifold analysis of the intrinsic structure of DNN is a important direction for further study. \ncons:\n 1. SVD is a standard tool for subspace and manifold analysis for decades of years.  I do not think using it in DNN is a big contribution. \n2. The authors should explain why choosing VGG19 for analysis.  Do other deep neural networks, such as Resnet, Googlenet, can  have the same phenomenon? \n3. Why the authors choose Persian Cat, Container Ship, and Volcano in the experiments?  Do other categories have the similar results? \n4. The authors can indicate the application scenario of this work.  For example, this work may guide to design better CNN structure for higher accuracy and lower computation cost.  It may help the readers better understand the values of this work.\n",1,1,1,1,1,-1,1,1,1,-1
Sy4c-3xRW-R1,"This paper propose an adaptive dropout strategy for class logits.  They learn a distribution q(z | x, y) that randomly throw class logits.   By doing so they ensemble predictions of the models between different set of classes, and focuses on more difficult discrimination tasks.  They learn the dropout distribution by variational inference with concrete relaxation.  \n\nOverall I think this is a good paper.  The technique sounds, the presentation is clear and I have not seen similar paper elsewhere  (not 100% sure about the originality of the work though).  \n\nPro:\n* General algorithm\n\nCon:\n* The experiment is a little weak.  Only on CIFAR100 the proposed approach is much better than other approaches.  I would like to see the results on more datasets.  Maybe should also compare with more dropout algorithms, such as DropConnect and MaxOut.",1,1,1,1,1,1,1,1,1,-1
Sy4c-3xRW-R2,"Pros\n- The proposed model is a nice way of multiplicatively combining two features :\n  one which determines which classes to pay attention to, and other that\nprovides useful features for discrimination. \n\n- The adaptive component seems to provide improvements for small dataset sizes   and large number of classes. \n\nCons\n- \""One can easily see that if o_t(x; w) = 0, then class t becomes neutral in the\n  classification and the gradients are not back-propagated from it. \"" : This does\nnot seem to be true.  Even if the logits are zero, the class would have a\nnon-zero probability and would receive gradients.  Do the authors mean\nexp(o_t(x;w)) = 0 ? \n\n- Related to the above, it should be clarified what is meant by dropping a\n  class.  Is its logit set to zero or -\\infty ?  Excluding a class from the\nsoftmax is equivalent to having a logit of -\\infty, not zero.  However, from the\nequations in the paper it seems that the logit is set to zero.  This would not\nresult in excluding the unit.  The overall effect would just be to raise the\nmagnitude of logits across the entire softmax. \n\n- It seems that the model benefits from at least two separate effects - one is\n  the attention mechanism provided by the sigmoids, and the other is the\nstochasticity during training.  Presently, it is not clear if only one of the\ncomponents is providing most of the benefits, or if both things are useful.  It\nwould be great to compare this model to a non-stochastic one which just has the\nmultiplicative effects applied in a deterministic way (during both training and\ntesting). \n\n- The objective of the attention mechanism that sets the dropout mask seems to\n  be the same as the primary objective of classifying the input, and the\nattention mechanism is prevented from solving the task by adding an extra\nentropy regularization.  It would be useful to explain more why this is needed. \nWould it not be fine if the attention mechanism did a perfect job of selecting\nthe class ? \n\nQuality\nThe paper makes relevant comparisons and is overall well-motivated.  However,\nsome aspects of the paper can be improved by adding more explanations. \n\nClarity\nSome crucial aspects of the paper are unclear as mentioned above. \n\nOriginality\nThe main contribution of the paper is similar to multiplicative gating.  The\nadded stochasticity and the model ensembling interpretation is probably novel. \nHowever, experiments are insufficient to determine whether it is this novelty\nthat contributes to improved performance or just the gating. \n\nSignificance\nThis paper makes incremental improvements and would be of moderate interest to\nthe machine learning community. \n\nTypos :\n- In Eq 3, the numerator has z_t. Should that be z_y ? \n- In Eq 5, the denominator has z_y.  Should that be z_t ?",1,1,1,1,1,-1,1,1,1,-1
Sy4c-3xRW-R3,"The paper discusses dropping out the pre-softmax logits in an adaptive manner.  This isn't a huge conceptual leap given previous work, for instance that of Ba and Frey 2013 or the sequence of papers by Gal and his coauthors on variational interprations of dropout.  In the spirit of the latter series of papers on variational dropout there is a derivation of this algorithm using ideas from variational inference.  The variational approximation is a bit odd in that it doesn't have any variational parameters, and indeed a further regulariser in equation (14) is needed to give the desired behaviour.  A fairly small, but consistent improvement on the base model and other similar ideas is reported in Table 1.  I would have liked to have seen results on ImageNet.  I don't find (the too small) Figure 2 to be compelling evidence that \""our dropmax effectively prevents\noverfiting by converging to much lower test loss\"".  The test loss in question looks like a noisy version of the base test loss with a slightly lower mean.  There are grammatical errors throughout the paper at a higher rate than would normally be found in a successful submission at this stage.  Figure 3 illustrates the idea nicely.  Which of the MNIST models from Table 1 was used?\n",1,1,1,1,1,1,1,1,1,-1
SyGT_6yCZ-R1,"This paper deals with early stopping but the contributions are limited.  This work would fit better a workshop as a preliminary result, furthermore it is too short.  Following a short review section per section. \n\nIntro: The name SFC is misleading as the method consists in stopping early the training with an optimized learning schedule scheme.  Furthermore, the work is not compared to the appropriate baselines. \n\nProposal: The first motivation is not clear.  The training time of the feature extractor has never been a problem for transfer learning tasks for example: once it is trained, you can reuse the architecture in a wide range of tasks.  Besides, the training time of a CNN on CIFAR10 or even ImageNet is now quite small(for reasonable architectures), which allows fast benchmarking. \nThe second motivation, w.r.t. IB seems interesting  but this should be empirically motivated(e.g. figures) in the subsection 2.1, and this is not done. \n\nThe section 3 is quite long and could be compressed to improve the relevance of this experimental section.  All the accuracies(unsup dict, unsup, etc) on CIFAR10/CIFAR100 are reported from the paper (Oyallon & Mallat, 2015), ignoring 2-3 years of research that leads to new numerical results.  Furthermore, this supervised technique is only compared to unsupervised or predefined methods, which is is not fair and the training time of the Scattering Transform is not reported, for example.  \n\nFinally, extracting features is mainly useful on ImageNet (for realistic images) and this is not reported here. \n\nI believe re-thinking new learning rate schedules is interesting,  however I recommend the rejection of this paper.",1,1,1,1,-1,1,1,1,1,-1
SyGT_6yCZ-R2,"This paper proposes a fast way to learn convolutional features that later can be used with any classifier.  The acceleration of the training comes from a reduced number of training epocs and a specific schedule decay of the learning rate.  \nIn the evaluation the features are used with support vector machines (SVN) and extreme learning machines on MNIST and CIFAR10/100 datasets. \n\nPros:\nThe paper compares different classifiers on three datasets. \n\nCons:\n- Considering an adaptive schedule of the learning decay is common practice in modern machine learning.  Showing that by varying the learning rate the authors can reduce the number of training epocs and still obtain good performance is not a contribution and it is actually implemented in most of the recent deep learning libraries, like Keras or Pytorch. \n- It is not clear why, once a CNN has been trained, one should want to change the last layer and use a SVN or other classifiers. \n- There are many spelling errors \n- Comparing CNN based methods with hand-crafted features as in Fig. 1 and Tab.3 is not interesting anymore.  It is well known that CNN features are much better if enough data is available.\n",1,1,1,1,-1,1,1,1,1,-1
SyGT_6yCZ-R3,"I am not sure how to interpret this paper.  The paper seems to be very thin technically, unless I missed some important details.  Two proposals in the paper are:\n\n(1) Using a learning rate decay scheme that is fixed relative to the number of epochs used in training,  and \n(2) Extract the penultimate layer output as features to train a conventional classifier such as SVM. \n\nI don't understand why (1) differs from other approaches, in the sense that one cannot simply reduce the number of epochs without hurting performance.  And for (2), it is a relatively standard approach in utilizing CNN features.  Essentially, if I understand correctly, this paper is proposing to prematurely stop training an use the intermediate feature to train a conventional classifier (which is not that away from the softmax classifier that CNNs usually use).  I fail to see how this would lead to superior performance compared to conventional CNNs.",1,1,1,1,-1,-1,1,1,1,-1
SyhcXjy0Z-R1,"The paper is relatively clear to follow, and implement.  \n\nThe main concern is that this looks like a class project rather than a scientific paper.  For a class project this could get an A in a ML class! \n\nIn particular, the authors take an already existing dataset, design a trivial convolutional neural network, and report results on it.  There is absolutely nothing of interest to ICLR except for the fact that now we know that a trivial network is capable of obtaining 90% accuracy on this dataset.",1,1,-1,1,-1,-1,1,1,1,-1
SyhcXjy0Z-R2,"\nAs one can see by the title, the originality (application of DCNN) and significance (limited to ATM domain) is very limited.  If this is still enough for ICLR, the paper could be okay.  However, even so one can clearly see that the architecture, the depth, the regularization techniques, and the evaluation are clearly behind the state of the art.  Especially for this problem domain, drop-out and data augmentation should be investigated. \n\nOnly one dataset is used for the evaluation and it seems to be very limited and small.  Moreover, it seems that the same subjects (even if it is other pictures) may appear in the training set and test set as they were randomly selected.  Looking into the referece (to get the details of the dataset -  from a workshop of the IEEE International Conference on Computer Vision Workshops (ICCVW) 2017) reveals, that it has only 25 subjects and 10 disguises . This makes it even likely that the same subject with the same disguise appears in the training and test set. \n\nA very bad manner, which unfortunately is often performed by deep learning researchers with limited pattern recognition background, is that the accuracy on the test set is measured for every timestamp and finally the highest accuracy is reported.  As such you perform an optimization of the paramerter #iterations on the test set, making it a validation set and not an independent test set.  \n\nMinor issues:\nmake sure that the capitalization in the references is correct (ATM should be capital, e.g., by putting {ATM} - and many more things).",1,1,1,1,1,-1,1,1,-1,1
SyhcXjy0Z-R3,"This paper is an application paper on detecting when a face is disguised,  however it is poorly written and do not contribute much in terms of novelty of the approach.  The application domain is interesting,  however it is simply a classification problem \n\nThe paper is written clearly  (with mistakes in an equation), however, it does not contribute much in terms of novelty or new ideas. \n\nTo make the paper better, more empirical results are needed.  In addition, it would be useful to investigate how this particular problem is different than a binary classification problem using CNNs",1,1,-1,1,1,-1,1,1,1,-1
Syhr6pxCW-R1,"Overall I like the paper and the results look nice in a diverse set of datasets and tasks such as edge-to-image, super-resolution, etc.  Unlike the generative distribution sampling of GANs, the method provides an interesting compositional scheme, where the low frequencies are regressed and the high frequencies are obtained by \""copying\"" patches from the training set.  In some cases the results are similar to pix-to-pix (also in the numerical evaluation) but the method allows for one-to-many image generation, which is a important contribution.  Another positive aspect of the paper is that the synthesis results can be analyzed, providing insights for the generation process.  \n\nWhile most of the paper is well written,  some parts are difficult to parse.  For example, the introduction has some parts that look more like related work (that is mostly a personal preference in writting). Also in Section 3, the paragraph for distance functions do not provide any insight about what is used, but it is included in the next paragraph (I would suggest either merging or not highlighting the paragraphs). \n\nQ: The spatial grouping that is happening in the compositional stage, is it solely due to the multi-scale hypercolumns?   Would the result be more inconsistent if the hypercolumns had smaller receptive field? \n\nQ: For the multiple outputs, the k neighbor is selected at random?\n",1,1,1,1,1,-1,1,1,1,-1
Syhr6pxCW-R2,"This paper presents a pixel-matching based approach to synthesizing RGB images from input edge or normal maps.  The approach is compared to Isola et al\u2019s conditional adversarial networks, and unlike the conditional GAN, is able to produce a diverse set of outputs. \n\nOverall, the paper describes a computer visions system based on synthesizing images, and not necessarily a new theoretical framework to compete with GANs.  With the current focus of the paper being the proposed system, it is interesting to the computer vision community.  However, if one views the paper in a different light, namely showing some \u201cblind-spots\u201d of current conditional GAN approaches like lack of diversity, then it can be of much more interest to the broader ICLR community. \n\nPros: \nOverall the paper is well-written \nMakes a strong case that random noise injection inside conditional GANs does not produce enough diversity \nShows a number of qualitative and quantitative results \n\nConcerns about the paper:\n1.) It is not clear how well the proposed approach works with CNN architectures other than PixelNet \n2.) Since the paper used \u201cthe pre-trained PixelNet to extract surface normal and edge maps\u201d for ground-truth generation, it is not clear whether the approach will work as well when the input is a ground-truth semantic segmentation map. \n3.) Since the paper describes a computer-vision image synthesis system and not a new theoretical result, I believe reporting the actual run-time of the system will make the paper stronger. \ Can PixelNN run in real-time?  How does the timing compare to Isola et al\u2019s Conditional GAN? \n\nMinor comments:\n1.) The paper mentions making predictions from \u201cincomplete\u201d input several times, but in all experiments, the input is an edge map, normal map, or low-resolution image.  When reading the manuscript the first time, I was expecting experiments on images that have regions that are visible and regions that are masked out.  However, I am not sure if the confusion is solely mine, or shared with other readers. \n\n2.) Equation 1 contains the norm operator twice, and the first norm has no subscript, while the second one has an l_2 subscript.  I would expect the notation style to be consistent within a single equation (i.e., use ||w||_2^2, ||w||^2, or ||w||_{l_2}^2)\n\n3.)  Table 1 has two sub-tables: left and right. The sub-tables have the AP column in different places. \n\n4.) \u201cDense pixel-level correspondences\u201d are discussed but not evaluated.\n",1,1,1,1,1,1,1,1,1,-1
Syhr6pxCW-R3,"This paper proposes a compositional nearest-neighbors approach to image synthesis, including results on several conditional image generation datasets.  \n\nPros:\n- Simple approach based on nearest-neighbors, likely easier to train compared to GANs. \n- Scales to high-resolution images. \n\nCons:\n- Requires a potentially costly search procedure to generate images. \n- Seems to require relevant objects and textures to be present in the training set in order to succeed at any given conditional image generation task.",1,1,1,1,-1,-1,1,-1,1,-1
SyJS-OgR--R1,"This paper interprets deep residual network as a dynamic system, and proposes a novel training algorithm to train it in a constructive way.  On three image classification datasets, the proposed algorithm speeds up the training process without sacrificing accuracy.  The paper is interesting and easy to follow.  \n\nI have several comments:\n1.\tIt would be interesting to see a comparison with Stochastic Depth, which is also able to speed up the training process, and gives better generalization performance.  Moreover, is it possible to combine the proposed method with Stochastic Depth to obtain further improved efficiency? \n2.\tThe mollifying networks [1] is related to the proposed method as it also starts with shorter networks, and ends with deeper models.  It would be interesting to see a comparison or discussion. \n[1] C Gulcehre, Mollifying Networks, 2016\n3. \tCould you show the curves (on Figure 6 or another plot) for training a short ResNet (same depth as your starting model) and a deep ResNet (same depth as your final model) without using your approach?",1,1,1,1,1,1,1,1,1,-1
SyJS-OgR--R2,"I enjoyed reading the paper.  This is a very well written paper, the authors propose a method for speeding up the training time of Residual Networks based on the dynamical system view interpretation of ResNets.  In general I have a positive opinion about the paper, however, I\u2019d like to ask for some clarifications. \n\nI\u2019m not fully convinced by the interpretation of Eq. 5: \u201c\u2026 d is inversely proportional to the norm of the residual modules G(Yj)\u201d.  Since F(Yj) is not a constant, I think that d is inversely proportional to ||G(Yj)||/||F(Yj)||, however, in the interpretation the dependence on ||F(Yj)|| is ignored.  Could the authors comment on that? \n\nSection 4. 1 \u201c Each cycle itself can be regarded as a training process, thus we need to reset the learning rate value at the beginning of each training cycle and anneal the learning rate during that cycle. \u201d Is there any empirical evidence for this?  What would happen if the learning rate is not reset at the beginning of each cycle?  \n\nQuestions with respect to dynamical systems point of view: Eq. 4 assumes small value of h.  However, for ResNet there is no guarantee that the h would be small (e. g. in Appendix C the values between 0.25 and 1 are used).  Would the authors be willing to comment on the importance of the value of h?  In figure 1, pooling (strided convolutions) are not depicted between network stages.  I have one question w.r.t. feature maps dimensionality changes inside a CNN: how does pooling (or strided convolution) fit into dynamical systems view? \n\nTable 3 and 4. I assume that the training time unit is a minute, I couldn\u2019t find this information in the paper.  Is the batch size the same for all models (100 for CIFAR and 32 for STL-10)?  I understand that the models with different #Blocks have different capacity, for clarity, would it be possible to add # of parameters to each model?  For multilevel method, would it be possible to show intermediate results in Table 3 and 4, e. g. at the end of cycle 1 and 2?   I see these results in Figure 6, however, the plots are condensed and it is difficult to see the exact number at the end of each cycle.  \n\nThe citation (E, 2017) seems to be wrong, could the authors check it?\n",1,1,1,1,1,-1,1,1,1,1
SyJS-OgR--R3,"\n\nThis paper proposes a new method to train residual networks in which one starts by training shallow ResNets, doubling the depth and warm starting from the previous smaller model in a certain way, and iterating.   The authors relate this idea to a recent dynamical systems view of ResNets in which residual blocks are viewed as taking steps in an Euler discretization of a certain differential equation.   This interpretation plays a role in the proposed training method by informing how the \u201cstep sizes\u201d in the Euler discretization should change when doubling the depth of the network.   The punchline of the paper is that the authors are able to achieve similar performance as \u201cfull ResNet training\u201d but with significantly reduced training time. \n\nOverall, the proposed method is novel \u2014 even though this idea of going from shallow to deep is natural for residual networks, tying the idea to the dynamical systems perspective is elegant.   Moreover the paper is clearly written.   Experimental results are decent \u2014 there are clear speedups to be had based on the authors' experiments.   However it is unclear if these gains in training speed are significant enough for people to flock to using this (more complicated) method of training. \n\nI only have a few small questions/comments:\n* A more naive way to do multi-level training would be to again iteratively double the depth, but perhaps not halve the step size.   This might be a good baseline to compare against to demonstrate the value of the dynamical systems viewpoint. \n* One thing I\u2019m unclear on is how convergence was assessed\u2026 my understanding is that the training proceeds for a fixed number of epochs (?) - but shouldn\u2019t this also depend on the depth in some way?  \n* Would the speedups be more dramatic for a larger dataset like Imagenet? \n* Finally, not being very familiar with multigrid methods from the numerical methods literature \u2014 I would have liked to hear about whether there are deeper connections to these methods.\n\n\n",1,1,1,1,1,-1,1,1,1,-1
SyjsLqxR--R1,"Summary:\n\nThis paper empirically studies adversarial perturbations dx and what the effects are of adversarial training (AT) with respect to shared (dx fools for many x) and singular (only for a single x) perturbations.  Experiments use a (previously published) iterative fast-gradient-sign-method and use a Resnet on CIFAR. \n\nThe authors conclude that in this experimental setting:\n- AT seems to defend models against shared dx's. \n- This is visible on universal perturbations, which become less effective as more AT is applied. \n- AT decreases the effectiveness of adversarial perturbations, e.g. AT decreases the number of adversarial perturbations that fool both an input x and x with e.g. a contrast change. \n- Singular perturbations are easily detected by a detector model, as such perturbations don't change much when applying AT. \n\nPro:\n- Paper addresses an important problem: qualitative / quantitative understanding of the behavior of adversarial perturbations is still lacking. \n- The visualizations of universal perturbations as they change during AT are nice. \n- The basic observation wrt the behavior of AT is clearly communicated. \n\nCon:\n- The experiments performed are interesting directions, although unfocused and rather limited in scope.  For instance, does the same phenomenon happen for different datasets?  Different models? \n- What happens when we use adversarial attacks different from FGSM?  Do we get similar results? \n- The papers lacks a more in-depth theoretical analysis.  Is there a principled reason AT+FGSM defends against universal perturbations? \n\nOverall:\n- As is, it seems to me the paper lacks a significant central message (due to limited and unfocused experiments) or significant new theoretical insight into the effect of AT.  A number of questions addressed are interesting starting points towards a deeper understanding of *how* the observations can be explained and more rigorous empirical investigations.\n\nDetailed:\n-\n",1,1,1,1,1,-1,1,1,1,-1
SyjsLqxR--R2,"This paper analyses adversarial training and its effect on universal adversarial examples as well as standard (basic iteration) adversarial examples.  It also analyses how adversarial training affects detection. \n\nThe robustness results in the paper are interesting and seem to indicate that interesting things are happening with adversarial training despite adversarial training not fixing the adversarial examples problem.  The paper shows that adversarial training increases the destruction rate of adversarial examples so that it still has some value though it would be good to see if other adversarial resistance techniques show the same effect.  It's also unclear from which epoch the adversarial examples were generated from in figure 5.  Further the transformations in figure 5 are limited to artificially controlled situations, it would be much more interesting to see how the destruction rate changes under real-world test scenarios. \n\nThe results on the detector are not that surprising since previous work has shown that detectors can learn to classify adversarial examples and the additional finding that they can detect adversarial examples for an adversarially trained model doesn't seem surprising.  There is also no analysis of what happens for adversarial examples for the detector. \n\nAlso, it's not clear from section 3.1 what inputs are used to generate the adversarial examples.  Are they a random sample across the whole dataset?\n\nFinally, the paper spends significant time on describing MaxMin and MinMax and the graphical visualizations but the paper fails to show these graphical profiles for real models.",1,1,1,1,1,-1,1,1,1,-1
SyjsLqxR--R3,"This paper investigates the effect of adversarial training.  Based on experiments using CIFAR10, the authors show that adversarial training is effective in protecting against \""shared\"" adversarial perturbation, in particular against universal perturbation.  In contrast, it is less effective to protect against singular perturbations.  Then they show that singular perturbation are less robust to image transformation, meaning after image transformation those perturbations are no longer effective.  Finally, they show that singular perturbations can be easily detected. \n\nI like the message conveyed in this paper.  However, as the statements are mostly backed by experiments, then I think it makes sense to ask how statistically significant the present results are.  Moreover, is CIFAR 10 experiments conclusive enough.",1,1,1,1,1,-1,1,1,1,-1
SyL9u-WA--R1,"This paper proposed a new parametrization scheme for weight matrices in neural network based on the Householder  reflectors to solve the gradient vanishing and exploding problems in training.  The proposed method improved two previous papers:\n1) stronger expressive power than Mahammedi et al. (2017), \n2) faster gradient update than Vorontsov et al. (2017) .\nThe proposed parametrization scheme is natrual from numerical linear algebra point of view and authors did a good job in Section 3 in explaining the corresponding expressive power.  The experimental results also look promising.  \n\nIt would be nice if the authors can analyze the spectral properties of the saddle points in linear RNN (nonlinear is better but it's too difficult I believe).  If the authors can show the strict saddle properties then as a corollary, (stochastic) gradient descent finds a global minimum",1,1,1,1,1,1,1,1,1,-1
SyL9u-WA--R2,"This paper suggests a reparametrization of the transition matrix.  The proposed reparametrization which is based on Singular Value Decomposition can be used for both recurrent and feedforward networks. \n\nThe paper is well-written and authors explain related work adequately.  The paper is a follow up on Unitary RNNs which suggest a reparametrization that forces the transition matrix to be unitary.  The problem of vanishing and exploding gradient in deep network is very challenging and any work that shed lights on this problem can have a significant impact.  \n\nI have two comments on the experiment section:\n\n- Choice of experiments.  Authors have chosen UCR datasets and MNIST for the experiments while other experiments are more common.  For example, the adding problem, the copying problem and the permuted MNIST problem and language modeling are the common experiments in the context of RNNs.  For feedforward settings, classification on CIFAR10 and CIFAR100 is often reported. \n\n- Stopping condition. \ The plots suggest that the optimization has stopped earlier for some models.  Is this because of some stopping condition or because of gradient explosion?  Is there a way to avoid this? \n\n- Quality of figures.  Figures are very hard to read because of small font.  Also, the captions need to describe more details about the figures.",1,1,1,1,1,1,1,1,1,-1
SyL9u-WA--R3,"The paper introduces SVD parameterization and uses it mostly for controlling the spectral norm of the RNN.  \n\nMy concerns with the paper include: \n\na) the paper says that the same method works for convolutional neural networks but I couldn't find anything about convolution.  \n\nb) the theoretical analysis might be misleading --- clearly section 6.2 shouldn't have title ALL CRITICAL POINTS ARE GLOBAL MINIMUM because 0 is a critical point but it's not a global minimum.  Theorem 5 should be phrased as \n\nall critical points of the population risk that is non-singular are global minima. \n\nc) the paper should run some experiments on language applications where RNN is widely used \n\nd) I might be wrong on this point, but it seems that the GPU utilization of the method would be very poor so that it's kind of impossible to scale to large datasets? \n",1,1,1,1,1,-1,1,1,-1,-1
SyMvJrdaW-R1,"Motivated via Talor approximation of the Residual network on a local minima, this paper proposed a warp operator that can replace a block of a consecutive number of residual layers.  While having the same number of parameters as the original residual network, the new operator has the property that the computation can be parallelized.  As demonstrated in the paper, this improves the training time with multi-GPU parallelization, while maintaining similar performance on CIFAR-10 and CIFAR-100. \n\nOne thing that is currently not very clear to me is about the rotational symmetry.  The paper mentioned rotated filters, but continue to talk about the rotation in the sense of an orthogonal matrix applying to the weight matrix of a convolution layer.   The rotation of the filters (as 2D images or images with depth) seem to be quite different from \""rotating\"" a general N-dim vectors in an abstract Euclidean space.   It would be helpful to make the description here more explicit and clear.",1,1,1,1,1,-1,1,1,-1,-1
SyMvJrdaW-R2,"Paper proposes a shallow model for approximating stacks of Resnet layers, based on mathematical approximations to the Resnet equations and experimental insights, and uses this technique to train Resnet-like models in half the time on CIFAR-10 and CIFAR-100.  While the experiments are not particularly impressive,  I liked the originality of this paper.",1,1,-1,1,-1,-1,1,-1,1,-1
SyMvJrdaW-R3,"The main contribution of this paper is a particular Taylor expansion of the outputs of a ResNet which is shown to be exact at almost all points in the input space.   This expression is used to develop a new layer called a \u201cwarp layer\u201d which essentially tries to compute several layers of the residual network using the Taylor expansion expression \u2014 however in this expression, things can be done in parallel, and interestingly, the authors show that the gradients also decouple when the (ResNet) model is close to a local minimum in a certain sense, which may motivate the decoupling of layers to begin with.   Finally the authors stack these warp layers to create a \u201cwarped resnet\u201d which they show does about as well as an ordinary ResNet but has better parallelization properties. \n\nTo me the analytical parts of the paper are the most interesting, particularly in showing how the gradients approximately decouple.   However there are several weaknesses to the paper (or maybe just things I didn\u2019t understand).   First,  a major part of the paper tries to make the case that there is a symmetry breaking property of the proposed model, which I am afraid I simply was not able to follow.   Some of the notation is confusing here \u2014 for example, presumably the rotations refer to image level rotations rather than literally multiplying the inputs by an orthogonal matrix, which the notation suggests to be the case.   It is also never precisely spelled out what the final theoretical guarantee is (preferably the authors would do this in the form of a proposition or theorem). \n\nThroughout, the authors write out equations as if the weights in all layers are equal, but this is confusing even if the authors say that this is what they are doing, since their explanation is not very clear.   The confusion is particularly acute in places where derivatives are taken, because the derivatives continue to be taken as if the weights were untied, but then written as if they happened to be the same. \n\nFinally the experimental results are okay  but perhaps a bit preliminary.   I have a few recommendations here:\n* It would be stronger to evaluate results on a larger dataset like ILSVRC.   \n* The relative speed-up of WarpNet compared to ResNet needs to be better explained \u2014 the authors break the computation of the WarpNet onto two GPUs, but it\u2019s not clear if they do this for the (vanilla) ResNet as well.   In batch mode, the easiest way to parallelize is to have each GPU evaluate half the batch.   Even in a streaming mode where images need to be evaluated one by one, there are ways to pipeline execution of the residual blocks, and I do not see any discussion of these alternatives in the paper. \n* In the experimental results, K is set to be 2, and the authors only mention in passing that they have tried larger K in the conclusion.   It would be good to have a more thorough experimental evaluation of the trade-offs of setting K to be higher values. \n\nA few remaining questions for the authors:\n* There is a parallel submission (presumably by different authors called \u201cResidual Connections Encourage Iterative Inference\u201d) which contains some related insights.   I wonder what are the differences between the two Taylor expansions, and whether the insights of this paper could be used to help the other paper and vice versa? \n* On implementation - the authors mention using Tensorflow\u2019s auto-differentiation.   My question here is \u2014 are gradients being re-used intelligently as suggested in Section 3.1?   \n* I notice that the analysis about the vanishing Hessian could be applied to most of the popular neural network architectures available now.   How much of the ideas offered in this paper would then generalize to non-resnet settings?\n\n",1,1,1,1,1,1,1,1,1,-1
SyProzZAW-R1,"Experimental results have shown that deep networks (many hidden layers) can approximate more complicated functions with less neurons compared to shallow (single hidden layer) networks.  \nThis paper gives an explicit proof when the function in question is a sparse polynomial, ie: a polynomial in n variables, which equals a sum J of monomials of degree at most c.  \nIn this setup, Theorem 4.3 says that a shallow network need at least ~ (1 + c/n)^n many neurons, while the optimal deep network (whose depth is optimized to approximate this particular input polynomial) needs at most  ~ J*n, that is, linear in the number of terms and the number of variables.  The paper also has bounds for neural networks of a specified depth k (Theorem 5.1), and the authors conjecture this bound to be tight (Conjecture 5.2).  \n\nThis is an interesting result, and is an improvement over Lin 2017 (where a similar bound is presented for monomial approximation).  \nOverall, I like the paper. \n\nPros: new and interesting result, theoretically sound.  \nCons: nothing major. \nComments and clarifications:\n* What about the ability of a single neural network to approximate a class of functions (instead of a single p), where the topology is fixed but the network weights are allowed to vary?  Could you comment on this problem? \n* Is the assumption that \\sigma has Taylor expansion to order d tight?  (That is, are there counter examples for relaxations of this assumption?)  \n* As noted, the assumptions of your theorems 4.1-4.3 do not apply to ReLUs,  Could you provide some further comments on this?\n\n",1,1,1,1,1,1,1,1,1,-1
SyProzZAW-R2,"The paper investigates the representation of polynomials by neural networks up to a certain degree and implied uniform approximations.  It shows exponential gaps between the width of shallow and deep networks required for approximating a given sparse polynomial.  \n\nBy focusing on polynomials, the paper is able to use of a variety of tools (e.g. linear algebra) to investigate the representation question.  Results such as Proposition 3.3 relate the representation of a polynomial up to a certain degree, to the approximation question.  Here it would be good to be more specific about the domain, however, as approximating the low order terms certainly does not guarantee a global uniform approximation.  \n\nTheorem 3.4 makes an interesting claim, that a finite network size is sufficient to achieve the best possible approximation of a polynomial (the proof building on previous results, e.g. by Lin et al that I did not verify).  The idea being to construct a superposition of Taylor approximations of the individual monomials.  Here it would be good to be more specific about the domain.  Also, in the discussion of Taylor series, it would be good to mention the point around which the series is developed, e.g. the origin.  \n\nThe paper mentions that ``the theorem is false for rectified linear units (ReLUs), which are piecewise linear and do not admit a Taylor series''.  However, a ReLU can also be approximated by a smooth function and a Taylor series.  \n\nTheorem 4.1 seems to be implied by Theorem 4.2.  Similarly, parts of Section 4.2 seem to follow directly from the previous discussion.  \n\nIn page 1 ```existence proofs' without explicit constructions'' This is not true, with numerous papers providing explicit constructions of functions that are representable by neural networks with specific types of activation functions. \n\n",1,1,1,1,1,-1,1,1,1,-1
SyProzZAW-R3,"Summary and significance: The authors prove that for expressing simple multivariate monomials over n variables, networks of depth 1 require exp(n) many neurons, whereas networks of depth n can represent these monomials using only O(n) neurons.  \nThe paper provides a simple and clear explanation for the important problem of theoretically explaining the power of deep networks, and quantifying the improvement provided by depth. \n\n+ves:\nExplaining the power of depth in NNs is fundamental to an understanding of deep learning.  The paper is very easy to follow.  and the proofs are clearly written.  The theorems provide exponential gaps for very simple polynomial functions. \n\n-ves:\n1. My main concern with the paper is the novelty of the contribution to the techniques.  The results in the paper are more general than that of Lin et al., but the proofs are basically the same, and it's difficult to see the contribution of this paper in terms of the contributing fundamentally new ideas.  \n2. The second concern is that the results apply only to non-linear activation functions with sufficiently many non-zero derivatives (same requirements as for the results of Lin et al.). \n3. Finally, in prop 3.3, reducing from uniform approximations to Taylor approximations, the inequality |E(\u03b4x)| <= \u03b4^(d+1) |N(x) - p(x)| does not follow from the definition of a Taylor approximation. \n\nDespite these criticisms, I contend that the significance of the problem, and the clean and understandable results in the paper make it a decent paper for ICLR.",1,1,1,1,-1,1,1,1,1,-1
SyqShMZRb-R1,"The paper presents an approach for improving variational autoencoders for structured data that provide an output that is both syntactically valid and semantically reasonable.   The idea presented seems to have merit ,  however, I found the presentation lacking.  Many sentences are poorly written making the paper hard to read, especially when not familiar with the presented methods.  The experimental section could be organized better.  I didn't like that two types of experiment are now presented in parallel.  Finally, the paper stops abruptly without any final discussion and/or conclusion.",1,1,1,1,1,-1,1,1,-1,-1
SyqShMZRb-R2,"Let me first note that I am not very familiar with the literature on program generation, \nmolecule design or compiler theory, which this paper draws heavily from, so my review is an educated guess.  \n\nThis paper proposes to include additional constraints into a VAE which generates discrete sequences, \nnamely constraints enforcing both semantic and syntactic validity.  \nThis is an extension to the Grammar VAE of Kusner et. al, which includes syntactic constraints but not semantic ones. \nThese semantic constraints are formalized in the form of an attribute grammar, which is provided in addition to the context-free grammar. \nThe authors evaluate their methods on two tasks, program generation and molecule generation.  \n\nTheir method makes use of additional prior knowledge of semantics, which seems task-specific and limits the generality of their model.  \nThey report that their method outperforms the Character VAE (CVAE) and Grammar VAE (GVAE) of Kusner et. al.   \nHowever, it isn't clear whether the comparison is appropriate: the authors report in the appendix that they use the kekulised version of the Zinc dataset of Kusner et. al, whereas Kusner et. al do not make any mention of this.  \nThe baselines they compare against for CVAE and GVAE in Table 1 are taken directly from Kusner et. al though.  \nCan the authors clarify whether the different methods they compare in Table 1 are all run on the same dataset format? \n\nTypos:\n- Page 5: \""while in sampling procedure\"" -> \""while in the sampling procedure\"" \n- Page 6: \""a deep convolution neural networks\"" -> \""a deep convolutional neural network\"" ""\n- Page 6: \""KL-divergence that proposed in\"" -> \""KL-divergence that was proposed in\"" "" \n- Page 6: \""since in training time\"" -> \""since at training time\"""" \n- Page 6: \""can effectively computed\"" -> \""can effectively be computed\"""" \n- Page 7: \""reset for training\"" -> \""rest for training\"" """,1,1,1,1,1,1,1,1,-1,-1
SyqShMZRb-R3,"NOTE: \n\nWould the authors kindly respond to the comment below regarding Kekulisation of the Zinc dataset?  Fair comparison of the data is a serious concern.  I have listed this review as a good for publication due to the novelty of ideas presented,  but the accusation of misrepresentation below is a serious one and I would like to know the author's response. \n\n*Overview*\n\nThis paper presents a method of generating both syntactically and semantically valid data from a variational autoencoder model using ideas inspired by compiler semantic checking. Instead of verifying the semantic correctness offline of a particular discrete structure, the authors propose \u201cstochastic lazy attributes\u201d, which amounts to loading semantic constraints into a CFG and using a tailored latent-space decoder algorithm that guarantees both syntactic semantic valid.  Using Bayesian Optimization, search over this space can yield decodings with targeted properties. \n\nMany of the ideas presented are novel.  The results presented are state-of-the art.  As noted in the paper, the generation of syntactically and semantically valid data is still an open problem.  This paper presents an interesting and valuable solution, and as such constitutes a large advance in this nascent area of machine learning. \n\n*Remarks on methodology*\n\nBy initializing a decoding by \u201cguessing\u201d a value, the decoder will focus on high-probability starting regions of the space of possible structures.  It is not clear to me immediately how this will affect the output distribution.  Since this process on average begins at high-probability region and makes further decoding decisions from that starting point, the output distribution may be biased since it is the output of cuts through high-probability regions of the possible outputs space.  Does this sacrifice exploration for exploitation in some quantifiable way?  Some exploration of this issue or commentary would be valuable.  \n\n*Nitpicks*\n\nI found the notion of stochastic predetermination somewhat opaque, and section 3 in general introduces much terminology, like lazy linking, that was new to me coming from a machine learning background.  In my opinion, this section could benefit from a little more expansion and conceptual definition. \n\nThe first 3 sections of the paper are very clearly written,  but the remainder has many typos and grammatical errors (often word omission).  The draft could use a few more passes before publication.\n",1,1,1,1,1,-1,1,1,1,-1
Syr8Qc1CW-R1,"This paper proposes to disentangle attributes by forcing a representation where individual components of this representation account for individual attributes.  \n\nPros: \n+ The idea of forcing different parts of the latent representation to be responsible for different attributes appears novel.  \n+ A theoretical guarantee of the efficiency of an aspect of the proposed method is given. \n\nCons: \n- The results are not very appealing visually.  The results from the proposed method do not seem much better than the baselines. What is the objective for the images in Fig. 4?  For example I'm looking at the bottom right, and that image looks more like a merger of images, than a modification of the image in the top-left but adding the attributes of choice. \n- Quantitative results are missing.  \n- Some unclarity in the description of the method; see below. \n\nQuestions/other:\n- What is meant by \""implicit\"" models?  By \""do not anchor a specific meaning into the disentanglement\""?  By \""circumscribed in two image domains\""?   \n- Why does the method require two images?  \n- In the case of images, what is a dominant vs recessive pattern?  \n- It seems artificial to enforce that \""the attribute-irrelevant part [should] encode some information of images\"".  \n- Why are (1, 0) and (1, 1) not useful pairs? \n- Need to be more specific: \""use some channels to encode the id information\"". \n",1,1,1,1,1,-1,1,1,1,-1
Syr8Qc1CW-R2,"Summary:\nThis paper investigated the problem of attribute-conditioned image generation using generative adversarial networks.  More specifically, the paper proposed to generate images from attribute and latent code as high-level representation.  To learn the mapping from image to high-level representations, an auxiliary encoder was introduced.  The model was trained using a combination of reconstruction (auto-encoding) and adversarial loss.  To further encourage effective disentangling (against trivial solution), an annihilating operation was proposed together with the proposed training pipeline.   Experimental evaluations were conducted on standard face image databases such as Multi-PIE and CelebA.  \n\n== Novelty and Significance ==\nMulti-attribute image generation is an interesting task but has been explored to some extent.  The integration of generative adversarial networks with auto-encoding loss is not really a novel contribution.  \n-- Autoencoding beyond pixels using a learned similarity metric. Larsen et al., In ICML 2016. \n\n== Technical Quality == \nFirst, it is not clear how was the proposed annihilating operation used in the experiments (there is no explanation in the experimental section).  Based on my understanding, additional loss was added to encourage effective disentangling (prevent trivial solution).  I would appreciate the authors to elaborate this a bit. \n\nSecond, the iterative training (section 3.4) is not a novel contribution since it was explored in the literature before (e.g., Inverse Graphics network).  The proof developed in the paper provides some theoretical analysis but cannot be considered as a significant contribution. \n\nThird, it seems that the proposed multi-attribute generation pipeline works for binary attribute only.  However, such assumption limits the generality of the work.  Since the title is quite general, I would assume to see the results (1) on datasets with real-valued attributes, mixture attributes or even relative attributes  and (2) not specific to face images. \n-- Learning to generate chairs with convolutional neural networks. Dosovitskiy et al., In CVPR 2015.\n-- Deep Convolutional Inverse Graphics Network. Kulkarni et al., In NIPS 2015.\n-- Attribute2Image: Conditional Image Generation from Visual Attributes. Yan et al., In ECCV 2016. \n-- InfoGAN: Interpretable Representation Learning by Information Maximizing Generative Adversarial Nets. Chen et al., In NIPS 2016.Yan et al., In ECCV 2016. \n\nAdditionally, considering the generation quality, the CelebA samples in the paper are not the state-of-the-art.  I suspect the proposed method only works in a more constrained setting (such as Multi-PIE where the images are all well aligned). \n\nOverall, I feel that the submitted version is not ready for publication in the current form.\n""",1,1,1,1,1,1,1,1,1,-1
Syr8Qc1CW-R3,"Pros:\n1. A new DNA structure GAN is utilized to manipulate/disentangle attributes. \n\n2. Non attribute part (Z) is explicitly modeled in the framework. \n\n3. Based on the experiment results, this proposed method outperformed previous methods (TD-GAN, IcGAN). \n\nCons:\n1. It assumes that each individual piece represents an independent factor of variation, which can not hold all the time.  The authors also admit that when two factors are dependent, this method might fail. \n\n2. In Lreconstruct, only min difference between A and A1 is considered.  How about A and A2 here?  It seems that A2 should also be similar with A since only one bit in A2 and A1 is different. \n\n3. Only one attribute can be \""manipulated\"" each time?  Is it possible to change more than one attribute each time in this method?",1,1,1,1,1,-1,1,1,1,-1
SyrGJYlRZ-R1,"This paper proposes a method to automatically tuning the momentum parameter in momentum SGD methods, which achieves better results and fast convergence speed than state-of-the-art Adam algorithm. \n\nAlthough the results are promising,  I found the presentation of this paper almost inaccessible to me. \n\nFirst, though a minor point, but where does the name *YellowFin* come from? \n\nFor the presentation, the motivation in introduction is fine,  but the following section about momentum operator is hard to follow.  There are a lot of undefined notation.  For example, what does the *convergence rate* mean (what is the measurement for convergence)?  And is the *optimal accelerated rate* the same as *convergence rate* mentioned above?  Also, what do you mean by *all directions* in the sentence below eq.2? \n\nThen the paper talks about robustness properties of the momentum operator.  But: first, I am not sure why the derivative of f(x) is defined as in eq.3, how is that related to the original definition of derivative? \n\nIn the following paragraph, what is *contraction*?  Does it have anything to do with the paper as I didn't see it in the remaining text? \n\nLemma 2 seems to use the spectral radius of the momentum operator as the *robustness*.  But how can it describe the robustness?  More details are needed to understand this. \n\nWhat it comes to Section 3, it seems to me that the authors try to use a local quadratic approximation for the original function f(x), and use the results in last section to find the optimal momentum parameter.  I got confused in this section because eq.9 defines f(x) as a quadratic function.  Is this f(x) the original function (non quadratic) or just the local quadratic approximation?  If it is the local quadratic approximation, how is it correlated to the original function?  It seems to me that the authors try to say if h and C are calculated from the original function, then this f(x) is a local quadratic approximation?  If what I think is correct, I think it would be important to show this. \n\nAlso, the objective function in SingleStep algorithm seems to come from eq.13, but I failed to get the exact reasoning. \n\nOverall, I think this is an interesting paper,  but the presentation is too fuzzy to get it evaluated.",1,1,1,1,1,-1,1,1,-1,-1
SyrGJYlRZ-R2,"The paper explores momentum SGD and an adaptive version of momentum SGD which the authors name YF (Yellow Fin).  They compare YF to hand tuned momentumSGD and to Adam in several deep learning applications. \n\n\nI found the first part which discusses the theoretical motivation behind YF to be very confusing and misleading:\nBased on the analysis of 1-dimensional problems, the authors design a framework and an algorithm that  supposedly ensures accelerated convergence.  There are two major problems with this approach: \n\n-First: Exploring 1-dim functions is indeed a nice way to get some intuition.  Yet,  algorithms that work in the 1-dim case do not trivially generalize to high dimensions, and such reasoning might lead to very bad solutions. \n\n-Second: Accelerated GD does not benefit over GD in the 1-dim case. And therefore, this is not an appropriate setting to explore acceleration.\nConcretely, the definition of the generalized condition number $\\nu$, and relating it to the standard definition of the condition number $\\kappa$, is very misleading.  This is since $\\kappa =1$ for 1-dim problems, and therefore accelerated GD does not have any benefits over non accelerated GD in this case. \nHowever, $\\nu$ might be much larger than 1 even in the 1-dim case. \n\n\nRegarding the algorithm itself: there are too many hyper-parameters (which depend on each other) that are tuned (per-dimension). \nAnd as I have mentioned, the design of the algorithm is inspired by the analysis of 1-dim quadratic functions. \nThus, it is very hard for me to believe that this algorithm works in practice unless very careful fine tuning is employed. \nThe authors mention that their experiments were done without tuning or with very little tuning, which is very mysterious for me. \n\nIn contrast to the theoretical part, the experiments seems very encouraging.  Showing YF to perform very well on several deep learning tasks without (or with very little) tuning.  Again, this seems a bit magical or even too good to be truth.  I suggest the authors to perform a experiment with say a qaudratic high dimensional function, which is not aligned with the axes in order to illustrate how their method behaves and try to give intuition.\n",1,1,1,1,1,-1,1,1,1,-1
SyrGJYlRZ-R3,"[Apologies for short review, I got called in late.  Marking my review as \""educated guess\"" since i didn't have time for a detailed review] \n\nThe paper proposes an algorithm to tune the momentum and learning rate for SGD.  While the algorithm does not have a theory for general non-quadratic functions,  experimental validation is extensive, making it a worthy contribution in my opinion.  I have personally tried the algorithm when the paper came out and can vouch for the empirical results presented here.",1,1,-1,1,-1,-1,1,1,1,-1
SySaJ0xCZ-R1,"This paper proposes a neural architecture search method that achieves close to state-of-the-art accuracy on CIFAR10 and takes much less computational resources.  The high-level idea is similar to the evolution method of [Real et al. 2017], but the mutation preserves net2net properties, which means the mutated network does not need to retrain from scratch. \n\nCompared to other papers on neural architecture search, the required computational resource is impressively small: close to state-of-the-art result in one day on a single GPU.  However, it is not clear to me what contribute to the massive improvement of speed.  Is it due to the network morphing that preserve equality?  Is it due to a good initial network structure?  Is it due to the well designed mutation operations?  Is it due to the simple hill climbing procedure (basically evolution that only preserve the elite)?  Is it due to a well crafted search space that is potentially easier? \n\nThe experiments in this paper does not provide enough evidence to tease apart the possible causes of this dramatic reduction on computational resources.  And the comparisons to other papers seems not fair since they all operate on different search space.  \n\nIn summary, getting net2net to work for architecture search is interesting.  And I love the results.  These are very impressive numbers for neural architecture search.  However, I am not convinced that the improve is resulted from a better algorithm.  I would suggest that the paper carefully evaluates each component of the algorithm and understand why the proposed method takes far less computational resources.",1,1,1,1,1,1,1,1,-1,-1
SySaJ0xCZ-R2,"This paper presents a method to search neural network architectures at the same time of training.  It does not require training from scratch for each architecture, thus dramatically saves the training time.  The paper can be understood with no problem.   Moderate novelty,  network morphism is not novel,  applying it to architecture search is novel. \n\nPros:\n1. The required time for architecture searching is significantly reduced. \n2. With the same number or less of parameters, this method is able to outperform previous methods, with much less time. \n\nHowever, the method described is restricted in the following aspects. \n\n1. The accuracy of the training set is guaranteed to ascend because network morphism is smooth and number of params is always increasing, this also makes the search greedy , which could be suboptimal.  In addition, the algorithm in this paper selects the best performing network at each step, which also hampers the discover of the optimal model.\ n\n2. Strong human prior, network morphism IV is more general than skip connection, for example, a two column structure belongs to type IV.  However, in the implementation, it is restricted to skip connection by addition.  This choice could be motivated from the success of residual networks.  This limits the method from discovering meaningful structures.  For example, it is difficult to discover residual network denovo.  This is a common problem of architecture searching methods compared to handcrafted structures. \n\n3. The comparison with Zoph & Le is not fair because their controller is a meta-network and the training happens only once.  For example, the RNNCell discovered can be fixed and used in other tasks, and the RNN controller for CNN architecture search could potentially be applied to other tasks too (though not reported).\n",1,1,1,1,-1,1,1,1,1,-1
SySaJ0xCZ-R3,"This paper proposes a variant of neural architecture search.   It uses established work on network morphisms as a basis for defining a search space.   Experiments search for effective CNN architectures for the CIFAR image classification task. \n\nPositives:\n\n(1) The approach is straightforward to implement and trains networks in a reasonable amount of time. \n\n(2) An advantage over prior work, this approach integrates architectural evolution with the training procedure.   Networks are incrementally grown; child networks are initialized with learned parameters from their parents.   This eliminates the need to restart training when making an architectural change, and drastically speeds the search. \n\nNegatives:\n\n(1) The state-of-the-art CNN architectures are not mysterious or difficult to find, despite the paper's characterization of them being so.   Indeed, ResNet and DenseNet designs are both guided by extremely simple principles: stack a series of convolutional layers, pool occasionally, and use some form of skip-connection throughout.   The need for architectural search is unclear. \n\n(2) The proposed search space is boring.   As described in Section 4, the possibly evolutionary changes are limited to deepening the network, widening the network, and adding a skip connection.   But these are precisely the design aspects that have been well-explored by human trial and error and for which good rules of thumb are already available. \n\n(3) As a consequence of (1) and (2), the result is essentially rigged.   Since only depth, width, and skip connections are considered, the end network must end up looking like a ResNet or DenseNet, but with some connections pruned.   There is no way to discover a network outside of the principled design space articulated in point (1) above.   Indeed, the discovered network diagrams (Figures 4 and 5) fall in this space. \n\n(4) Performance is worse than the best hand-designed baselines.   One would hope that, even if the search space is limited, the discovered networks might be more efficient or higher performing in comparison to the human designs which fall within that same space.   However, the results in Tables 3 and 4 show this not to be the case.   The best human designs outperform the evolved networks.   Moreover, the evolved networks are woefully inefficient in terms of parameter count. \n\nTogether, these negatives imply the proposed approach is not yet at the point of being useful in practice.   I think further work is required (perhaps expanding the search space) to resolve the current limitations of automated architecture search. \n\nMisc:\n\nTables 3 and 4 would be easier to parse if resources were simply reported in terms of total GPU hours.",1,1,1,1,1,1,1,1,1,-1
SysEexbRb-R1,"Authors of this paper provided full characterization of the analytical forms of the critical points for the square loss function of three types of neural networks: shallow linear networks, deep linear networks and shallow ReLU nonlinear networks.  The analytical forms of the critical points have direct implications on the values of the corresponding loss functions, achievement of global minimum, and various landscape properties around these critical points. \n\nThe paper is well organized and well written.  Authors exploited the analytical forms of the critical points to provide a new proof for characterizing the landscape around the critical points.  This technique generalizes existing work under full relaxation of assumptions.  In the linear network with one hidden layer, it generalizes the work Baldi & Hornik (1989) with arbitrary network parameter dimensions and any data matrices;  In the deep linear networks, it generalizes the result in Kawaguchi (2016) under no assumptions on the network parameters and data matrices.  Moreover, it also provides new characterization for shallow ReLU nonlinear networks, which is not discussed in previous work. \n\nThe results obtained from the analytical forms of the critical points are interesting,  but one problem is that how to obtain the proper solution of equation (3)?  In the Example 1, authors gave a concrete example to demonstrate both local minimum and local maximum do exist in the shallow ReLU nonlinear networks by properly choosing these matrices satisfying (12).  It will be interesting to see how to choose these matrices for all the studied networks with some concrete examples.",1,1,1,1,1,1,1,1,1,-1
SysEexbRb-R2,"This paper studies the critical points of shallow and deep linear networks.  The authors give a (necessary and sufficient) characterization of the form of critical points and use this to derive necessary and sufficient conditions for which critical points are global optima.  Essentially this paper revisits a classic paper by Baldi and Hornik (1989) and relaxes a few requires assumptions on the matrices.  I have not checked the proofs in detail but the general strategy seems sound.  While the exposition of the paper can be improved in my view this is a neat and concise result and merits publication in ICLR.  The authors also study the analytic form of critical points of a single-hidden layer ReLU network.  However, given the form of the necessary and sufficient conditions the usefulness of of these results is less clear. \n\n\nDetailed comments:\n\n- I think in the title/abstract/intro the use of Neural nets is somewhat misleading as neural nets are typically nonlinear.  This paper is mostly about linear networks.  While a result has been stated for single-hidden ReLU networks.  In my view this particular result is an immediate corollary of the result for linear networks.  As I explain further below given the combinatorial form of the result, the usefulness of this particular extension to ReLU network is not very clear.  I would suggest rewording title/abstract/intro \n\n- Theorem 1 is neat, well done! \n\n- Page 4 p_i\u2019s in proposition 1\nFrom my understanding the p_i have been introduced in Theorem 1 but given their prominent role in this proposition they merit a separate definition (and ideally in terms of the A_i directly).  \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tAre these characterizations computable i.e. given X and Y can one run an algorithm to find all the critical points or at least the parameters used in the characterization p_i, V_i etc? \n\n- Theorems 1, prop 1, prop 2, prop 3, Theorem 3, prop 4 and 5\n\tWould recommend a better exposition why these theorems are useful.  What insights do you gain by knowing these theorems etc.  Are less sufficient conditions that is more intuitive or useful(an insightful sufficient condition in some cases is much more valuable than an unintuitive necessary and sufficient one). \n\n- Page 5 Theorem 2\n\tDoes this theorem have any computational implications?  Does it imply that the global optima can be found efficiently, e.g. are saddles strict with a quantifiable bound? \n\n- Page 7 proposition 6 seems like an immediate consequence of Theorem 1 however given the combinatorial nature of the K_{I,J} it is not clear why this theorem is useful.  e.g . back to my earlier comment w.r.t. Linear networks given Y and X can you find the parameters of this characterization with a computationally efficient algorithm? \n",1,1,1,1,1,-1,1,1,1,-1
SysEexbRb-R3,"This paper mainly focuses on the square loss function of linear networks.  It provides the sufficient and necessary characterization for the forms of critical points of one-hidden-layer linear networks.  Based on this characterization, the authors are able to discuss different types of non-global-optimal critical points and show that every local minimum is a global minimum for one-hidden-layer linear networks.  As an extension, the manuscript also characterizes the analytical forms for the critical points of deep linear networks and deep ReLU networks, although only a subset of non-global-optimal critical points are discussed.  In general, this manuscript is well written.    \n\nPros:\n1. This manuscript provides the sufficient and necessary characterization of critical points for deep networks.  \n2. Compared to previous work, the current analysis for one-hidden-layer linear networks doesn\u2019t require assumptions on parameter dimensions and data matrices.  The novel analyses, especially the technique to characterize critical points and the proof of item 2 in Proposition 3, will probably be interesting to the community. \n3. It provides an example when a local minimum is not global for a one-hidden-layer neural network with ReLU activation. \n\nCons:\n1. I'm concerned that the contribution of this manuscript is a little incremental.  The equivalence of global minima and local minima for linear networks is not surprising based on existing works e.g. Hardt & Ma (2017) and Kawaguchi (2016).   \n2. Unlike one-hidden-layer linear networks, the characterizations of critical points for deep linear networks and deep ReLU networks seem to be hard to be interpreted.  This manuscript doesn't show that every local minimum of these two types of deep networks is a global minimum, which actually has been shown by existing works like Kawaguchi (2016) with some assumptions.  The behaviors of linear networks and practical (deep and nonlinear) networks are very different.  Under such circumstance, the results about one-hidden-layer linear networks are less interesting to the deep learning community. \n\nMinors:\nThere are some mixed-up notations: tilde{A_i} => A_i , and rank(A_2) => rank(A)_2 in Proposition 3.",1,1,1,1,-1,1,1,1,1,-1
SySisz-CW-R1,"This paper examines the nature of convolutional filters in the encoder and a decoder of a VAE, and a generator and a discriminator of a GAN.  The authors treat the inputs (X) and outputs (Y) of each filter throughout each step of the convolving process as a time series, which allows them to do a Discrete Time Fourier Transform analysis of the resulting sequences.  By comparing the power spectral density of the input and the output, they get a Spectral Dependency Ratio (SDR) ratio that characterises a filter as spectrally independent (neutral), correlating (amplifies certain frequencies), or anti-correlating (dampens frequencies).  This analysis is performed in the context of the Independence of Cause and Mechanism (ICM) framework.  The authors claim that their analysis demonstrates a different characterisation of the inference/discriminator and generative networks in VAE and GAN, whereby the former are anti-causal and the latter are causal in line with the ICM framework.  They also claim that this analysis can be used to improve the performance of the models. \n\nPros:\n-- SDR characterisation of the convolutional filters is interesting \n-- The authors show that filters with different characteristics are responsible for different aspects of image modelling \n\nCons:\n-- The authors do not actually demonstrate how their analysis can be used to improve VAEs or GANs \n-- Their proposed SDR analysis does not actually find much difference between the generator and the discriminator of the GAN  \n-- The clarity of the writing could be improved (e.g. the discussion in section 3.1 seems inaccurate in the current form).  Grammatical and spelling mistake are frequent.  More background information could be helpful in section 2.2.  All figures (but in particular Figure 3) need more informative captions \n-- The authors talk a lot about disentangling in the introduction, but this does not seem to be followed up in the rest of the text.  Furthermore, they are missing a reference to beta-VAE (Higgins et al, 2017) when discussing VAE-based approaches to disentangled factor learning \n\n\nIn summary, the paper is not ready for publication in its current form.  The authors are advised to use the insights from their proposed SDR analysis to demonstrate quantifiable improvements the VAEs/GANs.",1,1,1,1,1,1,1,1,1,-1
SySisz-CW-R2,"This work exploits the causality principle to quantify how the weights of successive layers adapt to each other.   Some interesting results are obtained, such as \""enforcing more independence between successive layers of generators may lead to better performance and modularity of these architectures\"" .  Generally, the result is interesting and the presentation is easy to follow.  However, the proposed approach and the experiments are not convincible enough.   For example,  it is hard to obtain the conclusion \""more independence lead to better performance\"" from the experimental results.  Maybe more justifications are needed.",1,1,1,1,1,-1,1,1,1,-1
SySisz-CW-R3,"The paper presents an application of a measure of dependence between the input power spectrum and the frequency response of a filter (Spectral Density Ratio from [Shajarisales et al 2015]) to cascades of two filters in successive layers of deep convolutional networks.  The authors apply their newly defined measure to DCGANs and plain VAEs with ReLUs, and show that dependency between successive layers may lead to bad performance.  \n\nThe paper proposed a possibly interesting approach,  but I found it quite hard to follow, especially Section 4, which I thought was quite unstructured.  Also Section 3 could be improved and simplified.  It would be also good to add some more related work.  I\u2019m not an expert, but I assume there must be some similar idea in CNNs.  \n\nFrom my limited point of view, this seems like a sound, novel and potentially useful application of a interesting idea.  If the writing was improved, I think the paper may have even more impact. \n\nSmaller details: some spacing issues, some extra punctuation (pg 5 \u201c. . Hence\u201d), a typo (pg. 7 \u201ctraining of the VAE did not lead to values as satisfactory AS what we obtained with the GAN\u201d)\n",1,1,1,1,1,1,1,1,1,-1
Syt0r4bRZ-R1,"Summary: the paper proposes a tree2tree architecture for NLP tasks.  Both the encoder and decoder of this architecture make use of memory cells: the encoder looks like a tree-lstm to encode a tree bottom-up, the decoder generates a tree top-down by predicting the number of children first.  The objective function is a linear mixture of the cost of generating the tree structure and the target sentence.   The proposed architecture outperforms recursive autoencoder on a self-to-self predicting trees, and outperforms an lstm seq2seq on En-Cn translation.  \n\nComment:\n\n- The idea of tree2tree has been around recently but it is difficult to make it work.  I thus appreciate the authors\u2019 effort.  However, I wish the authors would have done it more properly. \n- The computation of the encoder and decoder is not novel.  I was wondering how the encoder differs from tree-lstm.  The decoder predicts the number of children first, but the authors don\u2019t explain why they do that, nor compare this to existing tree generators.  \n- I don\u2019t understand the objective function (eq 4 and 5).  Both Ls are not cross-entropy because label and childnum are not probabilities.  I also don\u2019t see why using Adam is more convenient than using SGD. \n- I think eq 9 is incorrect, because the decoder is not Markovian.  To see this we can look at recurrent neural networks for language modeling: generating the current word is conditioning on the whole history (not only the previous word). \n- I expect the authors would explain more about how difficult the tasks are (eg. some statistics about the datasets), how to choose values for lambda, what the contribution of the new objective is. \n\nAbout writing:\n- the paper has so many problems with wording, e.g. articles, plurality. \n- many terms are incorrect, e.g. \u201cdependent parsing tree\u201d (should be \u201cdependency tree\u201d), \u201cconsistency parsing\u201d (should be \u201cconstituency parsing\u201d) \n- In 3.1, Socher et al. do not use lstm \n- I suggest the authors to do some more literature review on tree generation\n",1,-1,1,1,1,1,1,1,1,-1
Syt0r4bRZ-R2,"This paper proposes a tree-to-tree model aiming to encode an input tree into embedding and then decode that back to a tree.  The contributions of the work are very limited.   Basic attention models, which have been shown to help model structures, are not included (or compared).  Method-wise, the encoder is not novel and decoder is rather straightforward.  The contributions of the work are in general very limited.  Moreover, this manuscript contains many grammatical errors.   In general, it is not ready for publication.  \n\nPros:\n- Investigating the ability of distributed representation in encoding input structured is in general interesting.  Although there have been much previous work, this paper is along this line. \n\nCons:\n- The contributions of the work are very limited.  For example, attention, which have been widely used and been shown to help capture structures in many tasks, are not included and compared in this paper. \n- Evaluation is not very convincing.  The baseline performance in MT is too low.  It is unclear if the proposed model is still helpful when other components are considered (e.g., attention).  \n- For the objective function defined in the paper, it may be hard to balance the \""structure loss\"" and \""content loss\"" in different problems, and moreover, the loss function may not be even useful in real tasks (e.g, in MT), which often have their own objectives (as discussed in this paper).  Earlier work on tree kernels (in terms of defining tree distances) may be related to this work.  \n- The manuscript is full of grammatical errors, and the following are some of them:\n\""encoder only only need to\""\n\""For for tree reconstruction task\"" \n\""The Socher et al. (2011b) propose a basic form \""\n\""experiments and theroy analysis are done\""\n",1,1,1,1,-1,1,1,1,1,-1
Syt0r4bRZ-R3,"This paper presents a model to encode and decode trees in distributed representations.  \nThis is not the first attempt of doing these encoders and decoders.  However, there is not a comparative evalution with these methods. \nIn fact, it has been demonstrated that it is possible to encode and decode trees in distributed structures without learning parameters, see \""Decoding Distributed Tree Structures\"" and \""Distributed tree kernels\"". \nThe paper should present a comparison with such kinds of models.\n",1,1,1,1,1,1,1,1,-1,-1
SyVOjfbRb-R1,"Authors propose sampling stochastic gradients from a monotonic function proportional to gradient magnitudes by using LSH.  I found the paper relatively creative and generally well-founded and well-argued. \n\nNice clear example with least squares linear regression, though a little hard to tell how generalizable the given ideas are to other loss functions/function classes, given the authors seem to be taking heavy advantage of the inner product.  \n\nExperiments: appreciated the wall clock timings. \n\nSGD comparison: \u201cfixed learning rate. \u201d Didn't see how the initial (well constant here) step size was tuned?  Why not use the more standard 1/t decay? \n\nFig 1: Suspicious CIFAR100 that test objective is so much better than train objective?  Legend backwards? \n\nWhy were so many of the chosen datasets have so few training examples? \n\nPaper is mostly very clearly written,  though a bit too redundant and some sentences are oddly ungrammatical as if a word is missing - just needs a careful read-through. \n",1,1,1,1,1,-1,1,1,1,-1
SyVOjfbRb-R2,"The main idea in the paper is fairly simple:\n\n The paper considers SGD over an objective of the form of a sum over examples of a quadratic loss. \nThe basic form of SGD selects an example uniformly.    Instead,  one can use any probability distribution over examples and apply inverse probability weighting to retain unbiasedness of the gradient .\n\n  A good method (that builds on classic pps sampling) is to select examples with higher normed gradients with higher probability [Alain et al 2015]. \n\n  With quadratic loss,  the gradient increases with the inner product of the parameter vector (concatenated with -1) and the example vector x_i (concatenated with the label y_i).\n\n  For the current parameter vector \\theta,  we would like to sample examples so that the probability of sampling larger inner products is larger. \n\n  The paper uses LSH structures, computed over the set of examples, \n to quickly sample examples with large inner products with the current parameter vector \\theta.    Essentially, two vectors are hashed to the same bucket with probability that increases with their cosine similarity. \n So we select examples in the same LSH bucket as \\theta (for rubstness, we use multiple LSH mappings). \n\n\nstrengths:  simple idea that can work well in the context of sampling examples for SGD\n\nweaknesses: \n\n  The novelty in the paper is limited.  The use of LSH for sampling is a common technique to sample more similar vectors with higher probability.   There are theorems,  but they are trivial, straightforward applications of importance sampling.  \n\n The paper is not well written.  The presentation is much more complex that need be.  References to classic weighted sampling are  \n\n  The application is limited to certain loss functions for which we can compute LSH structures.   This excludes NN models and even the addition of regularization to the quadratic loss can affect the effectiveness.\n",1,1,1,1,-1,1,1,1,1,-1
SyVOjfbRb-R3,"The main contribution of this work is just a combination of LSH schemes and SGD updates.  Since hashing schemes essentially reduce the dimension, LSH brings computational benefits to the SGD operation.  The targeted issue is fundamentally important, and the proposed approach (exploiting LSH schemes) seems to be sound.   Specifically, LSH schemes fit into the SGD schemes since they hash two vectors to the same bucket with probability in proportional to their distance (here, inner product or Cosine similarity). \n\nStrengths:  a sound approach; a simple and straightforward idea that is shown to work well in evaluations. \n\nWeaknesses: \n1. The phrase of \""computational chicken-and-egg loop\"" in the title and also in the main body is misleading and not accurate.  The so-called \""chicken-and-egg\u201d issue concerns the causality dilemma: two causally related things, which comes the first.  In the paper, the authors concerned \""more accurate gradients\"" and \""faster convergence\""; their causality is very clear (the first leads to the second), and there is no causality dilemma.  Even from a computational perspective, \""SDG schemes aim for computational efficiency\"" and \""stochastic makes the convergence slow down\"" are not a causality dilemma.   The reason behind is that the latter is the cost of the first one, just the old saying that \""there is no such thing as a free lunch\"".  Therefore, this disordered logic makes the title very misleading, and all the corresponding descriptions in the main body are obscured by \""twisted\"" and unnatural logics.  \n \n2. The depth is so limited.  Besides a good observation that LSH fits well into SDG, there are no more in-depth results provided.  The theorems (Theorems 1~3) are trivial, with loose relations with LSH. \n\t \n3. The LSH schemes are not correctly referred to.  Since the similarity metric is inner-product, the authors are expected to refer to Cosine similarity and inner-product based LSHs, which were published recently in NIPS.  It is not in depth to assume \""any known LSH scheme\"" in Alg. 2.  Accordingly again, Theorems 1~3 are unrelated with this specific kind of similarity metric (Cosine similarity). \n\n4. As the authors tried hard to stick to the unnecessary (a bit bragging) phrase \""computational chicken-and-egg loop\"", the organization and presentation of the whole manuscript are poor. \n\n5. Occasionally, there are typos, and it is not good to use words in formulas. Please proof-read carefully.\n",1,1,1,1,-1,1,1,1,1,-1
SyXNErg0W-R1,"After the rebuttal:\n\nI do not think I had a major misunderstanding of the paper.  I was aware that the features mostly refers to the inputs to softmax.  In my point 4, I was suggesting that in order to have clustering performance, one might alternatively work on the softmax outputs instead of the inputs. \n\nMy opinion on this paper remains, and I think the contribution of this paper to machine learning is not very clearer at the current stage.  It might be the case that the considered scenarios indeed happen in computer vision related problems, but I am not an expert in that regard. \n\n========================================================================\n\nThis paper proposes a regularization to the softmax layer, which try to make the distribution of feature representation (inputs fed to the softmax layer) more meaningful according to the Euclidean distance.  The proposed isotropic loss in equation 3 tries to equalize the squared distances from each point to the mean, so the features are encouraged to lie close to a sphere.  Overall, the proposed method is a relatively simple tweak to softmax.  The authors show that empirically, features learned under softmax loss + isotropic regularization outperforms other features in Euclidean metric-based tasks. \n\nMy main concern with this paper is the motivation: what are the practical scenarios in which one would want to used proposed method? \n1. It is true that features learned with the pure softmax loss may not presents the ideal  similarity under the  Euclidean metric (e.g. the problem depicted in Figure 1),  because they are not trained to do so: their purpose is just to predict the correct label.   While the proposed regularization does lead to a nicer Euclidean geometry, there is not sufficient motivation and evidence showing this regularization improves classification accuracy. \n\n2. In table 2, the authors seem to indicate that not using the label information in the definition of Isotropic loss is an advantage.  But this does not matter since you already use the labels in the softmax loss. \n\n3. I can not easily think of scenarios in which, we would like to perform KNN in the feature space (Table 3) after training a softmax layer.  In fact, Table 3 shows KNN is almost always worse than softmax in terms of classification accuracy.  \n\n4. Running kmeans or agglomerative clustering in the feature space (Table 5) *using the Euclidean metric* is again ill-posed, because the softmax layer is not trained to do this.  If one really wants good clustering performance, one shall always try to learn a good metric, or , why do not you perform clustering on the softmax output (a probability vector?)  \n\n5.  The experiments on adversarial robustness and face verification seems more interesting to me,  but the tasks were not carefully explained for someone not familiar with that literature.  Perhaps for these tasks, multi-class classification is not the most correct objective, and maybe the proposed regularization can help, but the motivations are not given. \n\n\n\n",1,1,1,1,-1,-1,1,1,-1,-1
SyXNErg0W-R2,"The paper studies the problem of DNN loss function design for reducing intra-class variance in the output feature space.  The key contribution is proposing an isotropic variant of the softmax loss that can balance the accuracy of classification and compactness of individual class.  The proposed loss has been compared extensively against a number of closely related approaches in methodology.  Numerical results on benchmark datasets show some improvement of the proposed loss over softmax loss and center loss (Wen et al., 2016), when applied to distance-based classifiers such as k-NN and k-means.  \n\nPros: \n\n- The idea of isotropic normalization for enhancing compactness of class is well motivated \n\n- The paper is mostly clearly organized and presented. \n\n- Numerical study shows some promise of the proposed method. \n\nCons:\n\n-  The novelty of method is mostly incremental given the prior work of (Wen et al., 2016) which has provided a slightly different isotropic variant of softmax loss. \n\n- The training procedure of the proposed method remains unclear in this paper. \n\n\n",1,1,1,1,-1,1,1,1,1,-1
SyXNErg0W-R3,"In the centre loss, the centre is learned.  Now it's calculated as the average of the last layer's features \nTo enable training with SGD, the authors calculate the centre within a mini batch",1,-1,-1,1,-1,-1,1,-1,-1,-1
